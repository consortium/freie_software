<v2.05><e0>@Normal=<Ps100t0h100z12k0b0cKf"Helvetica">@9 ZÜ 2 in Inhalt=<Ps100t2h100.999z9k0b0cKf"Univers-CondensedLight">@99 Zü 3 in Inhalt=<Ps100t2h100.999z9k0b0cKf"Univers-CondensedLightOblique">@999 Zü 1 in Inhalt=<Ps100t0h100z9.5k0b0cKf"Univers-CondensedBold">@1 fliess normal=<Ps100t0h100z9.5k0b0cKf"FFScala">@6 Caps=<Ps100t0h100z9.5k0b0cKf"FFScala-Caps">@1 fliess kursiv=<Ps100t0h100z9.5k0b0cKf"FFScala-Italic">@3 hoch fliess=<P+s100t0h100z9.5k0b0cKf"Univers-Condensed">@4 Pfeil (Umschalt/Alt #)=<Ps100t0h100z9.5k0b0cKf"ZapfDingbats">@1 fliesshalbfett=<Ps100t20h100z9.5k0b0cKf"Univers-CondensedBold">@5 Klötzchen (kleines p)=<Ps100t0h100z9.5k0b0cKf"BulletsNstuff">@7 Glossar fett=<Ps100t10h100.999z9k0b0cKf"Univers-CondensedBold">@7 Glossar normal=<Ps100t2h100.999z9k0b0cKf"Univers-CondensedLight">@7 Glossar Pfeil Umsch/alt#=<Ps100t2h100.999z9k0b0cKf"ZapfDingbats">@7 Glossar kursiv=<Ps100t2h100.999z9k0b0cKf"Univers-CondensedLightOblique">@8 Kapitäl=<PHs100t10h100.999z9k0b0cKf"Univers-CondensedLight">@Normal=[S"","Normal","Normal"]<*L*h"Standard"*kn0*kt0*ra0*rb0*d0*p(0,0,0,0,0,0,g,"Deutsch")>@2  ZÜ 2=[S"","2  ZÜ 2"]<*L*h"Standard"*kn0*kt0*ra0*rb0*d0*p(0,0,0,12.5,0,0,g,"Deutsch")Ps100t10h100z9.5k0b0cKf"Univers-CondensedBold">@3 Fussnoten=[S"","3 Fussnoten"]<*L*h"Standard"*kn0*kt0*ra0*rb0*d0*p(0,0,0,9,0,0,g,"Deutsch")*t(11.339,0,"1  ")Ps100t2h100z7.5k0b0cKf"Univers-Condensed">@99 Glossar fliess=[S"","99 Glossar fliess"]<*L*h"Standard"*kn0*kt0*ra0*rb0*d0*p(5.669,-5.669,0,11,0,0,g,"Deutsch")Ps100t2h100.999z9k0b0cKf"Univers-CondensedLight">@0  Über 100%=[S"","0  Über 100%"]<*J*h"Standard"*kn0*kt0*ra0*rb0*d0*p(0,0,0,17,0,0,g,"Deutsch")*t(14.173,0,"1  ")Ps100t10h100z16k0b0cKf"Univers-CondensedBold">@2  ZÜ 1=[S"","2  ZÜ 1"]<*L*h"Standard"*kn0*kt0*ra0*rb0*d0*p(0,0,0,12.5,0,0,g,"Deutsch")Ps100t20h100z12k0b0cKf"Univers-CondensedBold">@1 fliess mit=[S"","1 fliess mit"]<*J*h"Standard"*kn0*kt0*ra0*rb0*d0*p(0,14.173,0,12.5,0,0,g,"Deutsch")Ps100t0h100z9.5k0b0cKf"FFScala">@1 fliesskursiv Zitat=[S"","1 fliesskursiv Zitat"]<*J*h"Standard"*kn0*kt0*ra0*rb0*d0*p(14.173,0,14.173,12.5,0,0,g,"Deutsch")Ps100t0h100z9.5k0b0cKf"FFScala-Italic">@1 fliess ohne=[S"","1 fliess ohne"]<*J*h"Standard"*kn0*kt0*ra0*rb0*d0*p(0,0,0,12.5,0,0,g,"Deutsch")Ps100t0h100z9.5k0b0cKf"FFScala">@1 fliessen Initial=[S"","1 fliessen Initial"]<*J*h"Standard"*kn0*kt0*ra0*rb0*d(1,3)*p(0,0,0,12.5,0,0,g,"Deutsch")Ps100t0h100z9.5k0b0cKf"FFScala">@0  Über 50%=[S"","0  Über 50%"]<*J*h"Standard"*kn0*kt0*ra0*rb0*d0*p(0,0,0,17,0,0,g,"Deutsch")*t(14.173,0,"1  ")Ps50t10h100z16k0b0cKf"Univers-CondensedBold">@2  ZÜ 3=[S"","2  ZÜ 3"]<*L*h"Standard"*kn0*kt0*ra0*rb0*d0*p(0,0,0,12.5,0,0,g,"Deutsch")Ps100t10h100z9.5k0b0cKf"Univers-CondensedBoldOblique">@99 Glossar fliess fett=[S"","99 Glossar fliess fett"]<*L*h"Standard"*kn0*kt0*ra0*rb0*d0*p(5.669,-5.669,0,11,0,0,g,"Deutsch")Ps100t10h100.999z9k0b0cKf"Univers-CondensedBold">@2  ZÜ 2:<*R>Volker GrassmuckFreie SoftwareZwischen Privat- und Gemeineigentum@3 Fussnoten:<z9.5f"FFScala">@2  ZÜ 2:<*R*p(0,0,0,18,0,0,g,"Deutsch")><Kz18>Volker Grassmuck<s60.001>Freie Software<s$>Zwischen Privat- <\n>und Gemeineigentum<*C*p(0,0,0,16,0,0,g,"Deutsch")>@3 Fussnoten:<z9.5f"FFScala">@99 Glossar fliess:<*p(0,0,0,11,0,0,g,"Deutsch")><z8>Bonn 2002© Bundeszentrale für politische Bildung (bpb)Redaktionsschluss: 30. November 2001Redaktion: Thorsten SchillingLektorat: Cornelia Schmitz, M.A., BonnProjektmanagement: Sabine BertholdDiese Veröffentlichung stellt keine Meinungsäußerung <\n>der Bundeszentrale für politische Bildung dar. <\n>Für inhaltliche Aussagen trägt der Autor die Verantwortung.Die Website zum Buch: http://freie-software.bpb.deTypografie: Hans Schlimbach AGD, Köln; Medienhaus Froitzheim AG, Bonn, BerlinUmschlaggestaltung: Mieke Gerritzen, nl.design, AmsterdamDruck: Bercker, KevelaerISBN 3-89331-432-6@0  Über 100%:Inhalt<t2h100.999z8f"Univers-CondensedLight">@99 Glossar fliess:<z8>@2  ZÜ 2:<t0z11>Vorworte<t$z$> @99 Glossar fliess:<*p(5.669,-5.669,0,15,0,0,g,"Deutsch")><*t(5.669,0,"1  "11.339,0,"1  ")>Thorsten Schilling<*p(5.669,-5.669,0,11,0,0,g,"Deutsch")><*t(5.669,0,"1  "11.339,0,"1  ")>Georg Greve <*t(5.669,0,"1  "11.339,0,"1  ")>	<f"Univers-CondensedLightOblique">Free Software Foundation Europe<f$> <\t><$f"Univers-CondensedLightOblique">13<f$><*p(5.669,-5.669,0,15,0,0,g,"Deutsch")><*t(5.669,0,"1  "11.339,0,"1  ")>Volker Grassmuck<*p(5.669,-5.669,0,11,0,0,g,"Deutsch")><*t(5.669,0,"1  "11.339,0,"1  ")>	<f"Univers-CondensedLightOblique">Initialize<f$> <\t><$f"Univers-CondensedLightOblique">16<*t(5.669,0,"1  "11.339,0,"1  ")>	History<f$><\t><$f"Univers-CondensedLightOblique">25<*t(5.669,0,"1  "11.339,0,"1  ")>	Credits<f$> <\t><$f"Univers-CondensedLightOblique">27<f$>@2  ZÜ 2:<*p(0,0,0,5,0,0,g,"Deutsch")><*p(0,0,0,12.5,0,0,g,"Deutsch")>Navigation<t2h100.999z9f"Univers-CondensedLight"> <\t><$f"Univers-CondensedLightOblique">29<a$>@99 Glossar fliess:<*t(5.669,0,"1  "11.339,0,"1  ")><*t(5.669,0,"1  "11.339,0,"1  ")>@2  ZÜ 2:<t0z11>1. Teil: Die rechtliche Ordnung <\n>des Wissens<\t><$t2h100.999z9f"Univers-CondensedLightOblique">31<a$>@99 Glossar fliess:<*t(5.669,0,"1  "11.339,0,"1  ")>@2  ZÜ 2:<*t(2.835,0,"1  "8.504,0,"1  ")>Eigentum<t2h100.999z9f"Univers-CondensedLight"> <\t><$f"Univers-CondensedLightOblique">36<a$>@99 Glossar fliess:<*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Allmende – vom Kollektiveigentum <*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>zum Privateigentum <\t><$f"Univers-CondensedLightOblique">37<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Wissen: vom Eigentum zur Allmende <*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>(Museen, Bibliotheken, Universitäten) <\t><$f"Univers-CondensedLightOblique">43<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Wissen: von der Allmende zum <*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Eigentum – Immaterialrecht <\t><$f"Univers-CondensedLightOblique">48<f$><*t(2.835,0,"1  "8.504,0,"1  ")>	<f"Univers-CondensedLightOblique">Patente<f$> <\t><$f"Univers-CondensedLightOblique">50<f$><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i><f"Univers-CondensedLightOblique">Die angloamerikanische  <\n>Copyright-Tradition<f$> <\t><$f"Univers-CondensedLightOblique">51<f$><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i><f"Univers-CondensedLightOblique">Die kontinentaleuropäische <\n>Droit d’auteur-Tradition <f$><\t><$f"Univers-CondensedLightOblique">58<f$><*t(2.835,0,"1  "8.504,0,"1  ")>	<f"Univers-CondensedLightOblique">Internationale Regelungen <f$><\t><$f"Univers-CondensedLightOblique">62<f$>@2  ZÜ 2:<*p(0,0,0,5,0,0,g,"Deutsch")><@9 ZÜ 2 in Inhalt><*p(0,0,0,15,0,0,g,"Deutsch")>Balance<@$p><t2h100.999z9f"Univers-CondensedLight"><\t><$f"Univers-CondensedLightOblique">65<a$><@9 ZÜ 2 in Inhalt>Akteure der rechtlichen <*p(0,0,0,11,0,0,g,"Deutsch")>Ordnung des Wissens<@$p><t2h100.999z9f"Univers-CondensedLight"><\t><$f"Univers-CondensedLightOblique">72<a$>@99 Glossar fliess:<*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>	<f"Univers-CondensedLightOblique">Autorinnen<f$><\t><$f"Univers-CondensedLightOblique">73<*t(2.835,0,"1  "8.504,0,"1  ")>	Verwertungsgesellschaften<f$><\t><$f"Univers-CondensedLightOblique">76<*t(2.835,0,"1  "8.504,0,"1  ")>	Rechteindustrie<f$><\t><$f"Univers-CondensedLightOblique">81<*t(2.835,0,"1  "8.504,0,"1  ")>	Öffentlichkeit<f$><\t><$f"Univers-CondensedLightOblique">85<f$>@2  ZÜ 2:<*p(0,0,0,5,0,0,g,"Deutsch")><*p(0,0,0,11,0,0,g,"Deutsch")><t0>Medientechnologie <\n>und ihre Digitalisierung<t2h100.999z9f"Univers-CondensedLight"><\t><$f"Univers-CondensedLightOblique">89<a$>@99 Glossar fliess:<*p(0,0,0,13,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Digitalmedien<\t><$f"Univers-CondensedLightOblique">94<f$><*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>	<f"Univers-CondensedLightOblique">Offline-Medien<f$><\t><$f"Univers-CondensedLightOblique">94<*t(2.835,0,"1  "8.504,0,"1  ")>	Online-Medien<f$><\t><$f"Univers-CondensedLightOblique">97<*t(2.835,0,"1  "8.504,0,"1  ")>	Sampling<f$><\t><$f"Univers-CondensedLightOblique">101<*t(2.835,0,"1  "8.504,0,"1  ")>	Kopierschutz<f$><\t><$f"Univers-CondensedLightOblique">102<f$>@2  ZÜ 2:<*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")><t0>Rechtliche Herausforderungen <\n>durch Digitalmedien<t2h100.999z9f"Univers-CondensedLight"><\t><$f"Univers-CondensedLightOblique">108<a$>@99 Glossar fliess:<*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>	<f"Univers-CondensedLightOblique">Hard- und Software<f$><\t><$f"Univers-CondensedLightOblique">111<*t(2.835,0,"1  "8.504,0,"1  ")>	Multimedia<f$><\t><$f"Univers-CondensedLightOblique">114<*t(2.835,0,"1  "8.504,0,"1  ")>	Sampling<f$><\t><$f"Univers-CondensedLightOblique">115<*t(2.835,0,"1  "8.504,0,"1  ")>	Öffentliche Wiedergabe<f$><\t><$f"Univers-CondensedLightOblique">116<*t(2.835,0,"1  "8.504,0,"1  ")>	Schrankenbestimmungen<f$><\t><$f"Univers-CondensedLightOblique">117<*p(0,0,0,3,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")><f$>@2  ZÜ 2:<@9 ZÜ 2 in Inhalt>Verträge statt Recht: Lizenzen<@$p><t2h100.999z9f"Univers-CondensedLight"><\t><$f"Univers-CondensedLightOblique">120<a$>@99 Glossar fliess:<*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i><f"Univers-CondensedLightOblique">Der Uniform Computer Information <\n>Transactions Act (UCITA)<f$><\t><$f"Univers-CondensedLightOblique">125<f$><*p(0,0,0,3,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>@2  ZÜ 2:<@9 ZÜ 2 in Inhalt>Code statt Recht: <*p(0,0,0,11,0,0,g,"Deutsch")>Rechtekontrollsysteme<@$p><t2h100.999z9f"Univers-CondensedLight"><\t><$f"Univers-CondensedLightOblique">130<a$>@99 Glossar fliess:<*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>	<@99 Zü 3 in Inhalt>Mark Stefik: Trusted Systems<@$p><\t><$f"Univers-CondensedLightOblique">132<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Kryptografie<@$p><\t><$f"Univers-CondensedLightOblique">141<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Widerrufung ›illegaler Geräte‹<@$p><\t><$f"Univers-CondensedLightOblique">142<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Content Scrambling System (CSS)<@$p><\t><$f"Univers-CondensedLightOblique">145<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Wasserzeichen<@$p><\t><$f"Univers-CondensedLightOblique">148<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i>Elekronische Bücher: E-Books in <\n>verschlüsseltem PDF<@$p><\t><$f"Univers-CondensedLightOblique">153<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i>Firewall um Deutschland: <\n>Rights Protection System (RPS)<@$p><\t><$f"Univers-CondensedLightOblique">156<f$><*t(2.835,0,"1  "8.504,0,"1  ")>	<@99 Zü 3 in Inhalt>Rechtliche Absicherung von <\n>	Rechtekontrollsystemen (RCS)<@$p><\t><$f"Univers-CondensedLightOblique">159<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Für eine informationelle Nachhaltigkeit<\t><$f"Univers-CondensedLightOblique">162<f$><*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")><*t(2.835,0,"1  "8.504,0,"1  ")>@2  ZÜ 2:<z11>2. Teil: Die Wissens-Allmende<\t><$t2h100.999z9f"Univers-CondensedLightOblique">177<a$>@99 Glossar fliess:<*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")><@999 Zü 1 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>Geschichte<@$p><\t><$f"Univers-CondensedLightOblique">177<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>›Wissenskommunismus‹ <*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>der Wissenschaften<\t><$f"Univers-CondensedLightOblique">177<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Internet<\t><$f"Univers-CondensedLightOblique">179<f$><*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>	<@99 Zü 3 in Inhalt>Frühphase<@$p><\t><$f"Univers-CondensedLightOblique">180<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Wachstum<@$p><\t><$f"Univers-CondensedLightOblique">185<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Institutionalisierung<@$p><\t><$f"Univers-CondensedLightOblique">186<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Netzwerkforschung<@$p><\t><$f"Univers-CondensedLightOblique">188<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i>Neue Architekturen, Protokolle <\n>und Dienste<@$p><\t><$f"Univers-CondensedLightOblique">189<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Internationalisierung<@$p><\t><$f"Univers-CondensedLightOblique">193<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Kommerzialisierung<@$p><\t><$f"Univers-CondensedLightOblique">195<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Wende ab 1990<@$p><\t><$f"Univers-CondensedLightOblique">195<*t(2.835,0,"1  "8.504,0,"1  ")><@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i>The Beginning of the <\n>Great Conversation<@$p><\t><$f"Univers-CondensedLightOblique">200<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Geschichte der Software-Entwicklung<\t><$f"Univers-CondensedLightOblique">202<f$><*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>	<@99 Zü 3 in Inhalt>Betriebssysteme<@$p><\t><$f"Univers-CondensedLightOblique">210<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Unix<@$p><\t><$f"Univers-CondensedLightOblique">211<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Berkeley Unix<@$p><\t><$f"Univers-CondensedLightOblique">214<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Das GNU-Projekt<@$p><\t><$f"Univers-CondensedLightOblique">217<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	GNU/Linux<@$p><\t><$f"Univers-CondensedLightOblique">227<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i>Von ›Free Software‹ zu <\n>›Open Source Software‹ und zurück<@$p><\t><$f"Univers-CondensedLightOblique">230<f$><*t(2.835,0,"1  "8.504,0,"1  ")><@999 Zü 1 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>Was ist freie Software, wie <\n>entsteht sie, wer macht sie?<@$p><\t><$f"Univers-CondensedLightOblique">233<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Quellcode und Objektcode<\t><$f"Univers-CondensedLightOblique">233<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Wie funktioniert ein Projekt <*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>der freien Software?<\t><$f"Univers-CondensedLightOblique">235<f$><*t(2.835,0,"1  "8.504,0,"1  ")>	<@99 Zü 3 in Inhalt>Core-Team und Maintainer<@$p><\t><$f"Univers-CondensedLightOblique">237<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Die Community<@$p><\t><$f"Univers-CondensedLightOblique">238<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i>Entscheidungsfindung: ›rough <\n>concensus and running code‹<@$p><\t><$f"Univers-CondensedLightOblique">239<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Code-Forking<@$p><\t><$f"Univers-CondensedLightOblique">240<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Die Werkzeuge<@$p><\t><$f"Univers-CondensedLightOblique">241<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Debugging<@$p><\t><$f"Univers-CondensedLightOblique">242<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Die Releases<@$p><\t><$f"Univers-CondensedLightOblique">245<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i>Institutionalisierung: Stiftungen und <\n>nicht profitorientierte Unternehmen<@$p><\t><$f"Univers-CondensedLightOblique">247<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i>Die Motivation: Wer sind die Leute <\n>und warum machen die das ...., wenn <\n>nicht für Geld?<@$p><\t><$f"Univers-CondensedLightOblique">249<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i>Software-Zyklus: Entwickler, <\n>Power-User, Endnutzer<@$p><\t><$f"Univers-CondensedLightOblique">254<f$><*t(2.835,0,"1  "8.504,0,"1  ")><@999 Zü 1 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>Die Software<@$p><\t><$f"Univers-CondensedLightOblique">259<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>BSD<\t><$f"Univers-CondensedLightOblique">261<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Debian GNU/Linux<\t><$f"Univers-CondensedLightOblique">263<f$><*t(2.835,0,"1  "8.504,0,"1  ")>XFree86<\t><$f"Univers-CondensedLightOblique">266<f$><*t(2.835,0,"1  "8.504,0,"1  ")>KDE<\t><$f"Univers-CondensedLightOblique">269<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Apache<\t><$f"Univers-CondensedLightOblique">271<f$><*t(2.835,0,"1  "8.504,0,"1  ")>GIMP<\t><$f"Univers-CondensedLightOblique">273<f$><*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")><@999 Zü 1 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>Lizenzmodelle<@$p><\t><$f"Univers-CondensedLightOblique">275<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>BSD-Lizenz<\t><$f"Univers-CondensedLightOblique">279<f$><*t(2.835,0,"1  "8.504,0,"1  ")>GNU General Public License<\t><$f"Univers-CondensedLightOblique">281<f$><*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>	<@99 Zü 3 in Inhalt>GPL und deutsches Recht<@$p><\t><$f"Univers-CondensedLightOblique">286<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Library / Lesser GPL<@$p><\t><$f"Univers-CondensedLightOblique">289<f$><*p(0,0,0,16,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Weitere offene Lizenzen<\t><$f"Univers-CondensedLightOblique">293<f$><*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i><@99 Zü 3 in Inhalt>Verhältnis von freiem und <\n>proprietärem Code<@$p><\t><$f"Univers-CondensedLightOblique">298<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Status abgeleiteter Werke<@$p><\t><$f"Univers-CondensedLightOblique">300<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Beispiele für Lizenzkonflikte<\t>301<@$p><*p(0,0,0,16,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Open Source Lizenzen aus Unternehmen<\t><$f"Univers-CondensedLightOblique">307<f$><*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")><@999 Zü 1 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>Gesellschaftliche Potenziale <\n>freier Software<@$p><\t><$f"Univers-CondensedLightOblique">318<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Freie Software in der Bildung<\t><$f"Univers-CondensedLightOblique">319<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Freie Software in den Nicht-G8-Ländern<\t><$f"Univers-CondensedLightOblique">323<f$><*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")><@999 Zü 1 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>Wirtschaftliche Potenziale<\n>freier Software<@$p><\t><$f"Univers-CondensedLightOblique">329<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Anwender von freier Software<\t><$f"Univers-CondensedLightOblique">339<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Erstellung freier Software<\t><$f"Univers-CondensedLightOblique">345<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Dienstleister rund um freie Software<\t><$f"Univers-CondensedLightOblique">349<f$><*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i><@99 Zü 3 in Inhalt>Systemhäuser, Hard- und <\n>Softwarehersteller<@$p><\t><$f"Univers-CondensedLightOblique">351<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Distributoren<@$p><\t><$f"Univers-CondensedLightOblique">354<@99 Zü 3 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>	Projekt-Hosting und Portale<@$p><\t><$f"Univers-CondensedLightOblique">356<f$><*t(2.835,0,"1  "8.504,0,"1  ")>	<@99 Zü 3 in Inhalt>Dokumentation<@$p><\t><$f"Univers-CondensedLightOblique">357<f$><*t(2.835,0,"1  "8.504,0,"1  ")><@999 Zü 1 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>Sicherheit<@$p><\t><$f"Univers-CondensedLightOblique">361<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Betriebssicherheit<\t><$f"Univers-CondensedLightOblique">364<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Gewährleistung und Haftung<\t><$f"Univers-CondensedLightOblique">365<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Kryptografie: Security by Obscurity <*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>vs. offene Verfahren<\t><$f"Univers-CondensedLightOblique">368<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Militärische Software und <*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Hochsicherheitsbereiche<\t><$f"Univers-CondensedLightOblique">377<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Vertrauenswürdige Instanzen<\t><$f"Univers-CondensedLightOblique">379<f$><*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")><@999 Zü 1 in Inhalt><*t(2.835,0,"1  "8.504,0,"1  ")>Für einen informationellen <\n>Umweltschutz<@$p><\t><$f"Univers-CondensedLightOblique">382<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Die kontrafaktische Wissens-Allmende<\t><$f"Univers-CondensedLightOblique">386<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Kollektive Intelligenz<\t><$f"Univers-CondensedLightOblique">390<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Von freier Software zu freiem Wissen<\t><$f"Univers-CondensedLightOblique">394<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Lizenzen<\t><$f"Univers-CondensedLightOblique">400<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Urheberrecht<\t><$f"Univers-CondensedLightOblique">402<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Wissensumweltschutz<\t><$f"Univers-CondensedLightOblique">405<f$><*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")><*t(2.835,0,"1  "8.504,0,"1  ")><*t(2.835,0,"1  "8.504,0,"1  ")>@2  ZÜ 1:<t0z11>Anhang<t2h100.999z9f"Univers-CondensedLight"><\t><$f"Univers-CondensedLightOblique">409<a$>@99 Glossar fliess:<*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")><*t(2.835,0,"1  "8.504,0,"1  ")>Glossar<\t><$f"Univers-CondensedLightOblique">409<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Abkürzungsverzeichnis<\t><$f"Univers-CondensedLightOblique">416<f$><*t(2.835,0,"1  "8.504,0,"1  ")>Internet-Ressourcen<\t><$f"Univers-CondensedLightOblique">423<f$><*p(0,0,0,11,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>	<\i><@99 Zü 3 in Inhalt>Wissensfreiheit und Bürgerrechte <\n>im Internet<\t>423<*t(2.835,0,"1  "8.504,0,"1  ")>	Internet<\t>423<*t(2.835,0,"1  "8.504,0,"1  ")>	Freie Software allgemein<\t>423<*t(2.835,0,"1  "8.504,0,"1  ")>	Unix<\t>424<*t(2.835,0,"1  "8.504,0,"1  ")>	Andere freie Softwareprojekte<\t>424<*t(2.835,0,"1  "8.504,0,"1  ")>	Freie Software für Schulen<\t>425<*t(2.835,0,"1  "8.504,0,"1  ")>	Gesetze<@$p><\t><$f"Univers-CondensedLightOblique">426<f$><*p(0,0,0,15,0,0,g,"Deutsch")><*t(2.835,0,"1  "8.504,0,"1  ")>Literatur<\t><$f"Univers-CondensedLightOblique">427<h100z9.5f"FFScala">@$:<@$p><t2z9.5f"FFScala">@0  Über 100%:Vorworte@1 fliess mit:@2  ZÜ 2:<*p(14.173,0,0,12.5,0,0,g,"Deutsch")>Thorsten Schilling@1 fliess mit:<t1>Was hat freie Software mit politischer Bildung zu tun? So berechtigt die Frage auf den ersten Blick scheint, spätestens nach der Lektüre dieses Buches sollte die Diskussion ihrer Antwort klarer umrissen sein. Am Anfang der Überlegungen, ein Buch mit diesem Gegenstand zu publizieren, stand jedenfalls der Gedanke: Freie Software (bzw. Open Source Software, beides wird der Kürze halber hier synonym verwendet) ist ein geeignetes Mittel und ein wichtiges Thema politischer Bildung.Anwendungen auf der Basis freier Software bieten gerade für kleine und mittlere Bildungsinstitutionen, Initiativen und Netzwerke attraktive Lösungen, um sich kostengünstig eine technische Infrastruktur zu geben. Dies betrifft nicht zuletzt den Bereich der Online-Anwendungen, das heißt der Programme um eigene Websites zu erstellen, sich mit Foren, Chats und vielem mehr interaktiv zu präsentieren und zu vernetzen. Interessant ist der Einsatz freier Software auch unter medienpädagogischen Gesichtspunkten: Die Möglichkeit, den zu Grunde liegenden Quellcode nachzuvollziehen und so die Funktionsweise der Programme zu studieren, ist eine Anregung zur Stärkung der Medienkompetenz durch Blicke hinter die Kulissen. Und sie gibt avancierten Nutzerinnen und Nutzern sehr schnell die Mittel in die Hand, selbst weiter zu programmieren und die vorhandene Software den eigenen Zwecken noch besser anzupassen. Die dabei erreichten Lösungen können der Allgemeinheit wieder zur Verfügung und zur Diskussion gestellt werden. Das wird sicher nicht die Mehrheit der Nutzerinnen und Nutzer sein, aber die Entwicklung des Interesses und der technischen Intelligenz ist gerade bei den jüngeren Generationen nicht zu unterschätzen.  Freie Software als ein Gut zu betrachten, dessen nachhaltige Entwicklung unter bestimmten Bedingungen im öffentlichen Interesse liegt, öffnet den Blick für weitere Perspektiven. Der Deutsche Bundestag hat am 9. November 2001 in einem Beschluss die Bundesregierung aufgefordert, »Open Source Software zu fördern und alle Voraussetzungen zur Einführung von Open Source in der Bundesverwaltung zügig zu schaffen«. Erste Initiativen dazu gibt es bereits. Im Ergebnis könnte –<\!q>ohne marktverzerrende Subventionen – ein Teil öffentlicher Infrastruktur als frei verfügbare Basis für viele andere Anwendungen geschaffen bzw. nachhaltig unterstützt werden. Vergleichbar z.B. mit den öffentlichen Investitionen in das Straßennetz inklusive der Autobahnen, gerade in Deutschland bis heute eine von der öffentlichen Hand finanzierte Grundlage für die Industrie- und Dienstleistungsgesellschaft. Darüber hinaus ist freie Software auch ein wichtiges Thema für die politische Bildung. Thomas Krüger, der Präsident der Bundeszentrale für politische Bildung, formulierte es in einer Rede so: <z12>»<z$>Es hat sich gezeigt, dass mit Entwicklungen wie freier Software, so genannten Peer-to-Peer Netzwerken und anderem eine grundlegende soziale und politische Dimension der neuen Informations- und Kommunikationstechnologien sich zu Wort meldet. Die darin breit angelegte Möglichkeit der Aufhebung der strikten Trennung von Sender und Empfänger hat große Schnittmengen mit der politischen Idee des mündigen Bürgers.« In der Geschichte der freien Software und bei der Diskussion ihrer Produktionsverhältnisse und aktuellen Entwicklungsperspektiven werden grundlegende Fragen der so genannten Informationsgesellschaft aufgeworfen. Das sind z.B. Fragen nach den rechtlichen Bedingungen, unter denen Wissen in Form von Softwarecode entwickelt und vertrieben wird, Fragen des Eigentums und des Urheberrechts, des Verhältnisses von privatwirtschaftlichen und öffentlichen Interessen, der gerechten Verteilung der Ressourcen des Wissens, der öffentlichen Organisationsformen kollektiver Intelligenz mithilfe digitaler Technologien und anderes mehr. Das Phänomen der freien Software erinnert dabei auch an eine in den Hintergrund geratene Tatsache: Die Informationsgesellschaft ist keine alleinige Erfindung der Industrie, und ihre Dynamik verdankt sie nicht nur den Kräften des Marktes. Viele ihrer grundlegenden Technologien und Standards sind unter akademischen oder wissenschaftsnahen Bedingungen mit massiver öffentlicher Förderung entstanden. Und so haben sich in diese Standards eben auch die Ideale, Wertvorstellungen und Regeln von Wissensreproduktion, wie sie die westliche Gesellschaft spätestens seit der Aufklärung etabliert hat, mit eingeschrieben: freier Austausch und Zugang zu den Quellen des Wissens, offene, kooperative und gleichberechtigte Kritik und Diskussion der Forschungsergebnisse als Grundbedingung für die weitere Entwicklung und Erfolgskontrolle. Der amerikanische Soziologe Robert K. Merton hat dies mit dem bewusst provokativen Wort vom »Wissenskommunismus der Wissenschaften« bezeichnet. Und es macht einen Teil des Erfolges freier Software aus, dass sie diese dem humboldtschen Wissenschaftsideal verbundene Form des Austausches und der kooperativen Entwicklung in neue Anwendungsfelder übersetzen konnte. Mittlerweile ist das Phänomen freier Software den akademische Zirkeln entwachsen und hat Facetten einer sozialen Bewegung angenommen: Freie und quell<\h>offene Softwareanwendungen werden über eine Vielzahl von Organisationsformen entwickelt, vertrieben und angewandt. Das reicht von genialen Einzelgängern über kleinere Initiativen, eigens gegründete NGOs bis hin zu mittelständischen Firmen. Und auch große Industrieunternehmen entdecken und nutzen zunehmend die Möglichkeiten freier Software für die Stärkung ihrer Position im globalen Wettbewerb.Wissen wächst durch Teilung, und die Mit-Teilung von Wissen ist eine wichtige Bildungsaufgabe. Informationen und Wissen können mit anderen geteilt werden, ohne dass sie dabei weniger werden, erst recht mithilfe digitaler Medien. Das macht einen grundlegenden Unterschied z. B. zu Industriegütern aus. Und das macht es gerade der Medien- und Softwareindustrie bis heute immer wieder schwer, unter den neuen Bedingungen der digitalen Revolution geeignete Wertschöpfungsmodelle zu finden und zu etablieren. Gleichwohl verdanken die neuen Technologien gerade dem Markt ihre massenweise Durchsetzung und die Lösung von den letztlich begrenzten Expertenkulturen der Wissenschaft: Ohne den »freien« Markt und seinen expansiven »Wissenskapitalismus<z12>«<z$> gäbe es die enorme Verbreitung des Internets und das exponentielle Wachstum nicht, das den allgemeinen Zugang zu diesen Technologien möglich macht<I>. <$>Einen Automatismus bei der Lösung sozialer Probleme kann es dabei allerdings nicht geben<I>. <$>Technologie, und sei es freie Software, ist keine Lösung, aber sie kann Möglichkeiten für nachhaltige Lösungsansätze bereitstellen. Freie Software als Kulturtechnik ist ein wichtiger Bestandteil dessen, was man in Anlehnung an einen juristischen Begriff die <x@1 fliess normal><t1>»<@$p><t1>Public Domain<x@1 fliess normal><t1>«<@$p><t1> nennen könnte – des öffentlichen Bereiches in der Informationsgesellschaft. Ihr Erfolg zeigt, dass es neben der marktwirtschaftlichen nach wie vor auch andere Erfolg versprechende Dynamiken der gesellschaftlichen Entwicklung gibt. Die Balance zwischen Öffentlichem und Privatem ist mit der Durchsetzung der neuen Technologien prekär geworden: Trotz langer Erfahrungen in der Verhinderung schädlicher Monopolbildungen wird gegenwärtig der Betriebssystemmarkt von einem Anbieter, die Märkte für populäre und für wissenschaftliche Information von den Großen Vier oder Fünf bestimmt. Freie Software ist deshalb auch ein geeignetes Thema für die Bildung über politische Ökonomie: die Entwicklung von Monopolen und Oligopolen in der Software- und der Wissensindustrie, ihre Folgen und ihre Alternativen. Es zeigt sich, dass viele Regeln, die das Gleichgewicht der Kräfte über lange Zeiträume mühsam genug etabliert und aufrechterhalten haben, ihre Wirksamkeit verlieren und neu justiert werden müssen. Die Diskussion da<\h>rüber ist mitten im Gange. Der Wettbewerb um die besten Lösungen ist ein Stück weit auch der offene Wettbewerb um die beste Balance zwischen beiden Sphären unter den Bedingungen der Informationsgesellschaft.  Das Buch von Volker Grassmuck zeigt nicht zuletzt auch für technische Laien, wie wertvoll diese Balance ist und dass wir sie nicht leichtfertig verloren geben sollten. <x@6 Caps><t1>Thorsten Schilling<x@1 fliess kursiv><t1>Bundeszentrale für politische Bildung/ bpb<@$p><\c>@2  ZÜ 2:<*p(14.173,0,0,10,0,0,g,"Deutsch")>Georg Greve@1 fliesskursiv Zitat:»Am Anfang war alle Software frei.« @1 fliess mit:@1 fliess ohne:Mit diesem Satz beginne ich häufig einführende Vorträge zum Thema Freie Software, da er verdeutlicht, dass Freie Software keinesfalls ein Phänomen der 90er-Jahre ist, obwohl es teilweise gerne so dargestellt wird. Natürlich ist das auch nur die halbe Wahrheit. Anfangs machten sich die Beteiligten keine Gedanken um die Freiheit von Software, die nur als »Dreingabe« zur Hardware betrachtet wurde. Es lässt sich zu Recht sagen, dass die eigentliche Taufe Freier Software durch Richard Stallman vollzogen wurde, der 1984 das GNU-Projekt und 1985 die <@1 fliess kursiv>Free Software Foundation<@$p> gründete. Dabei beschäftigte er sich zum ersten Mal mit der Frage, was Freie Software ist. Seine Definition Freier Software hat sich bis heute als die kompakteste und präziseste erwiesen.@1 fliess mit:Doch obwohl Freie Software je nach Betrachtungsweise bereits seit ungefähr 20 bzw. 40 Jahren existiert, hat sie sich bisher dem Verständnis der meisten Menschen entzogen. Auch heute hat nur eine sehr kleine Zahl von Menschen Freie Software wirklich verstanden. Die vielfältigen Versuche, Freie Software zu beschreiben oder zu er<t-2>k<t-3>lären, erschöpfen sich zumeist in reiner Phänomenologie. Und auch wenn <t$>sie zutreffend sind, sind Betrachtungen der Quelloffenheit (»Open Source«) und des Entwicklungsmodells in vieler Hinsicht eher hinderlich, da durch sie von den essentiellen Fragen abgelenkt wird.Insofern ist es erfrischend, ein Buch in Händen zu halten, das sich mit beachtlichem Erfolg um eine ausgewogene und gründliche Betrachtung bemüht.Doch was ist nun Freie Software? Die einfachste – und doch in vieler Hinsicht unzureichendste – Antwort auf diese Frage liefert die Definition Freier Software. Oft gibt schon die Stellung einer Frage die Tendenz der Antwort vor und manchmal ist eine Antwort unbefriedigend, weil die falsche Frage gestellt wurde.Die Welt der Freien Software ist geprägt von Dualismen. Freie Software ist sowohl kommerziell als auch nicht kommerziell, privat als auch öffentlich. Hinzu kommt, dass Software per se einen Dualismus darstellt. Sie ist nicht nur selber festgeschriebenes Wissen um Probleme und Problemlösungsstrategien, sie dient auch als Medium und Träger für Informationen und Erkenntnisse aller Art. Doch Software ist nicht nur abstraktes Wissen, sie ist auch kontrollierendes und ausführendes Organ in einem immer stärker von Computern durchdrungenen Alltag. Ihre Omnipräsenz macht das Wirtschafts- zum Kulturgut mit unmittelbarer Bedeutung für die gesamte Gesellschaft.Dabei ist Software selbst zunächst ultimativ statisch, sie ändert sich nicht ohne äußere Einflüsse. Aus dem Bezugssystem des in einer dynamischen Welt lebenden Betrachters bedeutet dies, dass Software degeneriert, sie ist vom permanenten Verfall bedroht.Software lebt nur durch die kontinuierliche Benutzung, Weiterentwicklung, Weitergabe und Pflege. Dieser Prozess ist Voraussetzung für die ungeheure Dynamik, die durch Software entfaltet werden kann, und wird bei unfreier (proprietärer) Software unterbrochen; sie wird gleichsam an eine Herz-/Lungenmaschine gekoppelt. Nur die in permanenter, eigenständiger Evolution befindliche Freie Software ist wahrhaft lebensfähig. Software ist also ebenso dynamisch wie auch statisch, die Reihe der Dualismen setzt sich fort.Analog zum Welle-Teilchen-Dualismus der Physik, durch den Photonen sowohl als Teilchen wie auch als Wellen zu betrachten sind, zeigt sich hier ein Softwaredualismus, dessen Verständnis eine wesentliche Aufgabe am Übergang zum Informationszeitalter darstellt. Dabei ist die Frage nach dem Umgang mit dem Wissen letztlich so alt wie die Menschheit selbst. Das älteste mir bekannte Zitat zu diesem Thema geht zurück auf Aurelius Augustinus, der in seinem »De doctrina christiana« schreibt: »Omnis enim res, quae dando non deficit, dum habetur et non datur, nondum habetur, quomodo habenda est.« Dieses Zitat zur Frage der Wissensvermittlung, das sich frei etwa mit »Denn jede Sache, die durch Weitergabe an andere nicht verliert, besitzt man nicht, wie man soll, solange sie nur besessen und nicht an andere weitergegeben wird« übersetzen lässt, wurde bereits im Jahre 397 unserer Zeitrechnung geschrieben.In der Vergangenheit war die Verbreitung und Vermittlung von Wissen mit einem zum Teil erheblichen Aufwand verbunden, erst heute besitzt die Menschheit die Möglichkeit, Wissen frei zirkulieren zu lassen. Schenkt man vor diesem Hintergrund der Idee des Softwaredualismus einen zweiten Blick, so wird offensichtlich, dass sich große Teile der Überlegung nicht nur auf Software im Speziellen sondern auch unmittelbar auf Wissen im Allgemeinen anwenden lassen; eventuell also von einem Wissensdualismus gesprochen werden muss, den es zu verstehen gilt. Das Verständnis dieser Problematik und die Lösung der dadurch entstehenden Fragen zählen zu den wichtigsten Aufgaben im Zusammenhang <t-1>mit dem so genannten Informationszeitalter. Gleichzeitig ist es ein Gebiet, <t$>das augenblicklich fast ausschließlich der freien Entfaltung der Interessen einer Minderheit auf Kosten der Gesellschaft überlassen wird.Da nachhaltige Lösungen von der Gesellschaft getragen werden müssen, ist es notwendig, ein Bewusstsein und Verständnis für diese Fragen zu schaffen. Denn nur wenn die Gesellschaft um diese Fragen weiß, kann dauerhaft sichergestellt werden, dass das Informationszeitalter nicht durch die blinde Durchsetzung von Minderheitsinteressen vorzeitig beendet wird.In diesem Sinne handelt es sich bei dem vorliegenden Buch sowohl um einen Meilenstein zum Verständnis und zur Sicherung des »digitalen Zeitalters« als auch um eine der vollständigsten Betrachtungen zum Thema Freier Software.<f"FFScala-Caps">Georg C. F. Greve<f$>,<@1 fliess kursiv>Präsident der Free Software Foundation Europe,<@1 fliess normal>Hamburg, den 20. August 2001 <@$p><\c>@2  ZÜ 1:Volker Grassmuck@2  ZÜ 2:Initialize@1 fliess mit:<*p(0,14.173,0,20,0,0,g,"Deutsch")>@1 fliessen Initial:<*h"mehr"><k10f"Univers-CondensedBold">A<k$f$>m Beginn der Überlegungen, die nun die Form dieses Buches angenommen haben, stand eine Beschäftigung mit Betriebssystemen. Um es gleich vorweg zu sagen, ich bin kein Informatiker, wenngleich durchaus technikaffin, wie man heute neudeutsch sagt. Nicht ohne Stolz kann ich berichten, dass mein erster Computer eine DEC PDP-11 war. Nun gut, nicht mein eigener, sondern der der Highschool in New Jersey, in der ich Mitte der 70er ein Jahr verbrachte. Doch die Erfahr<t-1>ung neben dem luftgekühlten Wandschrank und vor dem Blechterminal, <t$>auf dessen Endlospapier abwechselnd der Computer und ich klapper<\h>ten, blieb nicht ohne Folgen. Schon früh im Soziologiestudium konzentrierte ich mich auf Medien im Allgemeinen und den Computer im Besonderen.<@3 hoch fliess>1<@$p> Meine Diplomarbeit schrieb ich über den Wissens- und Menschbegriff der Künstlichen Intelligenz (Grassmuck, 1988). Für meine Promotionsforschung über Medien- und Diskursgeschichte verschlug es mich nach Japan. Dort lernte ich schließlich auch das Internet und dadurch die Unix-Welt kennen.<@3 hoch fliess>2<@$p> Seit William Gibsons Neuromancer-Trilogie Mitte der 80er-Jahre hatte diese Zone ein Bild und einen Namen: Cyberspace. Nun hatte ich ein Fenster entdeckt, durch das ich in diesen Raum aus nichts als Bits hineinsehen und -greifen konnte – ein Privileg, das damals nur wenigen außerhalb der Universität zuteil wurde. Und selbst dort nutzten vor allem die mathematisch ausgerichteten Fakultäten das Internet. Die Sozialwissenschaften verwendeten es allenfalls, um ihre Statistiken auf den Großrechnern des Rechenzentrums abarbeiten zu lassen. Als Untersuchungsgegenstand war das Netz noch undenkbar. @1 fliess mit:Das änderte sich im Herbst 1993, als der gerade frisch gekürte US-amerikanische Vizepräsident Al Gore seine Vision einer nationalen und bald darauf globalen »Informationsinfrastruktur« verkündete. Japan steckte sich, wie die meisten anderen Länder des Globus, sogleich an und verfiel in ein schweres Internetfieber. Mit seiner massenhaften Verbreitung wurde das Netz auch zu einem kulturwissenschaftlich akzeptablen <t-3>Thema. Im heimischen Berlin gründete sich 1994 die Projektgruppe Kultur<\h>r<t$>aum Internet<@3 hoch fliess>3<@$p> am Wissenschaftszentrum Berlin für Sozialforschung (WZB). Noch einmal vier Jahre später saßen nach einer <@1 fliess kursiv>mikro.lounge<@$p> zur Überwachung im Netz<@3 hoch fliess>4<@$p> Wolfgang Coy und ich in den Ledersesseln des WMF in der Johannisstraße, sinnierten über das anwesende Publikum und stellten mit nicht geringem Erstaunen fest, dass die Informatik von einer mathematisch-ingenieurwissenschaftlichen Disziplin zu einer Popkultur geworden war. Die Informatiker und Netzwerker, die das Internet bauen und weiterentwickeln, betrachten ihr Tun als rein technisches. Auch die Sozialwissenschaftler sehen die Technik meist als gegeben an und untersuchen dann in der Technikfolgeforschung ihre »Auswirkungen« auf die Gesellschaft. Die WZB-Forscherinnen dagegen gehen vom Internet als einem Kulturraum aus, der über »regierende Techniken« verfügt, die ihrerseits durch eine Regulierung bestimmt werden, die gerne als »kooperative Anarchie« bezeichnet wird. In ihrem Abschlussbericht heißt es, »dass in einem technisch konstituierten Raum, in dem alles Handeln die Form von Datenflüssen hat, auch die gemeinsamen Angelegenheiten dieses Handelns, sozusagen die res publica, eine technische Gestalt annimmt«.<@3 hoch fliess>5 <@$p>Sie untersuchten die Netzknoten, Übertragungsverfahren und Kommunikationsdienste, um dem »impliziten Designplan« des Internet auf die Spur zu kommen.Inspiriert von einer solchen Art, die Organisationsstrukturen von Gemeinschaften und Gesellschaft mit denen ihrer Maschinen und Medientechnologien zusammenzudenken, hatte mich nun also die Faszination von der Grundlagenschicht des Computers gepackt, dem Betriebssystem. Bei Andrew Tanenbaum und anderen Standardautoren las ich von System<\h>aufrufen und Dämonen, Prozessen und Threads, von Muscheln und virtuellen Maschinen. Nun zählen Betriebssysteme selbst unter Informatikstudenten nicht gerade zu den aufregendsten Themen. Für die meisten Computeranwender läuft das Betriebssystem ohnehin im Hintergrund ihres Bewusstseins. In Erscheinung tritt es allenfalls beim Systemstart oder bei Störungen. Den überwiegenden Teil seiner grundlegenden Operationen (Ressourcen verwalten, Aufgaben wie Input/Output steuern und koordinieren, Systemelemente koppeln) vollführt es im Verborgenen.Während ich über das vom Betriebssystem choreografierte Wechselspiel der Computerkomponenten las, erstand vor meinem inneren Auge das Bild einer Fabrik mit Werkshallen, Lagern, Fuhrpark und Buchhaltung. Der Anthropomorphisierung des Rechners kann man sich nur schwer entziehen. Tanenbaum speist diese allzu menschliche Neigung, z.B. wenn es heißt, dass Protokolle eine Stoppuhr starten, wenn sie eine Nachricht senden und eine Antwort erwarten, und die Nachricht erneut senden, falls sie in der erwarteten Zeit keine Bestätigung bekommen: »Dieser Vorgang wird solange wiederholt, bis der Sender gelangweilt ist und aufgibt« (Tanenbaum 1995, S. 534). »Betriebsmittel« und »Fairness« bei ihrer Allozierung, »Organisation«, »Kontrollflüsse«, »Macht« – das waren vertraute soziologische Begriffe inmitten eines scheinbar rein technischen Diskurses. Verfolgt man die Ausläufer des Betriebssystems weiter, so entfaltet sich ein umfassendes Netz von Beziehungen nicht nur in alle Bereiche von Computerhardware und -software, sondern auch in Wirtschaft, Politik, Recht, Soziologie, Psychologie und Kultur. Besonders evokativ war die Gegenüberstellung von Betriebssystemen mit einem monolithischen Makrokern und von verteilten Systemen mit einem Mikrokern. Tanenbaum, leidenschaftlicher Verfechter des zweiten Ansatzes, argumentiert rein technisch: Verteilte Systeme seien leistungsfähiger, wirtschaftlicher, flexibler, skalierbarer, könnten höhere Geschwindigkeiten erzielen und seien ausfallsicherer und daher zuverlässiger. »Falls der monolithische Kern der amtierende Champion ist, dann ist der Mikrokern der aufstrebende Herausforderer. [...] Der Mikrokern ist flexibel, da er fast nichts selbst erledigt. Er bietet nur vier minimale Bas<t-1>isdienste an: 1. einen Prozesskommunikationsmechanismus, 2. eine ein<\h>f<t$>ache Speicherverwaltung, 3. eine minimale Prozessverwaltung und ein einfaches Scheduling und 4. eine einfache I/O-Funktionalität« (Tanenbaum 1995, S. 477).Gerade hier drängt sich eine Parallele zwischen der Mikroorganisation von Rechnerkomponenten und der Makroorganisation sozialer Institutionen auf. Einerseits die zentralisierte, hierarchische Steuerung der großen Bürokratien in Wirtschaft, Staat und Gesellschaft mit klaren Befehlsketten nach dem Vorbild der militärischen Führung: der monolithische Makro-<@1 fliess kursiv>Kernel<@$p> als befehlshabender <@1 fliess kursiv>Colonel<@$p>. Auf der anderen Seite stehen neue Konzepte in den Organisationstheorien, die flache Hierarchien und verteilte, dezentrale, selbstorganisierende Einheiten anstreben: der Mikrokern als Mediator, der nur die Kommunikation und Synchronisation in einem Netzwerk ohne zentrale Instanzen regelt. In diesem zweiten Modell lässt sich ein Widerhall aus der Ökologie mit ihren Netzwerken von autonomen, aber interdependenten, heterogenen Elementen hören. Ebenso liegen Analogien zu den dezentralen Organisationsformen der sozialen Graswurzelbewegungen der 60er- und 70er-Jahre nahe. Etwa zur selben Zeit geriet das Betriebssystemmonopol Microsoft mit Kartellverfahren und wachsender Kritik an Missbrauchspraktiken in die Schlagzeilen. Als sein großer Gegenspieler trat das freie Betriebssystem GNU/Linux an. Zwar verfolgen beide einen Makrokernansatz, doch ist der Entstehungsprozess von Linux hochgradig verteilt und gewissermaßen um den minimalen Kern Linus Torvalds, den ursprünglichen Entwickler des Linux-Kerns, herum organisiert. Ein Beispiel für die unterschiedlichen sozialen und politischen Wertsetzungen der beide Modelle: Microsoft-Betriebssysteme wachsen, wie das sie produzierende Unternehmen, zu immer größeren Dimensionen aus, zielen auf die Auslas<\h>tung der jeweils neuesten Intel-Prozessorgeneration und zwingen so die Konsumenten immer wieder zum Nachkauf. GNU/Linux dagegen läuft auch auf vermeintlich obsoleten Plattformen und schont daher Budgets und andere Ressourcen. Dann fiel mir ein Aufsatz von Thomas Wulffen mit dem Titel »Betriebssystem Kunst« in die Hände. Er handelt von künstlerischen Positionen, die sich mit dem Kunstbetrieb selbst beschäftigen – also mit dem System von Museen und Galerien, Kuratorinnen und Kritikern, Zeitschriften und Hochschulen – und nicht auf etwas außerhalb der Kunst verweisen. Während Produzentinnen, Künstler und Rezipienten üblicherweise über ihre jeweiligen Benutzeroberflächen auf die Dienste des Betriebssystems zugreifen, thematisieren diese Positionen die inneren, »hardware-nahen« Abläufe des Systems. Ihr wesentliches Kennzeichen sei ihr Selbstbezug. Hier die Selbstreferenz des Computerbetriebssys<\h>tems, das ausschließlich am reibungslosen Weiterlaufen der Maschine »interessiert« ist. Dort die Fremdreferenz der Anwendungsprogramme, die sich der Betriebssystemsdienste bedienen, um sich auf etwas außerhalb der Welt des Rechners oder der Kunst, auf den Anwender und seinen Kontext zu beziehen. Tatsächlich verwenden ja vergleichsweise wenige Mensch einen Computer, um Puffer zu adressieren oder Prozesse vor Kollisionen zu bewahren, viele dagegen, um Texte zu schreiben oder Webseiten zu bauen. Wenn mich auch Wulffens technisch unbeholfene Einführung noch nicht überzeugte, bekräftigte sie mich doch darin, der Sache nachzugehen. Es folgten Diskussionen im Kreis von mikro.<@3 hoch fliess>6<@$p> Diese Gruppe von 15 Berliner Künstlerinnen, Theoretikern, Journalistinnen, Organisatoren und anderen Kulturproduzentinnen hatte sich Anfang 1998 zusammengefunden. Nein, behaupten zu wollen, wir seien von Tanenbaums Mikrokernen zu diesem Namen inspiriert worden, ginge dann doch zu weit.<@3 hoch fliess>7<@$p> Vielmehr drückt sich in beiden ein weit größerer Paradigmenwechsel aus, der kleinen beweglichen vernetzten Einheiten den Vorzug vor institutionellen Großstrukturen gibt. Die mikro.lounges zu Themen der digitalen Medienkultur gehen aus den lokalen, aber sich weltweit erstreckenden Kapillarstrukturen von mikro hervor. Auch die ersten Brain<\h>stormings über das Thema »Betriebssysteme von technischen und von sozialen Systemen« waren überaus angeregt und vielschichtig. Bald war klar, dass das Thema den Rahmen einer mikro.lounge sprengen würde, und es entstand die Idee, eine Konferenz daraus zu machen. Das Projekt wurde in seinem ersten Entwurf auch außerhalb von mikro neugierig aufgenommen, und so begannen die Vorbereitungen quasi aus dem Stand im Herbst 1998. Auch der Titel war bald gefunden: »Wizards of OS«. »Wizard« ist in Computer-Kreisen ein Ehrentitel und meint jemanden, der ein komplexes Stück Hard- oder Software so weit gemeistert hat, dass er damit zaubern kann. Besonders für Unix-Experten hat sich der Begriff in den USA so weit etabliert, dass »Unix Wizard« als Stellenbezeichnung verwendet wird.<@3 hoch fliess>8<@$p> Solche Leute, die mit Betriebssystemen, Computern, Netzen und vielleicht auch sozialen Systemen virtuos und visionär umgehen und Dinge damit tun können, an die nie ein Mensch gedacht hat, wollten wir uns einladen – Hacker eben, die ihrer eigenen Ethik zufolge Computer verwenden, um »Kunst und Schönheit« zu schaffen.<@3 hoch fliess>9<@$p> Und das »OS«? Richard Stallman fragte sofort misstrauisch, ob es für »Open Source« stehe. Richard und allen, die ähnliche Vermutungen hegen, sei erneut versichert, dass es von Anfang an und auch heute noch »Operating Systems«, Betriebssysteme, meint.<@3 hoch fliess>10<@$p> Tatsächlich griffen wir das parallel aufbrodelnde Phänomen der freien Software erst während der Planung der Konferenz auf. GNU/Linux stand zwar von Anfang an auf der Themenliste, doch erst zu Beginn 1999 erweiterte sie sich um freie Softwareprojekte, die im strengen Sinne keine Betriebssysteme sind.<@3 hoch fliess>11<@$p> Neben den Diskussionen innerhalb von mikro und in einem Netzwerk enger und lockerer Partner bildeten sich die Ideen zu den »Wizards of OS« – und damit zum vorliegenden Buch – auch maßgeblich im Rahmen des DFG-Forschungsprojektes »Von der Ordnung des Wissens zur Wissensordnung digitaler Medien.« Die Gespräche mit Wolfgang Coy und Jörg Pflüger, die mich in dieses Projekt im überregionalen Forschungsverbund »Medien Theorie Geschichte« eingeladen hatten, waren eine ständige Inspirationsquelle und ein wichtiges Korrektiv für meine Überlegungen zu Betriebssystemen und sozialen Systemen. Im Juli 1999 war es schließlich soweit. Die Konferenz »Wizards of OS – Offene Quellen und Freie Software«<@3 hoch fliess>12<@$p> im Haus der Kulturen der Welt Berlin brachte 50 Vortragenden und rund 700 Teilnehmer zu zwei randvollen Tagen mit Plenarpräsentationen, Workshops und informellen <\n>Gesprächen zusammen. Kernthema war das Phänomen der in offener Kooperation über das Internet erstellten freien Software, das vor allem mit dem Betriebssystem GNU/Linux kurz zuvor ins Bewusstsein der <\n>Öffentlichkeit getreten war. Der Geschichte, der Kultur, den Kooperationsmechanismen der freien Software und ihrer Bedeutung in Wirtschaft, Bildung und im Wissenstransfer mit der »Dritten Welt« wurde breite Aufmerksamkeit eingeräumt, doch gingen die Fragestellungen der Konferenz darüber hinaus. Welche Anregungen bieten die Kooperationsmechanismen der freien Software für neue Modelle der Ökonomie? Welche Impulse gehen von ihr für andere Bereiche der Wissensordnung aus, für Literatur, Wissenschaft und Netzkunst? Wie verhält sich dieses kollektiv geteilte Wissen zu den vorherrschenden Praktiken des geistigen Eigentums? <t-1>Einer der Höhepunkte der WOS 1 war fraglos das Panel zu geistigem Eigentum und gemeinfreiem Wissen, auf dem es dem Bostoner Gründer des GNU-<t-3>Projektes, Richard Stallman, dem Berliner Gentechnikexperten von Green<\h>p<t-1>eace, Benny Härlin, und dem New Yorker Kunstkritiker und Kurator Timothy Druckrey gelang, Korrespondenzen und parallele Entwicklungen in scheinbar sehr verschiedenen Wissensbereichen aufzuzeigen. Zu den weiteren Vortragenden zählten führende Vertreter freier Softwareprojekte wie Kalle Dalheimer (KDE) und Lars Eilebrecht (Apache), Anwender und Distributoren wie SuSE, Intershop, Apple und die tageszeitung, einer der deutschen Internet-Pioniere, Claus Kalle (Rechenzentrum der Universität Köln), der führende deutsche Mikroökonom Prof. Norbert Szyperski und der Netzwerkökonom Rishab Aiyer Ghosh (Neu Delhi), die Netzwerkpolitologin Jeanette Hofmann (Wissenschaftszentrum Berlin) der Kulturwissenschaftler Prof. Friedrich Kittler, der Verleger Tim O’Reilly, die Juristen Prof. Bernd Lutterbeck (TU Berlin) und RA Jürgen Siepmann (Freiburg), der Netzkünstler Alexei Shulgin (Moskau), Ingo Ruhmann (BMBF), Hubertus Soquat (BMWi), Peter Bingel (Freie Software und Bildung e.V.) und der mexikanische Filmemacher Fran Ilich. Am Tag vor den WOS 1 richteten wir für das Bundeswirtschaftsministerium ein Fachgespräch »Open Source-Software« aus. Einige der bereits Genannten trafen hier in einem intensiven ganztägigen Gespräch mit Vertretern von Ministerien und Behörden wie dem Bundesamt für Sicherheit in der Informationstechnik (BSI) zusammen. Der Widerhall all dieser verschiedenen Stimmen ist in das vorliegende Buch eingegangen.<t$>Waren uns an den Computerbetriebssystemen und am Internet Strukturmerkmale wie monolithisch – verteilt, makro – mikro, heterogen – homogen aufgefallen, so brachte die freie Software die Polaritäten frei – proprietär und offen – geschlossen hinzu. Solche schlichten Gegensatzpaare, zumal aufgeladen mit Wertungen wie »small is beautiful«, kann man natürlich nicht ohne einen gehörigen Rattenschwanz von Abers und Fragezeichen niederschreiben. Ist mikro wirklich besser als makro? Oder gibt es nicht vielmehr bestimmte Probleme, für die Großstrukturen nützlich, wenn nicht gar unabdingbar sind, und andere, die sich prima durch Netzwerke aus Mikroknoten lösen lassen? Die Welt ist nicht schwarz-weiß. Die genannten Begriffe sind ambivalent, von den großen, überfrachteten wie »offen« oder »frei« gar nicht erst anzufangen. Aber nehmen wir nur »frei – proprietär« im engeren Sinne von einerseits freier Software wie GNU/Linux und Gimp und andererseits gewöhnlicher kommerzieller Software wie die von Microsoft. Das scheint ein klare Sache zu sein. Dabei hat auch freie Software einen <@1 fliess kursiv>proprietarius<@$p>, einen Copyright-Eigentümer, nämlich den Autoren, der sie z.B. unter der GPL (<@1 fliess kursiv>GNU General Public License<@$p>) freistellt. Und was heißt »frei«? Man liest doch immer wieder, die GPL sei unter den Lizenzen der freien und Open Source-Software besonders »restriktiv«? Diese Fragen und ihre Hintergründe aufzuklä<t-1>ren, hofft dieses Buch ein Beitrag zu leisten. Den Ungeduldigen sei die Antwort zumindest auf die letzte Frage hier schon vorweggenommen: Wenn es von der GPL heißt, sie schränke die Verwendung der Software ein, ist dies nur halb richtig. Genauer schränkt sie ihre Einschränkung ein.<t$> In den folgenden beiden Semestern hatte ich dann die Gelegenheit, die Themen in einem Seminar am Institut für Informatik der Humboldt-Universität zu Berlin zusammen mit den Studenten und Gastvortragenden zu vertiefen. Der US-amerikanische Rechtsgelehrte Lawrence Lessig hatte gerade sein Buch »Code and Other Laws of Cyberspace« (1999)<@3 hoch fliess>13 <@$p>vorgelegt. Darin entwickelt er die These, dass es neben Recht, Normen und Markt vor allem die Architektur des Netzes selbst sei, die den Cyberspace reguliert. Mit »Code« oder »Architektur« meint er vor allem die grundlegenden Schichten, die Betriebssysteme, Protokolle, Infrastrukturen für technische Rechtekontrolle usw., die die Regeln in der digitalen Umgebung zementieren und die niemandem (mit Ausnahme von findigen Hackern) die Wahl lassen, sich an diese Regeln zu halten oder nicht. Der Code, der diese »gebaute Umgebung«  zu allererst konstituiert, bildet zugleich sein Gesetz. Ein Gesetz, das, von Maschinen interpretiert, sich so unausweichliche Geltung verschafft, wie kein juristisches Gesetz es je könnte. Verständlich, dass Regulierungsbestrebungen von Gesetzgebern, Industrien und Wizards, wie sie bereits die WZB-Forscherinnen untersucht hatten, an diesem Code ansetzen. Mit aller Leidenschaft, die ein Anwalt aufzubringen vermag,<@3 hoch fliess>14<@$p> richtet sich Lessig vor allem gegen die Copyright-Industrie, die mit Hilfe von Code eine perfekte Kontrolle über ihr geistiges Eigentum erlangen möchte. Dagegen macht er sich stark für eine »intellektuelle Allmende«, deren Voraussetzungen auch in zukünftigen Versionen des Cyberspace erhalten bleiben müsse. Da Larry einen längeren Gastaufenthalt an der <@1 fliess kursiv>American Academy<@$p> in Berlin verbrachte, nutzten wir die Gelegenheit, ihn im April 2000 zu einem Wizards of OS-Workshop in der Volksbühne einzuladen, bei dem es auch um zwei weitere Formen des geistigen Eigentums ging: um Softwarepatente (Daniel Rieck) und um Markennamen in ihrer Verwendung als Internet-Domain-Namen, deren Regulierung nun der <@1 fliess kursiv>Internet Corporation for Assigned Names and Numbers<@$p> (ICANN) unterstand (Jeanette Hofmann und Rob Blokzijl).<@3 hoch fliess>15<@$p> Dass die rechtliche, wirtschaftliche und technische Regulierung des so genannten geistigen Eigentums eine zentrale Rolle in der digitalen Wissensordnung spielen würde, hatte sich schon bei der Beschäftigung mit den Betriebssystemen sozialer Systeme erwiesen und bei der Konferenz über freie Software bestätigt. Lessigs Schlussfolgerung: »Offener Code bedeutet offene Kontrolle – es gibt eine Kontrolle, aber der Anwender ist sich ihrer bewusst. [...] offener Code ist eine Grundlage für eine offene Gesellschaft« (Lessig 1999, S. 107 f.).Die Beziehung zwischen Bits und Paragraphen, zwischen Programm<\h>code, Verhaltenskodex und juristischem Code und die Frage, wie Information überhaupt das Monopolrecht von jemandem sein kann, beschäftigten uns im WOS-Kreis weiter. Folglich griffen wir einen der zentralen Glaubenssätze aus der Gründungszeit des Internet für den nächsten WOS-Workshop auf: <@1 fliess kursiv>»Information wants to be Free<@$p> – Die digitale Wissensordnung zwischen Rechtekontrollsystemen und Wissens-Allmende«, im September 2000 im Kunstverein Hamburg (mit Gabriele Beger, Helmut Spinner, Florian Cramer, Marko Peljhan, Sally Jane Norman, Phil Agre und Wolfgang Coy).<@3 hoch fliess>16<@$p>In diesem Jahr hatte Napster bewiesen, dass ein neuer Code, eine neue Architektur Menschen auf ganz andere Weise miteinander in Beziehungen setzen und das Urheberrechtssystem gehörig in Unordnung bringen kann. Eine Gastprofessur an der Hochschule für Grafik und Buchkunst Leipzig bot die Gelegenheit, diesem Phänomen mit Studierenden und im nächsten öffentlichen Symposion in der WOS-Serie, »Napster und die Folgen« im Januar 2001 nachzugehen (mit Erik Möller, André Schirmer, Till Kreutzer, Thorsten Braun (nur schriftlich präsent) und Andreas A. Milles).<@3 hoch fliess>17<@$p>Im Oktober 2001 fand die zweite Konferenz  der »Wizards of OS« statt, diesmal mit dem Untertitel »Offene Kulturen und Freies Wissen«.<@3 hoch fliess>18<@$p> Sie wird viele der Gedankenstränge aus diesem Buch aufgreifen und weiterführen. Das Interesse an der Betriebssystemschicht der Wissensgesellschaft richtet sich dort unter dem Titel »Offene Infrastruktur« auf Standards und Geld. Ganz richtig, selbst »offenes, community-basiertes Geld« ist bereits im Einsatz.<@3 hoch fliess>19<@$p> Aus der freien Software stehen Projekte im Vordergrund, die die kooperative Sammlung, Erstellung, Bearbeitung und Weiterentwicklung von Wissen unterstützen. Neben dem Wissen, das Gemeineigentum einer offenen Gemeinschaft ist, wird auch das öffentliche Wissen der Gesellschaft in Bildung, Archiven, Rundfunk und Staat thematisiert. Und natürlich werden auch aktuelle Fragen des geistigen Eigentums angesprochen. Dieses Buch ist somit nur eine Momentaufnahme aus einem Prozess, in den es die geneigte Leserin und den geneigten Leser einladen möchte. Der Prozess ist offen für vielfältige Beteiligungen. Ich wünsche mir Leser, die nicht nur neugierig auf die Geschichte und die Bedeutung der freien Software, sondern auch mit Herz und Verstand, mit Engagement bei der Sache einer freien und offenen Wissensordnung dabei sind. »Die Freiheit des Wissens zu verteidigen, ist wahrscheinlich die wichtigste Aufgabe, die wir in der Zukunft vor uns haben,« sagte Prof. Dr. Norbert Szyperski auf den ersten Wizards of OS. Wenn Sie beim Kauf Ihrer nächsten Software an dieses Buch denken werden, hat es einen Teil zu dieser großen Aufgabe beigetragen. @2  ZÜ 2:History@1 fliess mit:@1 fliess ohne:Gilt von jedem Buch, dass es Produkt einer kollektiven Intelligenz ist, auch wenn diese sich im Haupt einer einzelnen Autorin manifestiert hat, so umso mehr von dem, das Sie gerade in der Hand halten. Es speist sich aus den »Wizards of OS 1«, den zahlreichen Diskussionen um sie herum, Aufsätzen zur Klärung des Konzepts, der Arbeit mit Wolfgang und Hugo an unserem Forschungsprojekt, den intensiven drei Tagen im Haus der Kulturen selbst, ihrer Nachbereitung mit der Arbeit an den Transkripten der Vorträge und Diskussionen und den folgenden Workshops und Seminaren in der WOS-Serie. @1 fliess mit:Dieser Text begann sein Dasein als Protokoll des Fachgesprächs des Bundeswirtschaftsministeriums. Eine weitere Ausarbeitung erfuhr er als Kapitel im Forschungsprojekt »Wissensordnung digitaler Medien«, das von der Deutschen Forschungsgemeinschaft (DFG) gefördert wurde. Er wird hier zusammen mit dem Kapitel »Urheberrecht« aus demselben Abschlussbericht in überarbeiteter und aktualisierter Fassung präsentiert. In allen wichtigen Punkten ist er auf den Stand vom Juli 2001 gebracht. Dem Ablaufen der Ereignisse hinterherschreiben zu wollen, ist natürlich ein müßiges Unterfangen. Insbesondere die Spätfolgen der Bauchlandung des Neuen Marktes, also der Computer- und Kommunikationsbranche insgesamt und damit auch des Sektors der freien Software, zeigen sich in ihrer ganzen Härte erst, während ich dies schreibe. Auch publizistisch hat sich in den vergangenen Wochen und Monaten einiges getan. Daraus möchte ich Sie auf drei Titel aufmerksam ma<t-1>chen, die hier noch keine Berücksichtigung finden konnten. Glyn Moodys<t$> »Die Software Rebellen. Die Erfolgsstory von Linus Torvalds und Linux« (deutsch: Linux-Magazin, 2001) ist eine spannende Reportage über das Entstehen von GNU/Linux von den Anfängen in Unix und dem GNU-Projekt über den öffentlichen Durchbruch 1999 bis zur Akzeptanz bei Hard- und Softwaregrößen wie IBM, SAP und HP. Andere wichtige Projekte wie Perl, TeX und Mozilla werden en passant skizziert, viele der Protagonisten kommen zu Wort.Pekka Himanen erzählt in seinem, zusammen mit Linus Torwalds und Manuel Castells geschriebenen Buch »Die Hacker-Ethik. Vom neuen Geist des Informations-Zeitalters« (deutsch: Riemann, 2001) im Wesentlichen die gleiche Geschichte nach und verfolgt dabei die These von einem grundlegenden Wandel der Arbeitsmotivation. Vergnügen, Neugier, Lust am Lernen, Enthusiasmus, Leidenschaft – das sind die Beweggründe, die Autoren freier Software immer wieder nennen. Diese »Arbeitsethik der Hacker« kontrastiert Himanen mit dem verinnerlichten Pflichtgefühl der »protestantischen Ethik«, die Max Weber als den treibenden »Geist des Kapitalismus« erkannte. Die freie Software bildet damit nicht nur eine Alternative zum vorherrschenden Softwareentwicklungsmodell, sondern erinnert auch an Spiel und Spaß als wichtige Elemente bei der Beantwortung der Frage nach dem Sinn des Lebens. Schließlich hat der »Revolutionär durch Zufall« Linus Torvalds zusammen mit David Diamond seine Version der Geschichte vorgelegt: »Just for Fun. Wie ein Freak die Computerwelt revolutionierte« (deutsch: Hanser, 2001) – ein Titel, mit dem er Himanens These von der Spaßkultur der freien Software untermauert.Um der Langsamkeit des alten Mediums Buch zu begegnen, wird die »Freie Software zwischen Privat- und Gemeineigentum« von einer Website begleitet. Unter <x@1 fliess normal><B>http://freie-software.bpb.de<@$p> finden Sie den vollständigen Text mit klickbaren URLs, Korrekturen, Ergänzungen und nützlichen Linklisten. Hier gibt es auch die Möglichkeit, Bugreports und anderes Feedback einzubringen und sich an der Fortschreibung des Textes zu beteiligen.@2  ZÜ 2:Credits@1 fliess mit:@1 fliess ohne:Zu Dank verpflichtet bin ich zu allererst den Mitgliedern von mikro und dem sehr viel größeren Team, das die »Wizards of OS« möglich gemacht hat. Hervorzuheben ist vor allem Thorsten Schilling, der nicht nur bei der Konzipierung und Planung der Konferenz ein unschätzbarer Partner in allen Ups-and-Downs war, sondern es auch möglich gemacht hat, dass dieses Produkt der kollektiven Intelligenz sich nun auch als Buch manifestiert. Thomax Kaulmann, Inke Arns und Ellen Nonnenmacher haben das Projekt kontinuierlich mitgetragen. Dank geht auch an Pit Schultz und Diana McCarty für fruchtbare Diskussionen. Ebenso an die weiteren mikros Tilman Baumgärtel, Andreas Broeckmann, Stefan Schreck, Sebastian Lütgert, Ulrich Gutmair, Martin Conrads, Vali Djordjevic, Walter van der Cruijsen und Micz Flor. @1 fliess mit:<*h"mehr">Nicht nur für vielfältige Inspirationen und dafür, dass sie mir den Rücken freigehalten haben, bin ich Wolfgang Coy und Hugo Pflüger verpflichtet. Die Konferenz wäre nicht möglich gewesen ohne Philip Yoram Bacher, Holger Friese, Philipp Haupt, Wilhelm Schäfer, Oleg Stepanov, Robert Fürst und Katja Pratschke. Susanne Schmidt, Sebastian Hetze, Jea<\h>nette Hofmann, Florian Cramer, Frank Ronneburg, Bernd Sommerfeld, Geert Lovink und Eike Dierks waren mir wichtige Gesprächspartner, führten mich in neue Welten ein und standen mit Rat und Tat zur Seite. Von den vielen, die den Prozess mit ihren Ideen und Taten bereicherten, seien besonders herzlich Sabine Helmers, Boris Gröhndahl, Andy <\h>Müller-Maguhn, Bernd Lutterbeck, Tim Pritlove, David Hudson, Armin Haase, Florian Rötzer, Angelika Meier, Wau Holland, Armin Medosch, Sally-Jane Norman, Andreas Küppers, Christian Kirsch, padeluun, Rena Tangens, Andreas Lange, Uwe Pirr, Gusztàv Hàmos, Detlef Borchers und Julia Lazarus erwähnt. <*h"Standard">Im Haus der Kulturen der Welt waren es besonders Maria Rosa Wonchee, Peter Winkels, Henrike Grohs und Bernd Scherer, die die Zusammenarbeit zu einem Vergnügen machten. Für das Fachgespräch des Bundesministerium für Wirtschaft und Technologie lässt sich das gleiche von Thomas Wahl und Bernd Bauche sagen. Von all den Unterstützern der WOS 1 seien besonders Eva Emenlauer-Blömers, Peter Weibel und Hubertus Soquat genannt. Viele wären noch zu nennen, wollte diese Liste vollständig sein, nicht zuletzt die Vortragenden und Workshopleiterinnen der verschiedenen WOS-Veranstaltungen und die Teilnehmer der Seminare WOS 1 und WOS 2 am Institut für Informatik der Humboldt-Universität zu Berlin und des Napster-Blockseminars an der Hochschule für Grafik und Buchkunst Leipzig. Um den Text in seiner vorliegenden Form haben sich besonders fünf Personen verdient gemacht. Peter Bingel von der Initiative »Freie Software und Schulen«, Georg Greve von der <@1 fliess kursiv>Free Software Foundation Europe<@$p>  sowie Alexander Stielau von der <@1 fliess kursiv>Berlin Unix User Group<@$p> (BUUG) sei Dank für die kritische Durchsicht des Manuskripts und wichtige Ergänzungen und Korrekturen. Herzlicher Dank geht auch an die Projektleiterin in der Bundeszentrale für politische Bildung, Sabine Berthold, und an meine Lektorin Conny Schmitz, die selbst bei geplatzten Abgabefristen und anderen Alarmstufe-Rot-Situationen noch ein aufmunterndes Wort für mich hatte. <@6 Caps>Volker Grassmuck,<@$p>vgrass<\@>rz.hu-berlin.deBerlin Kreuzberg, im August 2001<\c>@0  Über 100%:Navigation@1 fliess mit:<*p(0,14.173,0,22,0,0,g,"Deutsch")>@1 fliessen Initial:<k10f"Univers-CondensedBold">D<k$f$>ies ist kein Buch über freie Software. Vielmehr handelt es von Menschen, die an denselben Problemen arbeiten, ihre Antworten austauschen, voneinander lernen und gemeinsam etwas Größeres schaffen als die Summe der Teile, anstatt miteinander zu konkurrieren. Was sich ganz alltäglich anhören mag und doch in einem gesetzten Marktsegment vollkommen unwahrscheinlich ist, wuchs zu einer der großen Auseinandersetzungen des ausgehenden 20.<\!q>Jahrhunderts heran. Inzwischen haben freie Computerprogramme wie GNU/Linux und Apache ihre Qualität längst bewiesen. Microsoft darf man gerichtlich <\n>abgesichert als Monopol bezeichnen. Um den andauernden Streit von David und Goliath ist es ruhiger geworden, doch der Glaubwürdigkeitsverlust der proprietären, industriellen Produktion und Distribution von Wissensgütern und der Respektgewinn für seine Alternative – eine freie, offene Kooperation der kollektiven Intelligenz – werden uns noch lange beschäftigen. Wie Richard Stallman, einer der Gründerväter und Ban<\h>nerträger der Bewegung zu sagen pflegt, geht es nicht um die Software, sondern um die Gesellschaft, in der wir leben wollen.@1 fliess mit:Wie freie Software ist dieses Buch modular aufgebaut. Man kann also ganz nach Leseinteresse beginnen, wo die Neugier einen hinlenkt, und dann vor- oder zurückspringen. Das Buch ist in zwei große Teile gegliedert. Von der freien Software handelt der zweite. Einen möglichen Einstieg bietet etwa das Kapitel »Was ist freie Software?«. Für ein Verständnis der Wissensumwelt, in der sie entsteht, und der Wurzeln der technischen Kulturen, die sie hervorbringen, sind die beiden Geschichtskapitel zum »Internet« und zum Betriebssystem »Unix« hilfreich. Wer skeptisch ist, ob es sich bei der freien Software nicht doch nur um eine Eintagsfliege handelt, wird sich zunächst über ihre »gesellschaftlichen und wirtschaftlichen Potenziale« unterrichten wollen. Unter der Überschrift »Die Software« finden sich kurze Portraits einiger wichtiger freier Projekte. Installations- und Bedienungshilfen darf man hier nicht erwarten. Für jedes der Programme bietet der Fachhandel eine Auswahl guter Handbücher. Freie Software unterscheidet sich von »unfreier« vor allem durch ihre offenen, sozialen Prozesse der Wissenskooperation. Die Freiheit, gemeinsam Programmcode zu entwickeln, wird ihrerseits in einem anderen Code geschrieben. Neben einem Verhaltenskodex, der Ethik des <\n>offenen Austausches, sind es die Lizenzen der freien Software, die den Freiraum absichern, in dem eine offene Kooperation nur möglich ist. Ohne den Schutz dieser Lizenzen und der zu Grunde liegenden Urheberrechte hätte die freie Software niemals ihre heutigen Dimensionen <\n>erlangen können. Besonders bei den »Lizenzmodellen«, aber auch an vielen anderen Stellen im zweiten Teil wird die Frage auftauchen, welchen rechtlichen Status eine geordnete Menge von <@1 fliess kursiv>Bits<@$p> eigentlich hat. »Geistiges Eigentum ist die Rechtsform des Informationszeitalters«, schreibt der amerikanische Rechtsgelehrte James Boyle (1997). Wie es dazu kam, zeichnet der erste Teil nach. Er beginnt mit einem kurzen Überblick über die Geschichte des Urheberrechts, die heutigen Bestrebungen zu seiner internationalen Angleichung und das ökonomische Feld der an der Hervorbringung und Verbreitung von Wissen beteiligten Akteure. Mit neuen Medientechnologien wandeln sich die kreativen Praktiken, der Markt für ihre Produkte und dessen rechtliche Regulierung. Die beiden Hauptkapitel des ersten Teils beschäftigen sich mit den computergestützten Medien und den Bemühungen des Rechts, sie zu erfassen. Erst eine Vorstellung vom generellen Trend zur Verschärfung der Kontrolle durch die Wissensgüterindustrie lässt die Größe der Revolution erkennen, die die freie Software darstellt. Wer sich besonders für Fragen des gemeinschaftlichen Eigentums interessiert, wird mit dem ersten Kapitel über die Allmende beginnen wollen, den zweiten Teil über freie Software als Exemplifikation lesen und im abschließenden Kapitel einen Ausblick auf eine Bewegung finden, die nicht nur Software, sondern Wissen jeder Art gemeinschaftlich erstellt und pflegt. Und natürlich kann man dieses Buch auch einfach von vorne nach hinten durchlesen  <z12b-1>;-)<z$b$>  <\b>@0  Über 50%:1. Teil: @0  Über 100%:<*L>Die rechtliche Ordnung des Wissens@1 fliess mit:@1 fliess ohne:In jüngster Zeit liest man häufig dort, wo seit den 60er-Jahren »Informationsgesellschaft« gestanden hat, das Wort »Wissensgesellschaft«. Dieser noch nicht wirklich reflektierte Wandel in der Terminologie könnte eine Verschiebung der Gewichtungen andeuten. Mit der »Informationsgesellschaft« verbinden sich Vorstellungen vom Computereinsatz für die Steuerung des globalen Finanzsystems und von Werkzeugmaschinen, von Datenbanken und Expertensystemen, von Ras<\h>terfahndung und E-Commerce. Mit der »Wissensgesellschaft« tritt nun die maschinelle Seite zurück und das in den Vordergrund, was Menschen mit Information und Wissen machen. Nicht mehr Datenträger und -ka<\h>näle sind wichtig, also die Container, sondern der Content. Informatik und Telekommunikation können als gegeben betrachtet werden, bedeutsam sind jetzt vor allem Kultur und telekommunikatives Handeln. Die Vision der Künstlichen Intelligenz ist von der einer Kollektiven Intelligenz überholt worden, die in der computergestützten Vernetzung die Mittel erhalten hat, sich auszudrücken. @1 fliess mit:Der Philosoph der »Ordnung des Wissens« ist Helmut F. Spinner. Er beschreibt eine Neupositionierung des Wissens in allen Lebensbereichen: Wissen als Produktionsmittel, Wissen als »Unterhaltungs- und Verwaltungsmittel« (Spinner, 1994, S. 22), also zum einen als Ware, zum anderen als »Betriebssystem«, und Wissen als »Verkehrsmittel«, als Möglichkeit, Wege elektronisch abzukürzen – also das, was gewöhnlich Telekommunikation heißt. Spinner entwickelt das Konzept der Wissensordnung als dritte Basisstruktur der Gesellschaft neben der Wirtschafts- und der Rechtsordnung. Sie umfasst ihre bereichsspezifisch ausgebildeten Teilordnungen, hier besonders relevant die Eigentums-, Besitz-, Markt- und Verteilungsverhältnisse. Wissen als Produktivkraft, Konsumgut und Kontrollmittel, so Spinner, entwickelt seine eigenen Schutz- und Verwertungsbedingungen. Die »Wende von den vorherrschenden juristischen Schließungslösungen (durch Verbote, Zensur, Zweck<\h>bindung, Abschottung) zu ordnungspolitischen Offenheitslösungen (durch ›Waffengleichheit‹, informationelle Gewaltenteilung, Infrastrukturen der Kritik oder Gütekriterien gegen schlechtes Wissen u.dgl.)« (Spinner et.al., 2001, S.<\!q>12) liegt auch der vorliegenden Darstellung am Herzen.In diesem Teil des Buches wird die Schnittmenge, in der die Ordnung des Wissens, die Rechts- und die Wirtschaftsordnung ineinandergreifen, von der juristischen Seite aus betrachtet. Das Immaterialrecht kontrolliert die Produktion, die ökonomische Verwertung, die Rezeption, den Gebrauch und das Fortschreiben von Wissen. Ebensowenig wie die technische Seite der Medien, ist auch die rechtliche nur eine »Randbedingung« für die wissensimmanenten und die kulturellen, ethischen, sozialen und  ästhetischen Fragen des Wissens. Urheber- und Verwertungsrechte markieren das Spannungsfeld von Wissen als Ware und Wissen als öffentlichem Gut. Die rechtliche Regulierung der Wissensordnung strebt nach einer historisch wandelbaren Balance zwischen dem gesellschaftlichen Interesse an Urheber- und Investitionsschutz und dem an der ungehinderten Nutzung von Information durch alle Mitglieder der menschlichen Kulturgemeinschaft. Will man, so geht die Rechtslogik, dass Autoren ihre Werke veröffentlichen, so muss man einerseits Bedingungen schaffen, unter denen sie und ihre Verlage daraus Erträge erzielen können. Will man andererseits, dass neue Werke und neue Autorinnen heranwachsen, so muss man sicherstellen, dass bestehende Werke ohne allzu große finanzielle und sonstige Hemmschwellen zur Lehre und Inspiration für sie bereitstehen. Anders als im Falle materieller Waren sind die ausschließlichen Rechte an immateriellen Gütern auf einen bestimmten Zeitraum begrenzt. Darin drückt sich eine Anerkennung der Tatsache aus, dass jedes intellektuelle Produkt aus dem großen kulturellen Pool kollektiver Kreativität gekommen ist und dorthin zurückkehrt. Sind ein Foto, ein Film, ein Musikstück oder ein Roman einmal veröffentlicht, werden sie Teil der informationellen Umwelt in der Ausdehnung der jeweiligen medialen Reichweite und Teil des kulturellen Erbes. Es ist also nur natürlich, dass sich andere Mitglieder der Kulturgemeinschaft auf sie beziehen, sie zitieren und in eigen<t-1>en Kreationen darauf reagieren. Beide gesellschaftlichen Werte gleicher<\h>m<t$>aßen zu sichern, ist der Balanceakt, den das Urheber- und die anderen geistigen Eigentumsrechte ständig neu vollbringen. Wissens- oder Informationsrechte verändern sich maßgeblich in Abhängigkeit von medientechnischen Entwicklungen und den sich dadurch wandelnden ökonomischen Gegebenheiten der Verwertung von Wissen. Mediale Vervielfältigung setzte bis vor kurzer Zeit Produzententechno<\h>logie voraus. Eine Druckerei oder ein Plattenpresswerk stellten Investitionsgüter dar, die die Reproduktion der materiellen Informationsträger nur einer überschaubaren Anzahl von Akteuren erlaubte. Der Streit um »Raubkopien« und Aufführungsrechte wurde im 19. Jahrhundert zwischen Drucker-Verlegern und Theatern geführt, private »Endverbraucher« spielten dabei keine Rolle. Mit dem Rundfunk ergaben sich neue Fragen der Rechte auf Verbreitung von Information und ihrer Vergütung. Doch auch ist die Zahl der beteiligten Akteure durch die Mangel<\h>ressource Spektrum und den Kapitalbedarf für Studios und Sendeanlagen begrenzt.Fotokopierer, Audio- und Videokassettenrekorder brachten erstmals Reproduktionstechnologien in die Privathaushalte. Die regulative Antwort darauf waren in Kontinentaleuropa Pauschalabgaben für aufnahmefähige Geräte, Leermedien und Fotokopien. Beiden Seiten der Balance ist Rechnung getragen: Das Recht auf private Kopien ist gesichert und die Urheber erhalten über ihre kollektive Rechtewahrnehmung (die Verwertungsgesellschaften) Vergütungen. Ein markanter Einschnitt in der Entwicklung ergibt sich aus der Digitalisierung der Medientechnologien. Sie verändert Wissens- und Medien<\h>praktiken und stellt das Urheberrechtsregime vor neue Herausforderungen. Mit der ersten massenhaft verbreiteten digitalen Medientechnologie, der Compact Disc (CD), wurden Privatpersonen zunächst wieder in die Rolle von Konsumenten der industriell gefertigten Informationsträger verwiesen. Es vergingen Jahre, bis CD-Brenner auch für sie erschwinglich wurden.Auf der Produktionsseite senkte die neue Technologie die investitionelle Einstiegshürde. Kostengünstigere CD-Presswerke brachten in den 80er-Jahren eine Fülle von kleinen Musikverlagen, die <@1 fliess kursiv>Independents<@$p> oder <@1 fliess kursiv>Indies<@$p>, hervor. Musikautoren und -interpreten, die bislang durch die Auswahlraster der hochgradig konzentrierten Plattenindustrie fielen, erhielten jetzt eine Chance, ihre Werke an ein Publikum zu bringen. Eine Innovation der Produktionsmittel führte dazu, dass die Wissenslandschaft reicher und vielfältiger wurde. Seit Ende der 90er-Jahre führt ein Komprimierungsformat für Musik in CD-Qualität (<@4 Pfeil (Umschalt/Alt #)>’ <@$p>MP3) dazu, dass die Daten vollends von ihren materiellen Trägern abheben.Die Computertechnologie entwickelte sich anfangs auf einer von der Unterhaltungselektronik unabhängigen Linie.<@3 hoch fliess>1<@$p> Erst in den 70er-Jahren begann sich die Vorstellung zu etablieren, dass Software urheberrechtlich geschützt werden kann. Erst seit den 80ern konnte sie in den USA auch mit Hilfe von Patenten geschützt werden. Software und Daten jeglicher Art sind in der Anwendungsumgebung des Computers ihrer »Natur« nach frei kopierbar. Eine Kopierschutz musste in einem zusätzlichen Schritt mit Hilfe von Code, Hardware (<@4 Pfeil (Umschalt/Alt #)>’<@$p>Dongle) oder gesetzlichen Verboten hergestellt werden. Bezeichnenderweise war es nicht die Unterhaltungselektronik-, sondern die Computerindustrie, die CD-Geräte von <\n>einer reinen Abspieltechnologie in eine Abspiel- und Aufnahmetechnologie verwandelte. Mit dem CD-Brenner hielt die verlustfreie Reproduzierbarkeit großer digitaler Datenmengen Einzug in den Privathaushalt.Datennetze schließlich lösen digitale Informationen von der Notwendigkeit eines materiellen Trägers. Natürlich beruht auch das Internet auf einer extensiven materiellen Infrastruktur (Kabel, <@4 Pfeil (Umschalt/Alt #)>’<@$p>Router etc.), doch Inhalte wie Texte oder Software, die auf einer <@4 Pfeil (Umschalt/Alt #)>’<@$p>Newsgroup gepostet werden, verbreiten sich ohne weiteres Zutun durch das <@4 Pfeil (Umschalt/Alt #)>’<@$p>Usenet, so dass Kopien in kürzester Zeit auf Zehntausenden von Rechnern in aller Welt vorhanden sind. Von einem »Werkstück« zu reden, macht im Internet keinen Sinn mehr. Die Struktur des Netzes und die Kultur seiner Nutzerinnen führte nicht wenige Beobachter zu der Aussage, dass Information frei sein will.<@3 hoch fliess>2<@$p> Damit nicht sein kann, was nicht sein darf, arbeitet die Rechteindus<\h>trie unter Führung der Musikindustrie derzeit daran, in die freizügige Struktur des Internet umfassende Mechanismen zur Verwaltung und <t-1>Kontrolle urheberrechtlich geschützter Information einzuziehen. Urhe<\h><\h>be<t-2>r<\h>re<t$>chtskontrollsysteme implementieren privatwirtschaftlich-technologisch ein Verhältnis zwischen Rechteinhabern und Wissensnutzern, das bislang der öffentlichen Regelung durch Gesetzgeber und Rechtsprechung unterstand (vgl. Lessig, 1999). Ganz im Gegenteil des Versprechens von der »freien« Information zeichnet sich hier ein Grad von Kontrolle über die verschiedenen Nutzungsformen des Wissens bis hinunter zu kleinsten Informationseinheiten ab, wie er nie zuvor denkbar war. Und das, obgleich in Fachkreisen bis vor nicht allzu langer Zeit bezweifelt wurde, ob Information an sich überhaupt Eigentum sein könne oder solle.<@3 hoch fliess>3<@$p> Insbesondere für die in der freien Software zentralen Leitwerte der Veränderungs- und Verkehrsfreiheit ist der anhaltende Prozess der juris<\h>tischen Selbstreflexion auf die sich wandelnden Medienumwelten von Bedeutung. Das folgende Kapitel führt zunächst einen Kollektiveigentumsbegriff ein, die »Allmende«, aus deren Aufteilung im Mittelalter das Konzept des Privateigentums ursprünglich entstand. Das noch jüngere Konzept eines Eigentums an Immaterialgütern bildete sich anhand der gutenbergschen Drucktechnologie und parallel zu einem sich gegenläufig artikulierenden Anspruch auf öffentlichen Zugang zum Kulturerbe in Sammlungen, Bibliotheken und Archiven. Unter den Rechten des geistigen Eigentums konzentriert sich dieses Kapitel auf die beiden Traditionen des kontinentaleuropäischen Urheberrechts und des angelsächsischen Copyright-Rechts. Patent- und Markenschutzrecht können nur am Rande ange<\h>sprochen werden. Auch die im 20.<\!q>Jahrhundert neu hinzukommenden informationsrechtlichen Regulierungen wie Datenschutz, Medienrecht, Kommunikationsdiensterecht und Informationsfreiheitsgesetze werden nur dort aufgegriffen, wo sie die urheberrechtliche Materie berühren. Da es nicht um eine rechtswissenschaftliche Darstellung gehen soll, sondern um die Interaktion der Wirtschafts- und Rechtsordnung sowie der <\n>Medientechnologie mit der Wissensordnung, werden daraufhin die wichtigsten Akteure, ihre Interessen und ihre Stellung in der Urheberrechts<\h>praxis angesprochen. Da es in erster Linie die Dynamik der medientechnologischen Innovationen ist, die die rechtliche Entwicklung vorantreibt, werden diese als nächstes behandelt, wobei der Schwerpunkt auf den <\n>digitalen Medien und ihrer gesetzgeberischen Erfassung liegt. Urheberrecht respektive Copyright erhalten heute zunehmend Flankenschutz, aber auch Konkurrenz durch zwei weitere Mechanismen: Zum einen werden die rechtlichen Bestimmungen durch privatrechtliche Lizenzverträge zwischen den Wissensanbietern und den Wissensnutzern überformt, zum anderen wird eine umfassende technologische Infrastruktur in die Mediengeräte und das Wissen selbst eingezogen, die die Werke und ihre Nutzer auf immer feinporigere Weise kontrolliert und überwacht. Abschließend und im Vorgriff auf den zweiten Teil des Buches wird eine sich in jüngster Zeit herausbildende digitale Wissens-Allmende umrissen. Auch hier werden Lizenzverträge eingesetzt, doch nicht wie von der Rechteindustrie, um eine maximale Kontrolle über die Verwertung zu bewirken, sondern um den Prozess der offenen kooperativen Wissenserzeugung und -pflege gegen die Gefahren der Privatisierung abzusichern. @2  ZÜ 1:<\c>Eigentum@1 fliess mit:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Einerseits will Information teuer sein, da sie so wertvoll ist. Die richtige Information am richtigen Ort verändert Ihr Leben. Andererseits will Information frei sein, da die Kosten sie zu verbreiten ständig geringer werden. Und so streiten sich diese beiden Seiten miteinander.« <@1 fliess normal><*t(25.512,0,"1  ")>(<f"FFScala-Caps">Stewart Brand<f$$>, auf der ersten <@1 fliess kursiv>Hackers’ Conference<@1 fliess normal>, Herbst 1984) <@$p><*t(25.512,0,"1  ")><f"FFScala"><*t(25.512,0,"1  ")><f$>»Ich denke, dass jede allgemein nützliche Information frei sein sollte. Mit ›frei‹ beziehe ich mich nicht auf den Preis, sondern auf die Freiheit, Informationen zu kopieren und für die eigenen Zwecke anpassen zu k<t-2>önnen. Wenn Informationen allgemein nützlich sind, wird die Mensch<\h>h<t$>eit durch ihre Verbreitung reicher, ganz egal, wer sie weitergibt und wer sie erhält.« <@1 fliess normal>(<@6 Caps>Richard Stallman<@1 fliess normal>, ca. 1990) @1 fliess mit:@1 fliess ohne:Die Vorstellung, dass Information einem Einzelnen und nicht vielmehr allen gehören könnte, ist vergleichsweise jung.<@3 hoch fliess>4<@$p> Weder das römische noch das germanische Recht kannten das abstrakte Konzept von Immaterialgütern. Erst mit dem Entstehen eines neuen Wissens- und Autorenbegriffs in der Renaissance tauchten erste staatlich verliehene Schutzrechte für Erfindungen, für literarische und dann auch musikalische, dramatische und andere künstlerische Werke auf. In ihrer kodifizierten Form weichen sie auffällig von anderen Eigentumsrechten ab. Zwar sind Urheber- und Patentrechte, zumindest in wesentlichen Aspekten, frei übertragbar, können durch gerichtliche Verfügung geltend gemacht werden und sind gegen staatliche Eingriffe geschützt, d.h. sie tragen Merkmale des Eigentums, doch anders als materielle Güter, sind sie zeitlich und ihrer Geltung nach begrenzt. @1 fliess mit:Dem liegt das Konzept einer Balance zwischen einem Interesse an <\n>Urheber- und Investitionsschutz und einem gesellschaftlichen Interesse an der freien Verfügbarkeit von Wissen zugrunde. Diese Balance zwischen <@1 fliess kursiv>Private Domain<@$p> und <@4 Pfeil (Umschalt/Alt #)>’<@1 fliess kursiv>Public Domain<@$p> verändert sich geschichtlich und wird vor allem durch medientechnologische Entwicklungen immer wieder herausgefordert. In der Informationsgesellschaft spitzt sich der in Stewart Brands Motto angesprochene »Streit« der beiden Seiten zu. Da in Datennetzen die Transaktions- und Distributionskosten gegen Null <\n>gehen, stand einerseits noch nie so vielen Menschen eine so gewaltige Menge an Information auf Knopfdruck zur Verfügung. Andererseits ist Information zum zentralen Produktionsmittel und zur wichtigsten Ware geworden. Das Internet ist das ideale Medium, um Wissen frei zirkulieren zu lassen und Menschen bei der offenen, kooperativen Weiterentwicklung dieses Wissens zu unterstützen. Doch die gleichen Bedingungen seiner programmgesteuerten Übertragung, Speicherung und Veränderung werden heute benutzt, um eine umfassende Infrastruktur zu seiner Vermarktung und Kontrolle zu errichten.<@3 hoch fliess>5<@$p> Einerseits veralten Informationen durch ihren exponentiellen Zuwachs immer schneller, <\n>andererseits werden die rechtlichen Schutzfristen immer weiter ausgedehnt. Durch den Säurefraß stehen die papiergestützen Wissensspeicher vor der größten Wissenskatastrophe aller Zeiten, der nur durch eine groß angelegte Digitalisierung zu entkommen wäre – gleichzeitig werden die Etats der Bibliotheken gekürzt. Dem Spannungsfeld von geistigem Eigentum und freiem Wissen nähert sich dieser Abschnitt in drei Schritten. Zunächst wird mit der <t-1>»Allmende« oder englisch »<x@1 fliess kursiv><t-1>Commons«<@$p><t-1> an einen alten Kollektiveigentumsb<t$>egriff erinnert, der heute im Zusammenhang mit freier Software wieder auftaucht. Während die Allmende privatisiert wurde, gingen die in den Sammlungen der Reichen und Mächtigen gehorteten Kulturschätze den entgegengesetzten Weg: Die neu entstehende moderne bürgerliche Nation macht sie in Museen, Bibliotheken und Universitäten allen zugängl<t-1>ich. Schließlich wird in einem dritten Abschnitt die Gegenbewegung skiz<\h>zi<t$>ert, in der sich der Begriff des geistigen Eigentums etabliert und damit die rechtlichen Schutzmechanismen, denen das literarische und dann auch die anderen Formen künstlerischen Schaffens unterstellt werden.@2  ZÜ 2:Allmende – vom Kollektiveigentum zum Privateigentum@1 fliess ohne:Die gemeinsame Nutzung von natürlichen Ressourcen nennt sich mit <\n>einem althochdeutschen Wort »Al(l)mende«. Es stammt aus dem Bodenrecht und geht zurück auf <@1 fliess kursiv>al[gi]meinida<@$p>, »gemeinsam genutztes Land <\n>einer Gemeinde, einer festen Nutzergemeinschaft; allen gemeinsamer Grund, besonders Viehweide, Waldgebiet, Fischgewässer; sonstiges <\n>gemeindeeigenes Gelände, z.B. Wege, Plätze u.ä. auch innerhalb von Ortschaften; als Einrichtung meint es das Recht der gemeinschaftlichen Nutzung von Ländereien und auch die Nutzergemeinschaft« (<@6 Caps>Grimm<@$p>, 1998, S. 480 ff). Übrigens verweist auch das Wort »Ding« etymologisch auf eine Form der Allmende. Es geht zurück auf germanisch <@1 fliess kursiv>thing<@$p>, Volks- <t-1>und Gerichtsversammlung, und bezeichnet eine interessante Verbindung <t$>von Ort (dem <@1 fliess kursiv>thing<@$p>-Platz, an dem man sich traf), Zeit<\!q><@3 hoch fliess>6<@$p> und sozialer Institution, der »Volksversammlung aller Freien«, die über Krieg und Frieden beschließt, den Heerführer oder König wählt und in der Recht gesprochen wird.<@3 hoch fliess>7<\!q><@$p>Vielleicht liegt hier der Grund dafür, dass auch heute noch Gesetze, Verordnungen, amtliche Erlasse und Entscheidungen als öffent<\h>l<t-2>iche Dinge vom urheberrechtlichen Schutz ausgenommen sind (§ 5 UrhG).<t$> Analog bezeichnet im angloamerikanischen Eigentumsrecht »<@1 fliess kursiv>the Commons<@$p>« ein Stück Land für den kommunitären oder öffentlichen Gebrauch. In den amerikanischen Kolonien standen die <@1 fliess kursiv>Commons<@$p> für einen Dorfanger oder einen öffentlichen Platz. Auch hier ursprünglich als gemeinschaftliche Weide genutzt, wurden die zentralen <@1 fliess kursiv>Commons <@$p>zum <\n><t-2>Fokus des Gemeindelebens, wo Versammlungshäuser, Tavernen, Schmieden<t$> und Geschäfte errichtet und kommunalen sowie Erholungszwecken nachgegangen wurde.@1 fliess mit:Die überarbeitete Fassung des grimmschen Wörterbuches führt eine Fülle weiterer Fundstellen an, in denen auch eine bildliche Verwendung ablesbar ist. So ist im 14.<\!q>Jahrhundert vom rubinroten Mund einer Frau die Rede, der zur Allmende geworden sei. Ab Mitte des 19. Jahrhunderts heißt es von der Muttersprache, dass sie eine Allmende sei, worauf sich die Gelehrten und die Ungelehrten aller Fächer weideten (<@6 Caps>Grimm<@$p>, 1984).Das Recht an der Allmende entstand ursprünglich durch gemeinsame Urbarmachung, Waldrodung oder Erschließung z.B. von Moorgebieten. Unterschieden vom herrenlosen Wildland (engl.: <@1 fliess kursiv>waste<@$p>) war die Allmende Kollektiveigentum der Almendgenossen, also der Mitglieder eines Stammes oder eines Dorfes. In seinem Werk »Wirtschaft und Gesellschaft« nennt Max Weber diesen Prozess die »Schließung« einer Gemeinschaft, die die Quelle des »Eigentums« am Boden ebenso wie aller anderen Gruppenmonopole bilde. Als Beispiele nennt er den Zusammenschluss der Fischereiinteressenten eines bestimmten Gewässers, die Schließung der Teilnahme an den Äckern, Weide- und Allmendnutzungen eines Dorfes gegen Außenstehende, aber auch einen Verband der <\n>Diplomingenieure, der das rechtliche oder faktische Monopol auf bestimmte Stellen für seine Mitglieder gegen die nicht Diplomierten zu erzwingen sucht (<@6 Caps>Weber<@$p>, 1995, S. 140 ff). Hier wäre auch das Copyright selbst zu nennen, das die britische Krone im 16. Jahrhundert als Monopol an die Gilde der Drucker-Verleger (<@1 fliess kursiv>Stationers‘ Company<@$p>) verlieh. Als treibende Kraft wirkte dabei stets die Tendenz zum Monopolisieren bestimmter, in der Regel ökonomischer Chancen gegen Außenstehende. Innerhalb des Kreises der monopolistisch Privilegierten, so Weber, könnten Chancen »offen« bleiben, also »freie« Konkurrenz »unter sich« walten. Offenbar ist aber auch hier eine »Tendenz« am Werk, die darauf zielt, diese Systeme auch nach innen zu schließen. Ein Teil des Gemeinschaftsmonopols wird einzelnen Gemeinschaftern zur Monopolnutzung zugewiesen. Die privat zu nutzenden Flächen können im saisonalen Turnus neu verteilt werden oder bis auf Widerruf oder lebenslänglich und schließlich endgültig an den Einzelnen und seine Erben übergehen. Eine Abtretung der Verfügungsgewalt kann nicht zugelassen sein oder nur an Gemeinschaftsgenossen sowie schließlich auch an Dritte. Die verschiedenen Phasen der inneren Schließung der Gemeinschaft nennt Weber »Stadien der <@1 fliess kursiv>Appropriation<@$p> der von der Gemeinschaft monopolisierten sozialen und ökonomischen Chancen« (ebd., S.<\!q>143). Entsteht alles Eigen<t-1>tum aus der gemeinschaftlichen Schließung nach außen, so entsteht alles <t$>Privateigentum durch Schließung dieser Gemeinschaft nach innen. »Die völlige Freigabe der appropriierten Monopolchancen zum Austausch auch nach außen: Ihre Verwandlung in völlig ›freies‹ Eigentum bedeutet natürlich die Sprengung der alten monopolisierten Vergemeinschaf-<\h><\n>tung ...« (ebd.).Ging die Allmende einerseits durch innere Differenzierung in Privateigentum<@3 hoch fliess>8<@$p> über, so begannen, mit dem Entstehen von Zentralgewalten, auch Krone und Kirche sie für sich zu beanspruchen. Markgenossen konnten allenfalls Teilansprüche, wie Waldweide- und Holznutzungsrechte behaupten. Auch mit der Etablierung eines Eigentumsbegriffs an Grund und Boden wurde die Verfügung über Wild, Wasser und Mineralien häufig gesondert geregelt.<@3 hoch fliess>9<@$p> Im feudalen England konnte der Fürst Teile des <@1 fliess kursiv>Commons<@$p> für seine eigenen Zwecke appropriieren, sofern er genug Land für das Vieh seiner Untertanen übrig ließ. Im 19.<\!q>Jahrhundert ging dieses Recht der Aneignung auf den Staat über. Heute ist die gemein<\h>same Beweidung verschwunden, die <@1 fliess kursiv>Commons<@$p> sind öffentliche Grundstücke vor allem für Erholungszwecke. In Preußen wurden die Allmenden mit der Verordnung vom Juni 1821 aufgeteilt. In Bayern haben sie sich bis ins 20.<\!q>Jahrhundert erhalten.Trieben die Sesshaften ihr Kollektiveigentum immer weiter in die Gebiete der eigentumslosen nomadischen Stämme vor, so okkupierte <\n>seinerseits das im 12.<\!q>Jahrhundert entstehende Territorialrecht die gewohnheitsrechtlichen Strukturen der Markgenossenschaften: »Das duale System von kodifiziertem und traditionellem Recht, das Elemente eines subsidiären, dezentralen und den lokalen Verhältnissen angepassten Regelungssystems enthielt, zugleich aber unsystematisch war, öffnete der Willkür Tür und Tor« (<@6 Caps>Zingel<@$p>, 1995). In zahlreichen Ländern, wie z.B. Indien, kam zum Konflikt zwischen regionalen und nationalen Regulierungen auch noch das koloniale Rechtssystem hinzu. »Während Nutzungskonkurrenz auf lokaler Ebene noch geregelt werden konnte, trafen bei Nutzungskonkurrenz mit Nutzern von außerhalb auch verschiedene Rechtsauffassungen aufeinander, wobei diejenigen, die sich auf kodifiziertes Recht berufen konnten, die stärkere Position hatten. [...] Schwierig ist die Bestimmung des Umfangs der Allmende: Hier handelt es sich vielfach um Unland (<@1 fliess kursiv>waste<@$p>), als dessen Eigentümer sich der Staat empfindet. Häufig werden diese Flächen von der staatlichen Forstbehörde verwaltet, auch wenn es sich um Flächen ohne jeglichen Baumbestand handelt; solche Flächen werden sehr vielfältig genutzt, vor allem aber als Weide« (ebd.). Nach dem traditionellen System wurde im <@1 fliess kursiv>Thing<@$p>-Kreis eine Kontingentierung beschlossen, wenn die gemeinsame Nutzung die vorhandenen Ressourcen zu erschöpfen drohte. Doch wenn die »Dinge« außer Rand und Band gerieten und die Menschen aus der Allmendgenossenschaft in den individuellen und korporativen Egoismus fielen, versagte die Mäßigung und die »Tragödie« oder das »Dilemma der Allmende« trat zutage. Der Aufsatz »The Tragedy of the Commons« von 1968 machte den Humanökologen Garrett Hardin von der Universität Kalifornien berühmt. Darin beschäftigt er sich mit der malthusschen Bevölkerungsdynamik, den Unwägbarkeiten der Spieltheorie und den Fragen von unbegrenztem Wachstum in einer begrenzten Welt. Die Allmende könne funktionieren, argumentiert Hardin, solange die Bevölkerung immer wieder durch Makro- und Mikrofressfeinde dezimiert und die Ertragsfähigkeit einer Umwelt nur zu einem Minimum ausgelastet wird, also keine Knappheit herrscht. Doch wenn soziale Stabilität, ein friedlicheres Verhältnis zu gegnerischen Stämmen, Raubtieren und Krankheiten und damit Wachstum einsetzen, erzeuge »die inhärente Logik der Allmende    ... unbarmherzig ihre Tragik«.Als rationales Wesen werde jeder Allmendgenosse danach streben, seinen Vorteil zu maximieren. Als Viehhalter werde er sich fragen, welchen Nutzen es für ihn hat, seiner Herde ein weiteres Tier hinzuzufügen. Der Nutzen setzt sich aus einer negativen und einer positiven Komponente zusammen. Da er den zusätzlichen Ertrag erhält, ist sein individueller Nutzen annähernd +<\!q>1. Die negative Komponente ergibt sich aus der <\n>zusätzlichen Belastung der kollektiven Weide. Die aber wird von allen gemeinsam getragen und ist für ihn daher nur ein Bruchteil von –<\!q>1. Das Ergebnis der Abwägung sei somit, dass er ein weiteres Tier hinzufügt und noch eines und noch eines. Alle anderen verhalten sich in dieser Nutzungskonkurrenz »nach innen« genauso. Darin liege die Tragödie der Allmende: Jeder versuche, in einer begrenzten Umwelt seinen Nutzen grenzenlos auszuweiten, denn: »Freiheit der Allmende bedeutet den Ruin für alle.« In einem Interview fast 30 Jahre und eine Fülle von Diskussionen später präzisiert Hardin: »In einer übervölkerten Welt kann eine unregulierte Allmende unmöglich funktionieren« (<@6 Caps>Straub<@$p>, 1997).Hardin geht dabei nicht von der klassischen Allmende aus, wie sie in Europa bis ins Mittelalter üblich war, die ohne Selbstbeschränkungsmechanismen natürlich keinen Bestand gehabt hätte, sondern vom 20.<\!q>Jahrhundert, das sich der »Grenzen des Wachstums« schmerzhaft bewusst wird.<@3 hoch fliess>10<@$p> Längst ist aus einer sich kollektiv selbst regulierenden Gemeinschaft der Allmendgenossen eine Gesellschaft aus anonymen, egoistischen, individuellen und juristischen Personen geworden. Doch auch hier finde sich das Ganze wie von selbst, so will uns Adam Smith in seinem Fundamentalsatz über den freien Markt (1776 in <@1 fliess kursiv>The Wealth of Nations<@$p>) glauben machen. Von der Magie der »unsichtbaren Hand« geleitet, fördere der Egoist zugleich das Allgemeininteresse. Dieser Geist Adam Smiths muss laut Hardin »ausgetrieben« werden, wenn wir die heutigen Dilemmata in den Griff bekommen wollten. Die Wurzel des Übels sei Überbevölkerung durch Geburtenzuwachs und Immigration. Hardin richtet sich vor allem gegen das herrschende <@1 fliess kursiv>laissez faire<@$p> in der Fortpflanzung. Als »nicht zu verteidigende individuelle Freiheit«, die auch nicht durch Appelle an »Gewissen«, »Verantwortlichkeit« oder andere Formen der Mäßigung aufgefangen werde, müsse an ihre Stelle »wechselseitig vereinbarter, wechselseitiger Zwang« treten. <*h"mehr">Die Allmende und damit ihr Dilemma zeigt sich heute in verschiedenen Formen. In der Zeit, als Hardin seinen Aufsatz schrieb, wurde zum Beispiel gerade die Übernutzungsgefahr in der Abfallbeseitigung schmerzlich offenkundig. Als eine unerschöpfliche »inverse« Entsorgungs-Allmende war bis dahin das Wasser, die Luft und die Erde erschienen. Als der Müll katastrophisch zurückkehrte (vgl. <@6 Caps>Grassmuck<\!q>/<\!q>Unverzagt<@$p>, 1991), wurde deutlich, dass auch die allen gemeinsamen Natur<\h><t-1>elemente nur eine begrenzte »Ertragsfähigkeit« besitzen, also eine Fähigk<t$>eit, unseren Schmutzausstoß zu ertragen. Noch ganz am Anfang, so Hardin, stehe unser Verständnis der Übel einer »Allmende des Vergnügens«. Gemeint ist der nahezu unregulierte öffentliche Klangraum, der von hirnloser Musikbeschallung und Ultraschallflugzeugen at<\h>tac<\h>kiert, sowie der öffentliche Blickraum, der von Werbung verschandelt werde. Hierzu gehören auch die Erholungsressourcen, wie die Nationalparks, in denen sich immer mehr Besucher drängen und somit ihren Wert für <\n>alle senken.<@3 hoch fliess>11<@$p> <*h"Standard">Am häufigsten findet sich der Begriff »Allmende« heute im Zusammenhang mit der Umweltpolitik. Besonders, wenn vermeintlich unerschöpfliche globale Ressourcen wie Luft und Meere, die ökologischen Gleichgewichte der Meeresböden und die Antarktis mit großindustriellen Mitteln erschöpfbar gemacht werden, ist von »Tragik« die Rede. So umfasst »das Aufgabenfeld der Umweltpolitik als globaler Umweltpolitik auch die Verhältnisse in der allen Ländern gemeinsamen globalen Umwelt (›globale Allmende‹) und daraus abgeleitet die Beeinflussung der Beziehungen aller Länder zu dieser Umwelt [...], die selber nicht als Völkerrechtssubjekt handeln kann« (<@6 Caps>Kloke-Lesch<@$p>, 1998). Die Antwort auf die Dilemma-Logik liegt natürlich immer noch in kollektiven, auf dem globalen <@1 fliess kursiv>thing<@$p>-Platz beschlossenen Kontingentierungen, wie Fischfangquoten für internationale Gewässer oder nationale Obergrenzen für CO<+>2<$>-Emissionen. Das Problem dabei ist, dass eine Kooperationsverweigerung<@3 hoch fliess>12<@$p> auf kurze Sicht in der Regel mit nationalem Nutzen verbunden ist: »Die Auszahlungsmatrix des Dilemmas ändert sich erst auf lange Sicht, denn: Indem einzelne Staaten ihren (sofort anfallenden) Nutzen zu maximieren versuchen, verstärken sie als Weltgemeinschaft den (zeitlich verzögerten) Treibhauseffekt, der im Nachhinein jede kurzsichtig-rationale Nutzenmaximierung als fatal irrational überführen könnte« (<@6 Caps>Steurer<@$p>, 1999). Mit anderen Worten, es geht um den zeitlichen Planungshorizont. In einer Zeit, in der Firmen unter einem dreimonatigen Erfolgszwang gegenüber ihren Aktionären stehen und »flexibilisierte« Menschen sich auf immer kürzere Ereignisfolgen einstellen, stimmt diese Erkenntnis umso bedenklicher. @2  ZÜ 2:Wissen: vom Eigentum zur Allmende @1 fliess ohne:Während der Boden von Kollektiv- in Privateigentum überging, nahmen die geistigen Güter in der anbrechenden Moderne den entgegengesetzten Weg. Die Schatzkammern der Fürsten werden zugänglich gemacht – »zum Schmuck des Staates und Nutzen der Öffentlichkeit«.<@3 hoch fliess>13<@$p> In der gleichen Zeit, da das typografisch gedruckte Buch als das erste auf modern<t-1>e Weise massenproduzierte Industriegut einem besonderen Kopierrecht <t$>unterstellt wurde, verlangten insbesondere Wissenschaftler, Schriftsteller, Gelehrte und Künstler nach Bildung und nach freiem Zugang zu dem <t-2>in privaten Sammlungen der Reichen und Mächtigen angehäuften Wissen. <t$>@1 fliess mit:Die Gutenberg-Galaxis markierte eine tief greifende Umwälzung des intellektuellen Klimas. Am Ausgang des 13.<\!q>Jahrhunderts begann es in Italien plötzlich von »Persönlichkeiten« zu wimmeln. Im 15. Jahrhundert trat der <@1 fliess kursiv>uomo<@$p> <@1 fliess kursiv>universale<@$p> auf: Es »erhebt sich mit voller Macht das <@1 fliess kursiv>Subjektive<@$p>, der Mensch wird geistiges <@1 fliess kursiv>Individuum<@$p> und erkennt sich als solches« (<@6 Caps>Burckhardt<@$p>, 1976, S. 123). Diejenigen, die als Rezipienten Zugang zu den Kulturgütern verlangten, waren selbst Autoren, eine neue Art von individualisierten Werkemachern, die den Lohn für ihre Kreativität auf einem Markt eintreiben und daher ihre Werke schützen mussten. Mit den neuen Handelssubjekten veränderte sich der rechtliche Status von Verträgen. Die Autoren und Erfinder trieben eine neue Klasse von Immaterialrechten hervor, das Urheber- und das Patentrecht. Während sich das <t-1>Wissen der Vergangenheit zur Allmende öffnete, begann sich im gleichen <t$>Zuge die aktuelle Wissensproduktion zu proprietarisieren.Die öffentlichen Museen und Bibliotheken, die im 17. und 18.<\!q>Jahrhundert entstanden, gingen auf die Sammlungen der Könige, Fürsten und Bischöfe zurück, der reichen Händlerdynastien im Renaissance-Italien und ab dem ausgehenden 16.<\!q>Jahrhundert auf die systematischen Sammlungen der gelehrten Gesellschaften, wie der <@1 fliess kursiv>Royal Society<@$p> in London (1660) und der Akademie der Wissenschaften in Paris (1666). Hinzu kamen die kleineren Kabinette, die Kunst- und Wunderkammern der <\n>reichen Bürger, die ebenfalls ihren Weg in die Museen fanden. Bereits im Mittelalter gab es Klöster, die Funktionen von öffentlichen Leihbibliotheken erfüllten. Martin Luther forderte in seinem Manifest »An die Rathsherren aller Städte deutsches Lands« (1524) im öffentlichen Interesse staatlich organisierte obligatorische Kurse zur Beherrschung des neuen Informationsmediums Typendruck und die planmäßige Erweiterung des Systems von Bibliotheken (<@6 Caps>Gieseke<@$p>, 1991, S.<\!q>473 f.). Die ersten öffentlichen Bibliotheken datieren aus dem frühen 17. Jahrhundert, so in Oxford von 1602, in Mailand von 1609 oder in Rom von 1620 (vgl. <@6 Caps>Schreiner<@$p>, 1975). In der Neuen Welt erinnert sich Benjamin Franklin in seiner Autobiographie an die ersten Schritte zur Gründung der »Mutter aller nord<\h>amerikanischen Subskriptionsbibliotheken«: »Etwa zu dieser Zeit [1730] wurde bei unserem Clubtreffen von mir der Vorschlag eingebracht, dass es von Vorteil für uns wäre, da wir uns in unseren Disputationen häufig auf unsere Bücher bezogen, diese dort zusammenzutragen, wo wir uns trafen, so dass wir sie nach Bedarf zu Rate ziehen konnten. Indem wir <\n>also unsere Bücher in einer gemeinsamen Bibliothek zusammenstellten, hätte jeder von uns den Nutzen der Bücher aller anderen Mitglieder, was fast genauso vorteilhaft wäre, als wenn jeder sie alle besäße.«<@3 hoch fliess>14<@$p>An dieser Beobachtung Franklins wird der grundlegende Unterschied zwischen einer materiellen und einer informationellen Allmende deutlich. Im ersten Fall sinkt der anteilige Nutzen aus der gemeinsamen Ressource mit jedem neuen Allmendegenossen. Im Fall der gemeinsamen Bibliothek steigt der Nutzen für alle mit jedem neuen Mitglied, das weitere Bücher einbringt. Die Kosten/Nutzen-Bilanz der Allmende fällt für informationelle Güter viel günstiger aus. Die Kosten sind der Preis der erworbenen Bücher durch die Zahl der Mitglieder. Der theoretisch maximale Nutzen ist die Gesamtzahl der verfügbaren Bücher mal die Zahl der Mitglieder. Der mögliche Schaden durch egoistische Monopolisierung ist beschränkt. Sicher horten Menschen gelegentlich Bücher, die so dem <\n>allgemeinen Pool entzogen sind, aber in der Regel findet die temporäre Privatisierung ihre Grenze in der Zahl der Bücher, die jemand gleichzeitig lesen kann. Die gegensteuernde knappe Ressource ist hier Aufmerksamkeit. 250 Jahre und mehrere Generationen Medientechnologie später formuliert sich das gleiche Verhältnis im »Gesetz der Netzwerke«. Es wird Robert Metcalfe zugeschrieben, der sich als Erfinder des <@4 Pfeil (Umschalt/Alt #)>’<@1 fliess kursiv>Ethernet<@$p> wie kein anderer mit verteilten Netzen auskennt. Metcalfes Gesetz besagt, dass der Wert eines Netzes im Quadrat der Zahl der angeschlossenen Rechner und Nutzer steigt. Die Magie der Verbindung: Mit jedem Inkrement steigt die Gesamtheit möglicher Verbindungen zwischen Menschen, Computern, Telefonen, Faxgeräten, Mobiltelefonen oder Büchern exponentiell (vgl. <@6 Caps>Gilder<@$p>, 1993). Nicht nur Bücher, sondern auch andere natürliche und künstliche Gegenstände wurden seit derselben Zeit systematisch zusammengetragen. <t-1>Besonders im 16. und 17.<\!q>Jahrhundert entwickelten Krone und Kirche eine <t$>Sammelleidenschaft, deren Resultate in den Louvre oder das British Museum eingegangen sind. Im Italien der Renaissance sowie in Frankreich und England kam im 18. Jahrhundert eine neue Schicht von privaten Sammlern auf. Sie waren gleichermaßen am Vergnügen, am Studium und am Wissenszuwachs, wie an der Erhaltung ihrer Sammlungen interessiert, hatten allerdings anders als die feudalen Dynastien nicht die Gewissheit, dass sie auf ihre Nachfolgegenerationen übergehen würden. Wenn diese Garantie nicht im Familienverband gefunden werden konnte, dann musste eine Erbnachfolge anderswo gesucht werden, und die korporative Einheit gewährte die größte Sicherheit. Wenn ferner Wissen eine bleibende Bedeutung haben sollte, musste es gemeinfrei (<@1 fliess kursiv>in the <\n>public domain<@$p>) übertragen werden (<@1 fliess normal>vgl. Encyclopædia Britannica, 2000).<@$p> Die ersten öffentlichen Museen entstanden, wie im Falle der Medici, aus der Übertragung von Privatsammlungen an den Staat (1683 Oxford, 1734 Rom, 1743 Florenz). Der Individualisierung der Subjekte ist also der Übergang der Wissensschätze aus der privaten in die öffentliche Domäne komplementär. Um 1800 entstanden wissenschaftliche Museen. Die technisch-archä<\h>ologischen in Paris, Kopenhagen und Berlin zeichneten den Weg des Menschen vom Naturzustand zur Kultur nach. Der Vergangenheit der Nation verliehen sie zeitliche Tiefe: »Hervorgegangen aus einem Zusammentreffen von Patriotismus und Wissenschaft, haben sie die Spuren der materiellen Kultur auf einen der Kunst vergleichbaren Rang erhoben« (<@6 Caps>Pomian<@$p>, 1998, S.<\!q>105). Der Nationalismus war eine weitere wichtige Triebkraft für die Öffnung und den Ausbau der Sammlungen. Die Einheit von Volk, Territorium, Sprache und Geschichte im neuen Nationalstaat sollten die Bürger auch beim Gang durch das Kulturerbe in den Museen und in seinem Studium in den Bibliotheken erfahren. Auch Revolutionen können der öffentlichen Kultur zuträglich sein.<@3 hoch fliess>15<@$p> Die Idee, den Louvre mit seinen, seit Mitte des 16.<\!q>Jahrhunderts von den Monarchen zusammengetragenen Sammlungen zu einem öffentlichen Museum zu machen, kam schon früher im 18.<\!q>Jahrhundert auf, doch erst die Revolutionsregierung verwirklichte sie, als sie 1793 den Bürgern das <@1 fliess kursiv>Musée Central des Arts<@$p> in der <@1 fliess kursiv>Grande Galerie<@$p> öffnete. Im 19.<\!q>Jahrhundert setzte sich die Vorstellung durch, dass es eine legitime Investition von öffentlichen Mitteln darstellt, möglichst vielen Menschen Zugang zu Büchern geben. Nicht ohne Widerstand wurden Gesetze erlassen, die es lokalen Behörden ermöglichten, öffentliche Bibliotheken einzurichten. Die <@1 fliess kursiv>Boston Public Library<@$p>, die erste der großen öffentlichen Stadtbüchereien der USA und die erste, die durch direkte <\n>öffentliche Steuern finanziert wurde, hatte von Beginn an den Doppel<\h>charakter einer Bibliothek für wissenschaftliche Forschungszwecke und für ein allgemeines Leseinteresse. In England wurden 1850 die ersten öffentlich finanzierten Bibliotheken eröffnet. Die Bedeutung des niedrigschwelligen Zugangs zu geschriebenen Kulturgütern drückte sich in den meisten Ländern in einer Gesetzgebung aus, die sicherstellen sollte, dass allen ohne Gebühren gute Bibliotheken zur Verfügung standen <@1 fliess normal>(vgl. Encyclopædia Britannica, 2000). <@$p>Auch die Museen wurden ab dem 19.<\!q>Jahrhundert als öffentliche Orte der Bewahrung und Zurschaustellung nationaler Kulturgüter, der Forschung und der Bildung betrachtet, die es wert sind, mit öffentlichen Mitteln unterhalten zu werden. Im »Sondermilieu« der Universität reicht der öffentliche Charakter weiter zurück als bei Sammlungen. Seit den Athenern gehört es zu ihrem Wesen, dass das von ihr erzeugte und durch sie weitergegebene Wissen, anders als in geschlossenen und gar geheimen Forschungsstellen der Staaten oder der Industrien üblich, ungehindert zirkulieren können muss. Im Mittelalter wurde jemand, der Erfindungsgabe hatte, von irgendeinem Souverän eingesperrt, damit er um den Preis seines Kopfes Gold für ihn mache. Dagegen bildete sich die historische Errungenschaft der Freiheit von Forschung und Lehre heraus, auf der die Kooperation und der Wettbewerb um das beste Wissen für alle beruht. Die im Hochmittelalter entstandenen europäischen Universitäten bildeten einen Medienverbund aus Verarbeitung des gesprochenen Wortes zu handschriftlichen Büchern, Speicherung in Bibliotheken und Übertragung von Texten in einem eigenen Universitätspostsystem. In der frühen Neuzeit übernahmen dank Gutenbergs Erfindung Verlage die Produktion von Büchern, die entstehenden Territorialstaaten und später Nationalstaaten beanspruchten das Postmonopol. An den Universitäten entwarf die Gelehrtenrepublik des 19.<\!q>Jahrhunderts eine akademische Wissenschaftsverfassung, deren zentrales Element die Informationsfreiheit ist. Wissen, das an der Universität entsteht, ruht auf den Schultern vorangegangener Generationen und gehört folglich nicht dem Einzelnen, der einen kleinen Baustein einfügt, sondern der Wissenschaftlergemeinschaft insgesamt. Aus erkenntnisphilosophischen und methodologischen Gründen müssen Forschungsergebnisse veröffentlicht werden, <\n>damit die Gemeinschaft der Fachleute sie überprüfen, replizieren, kritisieren und fortschreiben kann. Das ist es, was Robert Merton mit dem »Wissenskommunismus« der Wissenschaften meinte.<@3 hoch fliess>16<@$p> Auch hier bildet sich somit eine Form von Allmendgemeinschaft, die sich von den Konzepten und Institutionen Eigentum, Interessen, Praxis und Staat abkoppelt, und den Wissensfluss »nach innen« offen hält (<@6 Caps>Spinner<@$p>, 1994, S.<\!q>91). Wissen als Gemeingut der Forschungsgemeinschaft kann von <\n>Kollegen frei nachvollzogen, überprüft und weiterentwickelt werden und in der Lehre frei der Reproduktion der Wissensträger in der nächsten Generation dienen. Durch diese fruchtbaren Bedingungen im »Sondermilieu« der Wissenschaften können die parallelen, kollektiven Bemühungen Ergebnisse hervorbringen, die kein Einzelner und kein einzelnes Team produzieren könnten. Im Übergang von Wissen als Privileg Weniger zu etwas, worauf alle im Volk Anspruch haben, zeigt sich eine fundamentale Wertsetzung. Am Ursprung und in der Basis der modernen Gesellschaften steht ein ausgeprägtes Gefühl für die moralische Verwerflichkeit von Zugangsbeschränkungen zu Wissen, Information, Bildung, Kultur und Weisheit. Versteht man die Rezeption von herausgehobenen kulturellen Artefakten als ihre »Nutzung«, kann man von öffentlichen Museen und Bibliotheken also durchaus als Allmenden im Sinne von Orten einer Kollektivnutzung sprechen. @2  ZÜ 2:Wissen: von der Allmende zum Eigentum – Immaterialrecht@1 fliess mit:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Es handelt sich um eine Art von Eigentum, die sich gänzlich von anderen Arten des Eigentums unterscheidet.« <@1 fliess normal>(aus dem ersten französischen Urheberrechtsgesetz von 1791)<@$p><*t(25.512,0,"1  ")>@1 fliess ohne:Während die Archive des Überlieferten frei zugänglich wurden, entstand für die Erschaffung neuen Wissens eine vollkommen neue Ökonomie. Die Kategorie des »geistigen Eigentums« tauchte auf. Die Zuweisung, Verwendung und Übertragung materieller Güter ist in den frühesten Rechtssystemen geregelt. Das römische Recht gewährte dem Eigentümer (<@1 fliess kursiv>proprietarius<@$p>) alle Rechte, Nutzungsprivilegien und die Übertragungsgewalt an einer Sache. Mit immateriellen Gütern tat sich das Recht jedoch erheblich schwerer. Das Mittelalter lehnte Abstrakta zumindest in irdischen Angelegenheiten ab. Der Status des Geldes z.B. war noch unklar. Wissen gegen Geld zu tauschen oder gar Geld gegen Geld war tabu. Das Zinsverbot begründete sich in der Vorstellung, dass der Wucherer Gottes Zeit stehle. Universitätsprofessoren wurden dafür angeprangert, dass sie sich für ihre Wissensvermittlung bezahlen ließen, da sie damit die gött<\h>liche Wahrheit verkauften. Bei knappen Kassen verminderten die Souveräne einfach den Edelmetallanteil der Münzen, was zu regelmäßigen Geldkrisen und zu der Frage führte, was es denn mit der bis heute mys<\h>teriösen Natur des Geldes auf sich habe und wem es gehöre. Im 14.<\!q>Jahrhundert lautete die theoretischen Antwort: allen. Doch da es einer für <\n>alle begeben, verwalten und kontrollieren musste, blieb es bei der praktischen Antwort: dem Souverän, der den Münzen sein Antlitz aufprägte (vgl. <@6 Caps>von Oresme<@$p>, 1999). @1 fliess mit:<t1>Noch schwieriger war es um den Stellenwert von Verträgen bestellt. Das römisch-kontinentale Recht (<x@1 fliess kursiv><t1>Civil Law<@$p><t1>) neigte dazu, die Möglichkeit von rechtlichem Eigentum an etwas, das man nicht anfassen kann, abzulehnen. Seit dem Mittelalter setzte sich zwar die Doktrin eines Quasieigentums an Rechten durch, die jedoch mit der Kodifizierung aufgegeben w<t$>urde. Das angloamerikanische Rechtssystem (<@1 fliess kursiv>Common Law<@$p>) war <t1>für die Vorstellung<t$> offener<t1>, dass man Eigentümer eines Rechtes, einer Verfügungsgewalt (z.B. über ein Grundstück) oder eines Privilegs sein kann.<t$> <*h"mehr">In Bezug auf Grund und Boden erkennt das <@1 fliess kursiv>Civil Law<@$p> den Besitzer eines Grundstücks als alleinigen Eigentümer an, während das <@1 fliess kursiv>Common Law<@$p> eine Vielzahl von Eigentumsrechten an Land kennt und daher nicht den Eigentümer eines Grundstücks, sondern den Eigentümer von Nutzungsanteilen an dem Grundstück anerkennt – eine immaterielle, gesetzliche Abstraktion eines materiellen Gegenstandes. Umgekehrt kam im <@1 fliess kursiv>Common Law<@$p> die Verschiebung von persönlichen Verträgen (z.B. Schuldscheinen) hin zu einem übertragbaren und somit handelbaren Gut, d.h. ihre Abstraktion zu einer Form von Eigentum, erst spät auf. Das galt auch für Bankkonten, Anleihen und Bürgschaften, die nur eine standardisierte Form von Vertragsrechten darstellen. Erst im 19. Jahrhundert wurden im <@1 fliess kursiv>Common Law<@$p> Schuldscheine frei übertragbar und unabhängig von weitergehenden Vereinbarungen mit dem ursprünglichen Kreditgeber einforderbar. In Kontinentaleuropa waren begebbare Wertpapiere zwar anerkannt, allerdings in einer rechtlichen (separates Handelsrecht, eigene Gerichtsbarkeit) und sozialen (nur unter Händlern) Nische. In Frankreich setzte die Bewegung zur Anerkennung von Eigentumsanteilen an Immaterialgütern früh ein, während in Deutschland erst in der Mitte des 20. Jahrhunderts Mechanismen entwickelt wurden, mit denen Immaterialrechte auf ähnliche Weise wie materielle Eigentumsrechte übertragen und geschützt werden konnten, obgleich sie noch immer nicht als Eigentum bezeichnet wurden. <*h"Standard"><t1>Mit dem Entstehen des gutenbergianischen Druckmarktes kam eine weitere Klasse von immateriellem Eigentum hinzu: staatlich verliehene Rechte. Die wichtigsten waren das Urheber-, das Patent- und das Marken<\h>schutzrecht. Das exklusive Recht, ein literarisches, wissenschaft<\h>liches, musikalisches, dramatisches oder anderes künstlerisches Werk zu veröffentlichen, zu reproduzieren und zu verkaufen sowie das Recht zur exklusiven Verwertung einer Erfindung nehmen beide ihren Ausgang im Italien des 15.<\!q>und im England des 16.<\!q>Jahrhunderts. Sie wurden zunächst als gildische Privilegien von der Krone verliehen. Zu den Metaphern, die diese Frühzeit für den Autor fand, gehörten der »Hirte«, ein »Gefäß für die göttliche Inspiration«, ein »Magier« oder ein »Geistesmonarch«. Eine weit verbreitete Vorstellung über die Beziehung von Autor und Werk war die der »Vaterschaft«. Schließlich setzte sich jedoch in Analogie zur privatisierten Allmende eine Vorstellung von literarischen Werken als einem Grundbesitz durch, der vom Autor bestellt werden konnte (vgl. <x@6 Caps><t1>Halbert<@$p><t1>, 1998, Kapitel 2). Mit zunehmender Privatisierung der gildischen Genossenanteile wurde daraus der kapitalistische Medienmarkt, wobei sich mit Verwertungsgesellschaften, Verlegerverbänden<t$> und Mediengewerkschaften korporative Strukturen bis heute erhalten haben. <\c>@2  ZÜ 3:Patente@1 fliess ohne:Das erste überlieferte Patent für eine industrielle Erfindung wurde 1421 an den Architekten und Ingenieur Filippo Brunelleschi vergeben. Es gab ihm für drei Jahre das Monopol, einen Lastkahn mit einer Winde herzustellen, der für den Transport von Marmorblöcken verwendet wurde. Im Laufe der folgenden zwei Jahrhunderte breiteten sich solche Privilegien für Erfinder von Italien über Europa aus. Häufig wurden sie im Zusammenhang mit der Einfuhr und Errichtung neuer Industrien vergeben. Bald wurde deutlich, dass zeitlich unbegrenzte Patente zu Monopolbildungen führten. Daher erließ das britische Parlament 1623 das <@1 fliess kursiv>Statute of Monopolies<@$p>. Es untersagte die meisten königlichen Monopole, nahm aber das Recht davon aus, <@1 fliess kursiv>Letters of Patent<@$p> für Erfindungen neuer Produkte für die Dauer von bis zu 14 Jahren zu verleihen. Die US-Verfassung von 1790 autorisierte in Artikel 1.8 den Kongress, ein Patentsystem zu errichten, das Erfindern für eine begrenzte Zeit das exklusive Recht an ihren Erfindungen gibt. Ein Jahr darauf verabschiedete Frankreich ein Patentgesetz. @1 fliess mit:Die Bedeutung »Schutzrecht für eine Erfindung« erlangte das Wort erst im 18. Jahrhundert. Vorher meinte »Patent« einen »landesherrlichen Befehl, amtliche Mitteilung, Freibrief, Berechtigungsschreiben, Bestallungsurkunde für Beamte und Offiziere«. Das mittellateinische <@1 fliess kursiv>litterae patentes<@$p> stand für »behördliche (landesherrliche, bischöfliche) gesiegelte, aber nicht verschlossene Briefe«. In ähnlichem Sinne war im 18. Jahrhun<\h>dert die Bedeutung »die in einem offen vorzuzeigenden amtlichen Brief bestätigte Anerkennung der Qualität einer Ware«. Patentartikel galten <\n>damals als besonders gut, da sie privilegiert, bevorrechtet, für die Nachahmung oder den Handel durch andere nicht freigegeben waren. Neben diesem gewerblichen Eigentum etablierte sich auch das Konzept des geistigen Eigentums (im Englischen: <@1 fliess kursiv>industrial<@$p> und <@1 fliess kursiv>intellectual property<@$p>). Diese Immaterialrechte sind frei übertragbar (nach dem kontinentaleuropäischen System mit Ausnahme der urheberpersönlichkeitsrechtlichen Anteile), können beliehen und vererbt, durch gerichtliche Verfügung geltend gemacht werden und sind gegen staatliche Eingriffe geschützt. Sie erfüllen somit die Bedingungen von Eigentumsrechten. Sie werden im deutschen Urheberrecht als »eigentumsähnliches« Recht behandelt. Die aktuellen <@4 Pfeil (Umschalt/Alt #)>’<@$p>WIPO-Verträge und EU-Richtlinien bezeichnen sie als »Eigentum«.<*h"mehr">Das Recht am geistigen Eigentum hat sich in zwei eigenständigen Traditionen entwickelt, dem angloamerikanischen <@1 fliess kursiv>Copyright<@$p> und dem kontinentaleuropäischen <@1 fliess kursiv>Droit d’auteur<@$p>. Der Kern der Unterscheidung liegt der Rechtsliteratur zufolge darin, dass das Copyright auf utilitären Prinzipien bei der Verfolgung eines öffentlichen Interesses beruht, während das Autorenrecht sich aus einem naturrechtlichen Verständnis der Persönlichkeitsrechte des Urhebers entwickelt. Mit der internationalen Angleichung in solchen Foren wie der <@1 fliess kursiv>World Intellectual Property Organization<@$p> (WIPO) und der Welthandelsorganisation (WTO) nehmen die Differenzen zwischen den beiden Traditionen ab. Dennoch bleiben die rechts<\h>philosophischen Hintergründe bedeutsam, wenn es darum geht, dass der Gesetzgeber oder die Gerichte einen Interessenausgleich zwischen den Rechtsinhabern und der Öffentlichkeit erzielt (vgl. <@6 Caps>Guibault<@$p>, 1997, S. 10).<*h"Standard">@2  ZÜ 3:Die angloamerikanische Copyright-Tradition@1 fliess ohne:Das britische Copyright nahm seinen Ausgang in einem von der Krone an die Drucker-Verleger (<@1 fliess kursiv>Stationers<@$p>) verliehenen Monopol (<@1 fliess kursiv>Patent<@$p>), wie es im späten 15. Jahrhundert bereits in Venedig in Kraft war. Auslöser war die 1476 nach England eingeführte Druckerpresse. Die Krone verlieh der Buchhändlergilde, der <@1 fliess kursiv>London Stationers‘ Company<@$p>, gegen eine Gebühr die <@1 fliess kursiv>litterae patentes<@$p> für das Drucken von Büchern. Die Mitglieder der Gilde, also fast alle Drucker in England, registrierten den Titel eines Manuskripts (<@1 fliess kursiv>Copy<@$p>) und respektierten dann das Recht des Eintragenden, dieses Werk exklusiv zu verwerten. <@1 fliess kursiv>Copyright<@$p> entstand somit als ein Kopierrecht der Verlage, nicht als ein Eigentumsrecht der Autoren. Noch handelte es sich um ein privatrechtliches Übereinkommen, das nur gegenüber anderen Gildenmitgliedern durchgesetzt werden konnte. Daher strebten die Druckerverleger eine öffentliche Bestätigung und Kodifizierung ihrer <\n>privatrechtlichen Regelung an (vgl. <@6 Caps>Loren<@$p>, 2000). @1 fliess mit:Diese erfolgte 1557 mit der königlichen <@1 fliess kursiv>Charter Mary<@$p>, die das Monopolrecht der <@1 fliess kursiv>Stationers‘ Company<@$p> bekräftigte und ihr zusätzlich das Recht verlieh, »ungesetzliche« Bücher aufzuspüren und zu vernichten. Die Gilde war nun auch in der Lage, gegen Nichtmitglieder vorzugehen, die registrierte Bücher nachdruckten. Zugleich wurde das Verlagswesen der Aufsicht des <@1 fliess kursiv>Star Chamber<@$p>-Gerichts unterstellt. Ziel des Staates war es nicht, die Verleger oder gar die Autoren zu schützen, sondern einerseits Einnahmen durch die Gebühren für das Gildenprivileg zu erzielen und andererseits eine Zensur über die verlegten Inhalte durchzusetzen.<t-2>Im Rahmen eines neu definierten Staatszieles, der »<x@1 fliess normal><t-2>Ermunterung zum <t$>Lernen«<@$p>, machte der <@1 fliess kursiv>Act of Anne<@$p> von 1710 das Copyright zu einem Leis<\h>tungsanreiz für Autoren und Verleger gleichermaßen. Danach konnten erstmals auch die Urheber das Copyright an ihren Werken erwerben. Die Versuche der <@1 fliess kursiv>Stationers<@$p>, ihre Monopolstellung durch ein naturrechtlich begründetes ewiges <@1 fliess kursiv>Common Law<@$p> Copyright des Urhebers sanktionieren zu lassen, wurden zurückgewiesen. Ebenso wie im Patent-, und Gebrauchsmuster- oder Markenrecht war die Eintragung in das jeweilige <\n>Register (die »Rolle«) konstitutive Voraussetzung für das Entstehen des Schutzes. Erst seit 1978 genießt in den USA ein Werk mit seiner Fixierung in einer greifbaren Form ohne weitere Bedingungen den Schutz des Copyrights.Der <@1 fliess kursiv>Act of Anne<@$p> etablierte die Vorstellung einer begrenzten Schutzfrist, die auf 28 Jahre festgelegt wurde. Damit wurde zugleich zum ersten Mal das Konzept einer geistigen <@1 fliess kursiv>Public Domain<@$p> formuliert, eines Bereichs des gemeinfreien Wissens, in dem Werke nach Ablauf der Schutzfrist eingehen und gegenüber der nur neu geschaffene Werke einen Schutz <t-1>erhalten können. Der Schutz umfasste nur s<t-2>pezifische Nutzungen (Druck, <t-1>Veröffentlichung und Verkauf), nicht aber <t$>z.B. den Weiterverkauf eines Werkes nach seinem Erstverkauf.<@3 hoch fliess>17<\!q><@$p><t-2>Der Positivismus des Präzedenzfallsys<\h>tems schuf in der Folgezeit einen Flickenteppich von Gesetzen, die nicht von einer Gesamtschau des Copyrights geleitet wurden. Noch heute wird das Copyright für alle öffentlichen Publikationen der Krone und des Parlaments von <x@1 fliess kursiv><t-2>Her Majesty’s Stationery Office<@$p><t-2> (HMSO) gehalten. Ihm steht der <x@1 fliess kursiv><t-2>Controller<@$p><t-2> vor, der seine Autorität vom <x@1 fliess kursiv><t-2>Letters Patent<@$p><t-2> ableitet, die auf jeden nachfolgenden <x@1 fliess kursiv><t-2>Controller<@$p><t-2> übertragen werden. Unabhängig vom HMSO werden die Copyrights für alle anderen Publikationen vom privatisierten <x@1 fliess kursiv><t-2>Stationery Office Limited ve<@$p><t-2>rwaltet.<x@3 hoch fliess><t-2>18<@$p>Die US-amerikanische Rechtsauffassung bildete sich am britischen Modell. Grundlage für Copyright- wie für das Patentrecht ist die US-Verfassung von 1790, die in Artikel 1, Sektion 8, Klausel 8 das Parlament ermächtigt, Autoren und Erfindern ein ausschließliches Recht an ihren Schriften und Erfindungen zu sichern, »um den Fortschritt der Wissenschaft und der nützlichen Künste zu fördern«: @1 fliesskursiv Zitat:<*t(25.512,0,"1  ")><*t(25.512,0,"1  ")>»Die Gestalter der Verfassung der Vereinigten Staaten, denen alle Monopole von vornherein verdächtig waren, kannten das Copyright aus seiner Geschichte als ein Werkzeug der Zensur und der Pressekontrolle. Sie wollten sicherstellen, dass das Copyright in den Vereinigten Staaten nicht als Mittel der Unterdrückung und Zensur eingesetzt würde. Daher banden sie das Copyright ausdrücklich an einen Zweck: den Fortschritt von Wissen und Bildung zu fördern. [...] Das Monopol des Copyright wurde zugelassen, aber nur als Mittel zu einem Zweck. Die Verfassung geht mit demselben Glauben an eine verrückte Idee an die Schaffung von Autorenwerken heran, wie Kevin Costner in dem Film ›<@1 fliess normal>Field of Dreams‹:<@$p> die Vorstellung, ›wenn man sie schützt, werden sie kommen.‹ Indem es dieses vermarktbare Recht an der Nutzung des eigenen Werkes etabliert, liefert das Copyright den ökonomischen Anreiz, Ideen zu schaffen und zu verbreiten. Der Oberste Gerichtshof erkannte: ›Die unmittelbare Wirkung unseres Copyright-Gesetzes ist es, dem ›Autoren‹ ein faires Einkommen für seine kreative Arbeit zu sichern. Doch das eigentliche Ziel ist es, durch diesen Anreiz die künstlerische Kreativität zum Wohle der breiten Öffentlichkeit anzuspornen‹« <@1 fliess normal><*t(25.512,0,"1  ")>(<@6 Caps>Loren<@1 fliess normal>, 2000).@1 fliess mit:@1 fliess ohne:Noch im selben Jahr setzte der US-Kongress den Verfassungsauftrag im ersten <@1 fliess kursiv>Copyright Act<@$p> um. Auch wenn das eigentliche Ziel ein anderes sein mag, steht beim Copyright das Werk als eine Handelsware im Vordergrund. Es gewährt seinem Besitzer ein Monopol auf sein Werk, das ihm die Kontrolle über dessen Nutzung und den pekuniären Vorteil daraus <\n>sichert. Es schützt die Investoren, d.h. Urheber und Verleger, gegen Angriffe auf ihre Investitionen geistiger und materieller Art. In der damaligen Debatte stand einem Naturrechtsverständnis der exklusiven Rechte <t-2>des Autors – in den Worten von Thomas Jefferson – die besondere »Natur«<t$> der Information entgegen:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")><*t(25.512,0,"1  ")>»Wenn die Natur es so eingerichtet hat, dass ein Ding sich weniger für ausschließliches Eigentum eignet als alle anderen Dinge, so ist es die Handlung des Denkvermögens, die wir Idee nennen. Ein Einzelner mag sie allein besitzen, solange er sie für sich behält, doch sobald sie preisgegeben wird, drängt sie sich in den Besitz eines jeden und der Empfänger kann sich ihrer nicht wieder entledigen. Zu ihrem eigentümlichen Charakter gehört es ferner, dass niemand weniger besitzt, weil all anderen die Idee ebenfalls besitzen. Wer eine Idee von mir bekommt, erhält Unterweisungen ohne die meinigen zu mindern; so wie derjenige, der seine Fackel an meiner entzündet, Licht erhält, ohne mich zu verdunkeln. Dass Ideen sich für die geistig-sittliche und gegenseitige Unterweisung der Menschen und die Verbesserung ihrer Lage frei vom einen zum andern über den ganzen Erdball verbreiten sollten, scheint die Natur auf eigentümliche und gütige Weise eingerichtet zu haben, als sie sie wie das Feuer machte, ausdehnbar über den ganzen Raum, ohne dass sie an irgend einem Punkt an Dichte verlören, und wie die Luft, die wir atmen, in der uns bewegen und unser physisches Dasein fristen, ungeeignet sie einzusperren oder in unseren alleinigen Besitz zu nehmen. Erfindungen können daher ihrer Natur nach nicht dem Eigentum unterstellt werden.«<@3 hoch fliess>19<@$p>@1 fliess mit:@1 fliess ohne:Das Copyright-Gesetz von 1790 umfasste das Recht, geschützte Werke zu drucken, zu verlegen und zu verkaufen, nicht aber sie aufzuführen oder Adaptionen davon herzustellen. Es legte die Schutzfrist auf 14 Jahre fest, mit der Möglichkeit, sie einmalig um weitere 14 Jahre zu verlängern. Bedingung für einen Schutz ist nicht ein Begriff der »Schöpfungshöhe« oder »Originalität«, wie er im kontinentaleuropäischen Urheberrecht im Vordergrund steht, sondern ein Mindestmaß an »Arbeit, Fertigkeit und Urteilsvermögen« (<@1 fliess kursiv>labour, skill and judgement<@$p>). Die materielle und geistige Beziehung des Urhebers zu seinem Werk endet mit der Übertragung an den Verleger. Diese wird nach dem allgemeinen Vertragsrecht geregelt (vgl. <@6 Caps>Ellins<@$p>, 1997). 1802 wurde das US-Copyright-Gesetz auf Bilddrucke und 1831 auch auf musikalischen Kompositionen ausgeweitet. Die so genannten moralischen oder Urheberpersönlichkeitsrechte des <@1 fliess kursiv>Droit d’auteur<@$p>-Modells erkennt das US-amerikanische Recht bis heute nicht an.@1 fliess mit:Der britische <@1 fliess kursiv>Copyright Act<@$p> von 1956 nimmt unter dem Einfluss der Tonträgerhersteller eine Trennung zwischen den Rechten der Autoren an Originalwerken (<@1 fliess kursiv>original works<@$p>) und denen der Hersteller (<@1 fliess kursiv>makers<@$p>) an Gegenständen (<@1 fliess kursiv>subject matters<@$p>) vor. Der <@1 fliess kursiv>Copyright, Designs and Patents Act<@$p> von 1988 (CDPA)<@3 hoch fliess>20<@$p> erfüllt die internationalen Verpflichtungen Großbritanniens im Rahmen der Berner Übereinkunft von 1971 und stellt eine gewisse Annäherung an das kontinentaleuropäische Urheberrechtsmodell dar. Der CDPA schrieb erstmals <@1 fliess kursiv>moral rights<@$p> (Persönlichkeitsrechte) fest, doch ist deren Substanz durch eine Vielzahl von Ausnahmen und Einschränkungen unterminiert. Ein Veröffentlichungsrecht, wie es in vielen <@1 fliess kursiv>Droit d’auteur<@$p>-Rechtsordnungen existiert, gibt es hier nicht. Das Urheberbezeichnungsrecht muss nach dem CDPA ausdrücklich geltend gemacht werden. Der Verzicht auf das Werkintegritätsrecht wird von den Produzenten regelmäßig zur Vertragsbedingung erhoben. Letztlich, schreibt Julia Ellins, bliebe auch die Durchsetzung der <@1 fliess kursiv>moral rights<@$p> dem Reich der Vertragsfreiheit überlassen (vgl. <@6 Caps>Ellins<@$p>, 1997).Die Unterscheidung in <@1 fliess kursiv>original works<@$p> und <@1 fliess kursiv>subject matters<@$p> wird durch den CDPA in einem umfassenden Werkbegriff aufgehoben, und damit auch die Möglichkeit der Unterscheidung in Urheberrechte und verwandte Schutzrechte. Außerdem wird der Begriff des <@1 fliess kursiv>author <@$p>darin auf die Produzenten von Tonträgern, Filmen und Sendungen sowie Verleger <\n>typografisch gestalteter Ausgaben ausgeweitet. Allen Investitionen geis<\h><t-1>tiger, schöpferischer, finanzieller, technischer und organisatorischer<t$> Art, die in das Endprodukt eingehen, wird Schutz gewährt. Der Begriff des »Autors« verbindet sich also nicht mit einer besonderen schöpferischen Tätigkeit, sondern mit einem neutralen Konzept des Schaffens. Das britische Copyright-Vertragsrecht hält keine besonderen Bestimmungen zum Schutz des Urhebers als der in den allermeisten Fällen schwächeren Partei in der Vertragswirklichkeit bereit. Zu den Neuerungen des CDPA gehört der Einschluss der Rechte ausübender Künstler und die Ergänzung der strafrechtlichen Sanktionen gegen die Verletzung von Künstlerrechten durch das Instrument der Zivilklage. Im Kampf gegen das <@1 fliess kursiv>Bootlegging<@$p>, das unautorisierte Aufzeichnen von Konzerten, wurde schließlich das neue Recht »von Personen mit ausschließlichen Aufnahmerechten an künstlerischen Darbietungen« eingeführt. Bei einer Übertragung des Copyright und unter Umständen auch bei einer Lizenzierung sind gegen eine Pauschalgebühr sämtliche Rechte betroffen. Der Produzent kann somit auch bei Altverträgen eine digitale Verwertung vornehmen, ohne eine Einwilligung des Urhebers einzuholen oder ihn dafür zu entschädigen (vgl. <@6 Caps>Ellins<@$p>, 1997).<*h"mehr">In den USA wurde die große Revision des Copyright-Gesetzes von 1976 mit zwei Hauptzielen vorgenommen.<@3 hoch fliess>21<@$p> Zum einen sollte das Recht den medientechnologischen Entwicklungen und ihren Auswirkungen auf die Kopierbarkeit von Werken angepasst werden. Zum anderen diente es der Vorbereitung auf den Beitritt der USA zum Berner Urheberrechtsabkommen. Schützte das Gesetz zuvor nur Werke, die die Autorin oder der Verleger förmlich registriert und in der vorgeschriebenen Form (»Copyright © [Jahr der Erstveröffentlichung] [Rechteinhaber]«) gekennzeichnet hatte, so gilt der Schutz seither automatisch für jedes Werk von dem Moment an, in dem es in einem materiellen Ausdrucksmedium (»<@1 fliess kursiv>in a tangible medium of expression<@$p>«) fixiert ist, und damit auch für nicht veröffentlichte Werke. Die Werkarten wurden spezifiziert. Dazu gehören <\n>literarische, musikalische und dramatische Werke, Pantomimen und choreografische Werke, bildliche, grafische und skulpturale Werke, Kino<\h>filme und andere audiovisuelle Werke sowie Klangaufnahmen. Auch die Herstellung abgeleiteter Werke und die öffentliche Aufführung und Zurschaustellung des Werkes sind seither geschützt. Auch die Beschränkungen des Copyright-Rechts veränderten sich. Erstmals werden die Rechtsdoktrinen des <@1 fliess kursiv>Fair Use<@$p> und des Erstverkaufes kodifiziert (s. u. »Ba<\h>lance«). <*h"Standard">Eine weitere zentrale Veränderung betrifft die Schutzdauer. Sie wurde auf die Lebenszeit des Autors plus 50 Jahre festgelegt. Für anonyme, pseudonyme und Auftragswerke betrug sie 75 Jahre nach der Erstveröffentlichung und bei nicht oder erst später veröffentlichten Werken 100 Jahre nach der Schöpfung. Copyrights, die vor 1976 angemeldet wurden und 28 Jahre geschützt waren, konnten um eine weitere Laufzeit von 47 Jahren bis zu einer Gesamtdauer von 75 Jahren nach Erstanmeldung geschützt werden. Hinter dieser Verlängerung steht einer der mächtigsten Akteure der Rechteindustrie, die Walt Disney Corp., und ihr wertvollstes geistiges Eigentum: Das Copyright für <@1 fliess kursiv>Mickey Mouse<@$p> wurde ursprünglich 1928 – für die Verwendung der Figur in dem Stummfilm »Plane Crazy«– angemeldet. Nach einer Schutzdauer von zweimal 28 Jahren drohte die weltberühmte Maus somit 1984 gemeinfrei zu werden. Die Gesetzesänderung verlängerte den Schutz bis 2004.<@3 hoch fliess>22<@$p> <*h"mehr">Als dieses Verfallsdatum näher rückte und die berühmteste Maus der Weltgeschichte in die <@1 fliess kursiv>Public Domain<@$p> zu fallen und damit Disneys Einnahmenstrom zu versiegen drohte, griff der Gesetzgeber erneut ein. Im <t-2>Oktober 1998 verabschiedete der amerikanische Kongress den <x@1 fliess kursiv><t-2>Sonny Bono <t$>Copyright Term Extension Act<@$p>, der die Schutzdauer um weitere 20 Jahre ausdehnte. Ein wichtiges öffentlich vorgebrachtes Argument war die Angleichung an die europäische Schutzdauer von 70 Jahren nach Tod des Autors. Geistiges Eigentum, hieß es, sei das ökonomisch bedeutendste Exportgut der USA. Künstler beklagten sich, dass ihr Copyright-Schutz zu Hause früher ende als in Europa. Während viele Werke aus den 20er-Jahren nur noch für spezialisierte Kreise von Interesse sind, gibt es einige, die weiterhin einen Marktwert besitzen. Die Rechte an George Gersh<\h>wins »Rhapsody In Blue« beispielsweise, 1924 zum Copyright angemeldet und nach der alten Schutzfrist ab dem 1.<\!q>Januar 2000 in der <@1 fliess kursiv>Public Domain<@$p>, wurden von United Airlines als Unternehmensmusik für den geschätzten Betrag von 500<\!q>000 Dollar angekauft. Mark Stefik, Vordenker der <\n>technischen Rechtekontrollsysteme am Xerox PARC, leistet Disney Schützenhilfe, indem er für das Copyright den zeitlichen Denkrahmen der amerikanischen Eingeborenen anführt, die für sieben Generationen planten, von den Großeltern bis zu den Großenkeln: »Diese zeitliche Perspektive aufzugreifen, könnte uns helfen, eine Institution zu schaffen, die einen dauerhaften Wert für die Menschheit darstellt« (<@6 Caps>Stefik<@$p>, 1996, S.<\!q>34).<*h"Standard">Doch es gab auch Kritik. Eric Eldred, der einen Online-Selbstverlag für gemeinfreie Bücher betreibt, gestartet als Leseanreiz für seine Töchter, erhielt wachsende Anerkennung und Auszeichnungen u.a. von der <@1 fliess kursiv>Nathaniel Hawthorne Society<@$p>. Eldred klagte wegen Verfassungswidrigkeit gegen das Gesetz. Er und andere Kritiker sahen in der Verlängerung einen Widerspruch zur Intention der US-Verfassung, die nicht die maximale Verwertung durch die Rechteinhaber, sondern in erster Linie das <\n>öffentliche Interesse an Zugang zu und Teilhabe an Wissensgütern schützt. Sie befürchteten innerhalb der nächsten 20 Jahre eine weitere Verlängerung der Schutzfrist, so dass diese sukzessive unendlich verlängert werde. Vor allem aber erbosten sie sich über die Art, wie das Gesetz verabschiedet wurde. Der <@1 fliess kursiv>Starr<@$p> <@1 fliess kursiv>Report<@$p> über die außerehelichen sexuellen Beziehungen des damaligen Präsidenten Bill Clinton war gerade erschienen. Presse und Öffentlichkeit waren von dem Skandal absorbiert. Mitten im Wahlkampf und kurz vor Ende der Legislaturperiode kam es zur mündlichen Abstimmung. Während die Öffentlichkeit nichts davon bemerkte, machte die Medienindustrie den Abgeordneten ihr Interesse an dieser Verlängerung deutlich. Mehr als 6,5 Millionen Dollar ließ sie in den laufenden Wahlkampf fließen. Disney war der größte Spender. Das Hauptargument von Harvard-Rechtsprofessor Lawrence Lessig,<@3 hoch fliess>23<@$p> dem <@1 fliess kursiv>pro bono<@$p> Rechtsvertreter von Eldred in der Klage gegen das »Mickey-Mouse-Gesetz«, lautet, dass die Privilegien, die die US-Verfassung den Autoren gibt, an die Intention gebunden seien, ökonomische Anreize dafür zu schaffen, dass neue Werke entstehen: »Da man einem Toten keinen Anreiz mehr geben kann – das neue Gesetz verlängerte die Schutzfrist rückwirkend auf Werke von bereits verstorbenen Autoren – besteht es den Test nicht. Eine rückwirkende Erweiterung dieser Privilegien dient keinem der Ziele, für welche die Copyright-Klausel geschaffen wurde. Solange Menschen die Bedeutung der <@1 fliess kursiv>Public Domain<@$p> nicht erkennen, befürchten wir, dass der Kongress fortfahren wird sie einzuzäunen« (nach <@6 Caps>Fonda<@$p>, 1999).Andere Rechtsgelehrte, wie Lydia Pallas Loren, lassen ebenfalls keinen Zweifel daran, dass die Ausweitung der Schutzfrist nicht den Autoren, sondern der Rechteindustrie nützt:  »Die Erfahrung hat gezeigt, dass der Zuwachs an Monopolrechten von Copyright-Inhabern nicht zu einem Zuwachs an neuen Werken führt. Es scheint unwahrscheinlich, dass eine kreative Person, wenn sie überlegt, wie sie ihre begrenzte Zeit und Mittel verwendet, <t-2>denkt: ›Ah, jetzt, da mein Copyright bis 70 Jahre nach meinem Tod gilt, investiere ich noch mehr in die Schaffung neuer <t$>Werke.‹ Die größere Reichweite dieser Monopole begünstigt stattdessen große Unternehmen, die eine beträchtliche Anzahl an Urheberrechten besitzen. Sie sind es, die den Großteil der Urheberrechte nach Entstehung der Werke von den ursprünglichen Autoren übertragen bekommen« (<@6 Caps>Loren<@$p>, 2000). Das Verfahren Eldred versus Reno ist in der ersten Instanz abschlägig entschieden worden. Auch das Berufungsgericht wies im Februar 2001 die Anfechtung der Copyright-Verlängerung zurück.<@3 hoch fliess>24<@$p> @2  ZÜ 3:Die kontinentaleuropäische Droit d’auteur-Tradition@1 fliess ohne:Ganz ähnlich wie in England regelte die französische Krone die Verlagsindustrie durch die Gewährung von Veröffentlichungsmonopolen, die zugleich Zensurzwecken dienten. 1789 beseitigte die Revolutionsregierung alle königlichen Privilegien, darunter auch die den Autoren und Verlegern verliehenen. Seither werden die Urheberrechte als ein dem Autor natürlich zufallendes Recht und als gerechte Belohnung für seine Arbeit und Kreativität erachtet. Diese Auffassung spiegelt sich auch in der Menschenrechtserklärung von 1793 wieder. Gegen eine naturrechtliche Auffassung einer Eigentümerschaft des Autors an seinem Werk argumentierten Autoren wie Condorcet für eine freie Zirkulation und Manipulation von Informationen und damit von Wissen als einem Gemeinschaftsbesitz (vgl. <@6 Caps>Halbert<@$p>, 1998, Kapitel 2). Tatsächlich enthalten die 1791 und 1793 erlassenen Gesetze über die Autorenrechte, die eine <@1 fliess kursiv>propriété littéraire et artistique<@$p>, einen »literarischen und künstlerischen Be<t-3>sitz« anerkannten, auch instrumentelle Züge, die die ökonomische Ef<\h>fi-<\n>zi<\h>enz und das öffentliche Interesse berücksichtigen (<x@6 Caps><t-3>Guibault<@$p><t-3>, 1997, S.<\!q>12). <t$>@1 fliess mit:Auch in Deutschland setzte das Urheberrecht mit obrigkeitlich an Drucker gewährten Privilegien ein, die jedoch durch die kleinstaatliche Struktur einer regen »Raubdruckerei« jenseits der Territorialgrenzen keinen Einhalt gebieten konnten. Die Alphabetisierung immer größerer Teile der Bevölkerung spitzte die ökonomische Problematik zu. Parallel dazu wandelte sich der Begriff des »Autors« grundlegend.<@3 hoch fliess>25<@$p> Juristen und Philosophen von Hegel und Kant (1987) bis Fichte bemühten sich um eine Gesamtschau des Problems und fanden ihre Haupt<\h>argumente in dem auf John Locke zurückgehenden Konzept des Naturrechts. Gegen Ende des 18.<\!q>Jahrhunderts setzte sich die Idee eines geistigen Eigentums des Urhebers am Ausdruck seiner Ideen durch, zunächst als Hilfskonstruktion zur Begründung der Verlegerrechte an den von <t-1>ihnen hergestellten physikalischen Vertriebsstücken, den Büchern, bald jedoch mit der Person des Urhebers im Zentrum. In der Debatte um die dogmatische Erfassung von Wesen und Inhalt des Urheberrechts, an der sich zahlreiche Intellektuelle beteiligten, setzte sich ein Modell durch, das den Schöpfungsakt des Urhebers ins Zentrum stellte. Da das Problem des grenzüberschreitenden Büchernachdrucks noch immer nicht behoben war, drängte das Verlagskapital zu Beginn des 19.<\!q>Jahrhunderts auf eine gesamtdeutsche Regelung. Preußen, dessen Verleger am meisten unter den »Raubdrucken« ihrer süddeutschen Kollegen zu leiden hatten, tat sich dabei besonders hervor. Es schuf 1837 mit dem »Gesetz zum Schutze des Eigenthums an Werken der Wissenschaft und Kunst in Nachdruck und Nachbildung« das ausführlichste und modernste Urheberrechtsgesetz<t$> seiner Zeit. 1870 wurde die erste gesamtdeutsche Regelung erzielt, das »Gesetz betr. das Urheberrecht an Schriftwerken, Abbildungen, musikalischen Kompositionen und dramatischen Werken«. Es stützte sich auf die Tradition des römischen Rechts sowie auf das Naturrecht und zog weder ein öffentliches Interesse noch einen besonderen pekuniären Anreiz für das kreative Schaffen in Betracht. 1901 wurde das »Gesetz betr. das Urheberrecht an Werken der Literatur und der Tonkunst« erlassen, das erst 1965 durch das noch heute geltende »Gesetz über Urheberrecht und verwandte Schutzrechte« (UrhG) abgelöst wurde. Ebenfalls 1901 folgten das Verlagsgesetz sowie 1907 das Kunsturhebergesetz. Das Sozialstaatsprinzip motivierte in den ersten beiden Jahrzehnten des 20. Jahrhunderts einen besonderen Schutz des Urhebers als wirtschaftlich und sozial schwächere Partei gegenüber dem Vertragspartner.Der Hauptunterschied zwischen Copyright- und <@1 fliess normal>Droit d’auteur<@$p>-Tradition liegt in der Stellung des Urhebers zu seinem Werk. In den Copyright-Ländern können (und werden in der Regel auch) die Copyright-Rechte vollständig an einen Verwerter übertragen. In den Ländern des Autorenrechts bleibt auch nach weitestgehendem Verkauf der Nutzungsrechte ein unzertrennliches Band zwischen Urheber und Werk bestehen. Das Urheberrecht unterscheidet die Rechte des Urhebers an seiner geistigen Schöpfung und die abgeleiteten Rechte der Verwerter und Mittler des Werkes, die diese vom Urheber erhalten können. Das Urheberrecht selbst ist nicht übertragbar. Die einzige Ausnahme bildet sein Übergang auf die Erben mit dem Tod des Urhebers (§ 28 u. 29 UrhG). Dritten werden allein die Nutzungsrechte eingeräumt. Die Regulierungseinheit ist in beiden Fällen das »Werk«. Das deutsche Urhebergesetz definiert es als eine »persönliche geistige Schöpfung« (§ 2 Abs. 2 UrhG). Da Schöpfung individuellen Geist voraussetzt, schützt es kein korporativ produziertes Wissen, sondern ausschließlich das von einzelnen oder mehreren (§ 8 UrhG) Individuen. Ein Werk muss einen geistigen Gehalt aufweisen. Für Musikwerke z.B. gilt, dass die »Handschrift« des Arrangeurs, sein Stil und sein Temperament spürbar sein müssen.<@3 hoch fliess>26<@$p> Die erforderliche Schöpfungs- oder Gestaltungshöhe findet ihre Grenze im »handwerklichen Durchschnittskönnen« und der <\n>Arbeit eines »Durchschnittsgestalters«.<@3 hoch fliess>27<@$p> Ein rein handwerkliches, mechanisch-technisches Zusammenfügen des Materials liegt außerhalb der urheberrechtlichen Schutzfähigkeit. Diese Grenze wird ihrerseits eingeschränkt durch das Konstrukt der »kleinen Münze«, demzufolge auch ein geringes Maß an Individualität einen Schutz begründet.Ein Werk muss in beiden Traditionen in einer für andere Menschen sinnlich wahrnehmbaren Form Ausdruck gefunden haben. Ideen sind in jeder, auch in juristischer Hinsicht frei.<@3 hoch fliess>28<\!q><\!q><@$p>Die Lehre von der Dichotomie von schutzfähiger Form und prinzipiell nicht schutzfähigem Inhalt ist komplex. Der informative Gehalt eines Werks wird von seinem konkret <t-1>verkörperten Ausdruck unterschieden. Die Gedanken des Schöpfers müs<\h>sen <t$>sinnlich wahrnehmbar in der Außenwelt in Erscheinung treten, um zum Gegenstand des Rechtsverkehrs werden zu können, doch zugleich ist die Identität des Werkes nicht von einem spezifischen medialen Träger abhängig. »Werk« im Sinne des Urhebergesetzes ist der vom sinnlich wahrnehmbaren Träger ablösbare geistige Gehalt (vgl. <@6 Caps>Ulmer<@$p>, 1980, S. 12). Ob z.B. ein Foto optochemisch erzeugt und dann gescannt oder direkt digital aufgezeichnet wird, berührt den Werkbegriff nicht. Ebensowenig, ob ein Musikstück in Form einer Schellackplatte oder einer MP3-Datei »verkörpert« ist. Heute ist die Form-Inhalt-Dichotomie weithin ersetzt durch die von Eugen Ulmer eingeführte Unterscheidung zwischen individuellen Zügen eines Werkes und dem darin enthaltenen Gemeingut (vgl. ebd., § 19 IV). Das Urheberrecht der Autorin an ihrem Werk ist tatsächlich ein ganzes Bündel von Rechten. Ein Teil davon sind die Nutzungsrechte: das <t-1>Recht auf Vervielfältigung, auf Verbreitung, öffentliche Wiedergabe, Übers<t$>etzung, Sublizenzierung usw. Der Urheber räumt in der Regel einen Teil oder alle diese Rechte zur einfachen oder ausschließlichen Verwertung einem Verlag ein. Sie können räumlich, zeitlich oder inhaltlich beschränkt e<t-1>ingeräumt werden (§ 32 UrhG). Im Verlagsvertrag müssen die eingeräumten Nutzungsarten einzeln aufgeführt werden, sofern sie eigenständig verkehrsfähige Nutzungsarten betreffen, z.B. ein Buch und seine Übersetzung. Andernfalls ist die Einräumung der Nutzungsrechte auf den vereinbarten Nutzungszweck beschränkt (§ 31 Abs. 5 UrhG). Zum Zeitpunkt des Vertragsabschlusses noch nicht bekannte Nutzungsarten können nicht eingeräumt werden. Verpflichtungen hierzu sind unwirksam (§ 31 Abs. 4 UrhG), d.h. der Urheberschutz relativiert auch die Maximen der Vertragsfreiheit. Im Zweifel verbleibt dem Urheber das Recht der Einwilligung (§ 37 UrhG). Als Zeitungsverlage begannen, ihre Bestände auf CD-ROM und in Online-Datenbanken zweitzuverwerten, hätten sie <\n>also für j<t$>eden Jahrgang vor der Einführung dieser eigenständig verkehrsfähigen Nutzungsart mit jedem Autor darin enthaltener Artikel einen neuen Vertrag abschließen müssen. Die Auseinandersetzung der IG-Medien, des DJV und der VG Wort mit den Verlegern über die Einhaltung dieser Verpflichtung dauern derzeit noch an. Nach einem umstrittenen Urteil zur Videozweitauswertung von Kinofilmen gilt diese nicht als eigenständig. Ein weiteres Bündel neben den vermögensrechtlichen sind die so genannten moralischen oder Urheberpersönlichkeitsrechte. Im Berner Urheberrechtsabkommen (s.u.) heißt es dazu: »Unabhängig von den wirtschaftlichen Rechten des Urhebers und selbst nach Übertragung der besagten Rechte, soll der Urheber das Recht haben, die Autorenschaft an seinem Werk zu beanspruchen und Einspruch gegen jegliche Entstellung, Verzerrung oder sonstigen Eingriffe in oder herabsetzende Handlungen in Bezug auf das besagte Werk zu erheben, die sich nachteilig auf seine Ehre oder seinen Ruf auswirken könnten« (Art. 6 bis Ziff. 1).Das deutsche Urhebergesetz umfasst entsprechend das Recht auf <\n>Urheberbezeichnung und Werkintegrität. »Der Urheber hat das Recht auf Anerkennung seiner Urheberschaft am Werk. Er kann bestimmen, ob das Werk mit einer Urheberbezeichnung zu versehen und welche Bezeichnung zu verwenden ist« (§ 13 UrhG). »Der Urheber hat das Recht, eine Entstellung oder eine andere Beeinträchtigung seines Werkes zu verbieten, die geeignet ist, seine berechtigten geistigen oder persönlichen Interessen am Werk zu gefährden« (§ 14 UrhG). Das weit gefasste Werk<\h>integritätsrecht schließt auch die Darstellung des Werkes in einem vom <t-1>Urheber unerwünschten Sachzusammenhang ein. Will z.B. der derzeitige Besitzer das Bild eines antifaschistischen Malers in einer den Nationalsozialismus verherrlichenden Ausstellung zeigen, so kann der Urheber dies verbieten. Ferner kann er ein Nutzungsrecht gegenüber dem Inhaber zurückrufen, wenn sich seine in dem Werk zum Ausdruck kommenden Überzeugungen geändert haben (§ 42 UrhG). Das wichtigste Persönlichkeitsrecht des Urhebers, das allen weiteren zu Grunde liegt, ist es zu bestimmen, ob und wie sein Werk zu veröffentlichen ist (§ 12 Ziff. 1 UrhG).<t$> Bis auf das Veröffentlichungsrecht, das mit der Erstveröffentlichung erlischt, wirken diese Rechte nach dem Verkauf von Nutzungsrechten an seinem Werk weiter. Sie bilden ein untrennbares Band zwischen dem Urheber und seinem Werk und erlöschen erst 70 Jahre nach seinem Tod. Das Copyright-Recht kennt keine solchen Konzepte. Mit der Abtretung des Copyrights an einen Verwerter endet die Kontrolle des Autors über sein Werk.@2  ZÜ 3:Internationale Regelungen@1 fliess ohne:Während des 19.<\!q>Jahrhunderts verabschiedeten die meisten Länder Gesetze zum Schutz der einheimischen künstlerischen und industriellen geistigen Güter, also der nationalen kulturellen Artefakte. Zugleich wurde deutlich, dass der wachsende grenzüberschreitende Verkehr von Immaterialgütern eine internationale Regulierung erforderlich machte. 1852 dehnte Frankreich den Urheberschutz auf die Werke der Autoren aller Nationalitäten aus. Die ersten zwischenstaatlichen Verträge waren das »Pariser Abkommen für den Schutz von Industriellem Eigentum« von 1883,<@3 hoch fliess>29<@$p> mit dem der Schutz von Patenten, Warenzeichen und industriellen Gestaltungen internationalisiert wurden, und das »Berner Abkommen zum Schutz von Werken der Literatur und Kunst« von 1886, das von ursprünglich 14 Staaten unterzeichnet wurde. Kern der Abkommen ist, dass die Unterzeichner den Bürgern der anderen Vertragsländer einen automatischen Schutz ihrer Werke gewähren. Das Berner Abkommen ging aus dem <@1 fliess kursiv>Droit d’auteur<@$p>-Geist hervor. Es setzt keine Anmeldung eines Werkes voraus und erkennt die Urheberpersönlichkeitsrechte an. Dies führte zu einem lang anhaltenden Widerstand der Copyright-Nationen. Erst 1988 traten England und die USA dem Berner Abkommen bei und revidierten ihre Copyright-Gesetze entsprechend, wobei das US-amerikanische Recht noch immer keine moralischen Autorenrechte anerkennt. Die USA, die bis zum Beginn des 20. Jahrhunderts ausländischem geis<\h>tigen Eigentum keinen Rechtsschutz gewährten, entsenden heute Regierungs- und Handelsmissionen in alle Welt (China, Thailand, Indien, <\h>Russland usw.), um dort die Verwertungsrechte der amerikanischen <\n>Software-, Musik- und Filmindustrie zu verteidigen. Das Berner Abkommen wurde 1971 grundlegend revidiert (<@1 fliess kursiv>Revidierte Berner Übereinkunft (<@4 Pfeil (Umschalt/Alt #)>’<@1 fliess kursiv>RBÜ<@$p>))<@3 hoch fliess>30<@$p> und 1979 noch einmal geändert. Teilgebiete des Urheberrechts erfassen das Rom-Abkommen über den Schutz der ausübenden <t-1>Künstler, der Hersteller von Tonträgern und der Sendeunternehmen vom <t$>26.<\!q>Oktober 1961<@3 hoch fliess>31<@$p> und das Genfer Tonträgerabkommen vom 29.<\!q>Oktober<\!q>1971.<@3 hoch fliess>32<@$p> Heute sind die meisten Länder der Erde in bilaterale und internationale Abkommen über geistiges Eigentum eingebunden. @1 fliess mit:Ein zweites umfassendes internationales Abkommen wurde 1952 in Genf verabschiedet und trat 1955 in Kraft: das Welturheberrechtsabkommen (<@1 fliess kursiv>Universal Copyright Convention<@$p> (UCC)), aktuell in der Pariser Fassung vom 24. Juli 1971. Die RBÜ geht der UCC vor, wenn beide Abkommen anwendbar sind (vgl. <@6 Caps>Siepmann<@$p>, 1999, Abs 74-75). Die UCC steht der US-amerikanischen Tradition näher, insofern sie eine Kennzeichnung durch das Coypright-Symbol verlangt. Nach dem Beitritt der USA zum Berner Abkommen hat die UCC an Bedeutung verloren. Für die Durchführung von Verwaltungsaufgaben, die sich aus dem Pariser und dem Berner Abkommen ergaben, wurden Büros eingerichtet. 1893 wurden die beiden zusammengelegt, um das <@1 fliess kursiv>United International Bureaux for the Protection of Intellectual Property<@$p> (BIRPI) in Bern zu etablieren. 1960 zog das BIRPI nach Genf um, um näher an den Vereinten Nationen und anderen dort ansässigen internationalen Organisationen zu sein. Ein Jahrzehnt später wurde, als Folge des Abkommens über die Errichtung der <@1 fliess kursiv>World Intellectual Property Organization<@$p> (<@4 Pfeil (Umschalt/Alt #)>’<@$p>WIPO), das BIRPI in die <\n>WIPO umgewandelt. 1974 wurde die WIPO eine Behörde der Vereinten Nationen mit dem Mandat, im Auftrag der Mitgliedsstaaten der UNO Fragen des geistigen Eigentums zu behandeln. Im März 2001 hatten 177 Nationen ihren Beitritt zur WIPO ratifiziert. Die WIPO verabschiedete im Dezember 1996 zwei Richtlinien zu Auto<t-2>renrechten in digitalen Medien, den <x@1 fliess kursiv><t-2>WIPO Copyright Treaty<@$p><t-2> (WCT)<x@3 hoch fliess><t-2>33 <\n><@$p>und den <@1 fliess kursiv>WIPO Performances and Phonograms Treaty<@$p> (WPPT),<@3 hoch fliess>34<@$p> die nun nach und nach in den nationalen Gesetzgebungen umgesetzt werden. Ebenfalls 1996 trat die WIPO in ein Kooperationsabkommen mit der Welthandelsorganisation (WTO) ein. Als Teil des Übereinkommens zur Errichtung der Welthandelsorganisation hatte die WTO 1994 das Abk<t-1>ommen über <x@1 fliess kursiv><t-1>Trade-Related Aspects of Intellectual Property Rights<@$p><t-1> (TRIPS)<x@3 hoch fliess><t-1>35<@$p><t-1> <t$>verabschiedet. Auch die Europäische Union bemüht sich um eine »Harmonisierung« der Urheberrechte der Mitgliedsländer mit dem Ziel, einen <@1 fliess kursiv>Single Market<@$p> für neue Produkte und Dienstleistungen der Informationsgesellschaft zu schaffen, die auf geistigem Eigentum beruhen. In Europa <\n>stehen nur die Rechtsordnungen Großbritanniens und Irlands in der Tradition des <@1 fliess kursiv>Copyrights<@$p>, während die übrigen 13 Staaten die <@1 fliess kursiv>Droit d‘au<\h>teur<@$p>-Philosophie vertreten, doch stehen Ersteren mächtige Vertreter des <@1 fliess kursiv>Common Law<@$p>-Rechtskreises zur Seite, allen voran die USA. Eine europaweite Harmonisierung wird ferner dadurch erschwert, dass für das Urheberrecht in sechs Ländern die Justizministerien zuständig sind, in sechsen die Kulturministerien und in dreien die Wirtschaftsministerien. 1995 verabschiedete die EU das <@1 fliess kursiv>Green Paper on Copyright and Related Rights in the Information Society<@$p>,<@3 hoch fliess>36<@$p> dem 350 Eingaben von interessierten Parteien folgten. Eine Konferenz in Florenz im Juni 1996 bildete den Abschluss des Konsultationsverfahrens. Seither arbeitete die Generaldirektion Binnenmarkt (vormals DG XV) an dem Richtlinienentwurf, der auch die Umsetzung der beiden »Internet-Abkommen« der WIPO von 1996 berücksichtigen muss. Am 21. Januar 1997 legte die Kommission dem Parlament und dem Rat einen Vorschlag für eine »Richtlinie des Euro<\h>päischen Parlaments und des Rates zur Harmonisierung bestimmter Aspekte des Urheberrechts und der verwandten Schutzrechte in der Informationsgesellschaft« vor. Das Parlament billigte den Vorschlag im Februar 1999 mit den von ihm vorgenommenen Änderungen,<@3 hoch fliess>37<@$p> doch seine Verabschiedung verzögerte sich noch um mehr als zwei Jahre. Zu den Kernpunkten, über die lange keine Einigkeit erzielt werden konnte, gehören die Schranken des Urheberrechts unter digitalen Bedingungen und der Schutz der technischen Mittel zur Durchsetzung von Copyright-Rechten. Am 9.<\!q>April 2001 schließlich erließ der EU-Ministerrat die Urheberrechtsrichtlinie, die nun innerhalb von 18 Monaten von den Mitgliedsstaaten in nationales Recht umgesetzt werden muss.<@3 hoch fliess>38<@$p> Sie beinhaltet unter anderem eine abschließende fakultative Liste der Ausnahmen vom Urheberrechtsschutz, zu denen auch private Vervielfältigungen gehören, und die Gewähr, dass solche Ausnahmen von Schulen, Bibliotheken usw. auch dann in Anspruch genommen werden können, wenn die Werke in Kopierschutz- und Rechtekontrollsysteme eingekapselt sind. Damit ist die erste Generation von Harmonisierungsrichtlinien der EU abgeschlossen.Neben WIPO, WTO und EU sind die G-8 Konferenzen zur Informationsgesellschaft ein weiteres Forum für Diskussionen und Verhandlungen über die internationale Angleichung der Urheberrechtsregularien. Zunehmend verlagert sich die Gesetzgebungsinitiative im Bereich des <\n>intellektuellen und industriellen Eigentums auf die globale Bühne. Vor allem durch die globalen Informationsflüsse im Internet wird die Notwendigkeit gesehen, die Gesetzgebung weltweit anzugleichen. @2  ZÜ 2:Balance@1 fliess ohne:<*h"mehr">Auch wenn geistige Güter einen ähnlichen Eigentumscharakter annehmen wie materielle, bleiben doch wesentliche Unterschiede. Anders als bei Sachen bedarf es bei der Bestimmung immaterieller Rechte der Abwägung zwischen den Interessen der Öffentlichkeit an einer weitgehenden Informationsfreiheit und dem Bedürfnis nach der Schaffung exklusiver individueller Informationsnutzungsrechte, die schöpferische Leis<\h>tungen belohnen und Anreiz für Innovation geben sollen, zwischen Informationsfreiheit und Urheberrecht.@1 fliess mit:Wer eine Einführung ins Immaterialrecht zur Hand nimmt, wird bald auf den Begriff der »Balance« stoßen. Auf der einen Waagschale liegen die Interessen der Öffentlichkeit und auf der anderen die der Urheber <\n>respektive der Besitzer von Copyright-Rechten. Die beiden Seiten des Gleichgewichts, das es immer wieder neu zu erzielen gilt, finden sich in vielen Rechtsquellen. Die Allgemeine Menschenrechtserklärung der Vereinten Nationen umreißt sie in Artikel 27 wie folgt: »(1) Jeder hat das Recht, am kulturellen Leben der Gemeinschaft frei teilzunehmen, sich an den Künsten zu erfreuen und am wissenschaftlichen Fortschritt und dessen Errungenschaften teilzuhaben. (2) Jeder hat das Recht auf Schutz der geistigen und materiellen Interessen, die ihm als Urheber von Werken der Wissenschaft, Literatur oder Kunst erwachsen.«<@3 hoch fliess>39<@$p>Im Grundgesetz der Bundesrepublik Deutschland (GG) sind die beiden Seiten in den Artikeln 5 (Informationsfreiheit) und 14 (Schutz des Eigentums) formuliert. Nach dem Urheberrecht stehen allein dem Urheber die exklusiven Rechte zu, sein Werk zu veröffentlichen, zu verbreiten, zu vervielfältigen, auszustellen und in unkörperlicher Form öffentlich wiederzugeben sowie anderen Personen Nutzungsrechte einzuräumen. Er soll den wirtschaftlichen Vorteil aus seinem geistigen Eigentum ziehen. Die Lehre von der »Stimulanz« durch Entlohnung gehört zu den Grund<\h>überzeugungen, auf denen sowohl das Urheber- als auch das Copyright-Recht basieren. Die Öffentlichkeit ist daran interessiert, dass Autoren <\n>ihre Werke öffentlich machen. Doch wenn das Werk die ausschließliche Kontrolle des Autors verlässt, wird es verwundbar gegenüber nicht autorisierter Verwertung. Das Urheberrecht sucht daher die monetären Anreize für den Autor gerade dann zu sichern, wenn er sein Werk öffentlich zugänglich macht.Als ebenso schützenswert werden die »Interessen der Allgemeinheit <t-4>an einem ungehinderten Zugang zu den Kulturgütern« angesehen (BVerfGE <t-2>3<t$>1, 229, 230). Wie jedes Eigentum, so unterliegt auch das geistige einer S<t-1>ozialbindung (Art.<\!q>14, Abs.<\!q>2 GG). Die ausschließlichen vermögensrecht<\h>li<t$>chen Ansprüche des Urhebers werden durch überwiegende Bedürfnisse der Allgemeinheit begrenzt. Geistiges Eigentum verpflichtet dazu, optimal zum geistigen, kulturellen und kulturwirtschaftlichen Fortschritt beizutragen. Ein »überwiegendes Bedürfnis der Allgemeinheit« besteht in dem ungehinderten Zugang zu Informationen. Dem hat der Gesetzgeber durch eine Reihe von Ausnahmeregelungen entsprochen, die das Urheberrecht beschränken.Das Urheberrecht ist ein »Sozialvertrag« über die Rechte von Autoren, von Rezipienten und der allgemeinen Öffentlichkeit. Auf seiner Grundlage muss eine Balance zwischen einem Ausschließlichkeitsrecht des Autors und einem Allgemeininteresse an einer möglichst weitgehenden Partizipation an Informationen, Ideen, Forschungsergebnissen und künstlerischen Schöpfungen erzielt werden. Im Streitfall muss der Ges<t-2>etzgeber die beiden Verfassungsgüter abwägen. Urheberrechtliches Schaf<\h>fe<t$>n basiert auf der Ausübung der Informationsfreiheit. Ohne die Aus<\h>einandersetzung mit vorhandenen geistigen Werken wäre eine Weiterentwicklung in Wissenschaft, Kunst und Literatur nicht möglich. Freie Information sichert daher Fortschritt und Bildung.Die gleiche Vorstellung einer Balance herrscht auch in den Copyright-Ländern. In der US-Verfassung heißt es in Art. 1 Ziff. 8: »Der Kongress soll die Macht haben, den Fortschritt der Wissenschaft und der nützlichen Künste zu fördern, indem er Autoren und Erfindern für eine begrenzte Zeit das ausschließliche Recht an ihren entsprechenden Schriften und Entdeckungen sichert.«<@3 hoch fliess>40<@$p> Loren weist darauf hin, dass es sich um die einzige von 18 Klauseln handelt, die ein Mittel (<@1 fliess kursiv>für eine begrenzte Zeit das Recht sichert <@$p>...) an einen ausdrücklichen Zweck (<@1 fliess kursiv>den Fortschritt zu fördern <@$p>...) bindet. Den Gründervätern sei nicht nur jede Form von Monopol verdächtig gewesen, auch die Geschichte des Copyright als Instrument der Zensur in England sollte in den USA nicht fortgeschrieben werden: »Sie haben daher den Zweck des Copyright ausdrücklich definiert: nämlich den Fortschritt von Wissen und Bildung zu fördern« (<@6 Caps>Loren<@$p>, 2000). Im gleichen Sinne schreibt auch Elkin-Koren: @1 fliesskursiv Zitat:<*t(25.512,0,"1  ")><*t(25.512,0,"1  ")>»Die Verfassung berechtigt den Kongress, den Urhebern Rechte nur in dem Umfang zu bewilligen, der den Grundsätzen der Wissens- und Bildungsförderung dient. Die Autorität der Verfassung gewährt den Urhebern keine privaten Eigentumsrechte an ihren Werken. Der Kongress hat den Auftrag, die Rechte der Urheber nur als Mittel zum Zweck zu schützen und nur in dem Umfang, in dem ein solcher Schutz dem Verfassungsziel der Fortschrittsförderung dient. Folglich sind alle Rechte und kommerziellen Erwartungen Einzelner von instrumentellem Charakter und nur zum Nutzen des Fortschritts geschützt. Anders ausgedrückt sind die den Urhebern gewährten Rechte bedingte Rechte, die von ihrer instrumentellen Rolle bei der Steigerung von Wissen und Bildung abhängen. Deshalb vermittelt das Copyright-Gesetz zwischen zwei sich widersprechenden öffentlichen Interessen: dem öffentlichen Interesse, die Erzeugung von Information zu motivieren und dem öffentlichen Interesse, einen maximalen Zugang zu Information zu gewährleisten« <@1 fliess normal>(<@6 Caps>Elkin-Koren<@1 fliess normal>, 1997).@1 fliess mit:@1 fliess ohne:In den <@1 fliess kursiv>Droit d’auteur<@$p>-Ländern ergibt sich eine Beschränkung der Rechtsposition des Urhebers aus der Sozialpflichtigkeit des Eigentums. Die so genannten Schranken des Urheberrechts (§ 45 ff.) betreffen u.a. die Nutzung von geschützten Werken in der Rechtspflege, im Kirchen- Schul- und Unterrichtsgebrauch (§ 46 UrhG) und in der Berichterstattung (§ 50). Journalisten gehören zu den von den Schranken Privilegierten, da sie einen öffentlichen Informationsauftrag haben. Die Wiedergabe von öffentlichen Reden (§ 48) und die Verwendung von Zitaten (§ 51), ausdrücklich auch für Zwecke von Kritik und Parodie – die die Rechteindustrie selten freiwillig lizenzieren würde –, ist erlaubt. Im Rahmen der aktuellen Berichterstattung dürfen in begrenztem Umfang auch Werke der bildenden Kunst reproduziert werden, ohne dass der Urheber hierzu seine Einwilligung geben muss. Ebenfalls ist es zulässig, ein Werk, das sich bleibend im öffentlichen Raum befindet, (zweidimensional) zu vervielfältigen. Bibliotheken haben für ihren öffentlichen Informationsauftrag ein Verleihrecht für alle veröffentlichten Werke (§ 17 Ziff. 2). Auch wissenschaftlichen Nutzungen räumt das Urhebergesetz einen besonderen Status ein. Die zitatweise Publikation von Werken in wissenschaftlichen Arbeiten ist zugestanden. Für wissenschaftlich-technische Werke gilt der Grundsatz der Freiheit wissenschaftlicher Gedanken und Lehren, wonach diese Gegenstand der freien geistigen Auseinandersetzung bleiben müssen. Die Form ist grundsätzlich schützbar, doch darf der Schutz nicht zu einer <\n>Monopolisierung der darin enthaltenen wissenschaftlichen Gedanken und Lehren führen (vgl. <@6 Caps>Saacke<@$p>, 1998, S. 35 f). @1 fliess mit:Schließlich ist es ohne Einschränkung der Nutzergruppe zulässig, einzelne Vervielfältigungsstücke eines Werkes zum privaten Gebrauch herzustellen. Eingeschlossen sind Fotokopien aus Büchern und Zeitschriften und die Audio- oder Videoaufzeichnung von durch Funk gesendeten Werken. Ausgenommen ist die Vervielfältigung von Noten und von vollständigen Büchern oder Zeitschriften, soweit sie nicht durch Abschreiben vorgenommen wird oder zu Archivzwecken geschieht, wenn als Vorlage für die Vervielfältigung ein eigenes Werkstück benutzt wird oder wenn es sich um ein seit mindestens zwei Jahren vergriffenes Werk handelt. In diesen Fällen ist die Einwilligung des Berechtigten einzuholen (§ 53). Die Ausnahme von der Ausnahme stellt Computersoftware dar: »Sowohl der Bundestag als auch die Organe der EU haben eine solche Schranke wegen der besonderen Anfälligkeit des Softwarewerkes für Urheberrechtsverletzungen abgelehnt« (<@6 Caps>Metzger/Jaeger<@$p>, 1999, S. 841). In allen anderen genannten Fällen gilt eine gesetzliche oder Zwangslizenz zur Vervielfältigung als erteilt, die jedoch nicht vergütungsfrei ist.Eine Vergütungspflicht für die genannten freien Nutzungen besteht für den Hersteller oder Importeur von Geräten zur Bild- und Tonaufzeichnung und Ablichtung, von Leermedien wie Audio- und Videokassetten sowie für den Betreiber von Fotokopiergeräten (§ 54 u. 54a). In den meisten Fällen sind Kopien ohne die Einholung einer ausdrücklichen Erlaubnis (Lizenz) des Rechteinhabers erlaubt. Die im Kaufpreis enthaltene gesetzlich festgelegte Pauschalgebühr (Anlage zum UrhG), z.B. zwei Pfennige pro A4-Fotokopie, wird von einer Verwertungsgesellschaft eingezogen und an die berechtigten Autoren ausgeschüttet.<@3 hoch fliess>41<@$p> Die Vergütungspflicht für die öffentliche Wiedergabe entfällt für Veranstaltungen der Jugend- und Sozialhilfe, der Alten- und Wohlfahrtspflege, der Gefangenenbetreuung sowie für Schulveranstaltungen, sofern sie nach ihrer sozialen oder erzieherischen Zweckbestimmung nur einem bestimmten abgegrenzten Kreis von Personen zugänglich sind (§ 52).Die Copyright-Rechtssysteme haben für die nicht gewerbliche private Vervielfältigung in geringem Umfang eine andere Lösung gefunden. Hier muss grundsätzlich fast jede Nutzung lizenziert werden. Ausnahmen vom Copyright-Schutz begründen sich in einem öffentlichen Interesse und einem Marktversagen. Da die Lizenzierungspflicht an praktische Grenzen stößt, es z.B. mit unzumutbarem Aufwand verbunden wäre, wenn man für die Anfertigung einer Fotokopie aus einem Buch den Rechteinhaber ausfindig machen und einen Nutzungsvertrag mit ihm aushandeln müsste, oder ein Rechteinhaber für Zitate in der kritischen Berichterstattung über eine Neuerscheinung keine Genehmigung erteilen möchte, hat sich das Konzept des <@1 fliess kursiv>Fair Use<@$p> etabliert. Bereits kurz nach der Verabschiedung des <@1 fliess kursiv>Act of Anne<@$p> 1709 erkannten die britischen Gerichte bestimmte nicht autorisierte Reproduktionen von copyrightgeschütztem Material, sofern sie der öffentlichen Bildung und Anregung geistiger Produktionen dienen, als <@1 fliess kursiv>Fair Abridgement<@$p> und später als <@1 fliess kursiv>Fair Use<@$p> an (vgl. <@6 Caps>Guibault<@$p>, 1997, S. 16). Die US-amerikanische <@1 fliess kursiv>Fair Use<@1 fliess normal>-Doktrin<@3 hoch fliess>42<@$p> hat ihren Ursprung in dem Gerichtsstreit Folsom gegen Marsh von 1841. Dabei ging es um die Verwendung von Privatbriefen George Washingtons in einer fiktionalisierten Biographie des Präsidenten ohne Genehmigung des Eigentümers der Briefe. Das Gericht entschied, dass es sich nicht um einen Copyright-Verstoß handle. In seiner Begründung stellte Richter Storey fest: »Kurz<\h>um, bei der Entscheidung solcher Fragen müssen wir immer wieder einen Blick auf die Ursprünge und die Gegenstände der vorgenommenen Auswahl werfen, auf die Menge und den Wert der verwendeten Materialien und auf den Grad, mit welchem die Verwendung den Verkauf des Originalwerkes beeinträchtigt, dessen Profit mindert oder aber dieses ersetzt« (nach <@6 Caps>Brennan<@$p>, o.J.).England, Australien, Neuseeland und Kanada haben die <@1 fliess kursiv>Fair Dealing-<@$p>Verteidigung für Vervielfältigungen zum Zwecke von Forschung, Bildung, Kritik, Rezension und Berichterstattung in ihren Copyright-Gesetzen kodifiziert.<@3 hoch fliess>43<@$p> Die USA nahmen die <@1 fliess kursiv>Fair Use<@$p>-Doktrin 1976 in den U.S. Copyright Act auf. Er geht über die entsprechenden Bestimmungen in den Commonwealth-Ländern hinaus, die Liste der <@1 fliess kursiv>Fair Use<@$p>-Zwecke ist offen (§ 107 U.S.C.). Zur gerichtlichen Feststellung, ob eine gegebene Nutzung fair war, gibt das Gesetz eine ebenfalls offene Liste von vier Kriterien vor, die an Richter Storeys Merkmale erinnern.<@3 hoch fliess>44<@$p> Die dem Richter aufgetragene Interpretation dieser <@1 fliess kursiv>Fair Use<@$p>-Kriterien hat im Laufe der J<t-1>ahre einen inkonsistenten Korpus von Fallrechtsurteilen hervorgebracht. <t$>Die Entscheidungen werden nicht von durchgängigen Prinzipien geleitet, sondern scheinen intiuitive Reaktionen auf individuelle Tatsachenmuster zu sein, bei denen häufig das Gewicht auf den Schutz des Privateigentums gelegt wird (vgl. <@6 Caps>Guibault<@$p>, 1997, S. 18).Die beiden Rechtssysteme haben also zwei verschiedene Lösungen für das Problem von privaten Kopien hervorgebracht. Das Copyright wählte eine generelle Lizenzierungspflicht plus gebührenfreiem <@1 fliess kursiv>Fair Use<@$p>. Das <@1 fliess kursiv>Droit d’auteur<@$p> entschied sich für eine Zwangslizenzierung plus Pauschalabgaben. Während die Urheberrechtsausnahmen unumstritten sind (problematisch werden sie allerdings bei digitalen Werkstücken und Online-Nutzungen; dazu s.u.) gibt es in den USA eine andauernde Debatte über den Status des <@1 fliess kursiv>Fair Use<@$p>. Einige Rechtsgelehrte sehen darin ein positives Recht, während andere es nur für eine Abhilfe gegen Marktversagen halten, dessen Rechtfertigung mit neuen technischen Mitteln zur einfachen Lizenzierung entfällt. Diese Debatte wird in den beiden folgenden Kapiteln über Digitalmedien und ihre Herausforderungen an das Recht wieder aufgegriffen.Beide Rechtssysteme kennen weitere Einschränkungen. Dazu gehört die Erstverkaufsdoktrin: Wer ein Werkstück (ein Buch, eine Schallplatte, ein Computerprogramm) rechtmäßig erworben hat, darf es ohne Erlaubnis des Urhebers weiterverkaufen.<@3 hoch fliess>45<@$p> Weitere Mittel zur Sicherung des öffentlichen Interesses und gegen den Missbrauch von Urheberrechten durch die Rechteinhaber finden sich in den Verfassungen, im Zivilrecht, im Verbraucherschutz sowie im Wettbewerbsrecht (vgl. <@6 Caps>Guibault<@$p>, 1997, S.<\!q>19 ff). Die grundlegendste Einschränkung gibt jedoch das Urheberrecht/Copyright selbst vor. Es sind dies die Freiheit der Ideen (des informativen Gehalts eines Werkes) und die, verglichen mit dem Sachenrecht, ungewöhnliche zeitliche Begrenzung der Schutzes. Darin drückt sich eine Anerkenntnis der Gesetzgeber aus, dass jede individuelle Schöpfung aus dem unermesslichen Bestand allen vorangegangenen Wissens hervorgekommen ist und dorthin zurückkehren wird. Ein einzelnes Werk – und sei es noch so genial – ist nicht mehr als ein Kräuseln auf dem Meer des kollektiven Wissens. Dagegen werden Monopolrechte nur zugestanden, um die Autorin in die Lage zu versetzen, weitere Werke zu schaffen. Vor diesem Hintergrund ist bereits die Ausdehnung der Schutzfrist über den Tod hinaus fragwürdig, da in der Regel auch größere Einnahmen Tote nicht dazu bewegen können, neue Werke zu schaffen. Da seit den ersten Schutzfristen von 28 Jahren nach Schöpfung die Umschlags- und Verfallsgeschwindigkeit des Wissens erheblich zugenommen hat, wäre eigentlich zu erwarten, dass auch die Schutzfristen kürzer werden. Tatsächlich zeigt die kontinuierliche Ausweitung der Schutzfrist in Deutschland und den USA auf 70 Jahre nach Tod des Autors, dass es nicht um die Interessen der Autorinnen oder der Öffentlichkeit geht, sondern um diejenigen der Rechteindustrie. Sie ist Teil einer generellen Tendenz der Verschiebung des Urheberrechts/Copyrights als einer Interessensabwägung der verschiedenen beteiligten Parteien hin zu einem Investitionsschutz. <t-1>Die Balance ist ein bewegliches Ziel. Urheberrechtliche »Werke«, also <t$>der Ausdruck von schöpferischen Ideen ist an Medien gebunden, und diese haben sich in den vergangenen 100 Jahren mit einer rasanten Dynamik verändert. Entsprechend verschiebt sich die daraus jeweils resul<t-1>tierende Allokation von Rechten zwischen Eigentümer und Nutzer. Erkki <t$>Liikanen, EU-Kommisar für die Informationsgesellschaft skizziert die aktuelle Situation so: @1 fliesskursiv Zitat:<*t(25.512,0,"1  ")><*t(25.512,0,"1  ")>»Wenn wir zu viele Barrieren errichten, wird sich der gesamte Sektor nicht entwickeln. Es wird keine Märkte geben. Das andere Risiko ist, dass es keine Lizenzeinnahmen geben wird. Aber ich denke, dass Lizenzgebühren einmal gezahlt werden sollten. Und wenn man das Werk dann später für private Zwecke nutzt, will man keine zweite Gebühr zahlen. [...] Ich bin sicher, wenn es keinen Markt für Künstler gibt, werden sie die Verlierer sein. Doch wenn Firmen in einem schwerfälligen, mehrstufigen System Tantiemen auf verschiedenen Ebenen bezahlen müssen, dann wird sich darauf keine keine Industrie aufbauen können. Wir alle werden dann verlieren. Was wir also tun müssen, ist, die richtige Balance zu finden« <@1 fliess normal>(nach <@6 Caps>Krempl<@1 fliess normal>, 4/2000).@1 fliess mit:@2  ZÜ 2:Akteure der rechtlichen Ordnung des Wissens@1 fliess mit:@1 fliess ohne:An der Schöpfung, Herstellung, Verbreitung und sonstigen Verwertung von Produkten des geistigen Eigentums sind eine Vielzahl von Menschen beteiligt, von der Autorin über den Toningenieur und den Setzer, den Agenten und die Rechtsanwältin bis zum Zeitungsverkäufer. Es lassen sich jedoch vier Hauptparteien unterscheiden, die auf unterschiedliche Weise am geistigen Eigentum interessiert sind:1.  <\i>die Urheber (bei Patenten: Erfinder) mit vermögens- und persönlichkeitsrechtlichen Interessen, die häufig kollektiv durch Verwertungsgesellschaften vertreten werden, 2.  <\i>die Verwerter und Mittler (Verleger, Bild- und Tonträger-, Software- und Datenbankhersteller, Betreiber von Rundfunk-, Kabel- und Internetdiensten, Betreiber von elektronischen Rechtekontrollsystemen) von Kleinstunternehmen bis zu weltweiten Oligopolen, mit Investitionsschutz- und Vermögensinteressen, 3.  <\i>die Rezipienten (auch als »Verbraucher« bezeichnet, obgleich im Falle von Wissen ja gerade nichts verbraucht wird) mit einem Interesse an ständig neuer, vielfältiger, kostengünstiger und zugänglicher Information, darunter auch an Kopien für den privaten Gebrauch,4.  <\i>die Öffentlichkeit, die an einer freien Zugänglichkeit und einem Austausch und einer kreativen Weiterschreibung von Wissen in Bildung, Bibliotheken, Museen und in der Wissenschaft interessiert ist. @2  ZÜ 3:Autorinnen@1 fliess ohne:In der Formationsphase des modernen Systems des geistigen Eigentums im 19.<\!q>Jahrhundert engagierten sich individuelle Künstler für die Verbesserung der Rolle der Autoren, in der Literatur z.B. Johann Wolfgang Goethe, in der Musik etwa Richard Strauss. Ab den 20er-Jahren des 20.<\!q>Jahrhunderts motivierte das Sozialstaatsprinzip den Gesetzgeber zu einem besonderen Schutz des Urhebers als der gegenüber den Verwertern wirtschaftlich und sozial schwächeren Partei.@1 fliess mit:Nicht selten werden Autoren mit der Rechteindustrie zusammengedacht, die bei der Verbreitung der Werke auf dem Markt in der Tat gemeinsame Interessen haben. Zumindest die Verwertungsgesellschaften – die ebenfalls nicht mit den Autorinnen gleichgesetzt werden dürfen – stehen in Urheberrechtsfragen häufig Schulter an Schulter mit der Rechteindustrie. Beim Erstverkauf ihrer Werke an die Verlage stehen die Autoren diesen jedoch als Antagonisten gegenüber. Die Rechteindustrie ist nicht kreativ. Sie vermarktet die künstlerischen Werke, für deren Zulieferung sie auf die Autorinnen angewiesen ist. Autoren sind in den seltensten Fällen zugleich Akteure der Rechteindustrie, wie etwa Michael Jackson, der die Rechte an den <@1 fliess kursiv>Beatles-<@$p>Werken erwarb. Als Verkäufer und Käufer haben die beiden Seiten entgegengesetzte Interessen: Der Autor möchte einen möglichst hohen Preis für sein Manuskript erzielen, der Verlag möchte ihm möglichst wenig, im Idealfall gar nichts zahlen. Im Buchverlagswesen ist heute in Deutschland ein <\n>Autorenanteil von sechs bis acht Prozent des Nettoladenpreises üblich. Nicht selten erhält der Urheber nicht einmal diesen, sondern muss selbst einen Druckkostenzuschuss beibringen. Nur eine Hand voll Bestsellerautorinnen ist in der Position, günstigere Konditionen auszuhandeln. Nimmt man den substanziellen Zweck des Urheberrechts, dem Autor <\n>eine Entlohnung zu sichern, um ihn im Interesse der Allgemeinheit zur Schaffung neuer Werke zu stimulieren, beim Wort, so muss man feststellen, dass die Urheberwirklichkeit dieses Ziel nicht einfüllt. Bücherschreiben ist eine brotlose Kunst. Die Interessen der kreativen Medienkulturschaffenden vertreten <\n>Verbände wie der Deutsche Journalisten Verband (DJV) oder die in ver.di aufgegangene Industriegewerkschaft (IG) Medien.<@3 hoch fliess>46<@$p> Für die Gewerkschaften stehen die Vergütungsinteressen und die Arbeits- und Lebensbedingungen ihrer Mitglieder sowie die tarifvertragliche Gestaltung oder Verbesserung der Konditionen der wachsenden Zahl von Freien und Selbständigen im Vordergrund. Häufig genug verpflichten Verlage Urheber und ausübenden Künstler, auf Rechte zu verzichten, die ihnen nach dem Urhebergesetz zustehen. Fotografen beispielsweise müssen in vielen Fällen ihr nach Gesetz nicht übertragbares Urheberpersönlichkeitsrecht auf Namensnennung abtreten. Auch die Verträge der Drehbuchautoren und anderer Urheber von Film- und Multimediawerken sehen in der Regel eine Übertragung ihrer Persönlichkeitsrechte vor.<@3 hoch fliess>47<@$p> Die Gewerkschaften sehen das Urheberrecht als das »Arbeitsrecht« der Autoren und ausübenden Künstler und engagieren sich für ein Urhebervertragsrecht der Kreativen.<@3 hoch fliess>48<@$p>Nun klagen auch die Buchverleger über geringe Gewinnmargen trotz Buchpreisbindung. Wie sieht es dagegen in der einnahmestarken Musik<\h>industrie aus? Von 40 Milliarden Dollar pro Jahr sollte doch einiges die Künstler erreichen. Hier gibt es noch weniger als in der Printindustrie verbindliche Richtwerte oder auch nur zuverlässige Zahlen über übliche Praktiken. In einem Aufsehen erregenden Artikel rechnete die Rocksängerin Courtney Love von der Band <@1 fliess kursiv>Hole<@$p> vor, was Musiker tatsächlich verdienen. Danach verkaufte die Band eine Millionen Platten, die der Plattenfirma einen Profit von 6,6 Millionen Dollar einbrachte und der Band einen von null Dollar: »Was ist Piraterie? Piraterie ist das Stehlen der Arbeit eines Künstlers, ohne die Absicht, dafür zu bezahlen. Ich spreche nicht von <@1 fliess kursiv>napster<@$p>-artigen Systemen. Ich spreche von Verträgen mit großen Plattenfirmen.«<@3 hoch fliess>49<@$p> Love, die von ihrer Plattenfirma <@1 fliess kursiv>Universal Records<@$p> in einen sieben Jahre andauernden Rechtsstreit verwickelt wurde, bezeichnet Labels als <@1 fliess kursiv>Gatekeeper<@$p>, die ihre Macht aus ihrer Kontrolle über die begrenzte Produktion, die knappe Sendezeit der Radios, die knappe Stellfläche in den Musikgeschäften, kurz, über den Zugang zur Öffentlichkeit beziehen. »Künstler zahlen 95 Prozent von dem, was sie verdienen an <@1 fliess kursiv>Gatekeeper<@$p>, weil wir bislang auf <@1 fliess kursiv>Gatekeeper<@$p> angewiesen waren, damit unsere Musik gehört wird. Sie haben nämlich ein System und wenn sie sich ent<t-1>scheiden, genügend Geld auszugeben – Geld, das ich ihnen dann schulde<t$> –, können sie abhängig von einer Menge willkürlicher Faktoren gelegentlich Dinge durch dieses System schleusen.« Dazu gehören diverse markt- und vertragstechnische Faktoren, nicht aber die Musik, nicht der Geschmack der Hörer und nicht ein finanzieller Anreiz für den Künstler, sein nächstes Werk zu schaffen. Unter den 273<\!q>000 in der <@1 fliess kursiv>American Federation of Musicians<@$p> organisierten Profimusikern, schreibt Love, liege das Durchschnittseinkommen bei etwa 30<\!q>000 Dollar im Jahr. Nur 15 Prozent von ihnen könnten ausschließlich von ihrer Musik leben:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")><*t(25.512,0,"1  ")>»Wir Künstler glauben gerne, dass wir eine Menge Geld verdienen können, wenn wir Erfolg haben. Aber es gibt Hunderte Geschichten von 60- und 70-jährigen Künstlern, die pleite sind, weil sie nie einen Pfennig für ihre Hits bekommen haben. Für einen Nachwuchskünstler ist es heute ein langer Weg zum echten Erfolg. Von den 32<\!q>000 Neuerscheinungen pro Jahr verkaufen nur 250 mehr als 10<\!q>000 Exemplare. Und weniger als 30 bekommen Platin [...] Künstler, die der Industrie Milliarden von Dollar eingebracht haben, sterben arm und unversorgt. Und sie sind nicht etwa Darsteller oder sonstwie sekundär Beteiligte. Sie sind die rechtmäßigen Eigentümer, Urheber und ausübenden Künstler von Originalkompositionen. Das ist Piraterie.« <@6 Caps>(Love 6/2000)<@$p><*t(25.512,0,"1  ")>@1 fliess ohne:Mit dem Internet seien die <@1 fliess kursiv>Gatekeeper<@$p> überflüssig geworden. Den Ressourcenmangel, den die Unternehmen verwalten, gebe es nicht mehr. Musiker und Hörer können in direkten Austausch miteinander treten. Zusammen mit ihrer 19-jährigen Webmistress betreibt Love daher ihre <\n>eigene Website. @1 fliesskursiv Zitat:<*t(25.512,0,"1  ")><t-1>»Da ich meine Musik im Grunde schon im alten System verschenkt habe, mache ich mir keine Sorgen über MP3-Dateien, Funktechnologien oder irgend welche anderen Bedrohungen meiner Copyrights. Alles, was meine Musik für mehr Menschen verfügbarer macht, ist großartig. [...] Ich werde es Millionen von Menschen erlauben, meine Musik umsonst zu bekommen, wenn sie möchten, und wenn sie sie ihnen gefällt, werden sie hoffentlich so freundlich sein, ein Trinkgeld zu hinterlassen.« (ebd.)@1 fliess ohne:@1 fliess mit:Wenn die Musiker nur einen kärglichen Anteil an den Ladenverkäufen ihrer Werke erhalten, wie sieht es dann mit den Vergütungen aus, die die Verwertungsgesellschaften von den Plattenfirmen pro Einheit, von den Veranstaltern für Aufführungen und für die Wiedergabe im Rundfunk einziehen und an die Autoren und ihre Verlage weitergeben? Eine Untersuchung der britischen Kartellbehörde über die Auszahlungsstruktur der <@1 fliess kursiv>Performance Rights Society<@$p>, die die Aufführungsrechte in England wahrnimmt, ergab, dass sich auch hier das von der Musikindustrie betriebene Starsystem widerspiegelt. Zehn Prozent der Komponisten erhalten 90 Prozent der Tantiemen.<@3 hoch fliess>50<@$p> Wie wussten schon die Bremer Stadtmusikanten zu sagen: »Etwas besseres als die Musikindustrie finden wir überall.« @2  ZÜ 3:Verwertungsgesellschaften@1 fliess ohne:Die Idee, dass sich Autoren zusammentun, um Verlagsfunktionen in eigener Regie und auf eigene Rechnung zu übernehmen, führte schon Ende des 18. Jahrhunderts zur Gründung erster Selbstverlagsunternehmen (vgl. <@6 Caps>Czychowski<@$p>, 1998). Das Modell hat sich mit dem »Verlag der Autoren« oder dem »Filmverlag der Autoren« bis heute erhalten. @1 fliess mit:Einmal veröffentlicht, konnten Noten von jedem Kaffeehausorchester gespielt werden, und vor allem das Abspielen der Schallplatte konnte in kommerziellen Etablissements beliebig oft für Tanz und Stimmung sorgen. Einen Tantiemenanspruch auf eine Teilhabe an jeder wirtschaftlichen Nutzung ihrer Werke begannen die Urheber Ende des 19.<\!q>Jahrhunderts geltend zu machen. Da nun ein Musikautor nicht mit jedem Kaffeehausbesitzer einen Nutzungsvertrag aushandeln konnte, gründeten sich seit der Jahrhundertwende Gesellschaften zur kollektiven Wahrnehmung der Verwertungsrechte der Urheber. Die Musikautoren waren dabei führend, bald folgten entsprechende Gesellschaften auch für literarische Autoren und bildende Künstler.Verwertungsgesellschaften sind private Einrichtungen, denen zur Wahrnehmung öffentlicher Aufgaben in vielen Ländern eine gesetzliche Monopolstellung zugewiesen wurde. Ihr Charakter liegt zwischen der quasi-gewerkschaftlichen Funktion einer Solidargemeinschaft der Urhe<\h>berinnen gegenüber den wirtschaftlich stärkeren Verwertern und einer quasi-amtlichen Funktion, die Einhaltung der Meldepflicht von Vervielfältigungsstücken, öffentlichen Aufführungen und – mit dem neuen <\n>Medium Radio – auch der Sendung zu kontrollieren. Die Gesellschaft für mus<t-2>ikalische Aufführungs- und mechanische Vervielfältigungsrechte <\n>(GEMA) <t$>wird z.B. gern als »Musikfinanzamt« bezeichnet. Der quasi-gewerkschaftliche Aspekt der Verwertungsgesellschaften als Anwälte der Kreativen und Tarifpartner der Rechtenutzer spricht aus folgendem Satz: »Nur Verwertungsgesellschaften mit einem umfangreichen, möglichst allumfassenden Repertoire sind in der Lage, ein Gegengewicht zur Marktmacht der Werknutzer, die durch die Zusammenschlüsse auf dem globalen Medienmarkt ständig wächst – über 80 Prozent ihrer Schallplattenumsätze macht die GEMA mit nur noch fünf Schallplattenproduzenten–, zu bilden« (vgl. <@6 Caps>Kreile/Becker<@$p>, 1997a, S. 638).Als Dachverband der Verwertungegesellschaften hat sich 1926 in Paris die <@1 fliess kursiv>Confédération Internationale des Sociétés d’Auteurs et Compositeurs<@$p> (CISAC)<@3 hoch fliess>51<@$p> gegründet. Heute umfasst sie 161 Mitgliedsorganisationen in 87 Ländern, die mehr als eine Million Urheber (<@1 fliess kursiv>creators<@$p>) mit mehr als 100 Millionen Werken aus den Bereichen Musik, Literatur, Film und bildende Kunst vertreten. 1994 sammelten die CISAC-Mitgliedsgesellschaften Tantiemen und Gebühren in Höhe von etwa fünf Milliarden Dollar ein. In Deutschland unterstehen die Verwertungsgesellschaften der Aufsicht des Deutschen Patentamtes sowie aufgrund ihres, zwar seit 1945 nicht mehr gesetzlichen, aber dennoch faktischen Monopols, dem Bundeskartellamt (§ 18 UrhWahrnehmungsgesetz). Mit der Zunahme der auch für Konsumenten zugänglichen Vervielfältigungstechnologien wie dem Tonband, stellte sich die Frage, ob auch für die Kopien von Werken für den privaten nicht gewerblichen Gebrauch ein gebührenpflichtiger Rechteerwerb erforderlich sei. Das US-amerikanische Rechtssystem bejahte dies, führte aber zugleich die Ausnahmeregelung des vergütungsfreien <@1 fliess kursiv>Fair-Use<@$p> ein. Das kontinentaleuropäische Recht entschied sich, die Herstellung einzelner Vervielfältigungsstücke eines Werkes zum privaten Gebrauch zuzulassen (§ 53 Abs. 1 UrhG), führte aber zugleich eine Pauschalabgabe für Reproduktionsgeräte und Leermedien ein. Die grundlegende Novellierung des deutschen »<@1 fliess normal>Gesetzes über Urheberrecht und verwandte Schutzrechte«<@$p> (UrhG) von 1965 etablierte einen Vergütungsanspruch des Urhebers gegenüber dem Hersteller und Importeur von Geräten und Bild- und Tonträgern, die erkennbar zur Vornahme von Vervielfältigungen bestimmt sind (§ 54 UrhG). Diese Rechte der Urheber nehmen kollektiv die GEMA und für den Sprachanteil von <t-1>Hörfunk- und Fernsehsendungen die Verwertungsgesellschaft (VG) Wort <t$>wahr. Ihnen gegenüber sind die Hersteller meldepflichtig. Die Erträge werden an die in ihnen organisierten Autorinnen verteilt. 1985 wurde auch für die Hersteller und Importeure von Fotokopiergeräten sowie für diejenigen, die solche Geräte für die Herstellung von Ablichtungen entgeltlich bereithalten, eine Vergütungspflicht in das Gesetz aufgenommenen (§ 54a UrhG). Wo das Gesetz nur von einer »angemessenen« Vergütung spricht, spezifiziert die Anlage zu Paragraph 54d, Abs. 1 UrhG die Höhe der einzelnen Vergütungen.Ebenfalls 1965 wurde das »Gesetz über die Wahrnehmung von Urheberrechten und verwandten Schutzrechten« erlassen. Demnach bedürfen juristische oder natürliche Personen, die von Urhebern mit der Wahrnehmung ihrer Nutzungsrechte, Einwilligungsrechte oder Vergütungsansprüche beauftragt werden, einer Erlaubnis durch die Aufsichtsbehörde, das Patentamt (§§ 1 und 2 UrhWG). Die Einnahmen sind nach einem öf<t1>fentlichen Verteilungsplan aufzuteilen, um ein willkürliches Vorgehen ausschließen. Der Verteilungsplan soll dem Grundsatz entsprechen, dass <t$>kulturell bedeutende Werke und Leistungen zu fördern sind <\n>(§ 7 UrhWG). Durch ihren Monopolstatus in Bezug auf die von ihr vertretenen Werke hat eine Verwertungsgesellschaft einen Abschlusszwang gegenüber Werknutzern. Die Verwertungsgesellschaft ist verpflichtet, auf Grund der von ihr wahrgenommenen Rechte jedermann auf Verlangen zu angemessenen Bedingungen Nutzungsrechte einzuräumen oder Einwilligungen zu erteilen (§ 11 UrhWG). Die Verwertungsgesellschaft hat Tarife aufzustellen und dabei auf religiöse, kulturelle und soziale Belange der zur Zahlung der Vergütung Verpflichteten einschließlich der Belange der Jugendpflege angemessene Rücksicht nehmen (§ 13 UrhWG). Schließlich soll die Verwertungsgesellschaft Vorsorge- und Unterstützungseinrichtungen für die Inhaber der von ihr wahrgenommenen Rechte oder Ansprüche einrichten (§ 8 UrhWG). Dieser Anspruch auf sozialen Schutz von Künstlern und Publizisten in der Renten-, Kranken- und Pflegeversicherung wurde durch das Künstlersozialversicherungsgesetz (KSVG) von 1983 weiter geregelt.<@3 hoch fliess>52<@$p>@1 fliess ohne:<@1 fliesshalbfett>GEMA<@3 hoch fliess>53<@$p> – Der Komponist und preußisch-königliche Generalmusikdirek<\h>tor Richard Strauss rief im Jahre 1903 in Berlin zusammen mit weiteren Komponisten wie Gustav Mahler einen Berufsverband und eine musikalische Verwertungsgesellschaft ins Leben. Aus ihr ging die »<@1 fliess normal>Gesellschaft für musikalische Aufführungs- und mechanische Vervielfältigungsrechte«<@$p> (GEMA) hervor. Heute vertritt sie 45<\!q>000 Komponisten, Textdichter und Musikverleger sowie, im Zuge der internationalen Zusammenarbeit, über eine Million ausländische Berechtigte. Im Jahr 2000 nahm sie 1,6 Milliarden Mark ein, 3,4 Prozent mehr als im Vorjahr. 1,3 Milliarden davon wurden an die Berechtigten ausgeschüttet. Die GEMA beansprucht das Alleinvert<t-1>retungsrecht des gesamten Repertoires ihrer Mitglieder. Diese haben also <t$>nicht die Möglichkeit, einzelne Werke nicht bei der <\n>GEMA anzumelden und deren Rechte selbst wahrzunehmen. Bei der Verwendung von Musik im Film besteht immerhin ein Rückfallrecht an den Autor. Werden Songs online verkauft, müssen die üblichen zwölf Prozent des Verkaufspreises an die GEMA abgeführt werden. Fragwürdig ist jedoch, dass auch Musiker, die ihre eigenen Stücke kostenlos zu Werbezwecken ins Internet stellen, Abgaben an die GEMA leisten sollen. Dadurch sieht der »<@1 fliess normal>Deutsche Rock- & Pop-Musikerverband«<@$p>, der die Interessen von 70<\!q>000 deutschen Bands vertritt, die Möglichkeiten von Musikern im Internet stark eingeschränkt. Ihr Sprecher rechnete vor, dass Bands mit 2<\!q>000 Mark jährlichen Vorauszahlungen rechnen müss<\h>ten, wenn sie pro Jahr zehn ihrer Songs ins Internet stellen: »Für die große Mehrheit der kreativen, unbekannten Musikgruppen und Interpreten ist diese Vorauszahlung nicht finanzierbar.«<@3 hoch fliess>54<@$p> <x@1 fliesshalbfett><t15>GVL<x@3 hoch fliess><t-5>55<@$p><t-5> – Die »<x@1 fliess normal><t-5>Gesellschaft zur Verwertung von Leistungsschutzrechten mbH«<@$p> wurde 1959 durch die Deutsche Orchestervereinigung und die Deutsche Landesgruppe der <@1 fliess kursiv>International Federation of the Phonographic Industry<@$p>, London (<@4 Pfeil (Umschalt/Alt #)>’<@$p>IFPI) gegründet. Sie nimmt die Zweitverwertungsrechte der ausübenden Künstler, Tonträgerhersteller, Videoproduzenten und Filmhersteller wahr. Das Inkasso führt die GEMA für die GVL mit durch. Für das Inkasso der Rekorder- und Leermedienabgaben ist die »<@1 fliess normal>Zentralstelle für Private Überspielungsrechte«<@$p> (ZPÜ), ein Zusammenschluss aller urheberrechtlichen Verwertungsgesellschaften, zuständig. <@1 fliesshalbfett>DMV <@$p>– Ist die GEMA für die mechanischen Rechte und die GVL für die Aufführungsrechte zuständig, so liegt die Wahrnehmung der graphischen Rechten der Musikverleger, also die Ansprüche aus der Verwertung der Noten, beim <@1 fliess normal>Deutschen Musikverleger-Verband e.V.<@3 hoch fliess>56<@$p><@1 fliesshalbfett>VG Wort<@3 hoch fliess>57<@$p> – Die <@1 fliess normal>Verwertungsgesellschaft Wort<@$p> wurde 1958 gegründet und kassiert die Tantiemen aus Zweitnutzungsrechten von Sprachwerken auch in Funk und Fernsehen ein. Berechtigte sind Autoren, Übersetzer und Verleger von schöngeistigen und dramatischen, journalistischen und wissenschaftlichen Texten, die der Urheber der VG Wort per Meldekarten angezeigt hat. Da mit Fotokopien, Audio- und Videocassetten jeweils Wort, Musik und Bild kopiert werden können, legen die beteiligten Verwertungsgesellschaften untereinander einen Verteilschlüssel für die <t-1>daraus erzielten Einnahmen fest, dem empirische Teststudien des <t$>kopier<\h>baren und vermieteten Materials zu Grunde gelegt werden. Seit der Reform des Urheberrechts von 1972 steht den Autoren und Verlagen auch für die Ausleihe ihrer Werke in öffentlichen Bibliotheken eine Vergütung zu. Weitere Einnahmen kommen aus Lesezirkelvergütungen für das Ausleihen und Vermieten von Werken, Vergütungen für die Nutzung von Artikeln in Pressespiegeln und für Nachdrucke in Schulbüchern.@1 fliess mit:Die Gebühren werden nach einem komplexen Verteilungssschlüssel einmal jährlich an die Autorinnen ausschüttet. Ein Teil der Einnahmen geht in die gesetzlich vorgeschriebenen Sozialeinrichtungen. Mehr als 220<\!q>000 Autoren und 5<\!q>500 Verlage haben Wahrnehmungsverträge mit der VG Wort abgeschlossen. Im vergangenen Jahr lagen ihre Einnahmen bei rund 127 Millionen Mark, von denen nach Abzug der Verwaltungs<\h>kosten 94 Millionen ausgeschüttet wurden.<@3 hoch fliess>58<@$p>@1 fliess ohne:<x@1 fliesshalbfett><t-0.5>VG Bild-Kunst<x@3 hoch fliess><t-0.5>59<@$p><t-0.5> – Bildende Künstler, Fotografen, Designer, Karikaturis<\h><t$>ten, Pressezeichner und Bildagenturen, sowie Filmproduzenten, Regisseure, Kameraleute, Cutter, Szenen- und Kostümbildner und Choreografen sind durch die 1968 gegründete <@1 fliess normal>Verwertungsgesellschaft Bild- Kunst<@$p> vertreten. Die Tantiemenquellen sind im Wesentlichen dieselben wie bei der VG Wort, hinzu kommen Museen, Kunstvereine und Artotheken. Eine weitere Besonderheit ist das Folgerecht, das Recht des bildenden Künstlers auf prozentuale Beteiligung am Erlös aus der Weiterveräußerung seines Werkes im Kunsthandel. Da er das Original seines Kunstwerks nur einmal verkaufen kann, wäre er ohne dieses Folgerecht von Wertsteigerungen am Kunstmarkt ausgeschlossen. 1992 nahm die VG Bild-Kunst aus Folgerechten Gebühren in Höhe von 4,2 Millionen Mark ein. <@1 fliesshalbfett>Internationale Dachverbände <@$p>– Zur Wahrnehmung der ausländischen Autorenrechte haben sich die Verwertungsgesellschaften zu europäischen und internationalen Vereinigungen zusammengeschlossen. Der Dachverband aller VGs ist die bereits genannte CISAC. Im Musikbereich vertritt das <@1 fliess kursiv>Bureau International des Sociétés gérant les Droits d‘Enregis<\h>trement et de Reproduction Mécanique <@$p>(BIEM) die Interessen der Urheber im mechanischen Recht. Das BIEM handelt mit dem internationalen Verband der Plattenindustrie IFPI ein Rahmenabkommen aus, demzufolge 9,009 Prozent des Händlerabgabepreises als Gebühren abzuführen sind. Weitere Dachverbände sind die GESAC (<@1 fliess kursiv>Groupement Européen des Sociétés d‘Auteurs et Compositeurs<@$p>) und EVA (<@1 fliess kursiv>European Visual Artists<@$p>). @1 fliess mit:Mit der Zunahme des internationalen Handels mit geistigem Eigentum wird der Ruf nach einer zentralen Rechte-Clearingstelle laut. Eine solche Clearingstelle würde in einer gewaltigen Datenbank Informationen über möglichst alle Werke und Leistungen bereithalten, über die daran bestehenden Rechte, über die Rechteinhaber und die Bedingungen, unter denen sie zu einer Lizenzierung bereit sind. Die Clearingstelle könnte stellvertretend direkt einen Linzenzvertrag mit dem Produzenten abschließen oder die Anfrage an den individuellen Rechteinhaber weiterleiten. Ein erster Schritt dazu ist das 1995 in Kooperation der GEMA, der französischen Verwertungsgesellschaften im Urheber- und mechanischen Recht <@4 Pfeil (Umschalt/Alt #)>’<@$p>SDRM/<@4 Pfeil (Umschalt/Alt #)>’<@$p>SACEM<@3 hoch fliess>60<@$p> und der britischen <@4 Pfeil (Umschalt/Alt #)>’<@$p>MCPS<@3 hoch fliess>61<@$p> gegründete <@1 fliess kursiv>Bureau for European Licensing<@$p>. Einen umfassenderen Ansatz stellt das im Aufbau befindliche <@1 fliess kursiv>Common Information System<@$p> der CISAC dar.<@3 hoch fliess>62<@$p>@2  ZÜ 3:Rechteindustrie@1 fliess ohne:Als treibende Kraft seit den Anfängen des Urheberrechts wirken die Verlage. Aus dem Handwerk, das zu Beginn noch die Funktionen von Verleger, Drucker und Buchhändler in einer Person vereinigte, ist über verschiedene Medientechnologiegenerationen hinweg das geworden, was nach der Frankfurter Schule »Kulturindustrie« heißt. Der Karlsruher Wissensphilosoph Helmut Spinner nennt ihre korporativen Akteure zeitgemäßer die »Datenherren« (<@6 Caps>Spinner<@$p>, 1998, S. 28). Generisch wird sie als »Rechteindustrie«, im Zusammenhang mit dem Internet auch als »Content-Industrie« bezeichnet, um anzugeben, dass ihre Produkte nicht Romane, Operetten, Sciencefiction-Filme oder Kinderserien sind, sondern die Verwertungsrechte daran, die Ware Inhalt, die die Medienkanäle f<t-1>üllt. Viele Künstler wie Courtney Love verbitten es sich ausdrücklich, ihre <t$>Arbeit als »Content« zu bezeichnen. Diese Rechte erwerben die Unternehmen von den Autoren und verwerten sie entweder selbst, wie das in der Printindustrie üblich ist, oder lizenzieren sie an einen Hersteller, <\n>etwa als Musikverlag an eine Plattenfirma oder an Dritte, wie z.B. an <@1 fliess kursiv>McDonalds<@$p>, die die Figuren des jeweils neuesten Disney-Films in ihrem Marketing einsetzen. @1 fliess mit:Die Rechteunternehmen sind <@1 fliess kursiv>Gatekeeper<@$p>, wie Courtney Love schrieb, zwischen den Kreativen und den Rezipienten ihrer Werke. Wenn sie etwas produzieren, dann ist es Massenaufmerksamkeit und die Kontrolle über ihren Rechtebestand. Auch sie genießen einen Schutz für ihre Leis<\h>tungen, aber eben keine Urheberrechte, sondern so genannte Leistungsschutzrechte, die Teil der »verwandten Schutzrechte« aus dem Titel des Urhebergesetzes sind. Es handelt sich zwar nur um abgeleitete Rechte, dennoch sind die primär berechtigten Autoren auf die Verwerter angewiesen, um ihre Werke verkaufen zu können. Das Land des Wissens wird von Urhebern und Rezipienten bevölkert – regiert wird es jedoch von den Datenherren. Diese Rechteindustrie ist in den letzten 20 Jahren nach Angaben der <@1 fliess kursiv>International Intellectual Property Alliance <@$p>(IIPA) in den USA mehr als doppelt so schnell gewachsen wie der Rest der Wirtschaft. Laut ihres Jahresberichts 2000 trug die gesamte Copyright-Industrie 1999 677,9 Milliarden Dollar oder 7,33 Prozent zum Bruttosozialprodukt der USA bei. Gegenüber 1998 stellte dies einen Zuwachs von 9,9 Prozent dar.<@3 hoch fliess>63<@$p> Sie stellt einen bedeutenden Teil der Arbeitsplätze, produziert mehr Exportgewinne als die Chemie-, Elektronik- oder Autoindustrie. Die IIPA muss es wissen. Sie ist eine Koalition von US-amerikanischen Rechteindustrievereinigungen, die zusammen mehr als 1<\!q>350 Unternehmen repräsentiere<t-1>n.<x@3 hoch fliess><t-1>64<@$p><t-1> Zusammengeschlossen haben sie sich, um sich für den Schutz ihrer Besitzstände gegen Rechte-»Piraterie« zuhause und im Ausland einzusetzen. Derzeit richtet sich die IIPA vor allem gegen die »inadequaten Copyright-Praktiken« in der Ukraine, in Brasilien, Russland und Uruguay. <t$>In anderer Konstellation und mit dem immer wieder hervorgehobenen Ziel der effektiven Selbstregulierung hat sich die Industrie im <@1 fliess kursiv>Global Business Dialogue on Electronic Commerce<@3 hoch fliess>65<@$p> (GBDe) zusammengefunden. Der GBDe repräsentiert eine weltweite Kooperation zwischen Vorstandsmitgliedern und leitenden Angestellten von Hunderten Unternehmen, die im Bereich des E-Commerce tätig sind. Der GBDe, der sich auf einer Konferenz in Paris im September 1999 konstituierte,<@3 hoch fliess>66<@$p> sieht die Herausforderung besonders in der grenzüberschreitenden Natur des Internet und setzt, wo immer möglich, auf technologische Lösungen. Der Umfang und die Gewichtung seiner Tätigkeit wird aus seinen neun Arbeitsgruppen (unter Leitung von) ersichtlich: Authentifizierung und Sicherheit (NEC Corporation), Vertrauen der Verbraucher (DaimlerChrysler), Content (The Walt Disney Company), Informations-Infrastruktur/Marktzugang (Nortel Networks), Geistige Eigentumsrechte (Fujitsu Limited), Gerichtsbarkeit (EDS), Haftung (Telefónica), Schutz persönlicher Daten (Toshiba Corporation), Steuern/Zölle (Deutsche Bank).Zur Frage von nicht autorisierten Inhalten im Internet will der GBDe unter dem Vorsitz von zwei Disney-Angestellten einen allgemeinen Selbstregulierungsrahmen für freiwillige <@1 fliess kursiv>Notice and takedown<@$p>-Prozeduren entwickeln, nach denen ein Internet-Anbieter Inhalte seiner Nutzer von seinen Servern entfernt, wenn er davon unterrichtet wird, dass sie vorgeblich gegen geistige Eigentumsrechte verstoßen. Dabei sollen auch Haftungsfragen berücksichtigt werden.<@3 hoch fliess>67 <@$p>Bei all dem immer wieder betonten Willen zur Selbstregulierung durch von der Industrie selbst eingeführte Verhaltensregeln und von ihr entwickelte Technologien richtet sich der GBDe sehr wohl auch an Regierungen, Verwaltungen, Parlamente und internationale Organisationen. Hier ist sein Ziel die rasche Unterzeichnung der WIPO-Copyright-Verträge, die Implementierung des TRIPs-Abkommens und die Angleichung des geistigen Eigentumsschutzes weltweit.Die Bundesbürger gaben laut Baseler Prognos-Institut 1996 130 Milliarden Mark für Medien und Kommunikation aus. 2010 sollen es schon 235 Milliarden sein. Für 1997 nannte Hubert Burda, Präsident des Verbandes Deutscher Zeitschriftenverleger, einen Gesamtumsatz der deutschen Medienindustrie von 160 Milliarden Mark. Unter den 50 größten Medienkonzernen der Welt fanden sich 1996 acht mit Sitz in Deutschland. Bertelsmann rangierte auf Platz 2 (hinter Time-Warner), die ARD auf Platz 8, Kirch auf Platz 20. Zu den weiteren Mediengrößen gehören die Springer AG, die WAZ-Gruppe, Bauer, Holtzbrinck und das ZDF.<@3 hoch fliess>68 <@$p>Bertelsmann war der weltgrößte integrierte Rechtekonzern, bis das Verlags- und TV-Unternehmen <@1 fliess kursiv>Time Inc.<@$p> 1989 das Film- und Musikunternehmen <@1 fliess kursiv>Warner Communications<@$p> übernahm. 1999 war das Jahr der »Elefantenhochzeiten« unter Telekommunikations-, Kabel-, Software- und Content-Unternehmen. Time-Warner wurde – in einem symbolischen Akt der Machtübernahme der alten Medien durch die digitalen Netze – Anfang 2000 vom weltgrößten Internet-Zugangsanbieter America On <\h>Line (AOL) geschluckt. Kurz darauf bemühte sich das Mega-Konglomerat um die britische Plattenfirma EMI, die Bands wie die <@1 fliess kursiv>Rolling Stones<@$p> und die <@1 fliess kursiv>Spice Girls<@$p> unter Vertrag hat.Die Rechteindustrie ist also durch eine hochgradige Konzentration gekennzeichnet. In der Musikbranche etwa teilen fünf Hauptakteure (Bertelsmann Music Group, PolyGram, Sony, Warner Chappell und EMI) den Weltmarkt mit 40 Milliarden Dollar unter sich auf. Ein Drittel der Einnahmen wird in den USA generiert, ein Drittel in Europa. In den vergangenen zehn Jahren hat sich der Markt vervierfacht. Zugleich beklagt die Industrie Einnahmeverluste durch privates Kopieren auf Cassetten (»Hometaping Kills Music«) und seit jüngstem auf CDs. Auch in Deutschland läuft eine Kampagne »Copy Kills Music«, die von der GEMA und dem Deutschen Musikrat getragen wird.<@3 hoch fliess>69<@$p> Ein weiteres Charakteris<\h>tikum ist die Tendenz zur medienübergreifenden Integration. Eine geeignete Konzernstruktur vorausgesetzt, kann ein Roman als Kinoverfilmung, Sound-Track, TV-Serie, Computerspiel, Web-Site, Kaffeetasse, Marketing für andere Produkte und vieles mehr maximal verwertet werden. Originäre Werke können so aus einer Hand für die verschiedenen Medienformate aufbereitet werden, die sich gegenseitig in der Aufmerksamkeit des Publikums verstärken.Die Digitalisierung der Medienbestände läutet eine neue Runde der Verwertung ein. Doch noch tun sich die alten Medien schwer mit den neuen. Die Filmstudios halten die wertvollsten Copyrights, doch ist es ihnen bislang am wenigsten gelungen, sie mit Hilfe digitaler Distribution gewinnträchtig umzusetzen. Dass eine Online-Distribution von Musik möglich ist, haben Netzfirmen wie <@1 fliess kursiv>MP3.com<@$p> der Musikindustrie vorgemacht, doch auch diese ist bei eigenen Schritten noch zögerlich. <*h"mehr">Entscheidend für diesen Akteur ist die Kontrolle möglichst über die gesamte »Wertschöpfungskette« der Rechteflüsse. Dafür ist mit weiteren Integrationsbewegungen von Unternehmen, Rechteportofolios, Technologien und rechtlichen Instrumenten zu rechnen. Tim Berners-Lee, der Vater des <@4 Pfeil (Umschalt/Alt #)>’<@1 fliess kursiv>World Wide Web<@$p>, unterscheidet vier Schichten des Netzes: das Übertragungsmedium, die Computerhardware, die Software und die Inhalte – er warnt eindringlich vor Versuchen ihrer vertikalen Integratio<t-1>nen. Monopolbildungen innerhalb einer Schicht seien weniger gefährlich, <t$>weil sichtbarer. Schichtübergreifende Integration könne heimtückischer sein und beeinflusse die Qualität von Information: »Aus Sicht der Softwareentwicklung ist [die Unabhängigkeit der Schichten] das Grundprinzip der Modularität. Aus wirtschaftlicher Sicht ist es die Trennung von horizontal konkurrierenden Märkten von der nicht konkurrierenden verti<\h>kalen Integration. Aus Informationssicht bedeutet dies redaktionelle Unabhängigkeit und Neutralität des Mediums« (<@6 Caps>Berners-Lee<@$p>, 1999, S.<\!q>192). <*h"Standard">Bei der Bildung des Konglomerats AOL-Time-Warner-EMI sicherten alle Beteiligten die Unabhängigkeit der Bestandteile zu. Doch auch der <@1 fliess kursiv>Merger<@$p> von Disney und ABC hat, trotz gegenteiliger anfänglicher Beteuerungen, sehr wohl dazu geführt, dass Disney seinen <@1 fliess kursiv>»Content« <@$p>verstärkt in die Fernsehkanäle von ABC gespielt hat. Ein weiteres Beispiel für die Bildung von Wissensmonopolen ist Bill Gates’ Unternehmen <@1 fliess kursiv>Corbis<@$p>. 1989 gegründet, hält es die Rechte an mehr <t-1>als 65 Millionen Bildern – die weltgrößte Sammlung digitaler Bild. <x@1 fliess normal><t-1>Corbis <@$p>lizenziert seine Bestände an kommerzielle Nutzer, wie Bildredaktionen von Zeitungen und Zeitschriften,<@3 hoch fliess>70<@$p> und an Konsumenten, die sie als Fotodrucke im Postkarten- oder Posterformat und als Download für die eigene Webseite bekommen können.<@3 hoch fliess>71<@$p> Die Bilder sind mit digitalen Wasserzeichen versehen, so dass ihre lizenzkonforme Nutzung überwacht werden kann. Zum <@1 fliess normal>Corbis<@$p>-Bestand gehören Werke von Fotografen wie Ansel Adams und Annie Griffith Belt, die digitalen Bildrechte an Sammlungen wie die der Nationalgalerie London oder der Hermitage in St.<\!q>Petersburg, das Fotoarchiv von <@1 fliess kursiv>United Press International<@$p> (12 Millionen Bilder) und die Fotos von Unterhaltungsstars und anderen Berühmtheiten des angekauften <@1 fliess kursiv>Lynn Goldsmith International<@$p>. Im Juni 1999 kaufte Corbis die Pariser Bildjournalismus-Agentur Sygma, die größte der Welt, mit Beständen von mehr als 40 Millionen Fotos.<@3 hoch fliess>72<@$p> Ob ein Foto in den Zeitschriften <@1 fliess kursiv>Newsweek<@$p> oder <@1 fliess kursiv>Stern<@$p>, einer Werbebroschüre oder auf einer Webseite auftaucht – die Wahrscheinlichkeit ist hoch, dass es Bill Gates gehört. @2  ZÜ 3:Öffentlichkeit@1 fliess ohne:Die gleiche strukturelle Interessenopposition wie zwischen Autoren und Rechteindustrie besteht zwischen Rechteindustrie und Medien-»Verbrauchern«. Erstere möchte möglichst teuer verkaufen, Letztere möglichst billig kaufen, im Idealfall umsonst. Doch anders als die anderen Parteien hat die Öffentlichkeit keine organisierte Interessenvertretung im Kräfteringen um die juristische, wirtschaftliche und technische Neuordnung des Wissens nach seinem Eintritt in den <@1 fliess kursiv>Cyberspace.<@$p> Dezidierte Konsumentenschutzorganisationen, die systematisch Lobbyarbeit für die Interessen der Endnutzer von digitalen Medienprodukten betreiben würden, gibt es nicht. @1 fliess mit:Eine Gruppe, die einer solchen Organisation am nächsten kommt, ist die <@1 fliess kursiv>Electronic Frontier Foundation <@$p>(<@4 Pfeil (Umschalt/Alt #)>’<@$p>EFF).<@3 hoch fliess>73<@$p> 1990 gegründet, engagiert sie sich vor allem für die Redefreiheit und den Datenschutz im Cyberspace. Sie liefert Expertisen für Gesetzgebungsverfahren, betreibt Öffentlichkeitsarbeit und organisiert Rechtsschutz, um die Grundrechte einschließ<\h>lich der Rechte der Urheber und den freien Fluss der Information bei der Formation neuer Technologien wie Kryptologie-Infrastrukturen und DVD zu erhalten. Die EFF unterhält dazu u.a. eine umfangreiche Ressourcen-Sammlung zu Copyright-, Patent- und Warenzeichenfragen.<@3 hoch fliess>74 <@$p>Sie arbeitet mit ähnlichen Gruppen, wie der <@1 fliess kursiv>Global Internet Liberty Campaign <@$p>(GILC)<@3 hoch fliess>75<@$p> und der <@1 fliess kursiv>Internet Free Expression Alliance <@$p>(IFEA)<@3 hoch fliess>76<@$p> zusammen und hat <@1 fliess kursiv>Electronic Frontier-<@1 fliess normal>Organisationen in Australien<@$p>,<@3 hoch fliess>77<@$p> Kanada<@3 hoch fliess>78<@$p> und Italien<@3 hoch fliess>79 <@$p>inspiriert. In Deutschland ist es vor allem der »<@1 fliess normal>Förderverein Informationstechnik und Gesellschaft«<@$p> (FITUG),<@3 hoch fliess>80<@$p> der sich in Sachen Netz und Politik in die öffentliche Debatte einschaltet. Stellvertretend für die Öffentlichkeit engagieren sich die professionellen Wissensvermittler für einen ungehinderten Zugang zu Wissen, <\n>allen voran die Bibliothekare und Archivare. In Europa ist es der Dachverband <@1 fliess kursiv>European Bureau of Library, Information and Documentation Associations<@$p> (EBLIDA),<@3 hoch fliess>81<@$p> der sich unter dem Motto »Lobbying for Libraries« in die Urheberrechtspolitik einschaltet, um die Schrankenbestimmungen, auf denen ihr öffentlicher Auftrag beruht (Erstverkaufsdoktrin und Verleihrecht), in den digitalen Wissensraum hinüberzuretten. In den USA ist neben der <@1 fliess kursiv>American Library Association <@$p>(ALA)<@3 hoch fliess>82<@$p> vor allem die <@1 fliess kursiv>Association of Research Libraries <@$p>(ARL)<@3 hoch fliess>83<@$p> eine der konsistentesten Stimmen in der öffentlichen Debatte, mit dem Ziel »einen fairen Zugang sowie eine <\n>effektive Nutzung des aufgezeichneten Wissens im Dienste von Unterricht, Forschung, Bildung und sozialen Dienstleistungen zu fördern«. Wie das EBLIDA ist das Tätigkeitsspektrum der ARL sehr viel breiter, doch Copyright-Fragen stehen natürlich auch hier im Zentrum.<@3 hoch fliess>84<@$p>Auch Wissenschaft und Bildung haben besondere Privilegien aus dem Urheberrecht und Copyright zu verteidigen (Kopierrechte für den Schul- oder Unterrichtsgebrauch und für den wissenschaftlichen Gebrauch). In Großbritannien befasst sich die <@1 fliess kursiv>Copyright in Higher Education Workgroup <@$p>(CHEW) mit den finanziellen und pädagogischen Barrieren für das Lehren und Lernen durch den restriktiven Einsatz des Copyrights.<@3 hoch fliess>85<@$p> Derzeit stehen die Auseinandersetzungen um den <@1 fliess kursiv>Higher Education Copying Accord<\!q><@3 hoch fliess>86<@$p> im Zentrum der Aktivitäten von CHEW, den die Hochschulen mit der <@1 fliess kursiv>Copyright Licensing Agency<@$p>, in der die britischen Verwertungsgesellschaften zusammengeschlossen sind, aushandeln. Eine deutschsprachige Ressourcensammlung zu Urheberrecht in Schule und Hochschule bietet das Juristisches Internet-Projekt Saarbrücken, Abteilung Urheberrecht, an.<@3 hoch fliess>87<@$p> Besonders der Fernunterricht ist durch neue »Digital-Gesetze« vor Herausforderungen gestellt.<@3 hoch fliess>88<@$p>Die Journalisten gehören natürlich zu den Autoren, haben aber andererseits ebenfalls einen besonderen öffentlichen Auftrag der Wissensvermittlung, zu deren Erfüllung ihnen das Urheberrecht gleichfalls eine <t-1>Sonderstellung einräumt (Zitierrecht, Wiedergabe von Reden, Auskunfts<\h>r<t$>echt gegenüber öffentlichen Instanzen, Zeugnisverweigerungsrecht). Zensur und Behinderung dieses Auftrags prangert die IG Medien sehr wohl an, doch in Bezug auf die Urheberrechtsentwicklung sind es ausschließlich die finanziellen Interessen ihrer Mitglieder, die sie durch eine Stärkung des Urhebervertragsrechts zu sichern sucht. Auf verschiedenen Ebenen finden sich inzwischen auch Ansätze zu einem Lobbying durch die Bewegung der freien Software. Im europäischen Kontext ist die von der Generaldirektion Informationsgesellschaft initiierte <@1 fliess kursiv>Working Group on Libre Software<\!q><@3 hoch fliess>89<@$p> aktiv, die Empfehlungen an die Europäische Kommission erarbeitet, die auch Urheberrechtsfragen berühren. Mit einer anhaltenden Kampagne und etwa 80<\!q>000 Unterschriften gegen die Einführung der Patentierbarkeit von Software in Europa haben sich die EuroLinux-Allianz<@3 hoch fliess>90<@$p> und ihr Mitglied, der Förderverein für eine Freie Informationelle Infrastruktur (FFII), hervorgetan.<@3 hoch fliess>91<@$p>In den USA ist schließlich noch die <@1 fliess kursiv>Home Recording Rights Coalition<@$p> (HRRC)<@3 hoch fliess>92<@$p> zu nennen, eine Koalition aus Konsumenten, Einzelhändlern und Herstellern von Audio- und Videogeräten mit Sitz in Washington. Gegründet wurde die HRRC im Zusammenhang mit dem Rechtsstreit Sony gegen Universal in den 70er-Jahren. Das Kinounternehmen hatte Sony auf Beihilfe zum Copyright-Verstoß verklagt, weil seine Betamax-Videorekorder benutzt wurden, um Fernsehsendungen aufzuzeichnen. Das US-Verfassungsgericht etablierte schließlich 1984 das Recht, im privaten Haushalt Kopien zum zeitversetzten Betrachten von Sendungen anzufertigen. Die HRRC tritt seither regelmäßig für das »Grundrecht« auf <@1 fliess kursiv>home taping <@$p>ein und preist sich damit, mehrfach dazu beigetragen zu haben, Versuche des Kongresses abzuwehren, eine als »Steuer« bezeichnete Tantiemenpauschale für Rekorder und Leermedien einzuführen.Alle genannten Gruppen sind wesentlich für die Förderung und Erhaltung der Wissensinfrastruktur, aus der alle kreativen Werke hervorgehen. Doch wie sich im Folgenden immer wieder zeigen wird, haben sie in der Auseinandersetzung um die Neustrukturierung der Wissensordnung unter digitalen Bedingungen die schwächste Position unter den vier genannten Akteuren. <\c>@2  ZÜ 1:<\3>Medientechnologie und ihre Digitalisierung@1 fliess mit:@1 fliess ohne:<*h"mehr">Die rechtliche Ordnung des Wissens umfasst – stärker in der Autorenrechtstradition – auch philosophische Aspekte der Beziehung zwischen dem Urheber, seinem Werk und dessen Lesern. Für die rechtspraktische Auseinandersetzung mit dieser flüchtigen immateriellen Materie stehen jedoch die Technologien der Reproduktion und der Vermittlung des Wissens und die damit einhergehenden wirtschaftlichen Gepflogenheiten im Zentrum. Gesetzgebung und Rechtssprechung in diesem Bereich setzten mit der Erfindung des Buchdrucks ein und veränderten sich mit jeder neuen Generation von Medientechnologie. Anfangs schützte das Urheberrecht/Copyright nur literarische Werke. Es folgte der Schutz von Musik, erst von Noten für öffentliche Aufführungen von Musikwerken und ab Ende des 19. Jahrhunderts auch von Schallplatten. Neue Technologien zur Bildreproduktion sowie die Fotografie führten wiederum zu einer Änderung des Rechts. Der Tonfilm konfrontierte es erstmals mit dem, was wir heute unter dem Begriff »Multimedia« kennen. Darin verbinden sich vielfältige Einzelrechte, wie die von Skript<\h>autor und Komponist, die der ausführenden Künstler (Schauspieler und Musiker), der Kameraleute an ihrer bildlichen Gestaltung, der Urheber von Werken der bildenden oder der Baukunst, die abgefilmt werden sowie die des Regisseurs an der Gesamtkomposition eines Filmwerkes. Die unterschiedlichen Rechtsnormen und die betroffenen Institutionen wie die verschiedenen Verwertungsgesellschaften mussten in Einklang gebracht werden. Die Rundfunktechnik schließlich machte einen Schutz von Text- und Tonwerken, die Fernsehtechnik auch von Filmwerken bei deren Ausstrahlung sowie einen Schutz der Werke selbst vor unautorisierter Weiterverbreitung erforderlich. Die am Text und am gutenbergschen Verlagswesen gebildeten Konzepte wirken bis heute: Als Computerprogramme in den 1970ern erstmals Urheberschutz erhielten, wurden sie ebenfalls als »literarische Werke« klassifiziert. @1 fliess mit:Bis zur Mitte des 20. Jahrhunderts setzte die Anfertigung von Reproduktionen in nennenswertem Umfang Investitionsgüter wie Druckmaschinen, Plattenpresswerke oder Filmkopieranlagen voraus. Rechtsstreitigkeiten wegen unautorisierten Kopierens richteten sich also nicht gegen Privatpersonen, sondern gegen »schwarze Schafe« innerhalb derselben Zunft. Dies änderte sich mit der Markteinführung von Konsumententechnologien, die nicht nur zum Abspielen etwa von Schallplatten, sondern auch zur Aufnahme und Vervielfältigung geeignet waren. Das Tonband in den 1960ern und das 1964 von Philips eingeführte <@1 fliess kursiv>Compact Cassetten<@$p>-Format waren die ersten, für Privatpersonen erschwinglichen Technologien, mit denen sich Konzerte aufzeichnen (<@1 fliess kursiv>Bootlegs<@$p>), Radiosendungen mitschneiden und Schallplatten kopieren ließen. Gegebenes Material kann in eigenen Kompilationen zusammgestellt und mit Hilfe von Mehrspurtonbandmaschinen so weitgehend verändert werden, dass abgeleitete Werke entstehen. Die Musikindustrie wehrte sich zunächst <\n>gegen die Cassetten-Rekorder, ohne allerdings die Gesetzgeber zum Einschreiten bewegen zu können. Bald jedoch begann sie selbst, voraufgezeichnete Musik auf Cassette zu vertreiben. In den 80er-Jahren wurden mit dem Fotokopierer und dem Videorekorder weitere private Vervielfältigungstechnologien verfügbar. Das Urheberrecht/Copyright sah sich jetzt nicht mehr nur einer überschaubaren Zahl von Profis gegenüber, sondern einer ungleich größeren Zahl von Individuen, die potenziell rechteverletzende Nutzungen geschützter Werke vornehmen können. <t-0.6>Da es nicht praktikabel ist, von all diesen Einzelpersonen zu verlangen, dass sie für jede Kopie einen Nutzungsvertrag mit dem Rechtein<\h>haber<t$> <\h>abschließen, begegneten die Autorenrechtsländer der Herausforderung durch die Einführung von Pauschalvergütungen für Aufzeichnungs- und Ablichtungsgeräte, Leermedien und Fotokopien. Die Vergütung für das unvermeidliche Kopieren wird von Verwertungsgesellschaften eingezogen und an die Berechtigten weitergegeben. Damit sind die rechtlich in die Pflicht Genommenen auf eine überschaubare Zahl von professionellen Herstellern, Importeuren und Betreibern reduziert. Es ergibt sich zwar eine gewisse Unschärfe, da nicht im Einzelnen überprüft werden kann, welches Material kopiert wird und ob es sich dabei überhaupt um geschützte Werke Dritter handelt. Trotzdem ist diese Vorgehensweise ein handhabbarer Kompromiss, der einen fairen Interessenausgleich unter Urhebern, Privatnutzern, Geräteindustrie und Rechteindustrie darstellt. <t1>Auch in den Copyright-Ländern mit ihrer generellen Lizenzierungspflicht und <x@1 fliess kursiv><t1>Fair-Use<@$p><t1>-Ausnahmen gibt es Ansätze zu einer ähnlichen <\n>Lösung. Die Revision des US-Copyright-Gesetzes von 1978 führte gesetz<\h>liche Pflichtlizenzen für bestimmte Zweitausstrahlungen in Kabel<\h>fern<\h>sehen- und Satelliten-Direktübertragungssystemen sowie für Münzgeräte zum Abspielen von Schallplatten (<x@1 fliess kursiv><t1>Jukeboxes<@$p><t1>) ein. Für die Festsetzung der Gebührenhöhe wurde das <x@1 fliess kursiv><t1>Copyright Royalty Tribunal<@$p><t1> unter der Ägide der Kongress-Bibliothek etabliert. Das Inkasso und die Umverteilung an die Berechtigten übernimmt die Copyright-Registrierungsbehörde.<x@3 hoch fliess><t1>1<@$p><t1> Dabei handelt es sich allerdings nicht um private, sondern ausschließlich um kommerzielle Nutzungen. Pauschalgebühren für Aufnahmegeräte und Leermedien führte die Revision des US-Copyright-Gesetzes von 1992 ein (s.u.).<t$>Eine bahnbrechenden Entscheidung für den Bereich von privaten Videokopien fällte das US-amerikanische <@1 fliess normal>Verfassungsgericht 1984. Universal City Studios und Walt Disney Corp. re<@$p>ichten 1976 eine Klage gegen <@1 fliess normal>Sony Corp.<@$p>, den Hersteller von Betamax-Videorekordern, ein. Der Vorwurf lautete, Sony unterstütze Copyright-Verstöße, da die Käufer seiner Geräte unberechtigterweise Fernsehprogramme aufzeichnen können. Sony gewann den Prozess, das Berufungsgericht hob das Urteil jedoch auf und machte damit private Videorekorder im Wesentlichen illegal. Das oberste Gericht, der <@1 fliess kursiv>Supreme Court<@$p>, hob schließlich im Januar 1984 das Urteil des Berufungsgerichts auf. Kern der Entscheidung <@1 fliess kursiv>Sony v. Universal <@$p>ist, dass seither die Aufzeichnung von Fernsehsendungen zum Zwecke des zeitversetzten Betrachtens als ein legitimer <@1 fliess kursiv>Fair Use<@$p> angesehen wird.<@3 hoch fliess>2<@$p> In der folgenden Rechtsprechung wurde die <@1 fliess kursiv>Time-shifting<@$p>-Freiheit von 1984 um das Recht auf eigene Anordnung (Erstellung von <\h>Kompilationen von Stücken aus verschiedenen Platten) und auf Ortsverschiebung (z.B. eine Cassettenaufnahme einer rechtmäßig erworbenen Platte, um sie im Auto zu hören) erweitert.Technische Systeme zur Verhinderung bestimmter Nutzungen, die bei digitalen Medien eine zentrale Rolle spielen werden, sind auch schon für analoge Medien eingeführt worden. Ein solches <@1 fliess kursiv>Analog Protection Sys<\h>tem<@$p> (APS), das größere Verbreitung gefunden hat, ist <@1 fliess kursiv>Macrovision<@$p>,<@3 hoch fliess>3<@$p> das das Kopieren von Videobändern verhindert. Auf den Bändern (z.B. für den Verleih in Videotheken) sind bestimmte Markierungen gesetzt, die entsprechende Schaltkreise, die inzwischen in allen neueren Videorekordern eingebaut sind, ansprechen und ein Kopieren verhindern. Die Firma <@1 fliess kursiv>Macrovision<@$p> bezieht Lizenzgebühren für jedes Band und jeden Rekorder. Zwar führt die Technologie auf älteren Geräten auch beim reinen Abspielen solcher Bänder zu Störungen, doch nimmt die Rechteindustrie die Beschwerlichkeiten einiger ihrer Kunden inkauf, da sie damit ein Maß an Kontrolle über ihr Eigentum erhält, wie es bis dato unvorstellbar war. Bürger, die sich hinreichend über die Behinderung ärgern, finden im Einzelhandel Geräte für etwa 150 Mark, die den Kopierschutz aufheben.<t-2>Das gleiche Schicksal erfuhr auch eine Schutztechnologie für das Satellitenfernsehen. In den 70ern wurden Fernsehsatelliten als Verteiltechnologie verwendet, um Programme zur Weiterverbreitung an lokale Kabel-TV-Inseln zu liefern. In den frühen 80ern entwickelte die General Instrument Corp. (GI) das <x@1 fliess kursiv><t-2>Scrambling<@$p><t-2>- oder »Zerhacker«-Protokoll <x@1 fliess kursiv><t-2>VideoCipher<@$p><t-2> (VC).<x@3 hoch fliess><t-2>4<@$p><t-2> Zur selben Zeit begannen die ersten Konsumenten, sich für noch stolze 10<\!q>000 Dollar Satellitenschüsseln zuzulegen und in den wachsenden Informationsstrom aus dem Himmel zu richten. Die Preise für die Empfangsstationen sanken, und was als <x@1 fliess kursiv><t-2>Business-to-Business<@$p><t-2> begonnen hatte, war zum Satelliten-Direkt-TV für den Endkunden geworden. 1986 begann <x@1 fliess kursiv><t-2>Home Box Office<@$p><t-2> (HBO), eine Tochter von Time Inc., sein Signal mit <x@1 fliess kursiv><t-2>VideoCipher<@$p><t-2> zu kodieren. Andere Satellitenprogramm-<\h>Anbieter wie CNN folgten.  GI vermarktete die Dekoder zunächst ausschließlich an Kabel-TV-Unternehmen. Konsumenten bot das Unternehmen, das die Exklusiv<\h>lizenz an der <x@1 fliess kursiv><t-2>VideoCypher<@$p><t-2>-Technologie und damit die vollständige Kontrolle über Hard- und Software inne hatte, die Geräte schlicht nicht an. Nach einem Aufschrei der Empörung brachte GI schließlich sechs Monate später VC II-Geräte für 700 Dollar auf den Markt. Jeder Dekoder verfügt über eine individuelle Kennung, die über einen Freischalt-Code angesprochen werden kann, der zusammen mit dem Fernsehstrom ausgestrahlt wird. Wer seine monatlichen Abonnementgebühren nicht bezahlt, dessen Bild bleibt verzerrt. Sechs Monate später war der erste Hack verfügbar: Ein kleiner Eingriff verschaffte Hunderten von Dekodern dieselbe Kennung. GI begann, den Autorisierungscode in kürzeren Abständen zu ändern. Auch die Hacker verfeinerten ihre Methoden. Es entstand ein ganzes Netzwerk aus Fernsehgeschäften, die die Manipulationen an den Dekodern vornahmen, und aus Kommunikationskanälen, über die die gehackten Codes verteilt wurden. Charles Platt (1994), der die Untergrundnetzwerke und ihre Wechselwirkungen mit den industriellen Akteuren sehr anschaulich schildert, schätzte die Zahl der Besitzer manipulierter Satelliten-Dekoder in den USA 1994 auf über eine Million. Ein ehemaliger GI-Mitarbeiter nennt eine Schätzung, derzufolge 80 Prozent der Satelliten-TV-Einzelhändler beteiligt seien. Die Freiheitsliebe, die sie dazu bringt, das Risiko einer Strafe von bis zu 500<\!q>000 Dollar und fünf Jahren Gefängnis auf sich zu nehmen, drückt einer von Platts Interviewpartnern so aus: <t$>@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»›Der Äther sollte für alle frei sein. Wenn ein Sendesignal unerlaubt über meinem Grund und Boden geht, dann sollte ich damit verdammt noch mal machen können, was ich will, da hat die Regierung sich nicht einzumischen.‹ [...] Die Piraten zitieren das Kommunikationsgesetz aus dem Jahre 1934, das die Nutzung von Stör- und Verschlüsselungsgeräten verboten hat und allen Amerikanern das Recht gab, jede Art von Radioübertragung zu nutzen. Diese Gesetzgebung basierte auf einem wichtigen Grundsatz: Das Radiospektrum ist eine begrenzte natürliche Ressource, wie Luft oder Sonnenlicht und jeder sollte Zugriff <\h>darauf haben« <@1 fliess normal>(<@6 Caps>Platt<@1 fliess normal>, 1994).<*t(25.512,0,"1  ")>@1 fliess ohne:Dieser gesetzliche Freiraum von 1934 wurde jedoch durch den <@1 fliess kursiv>Cable Communications Policy Act <@$p>von 1984 geschlossen. Seither ist die Verletzung privater verschlüsselter Satelliten-Kommunikation, damals vom Gesetzgeber noch für die Kabel-TV-Zulieferung gedacht, mit drakonischen Strafen belegt. @1 fliess mit:Es sieht also so aus, als würde die medientechnologische Entwicklung den freien Informationsfluss fördern. Audio- und Videocassettenrekorder sowie Fotokopierer erlauben es, mit Konsumentengeräten Informationen zu lesen und zu schreiben, sie nicht nur abzuspielen, sondern auch aufzuzeichnen, zu kopieren und in Grenzen zu modifizieren. Der Effekt wird sich beim Computer noch steigern. Er bewirkt eine Verbreitung von Medienkompetenz und damit der Voraussetzung für eine Teilnahme an den mediengestützten politischen und kulturellen Prozessen. Außerdem bringt er eine Fülle von »kleinen« Kommunikationen und von zwanglosen beiläufigen Kreationen hervor. Jeder ist ein Autor im medialen Austausch mit anderen. Vilém Flusser schrieb über die Lage nach der Einführung des Fotokopierers: »Der Mythos des Autors (und des Originals) verzerrt die Tatsache, dass Informationserzeugung ein Dialog ist ... Alle Autoren, Gründer, Stifter, Mosesse, Founding Fathers und Marxe (inklusive dem Göttlichen Schöpfer) sind angesichts der kybernetischen Verknüpfung der Dialoge und angesichts der Copyshops redundant geworden« (<@6 Caps>Flusser<@$p>, 1985, S. 83).Wenn der Kopierer die Autoren und mit ihnen die Monopole der Autorenrechte ins Wanken bringt, um wieviel mehr noch der Computer? Platts Fazit nach einer gründlichen Recherche über die vermeintlich so banale Welt des Satellitenfernsehens sieht so aus:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")><t-0.2>»Unterschätzen Sie niemals die Graswurzel-Opposition. [...] Für die meisten Menschen ist Datendiebstahl überhaupt kein moralisches Problem; es ruft noch nicht einmal Gewissensbisse hervor. Die Verbraucher sind heutzutage unbeeindruckt von den Rechten des Copyrights oder den damit verbundenen möglichen Strafen. [...] Tatsächlich geben Computer jedermann Macht. Wir haben das oft gehört, doch jetzt sehen wir, wohin das geführt hat. Seit acht Jahren bekämpft eine Handvoll Hacker einige große, wohlhabende  Unternehmen, und auch Hacker aus diesen reichen Unternehmen haben andere Unternehmen bekämpft – diese Unternehmen haben bisher nicht gewonnen. In einer Informationswirtschaft können die Pläne einer mächtigen Institution durchkreuzt werden oder sie kann sogar von ein paar cleveren, raffinierten Leuten, die einen Mikrocomputer haben, ruiniert werden« <x@1 fliess normal><t-0.2>(ebd.).<@$p><t-0.2>@1 fliess mit:@2  ZÜ 2:Digitalmedien@1 fliess mit:@1 fliess ohne:Mit dem Computer wird alles anders, sagen die einen. Eigentlich bleibt, mit einigen kleineren Anpassungen, alles beim Alten, sagen die anderen, zu denen vor allem die Bestandswahrer des Urheberrechts gehören. Eine sachlichere Betrachtung wird sich an die drei Grundoperationen eines jeden Mediensystems halten – Speichern, Übertragen, Modifizieren – und feststellen, dass sich alle drei durch den vernetzten Computer wandeln. Grundlegend neu sind das verlustfreie Kopieren von Werken, die Leichtigkeit, mit der die körperlosen Erscheinungsformen von Werken durchs Netz reisen, und ihre beliebige Modifizierbarkeit mit Hilfe der Universalmaschine. @1 fliess mit:@2  ZÜ 3:Offline-Medien@1 fliess mit:@1 fliess ohne:<t-1>Die <x@1 fliess kursiv><t-1>Compact Disc<@$p><t-1> (CD), 1982 von Philips und Sony eingeführt, war das <\n>erste für Konsumenten zugängliche digitale körperliche Übertragungsmedium. Die CD verwendet eine optische Speicherung, die mit einem Laser<\h>strahl abgetastet wird. Eine CD speichert ein Audio-Signal unkomprimiert mit einer Abtastrate von 44,1 KHz und einem Frequenzbereich von 2 bis 22 KHz. Neben den Audiodaten kann sie Zusatzinformationen wie Kennungen der Stücke, Aufnahmen und Rechteinhaber enthalten.<x@3 hoch fliess><t-1>5<@$p><t-1> Kleiner, handlicher und mit einer geringeren Abnutzung sollte sie die Vinyl-Schallplatte ablösen. Dies gelang so gründlich, dass 1991 japanische Zeitungen die Einstellung des letzten Vinyl-Presswerkes im Land vermeldeten. Audio-Connaisseure und eine neue DJ-Kultur verschafften der Schallplatte wenig später zwar ein neues Leben, doch die überwiegende Mehrzahl aller voraufgenommener Musik wird heute auf CD vertrieben. <t-2>@1 fliess mit:Die Computerwelt erschloss sich den neuen Massendatenträger mit einiger Verzögerung. 1986 fand auf Einladung von Microsoft die erste internationale Konferenz zur CD-ROM (<@1 fliess kursiv>Read-Only Memory<@$p>) in Seattle, Wa<\h>shington statt. Da die Computersoftware immer umfangreicher wurde, bot sich der neue Speicher als ideales Vertriebsmedium an. Seit den frühen 90er-Jahren werden viele PCs standardmäßig mit einem CD-ROM-Laufwerk ausgeliefert. Noch war die CD für die Endnutzerin ein reines Lesemedium. Die Errichtung einer CD-Produktion unter Reinraumbedingungen erforderte 1983 noch eine Investition von 50 Millionen Dollar. In den frühen 90ern brachte die HiFi-Industrie erste Haushaltsgeräte zum Schreiben von CDs auf den Markt, allerdings noch zu einem Preis, der außerhalb der Reichweite der meisten Konsumenten lag. Heute sind zwei Formate verfügbar, die einmal beschreibbare <@1 fliess kursiv>CD-Recordable<@$p> (CD-R) und die <@1 fliess kursiv>CD-Rewritable<@$p> (CD-RW), die sich wie eine Computerdiskette mehrfach beschreiben lässt.<@3 hoch fliess>6<@$p> Der neue optische Datenträger wurde auch für Bildinformationen erschlossen. Dazu gehören die FotoCD, die VideoCD<@3 hoch fliess>7<@$p> und die CD-i (interactive).<@3 hoch fliess>8<@$p> Auch die Hersteller von Videospielkonsolen gingen dazu über, ihre Spiele-Software auf CD auszuliefern. Das <@1 fliess kursiv>Digital Audio Tape <@$p>(DAT) ist ein digital beschreibbares Magnetband, das Mitte der 80er-Jahre von Sony und Philips entwickelt wurde, um den Konsumenten dieselbe Kopiermöglichkeit für CDs zu bieten, wie sie die Analog-Cassette für die Schallplatte darstellt. Bis dato waren Videorekorder (U-Matic, Beta, VHS) die einzigen Bandmaschinen, die in der für digitale Audioaufnahmen erforderlichen Geschwindigkeit Daten schreiben konnten. Tatsächlich wurde auch für DAT ein diagonal zum Band angeordneter Rotationsschreibkopf gewählt, wie er für Videocassettenrekorder entwickelt wurde. DAT speichert unkomprimierte Audiodaten in drei Auflösung: 16 Bit mit einer Abtastrate von 48 KHz, die von Studios verwendet wird, 16 Bit bei 44,1 KHz, dieselbe wie bei Audio-CDs, so dass bei einer Überspielung keine Konvertierung notwendig ist, und 12 Bit bei 32 KHz.<@3 hoch fliess>9<@$p> DAT-Geräte verfügen neben analogen auch über digitale Ein- und Ausgänge. So lassen sich perfekte Kopien erstellen, ohne dass die Daten erst verlustbehaftet in ein analoges Signal gewandelt werden müssten. Eine weitere Spur erlaubt das Einbringen von Sub-Code wie bei der CD. DAT wurde anfangs vor allem professionell im Studiobereich und bei Live-Aufnahmen eingesetzt, kaum jedoch für den Vertrieb von voraufgezeichneter Musik. DAT-Laufwerke werden auch als Backup-Speicher für Computerdaten eingesetzt. Die Cassetten sind dieselben, aber das Format <@1 fliess kursiv>Digital Data Storage <@$p>(DDS)<@3 hoch fliess>10<@$p> ist verschieden, so dass Audiorekorder und Computerlaufwerke die Bänder des anderen nicht ohne weiteres lesen können.Arbeiteten Sony und Philips bei CD und DAT zusammen, gingen sie 1992 getrennte Wege. Philips führte die <@1 fliess kursiv>Digital Compact Cassette<@$p> (DCC) ein, deren Geräte auch die ebenfalls von Philips entwickelten analogen Audio-Cassetten abspielen konnten. Anders als CD und DAT komprimiert DCC die Musikdaten, d.h. das dekomprimierte Signal enthält wen<t-1>iger Information als das Ausgangssignal. Entsprechend verlieren Kopien <t$>mit jeder Dekompression und Rekompression an Qualität. DCC konnte sich gegen die konkurrierenden Digitalformate nicht durchsetzen. Die Produktion wurde im Oktober 1996 eingestellt.<@3 hoch fliess>11<@$p>Ebenfalls 1992 brachte Sony die <@1 fliess kursiv>MiniDisc<@$p> (MD) als neues digitales Konsumenten-Audio-Format auf den Markt. MD verwendet eine überschreibbare magneto-optische Speicherung und eine verlustbehaftete Kompression mit einem Verhältnis von 6:1. Sony gesteht zu, dass die Qualität nicht an die CD heranreicht, aber zielt vor allem auf mobile Verwendung (MD-Walkman). Auch auf MD wird voraufgenommene Musik angeboten.<@3 hoch fliess>12<@$p> Die im November 1996 auf dem japanischen Markt eingeführte DVD gilt als das nächste universale digitale Speichermedium nach der CD in all ihren Varianten. Anfangs wurde sie für die großen Datenmengen von Videoinformation plaziert, weshalb DVD für <@1 fliess kursiv>Digital Video Disc<@$p> stand. Nachdem Anwendungen für Computersoftware und (zumindest die Option auf) Audio hinzukam, heißt sie heute <@1 fliess kursiv>Digital Versatile Disc.<@$p> Eine gewöhnliche DVD fasst mehr als zwei Stunden hochqualitatives Video, eine doppelseitige doppellagige DVD acht Stunden in hoher Qualität oder 30 Stunden in VHS-Qualität. Bis zu acht Audiospuren mit jeweils bis zu acht Kanälen können Synchronisationen in verschiedenen Sprachen und Surround-Sound-Systeme unterstützen. DVD ermöglicht eine nahtlose Verzweigung des Videostroms für so genannte interaktive Filme oder eine Auswahl aus bis zu neun Kamerapositionen bei der Wiedergabe. Neben den üblichen Sub-Code-Informationen können bis zu 32 Untertitel oder Karaoke-Spuren enthalten sein. Video wird meist mit verlustbehafteter MPEG-2-Kompression gespeichert, die bei der Darstellung gelegentlich Artefakte erzeugen kann. Wie die CD gibt es DVDs in verschiedenen physikalischen und Applikationsformaten. Die ersten beschreibbaren DVDs für Computerdaten kamen im Herbst 1997 auf den Markt.Nahm bei analogen Kopierverfahren die Qualität von Generation zu Generation rapide ab, so kann ich jetzt einige Kopien einer Musik-CD an Freunde verschenken, die wiederum Kopien weiterverschenken und so fort, bis schließlich die gesamte Weltbevölkerung damit versorgt ist, und die letzte Kopie wird originalgetreu wie das ursprüngliche Vertriebsstück klingen. Jede digitale Kopie ist ein Master. Der einzige »Mehrwert«, den die kommerzielle Kopie bietet, ist der Aufdruck auf der CD und das Book<\h>let. Doch wer einmal eine Viertelstunde neben den Farbkopierern eines Copyshops zugebracht hat, weiß, dass viele auch das nicht als ein wirkliches Problem sehen. Zu den Zentren der industriellen Fertigung von CD-Raubkopien gehören Hong Kong, Macau, Bulgarien und Russland. Davon betroffen sind nicht nur die Musik-, Film- und Softwareindustrien in den USA und Europa. Sehr viel massiver sind die Auswirkungen auf diese Industrien in den »Piratenländern« selbst. Eine eigenständige Softwarebranche kann sich nicht entwickeln, wenn z.B. russische Programme sofort nach der Markteinführung auf dem Schwarzmarkt für umgerechnet vier bis fünf Mark angeboten werden. Selbst die Filmindustrie in Hong Kong, die zweitgrößte nach Hollywood, leidet darunter, dass Filme zur selben Zeit oder noch bevor sie in den Kinos anlaufen für die Hälfte des Preises einer Eintrittskarte auf VideoCD vertrieben werden.<@3 hoch fliess>13<@$p>@2  ZÜ 3:Online-Medien@1 fliess ohne:<*h"mehr">Offline-Medien sind aus der Sicht der Rechteindustrie problematisch <\n>genug. Noch sehr viel ernsthafteres Kopfzerbrechen bereiten ihr die digitalen Kanäle für unköperliche Übertragung, allen voran das Internet. Die klassischen Rundfunkmedien senden von einem Zentrum an alle, wobei die Kontrolle über den Zugang zum Sender gewährleistet ist.<@3 hoch fliess>14<@$p> Mit individuellen Kennungen in den Dekodern wird es möglich, vom Sendezentrum aus jede einzelne Empfangsstation anzusprechen und so die Kontrolle auf die Endgeräte auszuweiten. Im Internet jedoch hat jeder Knoten nicht nur Zugriff auf alle verfügbaren Inhalte des gesamten Netzes, sondern kann auch selber »senden« – potenziell an alle anderen Knoten. <\h>Eine solche Bidirektionalität gab es bislang nur im Telefon, jedoch (mit Ausnahme von Konferenzschaltungen) beschränkt auf Punkt-zu-Punkt-Verbindungen, und, durch rechtliche Restriktionen inhaltlich stark eingeschränkt, im Amateurfunk.@1 fliess mit:<@1 fliess kursiv>Bulletin Board Systems<@$p> (deutsch: Mailboxen) waren zunächst lokale Strukturen für den Zweiwegaustausch von digitalen Informationen. In den 80er-Jahren entstanden jedoch mit Anbietern wie CompuServe und AOL bereits weltweite Strukturen. Die massenhafte Verbreitung des Internet ab Mitte der 90er schließlich machte es für fast jeden in der <\h>»Ersten Welt« erschwinglich, einen Netzknoten einzurichten und allen anderen Angeschlossenen eigene Inhalte zugänglich zu machen – oder eben geschützte Inhalte Dritter. Alle digitalen Werkarten können über das Internet weltweit übertragen (per <@4 Pfeil (Umschalt/Alt #)>’<@$p> ftp und E-Mail), ausgestrahlt (»gestreamt«) und zum Abruf auf Webservern bereitgestellt werden. <*h"mehr">Zu den Kanälen für die unkörperliche Verbreitung von digitalen Werken gehören ferner die terrestrische und die Satellitenaustrahlung von Radio- und Fernsehen (<@1 fliess kursiv>Digital Audio Broadcast<@$p> (DAB) und <@1 fliess kursiv>Digital Video Broadcast<@$p> (DVB)) und das Kabel-TV-Netz, dessen Digitalisierung und Zweiwegschaltung in Deutschland jedoch erst 1999 begann.<@3 hoch fliess>15 <@$p>Die Digitalisierung der Rundfunk- und Kabelinfrastruktur ist mit hohen Investitionen verbunden, die die Bildung von Allianzen aus Sendeunternehmen, Rechteindustrien und Telekommunikationsunternehmen motivieren. Der digi<\h>tale Telefondienst <@4 Pfeil (Umschalt/Alt #)>’<@$p> ISDN hat eine gewisse Relevanz für den Vertrieb von Werken, jedoch nur in Verbindung mit dem Internet. Mobiltelefonie wird erst mit neuen breitbandigen Standards wie <@4 Pfeil (Umschalt/Alt #)>’<@$p> UMTS genügend Kapazität aufweisen, um mehr als kurze Textbotschaften austauschen zu können.<*h"Standard">Das Netz erlaubt es, vier grundlegende Kostenfaktoren der Medienindustrie, die Kosten für Druck oder Pressung, für Lagerhaltung, Transport und für den Handel, die auch bei den physischen digitalen Speichern anfallen, drastisch zu reduzieren. Diese Attraktivität des Internet für die Medienindustrie wird einerseits durch die Schwierigkeit konterkariert, die Werke zu kontrollieren. Andererseits bereitet die eingeschränkte Übertragungskapazität des Internet Probleme beim Transport von hochauflösenden Musik- oder gar Videodaten. Die 4,7 <@4 Pfeil (Umschalt/Alt #)>’<@$p> Gigabyte einer DVD zu übertragen, würde über ein 56 <@4 Pfeil (Umschalt/Alt #)>’<@$p> Kbps-Modem mehr als eine Woche dauern. Die Lösung liegt in einer kräftigen Komprimierung. Die <@1 fliess kursiv>Moving Picture Experts Group <@$p>(MPEG)<@3 hoch fliess>16<@$p> ist eine Arbeitsgruppe der <@1 fliess kursiv>International Organization for Standards <@1 fliess normal>und der <@1 fliess kursiv>International Electrotechnical Commission <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’<@1 fliess kursiv> <@$p>ISO/IEC), die sich mit der Entwicklung und Standard<t-1>isierung von Codierungsverfahren für digitales Audio und Video befasst. <t$>Sie konstituierte sich 1988 im Zuge der Standardisierung von MPEG-1. Die Videokompression von MPEG-1 wird z.B. auf der Video-CD verwendet. MPEG-1 Layer 3 – kurz MP3 – ist der heutige Standard für die Musik<\h>codierung im Netz.MP3 wurde maßgeblich am Fraunhofer Institut für Integrierte Schaltungen (IIS) in Erlangen entwickelt.<@3 hoch fliess>17<@$p> 1987 begann das IIS im Rahmen eines EU-Projektes über psychoakustische Komprimierung zu forschen. Das Verfahren geht von der Beobachtung aus, dass bestimmte Klanganteile andere »maskieren«, diese also vom menschlichen Ohr nicht wahrgenommen und deshalb ohne subjektive Qualitätsreduktion eingespart werden können. Der Maskierungseffekt ist abhängig vom syntaktischen Gehalt des jeweiligen Musikstücks. Die Datenreduktion kann also nicht »mechanisch« vorgenommen werden, sondern setzt eine Analyse des Ausgangsmaterials voraus. In zahlreichen Hörtests verfeinerten die IIS-Forscher den Kompressionsalgorithmus. Schließlich gelang es ihnen, die 1,4 MBit/s einer CD ohne wahrnehmbaren Klangverlust um den Faktor 12 auf 112 KBit/s zu vermindern. Damit lässt sich hochqualitative Musik über die beiden 64 KBit/s-Datenkanäle des damals gerade eingeführten ISDN in Echtzeit übertragen. MP3 wurde von ISO/IEC als Standard übernommen und vom Frauenhofer-Institut patentiert. MP3 verbreitete sich Mitte 1997 wie ein Lauffeuer im Internet. Entgegen der üblichen Praxis der ISO-Standardisierung, gedruckte Versionen der Spezifikation zu verkaufen, hatte das Fraunhofer IIS nicht nur die Spezifikation, sondern auch eine Referenzimplementation des <@4 Pfeil (Umschalt/Alt #)>’<@$p> Encoders auf seinen Webseiten zur Verfügung gestellt. Sie wurde vielfach studiert und bald erschienen im Internet kostenlose En- und Decoder und ein Fülle von Musik im MP3-Format. Etwa ein Jahr später begann das IIS Abmahnungen an alle zu verschicken, die auf ihren Internetseiten freie MP3-Encoder anboten. Das Institut machte die Betroffenen darauf aufmerksam, dass das IIS ein Patent an der Technologie besitze. Nutzer hatten entweder eine Lizenz zu erwerben oder die Verbreitung ihrer Encoder einzustellen.<@3 hoch fliess>18<@$p> <t-0.4>Während das IIS die patentrechtlichen Ansprüche an seinem geistigen Eigentum geltend machte, beklagte die Musikindustrie die zunehmende Verletzung ihrer verwertungsrechtlichen Ansprüche, da viele der im Internet verbreiteten Musikstücke nicht von den Anbietern, sondern von kommerziell vertriebenen CDs stammten. Sie entdeckte Zehntausende illegale MP3-Stücke im Netz und behauptete Umsatzeinbußen. Die <x@1 fliess kursiv><t-0.4>Recording Industry Association of America <x@1 fliess normal><t-0.4>(<x@4 Pfeil (Umschalt/Alt #)><t-0.4>’<x@1 fliess kursiv><t-0.4> <@$p><t-0.4>RIAA) z.B. verklagte die Firma Diamond, die den ersten portablen MP3-Player namens »Rio« vorgestellt hatte und die Firma Lycos, die eine auf MP3-Dateien spezialisierte Suchmaschine betreibt. Sie schickt regelmäßig Abmahnungen an <x@4 Pfeil (Umschalt/Alt #)><t-0.4>’<@$p><t-0.4><\!q>ISPs,<t$> Universitäten und andere Netzbetreiber, auf deren Servern sie nicht lizenzierte Musikdateien entdeckt. Die RIAA startete ein Anti-»Piraterie«-Programm namens <@1 fliess kursiv>soundbyting.<@3 hoch fliess>19<@$p> Die deutsche Sektion der IFPI zog mit einer ähnlichen Kampagne nach (»Copy kills Music«).<@3 hoch fliess>20<@$p>Im Dezember 1998 schloss sich die RIAA mit ihrem japanischen Pendant RIAJ und der <@1 fliess kursiv>International Federation of the Phonographic Industries <@$p>(IFPI) zur <@1 fliess kursiv>Secure Digital Music Initiative <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’<@$p> SDMI)<@3 hoch fliess>21 <@$p>zusammen, um auf Hard- und Softwarehersteller, den Gesetzgeber und die Öffentlichkeit einzuwirken. Während sich die SDMI einerseits mit der Entwicklung von technischen Schutzsystemen für MP3 und anderen Datenformaten beschäftigt – das Fraunhofer IIS spielt hierbei eine führende Rolle –, betreibt sie andererseits eine beispiellose Kampagne, um MP3 pauschal zu verdammen, da es zu nichts anderem als dem »Raub« von geschützter Musik tauge. All die Musiker, die ihre eigenen Stücke im MP3-Format verschenken, um z.B. ein neues Album zu bewerben oder weil sie darin eine Chance sehen, die <@1 fliess kursiv>Gatekeeper<@$p>-Funktion der Musikindustrie zu umgehen, werden geflissentlich ignoriert.Die Vorteile des Internet kommen dann voll zum Tragen, wenn Autorinnen auf die Kontrolle über die Nutzung ihrer Werke verzichten. Junge Bands, die ihre Musik bekannt machen möchten, akademische Autoren, die ihre Texte für den globalen wissenschaftlichen Austausch verfügbar machen, Hobbyisten jeglicher Couleur, die anderen stolz ihre Kreationen zeigen wollen, oder Programmierer, die gemeinsam mit Tausenden anderer freie Software entwickeln – sie alle können die genannten Kosteneinsparungen ausnutzen und das seit Jahren, ohne dass sie erst darauf warten müssten, dass die Gerätehersteller, Hollywood und die Großen Fünf der Musikindustrie sich auf einen Schutzstandard einigen. <t-1>Der Verzicht auf die maximale Inanspruchnahme der Verwertungsmöglichkeiten, die ihnen das Urheberrecht/Copyright gibt, und die besondere S<t$>truktur des Netzes, in dem jeder Empfänger auch ein Sender ist, erlauben es den Urhebern, ihre Werke unter Umgehung von oligopolistischen Verlagen, Herstellern und Vertrieben direkt zu den Rezipienten zu bringen. Und wer Interessenten findet, wird – nach dem Shareware-Prinzip, durch den Postversand von eigenen CDs oder <@1 fliess kursiv>Printing-on-Demand <@$p>der eigenen Bücher – auch finanziell nicht leer ausgehen. @2  ZÜ 3:Sampling@1 fliess ohne:Auch das Material auf analogen Datenträgern wie Zelluloid und Magnetband lässt sich bereits modifizieren. Mit Hilfe von Retusche, Schnitt, Mehrspuraufnahmen, Effekten (Hall, Echo) usw. können aus eigenem oder dem Material Dritter neue abgeleitete Werke erstellt werden. Liegen die Daten jedoch in digitaler Form in einer programmgesteuerten Umgebung vor, sind ihren Modifikationsmöglichkeiten keine Grenzen gesetzt. Am dramatischsten trat dieser Effekt durch die digitale Bildbearbeitung ins öffentliche Bewusstsein, die inzwischen bei fast jedem Foto in Zeitungen und Zeitschriften vorgenommen wird. War die Vorstellung, dass Fotos nicht lügen, schon durch analoge Montage und Retusche fragwürdig, so trugen die spurlosen digitalen Manipulationen den verblie<\h>benen Glauben an die Authentizität eines Originals auf dem Friedhof der Mediengeschichte zu Grabe. Der Werkbegriff verändert sich grundlegend. Kleinste Werkbestandteile (ein Augenpaar, ein Gitarren-Riff, eine Handbewegung) können extrahiert, verändert und in eigenen Schöpfungen weiterverwendet werden. <@1 fliess kursiv>Sampling<@$p> wurde zum Inbegriff einer neuen – postmodernen – ästhetischen Praxis der 80er, nicht nur in der Musik, sondern auch im Graphik-Design, in Video und Text.@1 fliess mit:Der erste analoge Musik-Sampler, das <@1 fliess kursiv>Mellotron<@$p>, wurde 1963 erfunden, im selben Jahr, als Dr. Moog seinen ersten Synthesizer baute. Es bestand aus einer Klaviatur, hinter deren Tasten sich jeweils kurze Magnetbandschleifen befanden. Auf ihnen konnten beliebige Klänge (<@1 fliess kursiv>Samples<@$p>) aufgenommen und durch Anschlag der Taste wiedergegeben werden. Um 1980 herum folgte die erste Generation von digitalen Samplern. Wie das Mellotron waren sie so kostspielig, dass nur die größten Studios sie sich leisten konnten. Experimentelle Musiker wie Brian Eno arbeiteten damit. Künstler wie Bob Ostertag oder John Zorn verliehen dem Sampling die Ehrwürdigkeit der U-Musik. Sampler haben sich in den Studios und bei Live-Sets als Produktionshilfe und als Musikinstrument fest etabliert. Die Gruppe »Kraftwerk« entwickelten ihre Vision von den Kindern, die nach der Schule am Heimcomputer ihre eigene Musik basteln. Mit der Demokratisierung der Musikproduktionsmittel kündige sich die Zeit an, da Sampling zur neuen Volksmusik würde.Zu den weiteren Möglichkeiten, die die digitale Umgebung eröffnet, gehört die Durchsuch- und Verknüpfbarkeit der Information auf der eigenen Festplatte, im <@4 Pfeil (Umschalt/Alt #)>’<@$p> Intranet und im Internet. Die Verbindungen, die im analogen Wissensraum mühsam über Kataloge, Register, Zettelkäs<\h>ten oder das Gedächtnis hergestellt werden müssen, werden im digitalen durch Suchmaschinen, Links und algorythmische Operationen unterstützt. @2  ZÜ 3:Kopierschutz@1 fliess ohne:Schießlich erlauben es ihre programmierbaren Existenzbedingungen, Schutzsysteme für digitale Werke zu schaffen, die die Einhaltung der <\n>urheberrechtlichen und lizenzvertraglichen Nutzungsbedingungen technisch erzwingen. Die Daten von Bildern, Musikstücken, Videos und Texten werden in »kryptografische Umschläge« gehüllt und mit »Wasserzeichen« versehen, die entsprechend ausgerüstete Endgeräte steuern und ausschließlich die vom Rechteinhaber autorisierten Nutzungen zulassen. @1 fliess mit:Den Bemühungen der Rechteindustrie, solche Kontrollinfrastrukturen aus Hard- und Software zu errichten, auf die das folgende Kapitel genauer eingeht, stehen zwei grundsätzliche Probleme entgegen. Erstens ist im Prinzip jede Operation, die ein Computer vornimmt, von einem Computer auch wieder rückgängig zu machen, und muss es auch sein, da autorisierte Nutzer das Werk schließlich dafür bezahlt haben, es zu rezipieren.<@3 hoch fliess>22<@$p> Zweitens muss ein Werk bei noch so starkem kryptografischem Schutz während der Speicherung und Übertragung letztlich zu seiner legitimen Darstellung und Nutzung entschlüsselt werden. Spätestens dann ist es gegenüber Angriffen verwundbar. Lange bevor die traditionelle Medienindustrie begann, sich Gedanken über technischen Rechteschutz zu machen, war die Computersoftware-Industrie mit den neuen Herausforderungen und Möglichkeiten konfrontiert. Seit den 70ern entwickelte diese sozusagen im Computer heimische Branche immer ausgefeiltere Kopierschutzsysteme. Aber ganz gleich, ob sie auf Software oder Hardware <@1 fliess kursiv>(<@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Dongle)<@$p> basierten – Hacker benötigten nie sehr lange, um die Systeme zu brechen oder zu umgehen. Für diejenigen, die sich den Ladenpreis der Programme nicht leisten konnten, waren bald kopierschutzbefreite Versionen von »dBase« oder »Lotus 1-2-3« verfügbar. Unterdessen hatten zahlende Kunden darunter zu leiden, dass sie keine Sicherungskopien anfertigen konnten, die Programme von der Festplatte nicht ordnungsgemäß liefen oder sie ihren Dongle verlegt hatten. Der sich gegenseitig hochschaukelnde Wettlauf zwischen Schützern und Hackern und die wachsende Verärgerung der autorisierten Nutzer führte schließlich dazu, dass die Softwareverleger den Kopierschutz im Wesentlichen aufgaben. Keiner von ihnen ist darüber bankrott gegangen. Viele Nutzer sind bereit, Software regulär zu kaufen, wenn sie den Preis für angemessen halten und einen Mehrwert in Form von gedruckter Dokumentation und technischem Support erhalten. <*h"mehr">Im Falle der Musik-CD wurde die Entwicklung von der Geräte<\h>in<\h><\h>dus<\h>trie vorangetrieben. Die Rechteindustrie begann erst mit einiger Verzögerung, Kopierschutzmaßnahmen einzuführen. Dazu gehören Abweichungen vom Standard, Veränderungen im Dateiverzeichnis und irre<\h>führende Informationen im <@1 fliess kursiv>Error Correction Code <@$p>(ECC), die der Kopie Fehler hinzufügt, so dass sie bereits in der ersten Generation unlesbar werden kann. Programme, die die Date<@1 fliess normal>n Bit für Bit au<@$p>slesen und auf den Rohling schreiben (eine <@1 fliess kursiv>Image<@$p>-Kopie), umgehen viele dieser Maßnahmen. Auch gegen die meisten anderen Schutztechniken ist spezialisierte Kopiersoftware verfügbar.<@3 hoch fliess>23<@$p><*h"Standard">Anders als bei der Einführung von CD-Brennern war die Musikindus<\h>trie bei der Entwicklung der DAT-Technologie von Anfang an beteiligt. In den Verhandlungen mit der Geräteindustrie wurden verschiedene technische Systeme vorgeschlagen, darunter eine Geldkarte, von der bei jeder Kopie eines geschützten Musikstücks automatisch ein bestimmter Betrag abgezogen werden sollte.<@3 hoch fliess>24<@$p> Schließlich einigten sich die beiden Seiten auf das <@1 fliess kursiv>Serial Copy Management System <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’<@1 fliess kursiv> <@$p>SCMS). Die beiden SCMS-Kopier-Bits werden im Sub-Code-Kanal über das ganze Band geschrieben und können drei Werte annehmen:<*t(31.181,0,"1  ")>00	Kopieren unbeschränkt erlaubt,<*t(31.181,0,"1  ")>11	eine Folgegeneration von Kopien erlaubt,<*t(31.181,0,"1  ")>10	Kopieren verboten.@1 fliess ohne:Liest ein Rekorder aus der Vorlage den Wert 10, weigert er sich, eine Kopie anzufertigen. Liest er 11, setzt er den Wert auf der Kopie auf 10. Liest er 00, schreibt er abhängig vom Gerät wieder 00 oder 11. Bei einem analogen Eingangssignal ohne SCMS-Information schreibt er 11. CDs haben einen SCMS-Wert von 11, so dass die gesetzlich zugestandene Kopie für Datensicherung und ortsversetztes Hören möglich ist, von dieser aber keine weiteren Kopien erstellt werden können.@1 fliess mit:Nachdem eine technische Lösung gefunden war, stand die Rechteindustrie vor dem Problem, dass sich niemand freiwillig Geräte mit einer eingeschränkten Kopierfunktion kaufen würde, für deren Einschränkung er zudem auch noch extra bezahlen müsste. Also wurde der Gesetzgeber auf den Plan gerufen, der 1992 das US-Copyright-Gesetz um den <@1 fliess kursiv>Audio Home Recording Act <@$p>(AHRA)<@3 hoch fliess>25<@$p> ergänzte, der maßgeblich auf die DAT-Technologie zugeschnitten war. Der AHRA verbietet die Herstellung, Einfuhr und Verbreitung von Geräten für digitale Audioaufnahmen, die nicht über ein SCMS oder ein vergleichbares System verfügen. Gleichzeitig verbietet er Geräte, deren primärer Zweck es ist, solche Kopierschutzmechanismen zu umgehen, zu entfernen oder zu deaktivieren. Schließlich kodifiziert er erstmals in den USA ein Gebührensystem für digitale Audioaufnahmegeräte und Leermedien.<@3 hoch fliess>26<@$p> Immerhin stellt der AHRA die Hersteller und Importeure von Geräten und die Konsumenten, die sie für nicht kommerzielle digitale oder analoge Musikkopien verwenden, von der Verfolgung wegen vermeintlicher Copyright-Verstöße ausdrücklich frei. Auch teurere professionelle DAT-Geräte für den Studiogebrauch ignorieren SCMS. Spätestens seit Mitte der 90er waren alle DAT-Rekorder für den Konsumentenmarkt mit SCMS ausgerüstet. Technisch ist es mög<t-1>lich, die SCMS-Information aus dem Datenstrom herauszufiltern. Geräte<t$> für diesen Zweck, die zwischen zwei DAT-Rekorder gesteckt werden, sind in den USA ab 200 Dollar zu haben.<@3 hoch fliess>27<@$p>Das bei der DVD verwendete Kopierschutzverfahren ist identisch mit SCMS, heißt hier aber <@1 fliess kursiv>Copy Generation Management System <@4 Pfeil (Umschalt/Alt #)>’<@1 fliess kursiv> <@$p>(CGMS). Darüber hinaus werden die Videodaten mit dem <@1 fliess kursiv>Content Scrambling <\h>System<@$p> (<@4 Pfeil (Umschalt/Alt #)>’<@$p> CSS) verschlüsselt, die Überspielung auf einen analogen Videorekorder wird von Macrovision blockiert und ein Regionalcode verhindert das Abspielen von DVDs aus anderen Regionen. Verzögerte sich bereits die Markteinführung von Video-DVD-Geräten aufgrund weiterer Anforderungen der Filmindustrie, ist der Effekt bei der Audio-DVD noch ausgeprägter. Seit 1996 wird eine marktfertige Technologie zurückgehalten, weil bislang keine Einigung über eine Rechtekontrolltechnologie zwischen Musik- und Geräteindustrie erzielt werden konnte. Das DVD-Forum verabschiedete die endgültigen DVD-Audio 1.0 Spezifikationen – noch immer ohne Kopierschutz – im Februar 1999. Erste Geräte waren für Mitte 2000 angekündigt. Doch die offenkundig <\n>gewordenen Schwächen von CSS und weitere Forderungen der <@1 fliess kursiv>Secure Digital Music Initiative<@$p> <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’<@$p> SDMI) zur Verschlüsselungs- und zur Wasserzeichentechnologie verhinderten auch diesen Termin.Bei Werkstücken, die aus nichts mehr als einer Kette von Daten bestehen, muss die Information, die die Einschränkung ihrer Nutzungen steuert, möglichst untrennbar in diese Daten selbst eingebettet werden. Die Endgeräte enthalten dann Mechanismen, die diese Information interpretieren und Nutzungen unter bestimmten Bedingungen zulassen, unter anderen nicht. Der MP3-Standard beinhaltet keinen solchen Rechtekontrollmechanismus. In einem zweiten Schritt entwickelte das Fraunhofer IIS das <@1 fliess kursiv>Multimedia Protection Protocol <@$p>(MMP).<@3 hoch fliess>28<@$p> Dateien werden dabei kryptografisch »eingekapselt«. In einen Dateivorspann und in regelmäßigen Abständen auf die ganze Datei werden dazu Informationen über die Rechteinhaber verteilt und die erlaubten Nutzungen eingefügt. Darin kann stehen, dass z.B. eine MP3-Datei nur vom Käufer angehört, nur vom Server, nicht aber von einer lokalen Kopie abgespielt oder nicht auf CD gebrannt werden kann. 1995 vorgestellt, war MMP eines der ersten Schutzsysteme im Internet. Möchte eine Kundin Multimedia-Information bei einem E-Commerce-Anbieter kaufen, muss sie sich zunächst registrieren. Der »Medien-Distributor« schickt ihr dann einen individuellen Registrierungs-Code, den sie in ihre soft- und hardwarebasierten Medien-Player einträgt. Bestellt sie nun ein Musikstück, geht ihre Kennung in die Verschlüsselung der Datei ein. Diese Datei ist beliebig kopierbar, aber angehört werden kann sie nur mit einem Player, der die Kennung enthält, auf die sie kryptiert wurde. Zu den ersten Nutzern von MMP gehörten das von der EU geförderte Projekt <@1 fliess kursiv>Musik on DEmand<@$p> (MODE)<@3 hoch fliess>29<@$p> und das vom Südwestfunk (heute Südwestrundfunk) in Zusammenarbeit mit der Deutschen Telekom entwickelte Pilotprojekt »SWR on Demand«, das auf der Berliner Funkausstellung 1995 vorgestellt wurde. An der Entwicklung waren auch die GEMA und die Vereinigung der Deutschen Phonographischen Industrie beteiligt, die mit der Telekom Rahmenverträge abschlossen. Derzeit werden im <@1 fliess kursiv>Audio on Demand<@$p>-System<@3 hoch fliess>30<@$p>, das die Telekom jetzt allein weiterbetreibt, 70<\!q>000 Titel angeboten. Zusätzlich zu MMP werden hier die MP3-Dateien durch ein Rückrufsystem gesichert: Die Musik wird über das Internet nur ausgewählt, dann kappt der Server die Internet-Verbindung und wählt die bei der Registrierung angegebene ISDN-Nummer des Kunden, um die Dateien zu übertragen. Über das Abrechnungsverfahren brauchte sich die Telekom keine Gedanken zu machen. Die Kosten bucht sie einfach auf die Telefonrechnung. Inzwischen gibt es eine ganze Reihe solcher Kontrollsysteme. Die MPEG-Familie von Codierungsstandards wird weiterentwickelt. So ist MPEG-1 bereits weitgehend durch MPEG-2 ersetzt, das in Dekodern für digitales Fernsehen und für DVD-Video Verwendung findet. MPEG-4 und MPEG-7, das <@1 fliess kursiv>Multimedia Content Description Interface<@$p>, sollen die neuen Standards für Multimediadaten im Internet werden. Anders als bei MP3, bei dem ein Rechtekontrollmechanismus extern hinzugefügt werden muss, enthalten die aktuellen Standards einen solchen von vornherein. Urheber können zwar noch die Rechtekontrolle auf »Kopieren unbeschränkt erlaubt« setzen, doch die Option, keine Rechtekontrolle zu verwenden, werden sie bald nicht mehr haben. Sah es eine Zeit lang so aus, als würde die technologische Entwicklung die Balance zugunsten der Öffentlichkeit verschieben, lässt sie jetzt das Pendel in die andere Richtung zurückschlagen. Digitalen Daten, die ihrer Natur nach beliebig oft vervielfältigt werden können, wird durch zusätzliche Mechanismen ein Werkstückcharakter verliehen. Anders als bei einem Buch oder einer CD wird dieses Werkstück außerdem auf den Käufer maßgeschneidert. Konnte er materielle Werkstücke nach ihrer Lektüre auf dem Second-Hand-Markt weiterverkaufen, sind die digitalen für Dritte nutzlos. Nutzlos werden sie selbst für den rechtmäßigen Besitzer, wenn in einigen Jahren die Technologie sich wandelt und die Daten- oder die Schutzformate nicht mehr lesbar sind.<@3 hoch fliess>31<@$p>@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»[D]er zentrale wunde Punkt der ganzen IP-Ökonomie (IP im Sinne von Intellectual Property, aber auch Internet): Sie floriert unter anderem nur deswegen, weil die Knappheit der Güter, die den Austausch darunter treibt, eine rein virtuelle Knappheit ist. Konsequent umgesetzt führt die IP-Ökonomie jedoch zu einer kulturellen Verarmung, weil sie nicht nachhaltig ist. Eine Umsetzung von Intellectual Property nach den Ideen, die hinter dem DMCA <@1 fliess normal>[dem US-Digital Millennium Copyright Act (s.u.)] <@$p>stehen, bedeutet eigentlich, dass jedem Subjekt eines Kulturraums Kommunikationsbeschränkungen auferlegt werden... Ultimativ ist in einem solchen ökonomischen Modell aber jede Äußerung, jeder Mausklick, ja jeder Atemzug von mir geldwert« <@1 fliess normal>(<@6 Caps>Köhntopp<@1 fliess normal>, 2/2000).@1 fliess mit:<\c>@2  ZÜ 1:Rechtliche Herausforderungen <\n>durch Digitalmedien@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")><*t(25.512,0,"1  ")> »Das Copyright-Gesetz ist völlig veraltet. Es ist ein Artefakt aus der Zeit Gutenbergs. Da es  sich um einen reaktiven Prozess handelt, wird es wahrscheinlich vollkommen zusammenbrechen müssen, bevor es korrigiert wird.« <@1 fliess normal>(<@6 Caps>Negroponte<@1 fliess normal>, 1995)@1 fliess mit:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Das Recht auf geistiges Eigentum kann ebensowenig  ausgebessert, rückwirkend angepasst oder erweitert werden, um der digitalen Welt gerecht zu werden, wie das Immobilienrecht dahingehend revidiert werden kann, eine Zuteilung des Sendespektrums abzudecken (was hier versucht wird, erinnert jedoch ein wenig daran). Wir müssen ganz neue Methoden entwickeln, die den neuen Umständen gerecht werden.« <@1 fliess normal>(<@6 Caps>Barlow<@1 fliess normal>, 1994)<@$p><*t(25.512,0,"1  ")>@1 fliess ohne:Das Urheberrecht sieht sich in den digitalen Medien mit ungleich größeren Problemen konfrontiert als in den analogen. Bislang waren Werke an Werkstücke gebunden. Sie existierten nicht anders denn als physische Erscheinungsformen. Ein Kopiervorgang erforderte erhebliche Investitionen. In Zeiten des Bleisatzes musste ein Buch vollständig neu gesetzt werden, um es nachdrucken zu können. Mit der Reprofotografie konnten die Druckplatten mit einem wesentlich geringeren Aufwand direkt von der veröffentlichten Vorlage erstellt werden. Durch den Fotokopierer wurde das <\h>Erstellen einzelner Kopien zu einem Kinderspiel. Die Kopplung des informationellen Gehalts an seinen materiellen Träger wurde mit jedem Schritt lockerer, bis die Digitalisierung sie schließlich vollends aufhob. Der beliebigen Vervielfältigung der Form eines Werkes sind – zumindest ökonomisch – keine Grenzen gesetzt. Die Handhabung von Atomen ist kostspielig, Bits dagegen lassen sich fast umsonst kopieren und verbreiten. Die technischen und juristischen Antworten auf diese Herausforderung zielen darauf, mit großem Aufwand den Werkstückcharakter aus der analogen Welt künstlich in der digitalen nachzubilden. Auch hier bestätigt sich Marshall McLuhans Beobachtung, dass wir in der formativen Phase der neuen Medien in sie hineinprojizieren, was uns von ihren Vorläufern vertraut ist.@1 fliess mit:Die Industrie reagiert, wie im Fall von DVD und MP3 gezeigt, auf technologische Herausforderungen mit technologischen Lösungen. Die Gesetzgebung flankiert diese Lösungen. Der im Zusammenhang mit der DAT-Technologie bereits genannte US-amerikanische <@1 fliess kursiv>Audio Home Recording Act<@$p> (AHRA) von 1992 ist ein frühes Beispiel dafür. Geräte ohne Rechteschutzsysteme werden verboten, ebenso Technologien zu ihrer Umgehung. Doch die Rechtspraxis führte viele amerikanische Beobachter, wie den Chef des MIT-Media-Labs Negroponte und den Sprecher der <@1 fliess kursiv>Electronic Frontier Foundation <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’<@1 fliess kursiv> <@$p>EFF) Barlow, Mitte der 90er-Jahre dazu, die überlieferten Copyright-Mechanismen grundsätzlich in Frage zu stellen. Als schädlich erachtet auch der Gründer des GNU-Projekts, Richard Stallman, das traditionelle Copyright in der neuen digitalen Welt: @1 fliess ohne:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Das Copyright ist eine künstliche Einschränkung, die dem Fortschritt dienen soll. Die Frage ist aber, ob der Fortschritt, der auf diese Weise gefördert wird, die ganzen Probleme wert ist, die dadurch verursacht werden. [...] Das Copyright war in Ordnung, als die Druckerpresse das wichtigste Instrument zur Verbreitung von geistigem Eigentum war. Der Leser musste kein Recht aufgeben, von dem er tatsächlich hätte Gebrauch machen können – es entstand also kein Verlust. Im Computerzeitalter stimmt das nicht mehr. Die Einschränkungen durch das <\n>Copyright wirken sich negativ aus. Anstelle einer Einschränkung nur für die Herausgeber haben wir heute eine Einschränkung der Leser. Und das macht die Sache schlecht. Das Copyright hat jetzt einen schädlichen Effekt, der vorher nicht da war.«<\!q><@3 hoch fliess>1<@$p><*t(25.512,0,"1  ")>@1 fliess ohne:In der Rechtsdebatte dagegen herrschte die Auffassung vor, dass für das Urheberrecht durch die Digitalisierung im Prinzip alles beim Alten bleibe: »Die tragenden Prinzipien bleiben im Wesentlichen unangetastet; zur Lösung der anstehenden Probleme erscheinen einzelne Maßnahmen der Anpassung und Klarstellung erforderlich, aber auch ausreichend.«<@3 hoch fliess>2 <@$p>Die Digitalisierung eines Werkes gelte nicht als Bearbeitung oder Umgestaltung. Sie schaffe keine neue Werkart. Daher seien grundsätzlich dieselben Regelungen anwendbar, wie für analoge Werke. Charakteristisch ist die Haltung der GEMA, die Datenträger wie Disketten, Festplatten, CD-I und CD-ROM analogen Datenträgern wie Schallplatten oder Cassetten gleichstellt und für deren Musikanteil gleichermaßen Tantiemen erhebt. Auch für den Online-Bereich überträgt sie überlieferte Kategorien. Ein <@1 fliess kursiv>Music-on-Demand<@$p>-System stuft sie auf die gleiche Weise ein wie ein Plattenpresswerk. Beide sind verpflichtet, der GEMA eine monatliche Aufstellung ihrer Kunden, Katalognummern, Titel und Auflagen der ge<\h>pressten Platten respektive der auf dem Server angebotenen und der abgerufenen Musikstücke vorzulegen.<@3 hoch fliess>3<@$p> Dennoch ist es unverkennbar, dass urheberrechtliche Grundbegriffe wie »Urheber«, »Werk« und »Leis<\h>tung« durch die Digitalisierung einem Wandel unterliegen. So wurde z.B. debattiert, ob eine digital gespeicherte Fotografie aus urheberrechtlicher Sicht als Fotografie, als Datenbank oder als Computerprogramm anzusehen sei (vgl. <@6 Caps>Ellins<@$p>, 1997, S. 342 ff.). @1 fliess mit:<t-2>Die Internationalisierung des Wissensraumes hatte bereits ab Mitte des 19. Jahrhunderts zu ersten Rahmenabkommen geführt. Die Globalisierung durch grenzüberschreitende Direktsatelliten, vor allem aber durch das Internet verstärkte diesen Bedarf nach einer Vereinheitlichung der nationalstaatlichen Regelungen und einer Anpassung an die neuen Sachverhalte. So hat der <x@1 fliess kursiv><t-2>WIPO Copyright Treaty<@$p><t-2> (WCT) vom Dezember 1996 ein neues exklusives Recht des Urhebers zur »Kommunikation an die Öffentlichkeit« eingeführt, das die Bereitstellung von Werken im Internet erfasst. Neben die Veröffentlichung in Form von physischen Verbreitungsstücken und die Sendung in Rundfunk und Kabelsystemen trat die Abbrufbarkeit durch Mitglieder der Öffentlichkeit. Dieses »Zugänglichmachen« kann als ein Konzept eines genuinen <x@1 fliess kursiv><t-2>lex digitalis<@$p><t-2> gesehen werden.<t$>Andere Differenzen zwischen den nationalen Rechtsräumen sträuben sich gegen eine Harmonisierung. So haben viele Länder Auflagen für die Ein- und Ausfuhr sowie für die Verwendung von Kryptografieprodukten verhängt, die mit Hilfe von Servern in andere Ländern leicht unterlaufen werden können. Pornografie und nationalsozialistische Embleme sind in einigen Ländern verboten, im Internet jedoch nur einen Mausklick weit entfernt. Einige Länder haben Bestimmungen über den <t-1>Anteil lokaler Inhalte von Rundfunkprogrammen erlassen. Firmen können <t$>nationales Steuerrecht unterlaufen, indem sie ihre E-Commerce-Server in Steueroasen wie die Bahamas auslagern. Auch das nationalstaatlich geregelte Markennamenrecht steht durch den grenzen- und branchen<\h>überschreitenden Digitalraum vor Problemen. Wessen Anspruch auf den Internet-Domain-Namen »apple.com« soll überwiegen, der des US-amerikanischen Computerherstellers oder der des britischen Schallplattenunternehmens?Die EU-Datenbankrichtlinie vom 11. März 1996 schuf ebenfalls einen neuen Schutzgegenstand. Es handelt sich um ein <@1 fliess kursiv>sui-generis-<@$p>Recht für den Investitionsschutz an Sammlungen, die den urheberrechtlichen Anforderungen an die kreative Schöpfungshöhe nicht genügen. In der Verschiebung von Urheber- zu Leistungsschutzrechten drückt sich eine Angleichung der <@1 fliess kursiv>Droit d‘auteur<@$p>-Tradition an die Copyright-Länder aus, die nicht nur Produkte schützen, die ein Autor dank seiner Kreativität, sondern auch »im Schweiße seines Angesichts« geschaffen hat. Dem Unterschied wurde durch die kurze Schutzdauer für Datenbanken von 15 Jahren Rechnung getragen.Eine Reihe von »digital-weltlichen« Rechtsabkommen sind bereits genannt worden, wie die Revidierte Berner Übereinkunft und die beiden WIPO-Verträge vom Dezember 1996. In Deutschland diente der Diskussionsentwurf eines »Fünften Gesetzes zur Änderung des Urheberrechtsgesetzes« aus dem Juli 1998<@3 hoch fliess>4<@$p> dazu, die internationalen Verpflichtungen umzusetzen und das Urheberrecht den Entwicklungen im Bereich der Informations- und Kommunikationstechnologie, insbesondere der digitalen Technologie, anzupassen. Nachdem im April 2001 die »EU-Richtlinie zur Harmonisierung des Urheberrechts« verabschiedet wurde, ist es nun an der Bundesregierung, das nationale Urhebergesetz entsprechend anzupassen. In den USA war es der <@1 fliess kursiv>Digital Millennium Copyright Act<@$p> (DMCA)<@3 hoch fliess>5<@$p> von 1998, der das Copyright-Gesetz auf die Höhe der neuen Zeit brachte. Das Gesetz wurde wohl kaum im Jahr-2000-Rausch betitelt. Hier markiert der Name eines Gesetzes eine neue Epoche, die nur zufällig mit einer runden Zahl im christlichen Kalender zusammenfällt. Der Titel sagt: Mit der Digitalität wird alles anders.Im Folgenden sollen die zentralen Punkte der Digital-Gesetze angesprochen werden.@2  ZÜ 3:Hard- und Software@1 fliess ohne:Grundlage für die Erzeugung und Nutzung digitaler Werke sind Hard- und Software. Hardware fällt überwiegend in den Zuständigkeitsbereich des Patentrechts, das Layout der Schaltkreise im Inneren der Computerchips ist mit Hilfe des Urheberrechts erfasst worden. Vor dem Hinter<t-1>grund des damaligen »Chip-Kriegs« zwischen den USA und Japan wurde<t$> 1984 der <@1 fliess kursiv>U.S.-Semiconductor Chip Protection Act<@$p> erlassen. Im Dezember 1986 folgte die EU-Richtlinie 87/54/EEC über den Rechtsschutz für die Topografien von Halbleiterprodukten diesem Beispiel. @1 fliess mit:Der rechtliche Status von Software war in den 60er-Jahren noch ungeklärt. 1974, also noch bevor eine eigenständige Softwareindustrie entstanden war, berief der US-Kongress eine Kommission ein, die sich mit <t-1>dem rechtlichen Status von Computerwerken befassen sollte. Die CONTU <t$>(<@1 fliess kursiv>Commission on New Technological Uses of Copyrighted Works<@$p>) empfahl, Computerprogramme zukünftig als »literarische« Werke unter den Schutz des Copyright zu stellen. Die US-Copyright-Reform von 1976 folgte dieser Empfehlung, stellte spezifischere Regulierungen jedoch bis zum Abschlussbericht der CONTU aus, dessen Formulierungen fast wörtlich in der nächsten Revision von 1980 übernommen wurde. Überdies können Programme, die Teil eines Produktes oder eines Prozesses sind, seither in den USA auch durch Patente geschützt werden. <t-1>Das Revidierte Berner Übereinkommen von 1979 folgte dem Beispiel und unterstellte Computerprogramme dem Schutz für literarische Werke. Im Mai 1991 verabschiedete die Europäische Kommission die »<x@1 fliess normal><t-1>Richtlinie zum Schutz von Computerprogrammen«<@$p><t-1> (91<\!q>/<\!q>250<\!q>/<\!q>EEC).<x@3 hoch fliess><t-1>6<@$p><t-1> In Deutschland wurde zu Beginn der Debatte über Multimedia-Produkte verschiedentlich die Auffassung geäußert, eine solche Anwendung einschließlich der Benutzeroberfläche, der integrierten Medien, Steuerungssoftware und Hardware sei eine einheitliche technische Anwendung und daher insgesamt als Computerprogramm zu schützen. Durchgesetzt hat sich jedoch eine Trennung der Software und der durch sie präsentierten Inhalte. In den Regelungen zu Datenbanken, sowohl in der EU-Richtline (Art. 1 Abs. 1) wie im Urhebergesetz drückt sich diese Trennung aus. In Paragraph 4 Abs.<\!q>2 Satz 2<\!q>UrhG heißt es: »Ein zur Schaffung des Datenbankwerkes oder zur Ermöglichung des Zugangs zu dessen Elementen verwendetes Computerprogramm ist nicht Bestandteil des Datenbankwerkes.«<x@3 hoch fliess><t-1>7<@$p><t-1><t-0.5>In Deutschland genossen Computerprogramme bis 1993 keinen <\n>wirkungsvollen gesetzlichen Urheberrechtsschutz. Seither zählen Computerprogramme in Umsetzung der EU-Richtlinie neben Schriftwerken und Reden ebenfalls zu den geschützten Sprachwerken (§2 Abs. 1<\!q>UrhG). Die Computerrechtsnovelle fügte ferner die »Besonderen Bestimmungen für Computerprogramme« (§<\!q>69a ff.) ein. Während an alle andere Werke das Kriterium der kreativen Schöpfungshöhe angelegt wird, sind für die Schutzfähigkeit von Programmen keine anderen Kriterien, insbesondere nicht qualitative oder ästhetische, anzuwenden. Die Übertragung des Konzepts der Kopie auf das Medium Computer führte in der Anfangszeit zu einer der absurdesten Episoden des Digitalrechts – dem Verbot des Kopierens in den Arbeitsspeicher (<x@1 fliess kursiv><t-0.5>Random Access Memory<@$p><t-0.5> – RAM).<t$>@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Die Extrapolation der Analogie sagt, dass  sogar das Übertragen von Bits in einen Computerspeicher unter ›kopieren‹ fällt und somit das Urheberrecht verletzt. Moderne Computer haben aber keinen einfachen, undifferenzierten Speicher. Ein typischer Fall wäre z.B., wenn ein Computer Daten von einem Gerät empfängt und die Bits normalerweise in einen Eingangspuffer geladen werden. Sie können dann in den Hauptspeicher kopiert werden und anschließend in einen High-Speed-Cache (Pufferspeicher), parallel zum Hauptspeicher. Dann würden sie in einen Zwischenspeicher kopiert, bevor sie auf dem Bildschirm erscheinen. Wenn man es ganz genau nimmt, müsste man sagen, dass das Urheberrecht von einem Computer mit moderner Architektur mehrfach verletzt wird, bevor überhaupt jemand das Werk sieht« <@1 fliess normal><\h>(<@6 Caps>Stefik<@1 fliess normal>, 1997a).@1 fliess mit:<@1 fliess normal>@1 fliess ohne:Dass all diese »Kopien« nichts mit dem Nachdrucken eines Buches gemein haben, erscheint jedem Computerbenutzer als Selbstverständlichkeit, und auch der Abschlussbericht der CONTU erkannte dies an.<@3 hoch fliess>8<@$p> Das hinderte US-Gerichte nicht, in mehreren umstrittenen Fällen in nichts als dem Einschalten eines Computers und dem Starten eines Programms einen copyrightverletzenden Kopiervorgang zu erkennen.<@3 hoch fliess>9<@$p>@1 fliess mit:Auch in Europa war die »RAM-Kopie« virulent, besonders verstärkt durch das Internet, denn auch hier müssen natürlich alle im Netz bereitgestellten Daten zur Darstellung auf den Rechner der Betrachterin geladen werden. Paragraph 69c Nr.<\!q>1<\!q>UrhG besagt: »Soweit das Laden, Anzeigen, Ablaufen, Übertragen oder Speichern des Computerprogramms eine Vervielfältigung erfordert, bedürfen diese Handlungen der Zustimmung des Rechtsinhabers.« Theoretisch könnte der Rechtsinhaber den Käufern seiner Software diese Zustimmung in einem Lizenzvertrag verweigern. In Paragraph 69<\!q>d Ziff. 1 wird eingeschränkt, dass diese Handlungen nicht der Zustimmung bedürfen, »wenn sie für eine bestimmungsgemäße Benutzung [...] des Computerprogramms notwendig sind«, doch nur »soweit keine besonderen vertraglichen Bestimmungen vorliegen«. Die EU-Richtlinie vom April 2001 beseitigt diese Unsicherheit endlich: Danach sind »technische Vervielfältigungshandlungen, die als wesentlicher und unerlässlicher Teil eines technischen Verfahrens bei einer netzvermittelten Übertragung« gelten, erlaubt.@2  ZÜ 3:Multimedia@1 fliess ohne:Multimedia-Produkte sind eines der zentralen Themen in der deutschsprachigen Rechtsliteratur. Sie erschweren eine Abgrenzung der einzelnen Werk- und Leistungsarten. Wird eine Multimedia-Anwendung als solche oder als Summe der enthaltenen Werke und Leistungen geschützt? Worauf soll das Kriterium der Originalität Anwendung finden? Auch das Rechtemanagement kompliziert sich gegenüber einfachen Werkarten, da unter Umständen eine große Zahl von Rechteinhabern aufgespürt und Lizenzbedingungen und -gebühren ausgehandelt werden müssen.@1 fliess mit:Multimedia-Werke haben alle Merkmale mit digitalen Trägern nur einer Werkart (Musik, Text, Bild) gemein, aber darüber hinaus ergeben sich besondere Probleme durch die Kombination von Text, Musik, Foto, Film zu einem neuen Ganzen. Nach der möglichst weiten Bestimmung des Rechtssystems ist Multimedia durch die Digitalisierung der zusammengeführten Bestandteile, durch die Kombination früher medial getrennter Inhalte und durch Interaktivität charakterisiert (vgl. <@6 Caps>Saacke<@$p>, 1998, S. 20). Es handelt sich somit um einen mehrschichtigen Schutz, der das einzelne aufgenommene Werk, das Multimediaprodukt als strukturierte Gesamtheit und die zu Grunde liegende Software umfasst. In der Literatur wurde Multimedia in Analogie zu drei bestehenden Werkarten diskutiert, zu Computerprogrammen, filmähnlichen Werken oder zu Sammelwerken. Bei Filmwerken – in ihrer Verbindung von aufführenden Künsten, Bild und Ton die »multimedialsten« vor dem Computer – herrscht die Auffassung vor, dass hier die Verwertungsrechte für die einzelnen schöpferischen Beiträge an den Produzenten übertragen werden. Keine andere Werkart kennt eine solche Übertragung. Bei einer Auffassung von Multimedia als »filmähnlichem Werk« (§ 2 Abs. 1 Nr. 6 UrhG) würde diese Übertragung auch wirksam sein, selbst wenn eine CD-ROM nicht ausschließlich aus Filmsequenzen besteht.In der Umsetzung der EU-Datenbankrichtlinie vom März 1996 wurde Paragraph 4 UrhG geändert. Danach sind Sammelwerke als »Sammlungen von Werken, Daten oder anderen unabhängigen Elementen, die aufgrund der Auswahl oder Anordnung der Elemente eine persönliche geistige Schöpfung sind« definiert und unbeschadet der Urheberrechte an den einzelnen Elementen geschützt. Datenbankwerke sind als solche Sammelwerke bestimmt, deren »Elemente systematisch oder methodisch angeordnet und einzeln mit Hilfe elektronischer Mittel oder auf andere Weise zugänglich sind«. Die Software für die Schaffung des Datenbankwerkes oder für den Zugang zu ihm ist nicht Bestandteil des Datenbankwerkes. Ein subtiler Unterschied wird zwischen einem Datenbank<@1 fliess kursiv>werk<@$p>, für das das Kriterium der Schöpfungshöhe gilt, und einer Daten<@1 fliess kursiv>bank <@$p>vorgenommen, die keine »persönliche geistige Schöpfung«, sondern eine »wesentliche Investition« erfordert (§ 87a<\!q>UrhG). Multimediawerke wie Computerspiele, deren Bestandteile neu geschaffen werden, fallen in die erste Kategorie. Dagegen sind reine Datenaggregationen, wie eine elektronische Bibliographie oder ein Telefonbuch, Datenbanken im Sinne des Paragraphen 87a mit einer eingeschränkten Investitionsschutzdauer von 15 Jahren nach Veröffentlichung (§ 87<\!q>d). Auch bei wissenschaftlich-technischen Werken kann der individuelle Gestaltungsspielraum durch Konventionen, Logik des Gegenstandes (alphabetische oder chronologische Anordnung) und technische Notwendigkeiten (Standard-Datenbanksoftware, Kompatibilität) eingeschränkt sein. Um die bestehenden Urheberrechte an den aufgenommenen Elementen zu wahren, bleibt der Autor des Multimediawerkes verpflichtet, diese Rechte einzeln zu ermitteln und Nutzungslizenzen zu erwerben. Die Multimedia-Industrie hat dies als Investitionshemmnis beklagt, doch inzwischen wird hier technische Abhilfe gesucht. Online-Datenbanken wie das <@1 fliess kursiv>Multimedia Clearing House<@$p> (CMMV) der GEMA oder das <@1 fliess kursiv>Common Information System <@$p>(CIS) der CISAC sollen es Verwertern einfacher machen, Rechte an Multimedia-Werkteilen zu klären und zu lizenzieren.@2  ZÜ 3:Sampling@1 fliess ohne:<@1 fliess normal>Sampling-Tec<@$p>hnologien warfen eine der frühesten, bis heute anhaltende Urheberrechts-Kontroverse um die digitale Weiterverwendung von Werken auf. Mit Hilfe von <@1 fliess kursiv>cut-and-paste<@$p> (»ausschneiden und einfügen«) ist es möglich, Elemente aus bestehenden Werken zu entnehmen und in eigenen Werken zu zitieren, zu collagieren und zu verfremden. Die Frage gilt somit der kleinsten schützbaren Informationseinheit (vgl. <@6 Caps>Münker<@$p>, 1995). Nach vorherrschender Auffassung ist das Schutzkriterium eines Musik-Samples nicht eine minimale Anzahl von Takten oder Sekunden, sondern die Erkennbarkeit der Melodie. Auch eine Tonfolge aus drei Noten kann bereits so charakteristisch sein, dass sie dieses Kriterium erfüllt. Die GEMA vertritt hier eine eigene Auffassung: »Wird als Klangquelle ein Tonträger oder Mitschnitt einer Rundfunksendung verwendet, sind die Zustimmung der berechtigten ausübenden Künstler, der Tonträgerhersteller oder der Sendeanstalt selbst dann einzuholen, wenn nur kleinste Klangteile, unabhängig von der Erkennbarkeit der Melodie, verwendet werden.«<@3 hoch fliess>10<@$p>@1 fliess mit:@2  ZÜ 3:Öffentliche Wiedergabe@1 fliess ohne:Eine bevorstehende Anpassung des Urheberrechts betrifft das Recht der öffentlichen Wiedergabe und die entsprechenden Schrankenregelungen. Die internationalen WIPO-Verträge (Artikel 8 WCT sowie Artikel 10 und 14 WPPT), die aktuelle »<@1 fliess normal>EU-Richtlinie zur Harmonisierung bestimmter Aspekte des Urheberrechts und verwandter Schutzrechte« sowie der deutsche Diskussionentwurf eines »Fünften Gesetzes zur Änderung des Urheberrechtsgesetzes«<@$p> sehen ein neues Ausschließlichkeitsrecht für Urheber, ausübende Künstler und Tonträgerhersteller vor, das so genannte <@1 fliess kursiv>right of making available.<@$p> Dieses Recht umfasst das zur Verfügung Stellen geschützter Inhalte zum individuellen Abruf in digitalen Netzen. Im deutschen Entwurf ist es als Teil des Rechts der öffentlichen Wiedergabe konzipiert und umfasst nicht nur <@1 fliess kursiv>pull media <@$p>(auf Abruf des Empfängers), sondern auch <@1 fliess kursiv>push media<@$p>, bei denen der Werkverwerter den Akt der Zugänglichmachung steuert. Bislang definiert das Urhebergesetz »Öffentlichkeit« als »eine Mehrzahl von Personen«, die nicht durch gegenseitige B<t-1>eziehungen persönlich untereinander verbunden sind (§<\!q>15 Abs. 3<\!q>UrhG).<t$> Versteht man den Begriff so, dass die Verwertung gegenüber den Adressaten <@1 fliess kursiv>gleichzeitig<@$p> erfolgen muss, dann wäre die Wiedergabe im Rahmen von <@1 fliess kursiv>On-demand<@$p>-Diensten keine öffentliche Wiedergabe in diesem Sinne des Begriffs. Der Entwurf stellt deshalb klar, dass eine öffentliche Wiedergabe auch dann vorliegt, wenn das Werk »für eine Mehrzahl von Angehörigen der Öffentlichkeit zugänglich oder gleichzeitig wahrnehmbar gemacht wird oder aufgrund eines an die Öffentlichkeit gerichteten Angebotes für einen einzelnen Angehörigen der Öffentlichkeit zugänglich gemacht wird« (§ 15 Abs. 2 E).<@3 hoch fliess>11<@$p>@1 fliess mit:Dies hat Auswirkungen auf die Schrankenregelungen des Urheberrechts, also jene Regelungen, die im Interesse der Allgemeinheit das ausschließliche Verwertungsrecht der Urheber einschränken, wie private Einzelkopien, Zitate oder Pressespiegel. Der <@1 fliess normal>Diskussionsentwurf <@$p>unternimmt dabei bewusst noch keine durchgängige Neuregelung, da diese erst jetzt nach der Verabschiedung der EU-Richtlinie möglich ist. Die Verzögerung in Brüssel, für die vor allem die schwierige Einigung über die Schrankenregelungen verantwortlich war, führte in der »nachgeordneten Instanz« zu einem Gesetzgebungsstau. Absehbar ist jedoch bereits, dass mit dem auschließlichen Recht der Urheber auf die Zugänglichmachung ihrer Werke im Internet den Versuchen der Verleger, Werke aus den alten Medien ohne zusätzliche Vergütung online anzubieten, endgültig ein Ende gesetzt ist (vgl. <@6 Caps>Lühr<@$p>, 1998).@2  ZÜ 3:Schrankenbestimmungen@1 fliess ohne:Auch die Einschränkungen des Urheberrechts können nicht ohne weiteres auf die digitale Welt übertragen werden. Dazu gehört die Erstverkaufsregel, derzufolge der Käufer eines Buches dieses nach der Lektüre weiterverschenken oder verkaufen kann. Im Falle eines digitalen Werkstückes könnte man meinen, dass der rechtmäßige Käufer es auf seine Homepage stellen und an alle Interessierten verschenken könnte. Anders als bei materiellen Werkstücken erhöht diese Art der Nutzung jedoch die Zahl der nutzbaren Kopien, so dass theoretisch die gesamte Online-Bevölkerung der Welt damit versorgt werden könnte. Computerprogramme dürfen weitergegeben werden, aber ausschließlich in Form des originalen Vertriebsstückes und nachdem alle installierten oder Sicherungskopien gelöscht wurden.<@3 hoch fliess>12<@$p> Die vorherrschende Auffassung in den Diskussionen um Digital-Gesetze ist, dass die Erstverkaufsdoktrin im Netz obsolet sei. Kritische Rechtsgelehrte sehen darin die Gefahr einer weiteren Monopolisierung des Mediensektors: »Die Erstverkaufsdoktrin <\h>abzuschaffen würde die Macht der Eigentümer steigern, den Sekundärmarkt ihrer Werke zu kontrollieren und würde die Informationsverbreitung weiter zentralisieren« (<@6 Caps>Elkin-Koren<@$p>, 1996).@1 fliess mit:Es ist auch die Erstverkaufsdoktrin, die es Bibliotheken erlaubt, die erworbenen Werkstücke zu verleihen, ohne eine besondere Lizenz vom Rechteinhaber einzuholen. Bibliothekare und Archivare warnen daher vor den Regelungen, wie sie unter anderem die EU-Richtlinie vorsieht. Das <@1 fliess kursiv>European Bureau of Library, Information and Documentation Associations <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’<@$p> EBLIDA) stellt in einem Positionspapier fest, dass die Richtlinie zur Folge hätte, dass Bibliotheken und Archive ohne eine besondere Lizenz es ihren Nutzern nicht gestatten dürften, geschütztes elektronisches Material in ihren Räumen – geschweige denn übers Netz – anzusehen oder anzuhören, davon für private und Bildungszwecke Kopien anzufertigen, es im Zuge der Fernleihe anderen Bibliotheken zugänglich zu machen oder für Archivzwecke eigene Kopien anzufertigen (<@6 Caps>Eblida<@$p>, 1998).Eine technologische Antwort kommt von Mark Stefik, der mit der Feststellung beginnt: »ohne Kopiekontrollen würden die digitalen Bibliotheken zu Zentren für die kostenlose Verbreitung« (<@6 Caps>Stefik<@$p>, 1997a). Mit einer entsprechend konzipierten <@1 fliess normal>Infrastruktur aus technischen Rechtekontrollsystemen<@1 fliess kursiv> <@$p>(<@1 fliess kursiv>Rights Control System,<@$p> <@4 Pfeil (Umschalt/Alt #)>’<@$p> RCS) dagegen, könne ein Autor oder ein Verleger seine digitalen Werke mit einem Verleihrecht versehen, aber die Möglichkeit sperren, Kopien zu erstellen oder Ausdrucke anzufertigen. Indem RCSs einen Werkstück<\h>charakter in den Digitalraum einführen, könnte eine Bibliothek, wie bislang, eine bestimmte Anzahl von Werkstücken ankaufen, und nur diese Zahl kann gleichzeitig ausgeliehen werden. Vorteilhaft für die Verleger könnte jedes Bibliotheks<\h>exemplar einen »Kauf mich«-Knopf enthalten, falls der Leser sich entscheidet, ein eigenes Exemplar erwerben zu wollen. Stefik argumentiert, dass Bibliotheken unter dem ständigen Druck der Kostenreduktion stünden. Digitale Werkstücke in einem RCS würden die Handhabung von physikalischen Büchern und CDs beseitigen und den Leihverkehr automatisieren. Mehr noch könnte sie eine neue Einnahmequelle darstellen. »Nehmen wir einmal an, eine Bibliothek kauft zehn digitale Exemplare eines Bestsellers, um diese kostenlos an ihre Benutzer auszuleihen und zehn weitere Exemplare, die gegen eine Gebühr abgegeben werden. Diejenigen, die wirtschaftlich denken, können warten, bis kostenlose Exemplare verfügbar werden. Nutzer, die es eilig haben und bereit sind, mehr Geld auszugeben, können die gebührenpflichtigen Exemplare ausleihen, anstatt zu warten. Die Einkünfte aus den gebührenpflichtigen Exemplaren können die kostenlosen Exemplare finanzieren« (ebd.). Da aber bislang kein einziges RCS auch nur eine Anstrengung unternommen hat, ein solches Verleihrecht zu implementieren, kann Stefiks Szenario ins Reich der Sciencefiction verwiesen werden.In der Diskussion um die neuen Digital-Gesetze wird die unbestrittene Notwendigkeit von Schranken im Hinblick auf den so genannte Drei-Stufen-Test behandelt. Er ähnelt dem von Richter Storey im 19.<\!q>Jahrhundert etablierten und in die <@1 fliess kursiv>Fair-Use<@$p>-Doktrin des Copyright eingegangenen Test (s.o. unter »Balance«). In seiner aktuelle Fassung geht er von Arti<\h>kel<\!q>9(2) des Revidierten Berner Übereinkommens (RBÜ) aus: »Es sollte Sache der Gesetzgebung in den Mitgliedsländern sein, die Reproduktion solcher Werke [1] in bestimmten, besonderen Fällen zu erlauben, [2] vorausgesetzt, dass eine solche Reproduktion nicht im Widerspruch mit einer ›normalen Verwertung‹ des Werkes steht und [3] dass sie in Bezug auf die rechtmäßigen Interessen des Autors keine unzumutbaren Nachteile darstellt.«<@3 hoch fliess>13<@$p> Die Möglichkeit von Schranken ist somit grundsätzlich auch im digitalen Wissensraum gegeben, doch handelt es sich bei »normaler Verwertung« und »unzumutbaren Nachteilen« um auslegungsfähige Kriterien. Guibault fragt:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Was aber begründet die normale Verwertung eines Werkes in der <\n>digitalen vernetzten Welt? Wenn die Technologie es den Inhabern von Urheberrechten ermöglicht, jede Nutzung ihrer Werke auf dem Informations-Highway zu kontrollieren und für jede autorisierte Nutzung <\n>Gebühren einzutreiben, bedeutet das zwangsläufig, dass sie das auch dürfen? Würde die Auferlegung einer Copyright-Beschränkung in diesem Fall automatisch die normale Verwertung des Werkes verhindern? Es ist offensichtlich, dass der Drei-Stufen-Test von Artikel 9(2) des Berner Abkommens ein wichtiger Faktor ist, der bei der Einführung der Copyright-Beschränkungen in der digitalen vernetzten Welt in Betracht gezogen werden muss; unserer Meinung nach sollte er aber sicherlich nicht der einzige Faktor sein, der berücksichtigt werden muss« <\n><@1 fliess normal>(<@6 Caps>Guibault<@1 fliess normal>, 1997).@1 fliess mit:@1 fliess ohne:Elkin-Koren analysiert die Verschiebung in der Balance zwischen Rechteinhabern und Nutzern und zeigt, dass die überkommenen Konzepte von privater und öffentlicher Informationsübermittlung nicht mehr tragen, wenn technische Zugangsysteme es der Rechtindustrie erlauben, jeden individuellen Zugang zu Werken zu kontrollieren, dabei aber effektiv überhaupt nichts mehr an die Öffentlichkeit kommunizieren:@1 fliess mit:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Der Mangel an Verständnis für die Art und Weise, in der der Copyright-Diskurs den Bedürfnissen bestimmter Technologien dient, begüns<\h>tigt die Interessen der Stakeholders. Die Rechte, die notwendig waren, um die Interessen der Verleger und Sendeanstalten zu schützen, sind möglicherweise nicht länger vertretbar. Die Copyright-Reform sollte nicht auf die Übersetzung bestehender Copyright-Konzepte zielen, um neue technologische Gebiete abzudecken. Stattdessen sollte sie die <\h>Möglichkeiten und auch die Bedrohungen für Wissenschaft und Bildung im Cyberspace ermitteln« <@1 fliess normal>(<@6 Caps>Elkin-Koren<@1 fliess normal>, 1996).<*t(25.512,0,"1  ")><*t(25.512,0,"1  ")>@2  ZÜ 2:Verträge statt Recht: Lizenzen@1 fliess mit:@1 fliess ohne:Die Nutzungsrechte an einem Werk werden in den meisten Fällen zwei Mal übertragen, vom Urheber an den Verleger und vom Verleger an den »Endnutzer«. Ein (Musik-, Text-, Bild-) Autor erschafft als freier Unternehmer ein Werk, dessen Verwertungsrechte er an einen Verlag verkauft. Verlag und Urheber können dazu im Prinzip einen Vertrag frei aushandeln, der von beiden Seiten unterschrieben wird. »Im Prinzip«, weil tatsächlich die meisten Verlage den meisten Autoren nicht verhandelbare »Friss-oder-stirb«-Verträge vorlegen. Eine Vertragsfreiheit besteht für alle außer Bestsellerautoren nur in der Theorie. @1 fliess mit:Die Schutznorm, die den pauschalen »Abkauf« des Urheberrechts verbietet, wurde 1965 in das deutsche Urhebergesetz eingeführt, da es immer wieder zu massiven Übervorteilungen der Urheber gekommen war. Bereits mit dem Verlagsgesetz von 1901 erkannte der Gesetzgeber die Schutzbedürftigkeit der Autoren als der wirtschaftlich schwächeren Partei gegenüber dem Verleger an.<@3 hoch fliess>14<@$p> Die Vertragswirklichkeit orientiert sich jedoch seit langem nicht mehr am Verlagsgesetz. Auch das Urhebergesetz selbst enthält Rechte, auf die vertraglich nicht verzichtet werden kann, z.B. die Ungültigkeit der Einräumung noch nicht bekannter Nutzungsarten (§<\!q>31 Abs.<\!q>4<\!q>UrhG). Als Mitte der 90er-Jahre CD-ROMs und Internetarchive aufkamen, stellten sie eine eigenständige Nutzungsart dar, die zum Zeitpunkt des Vertragsabschlusses über die Text- und Bildverwertungen in den Printausgaben der Zeitungen und Zeitschriften noch nicht bekannt war. Dennoch weigerten sich die Verlage, die Autoren dafür zu vergüten<k-10>.<@3 hoch fliess>15 <@$p>Tatsächlich müssen sich Autoren häufig vertraglich verpflichten, die Verwertungsrechte an ihrem Werk für »alle Ausgaben und Auflagen« in allen Nutzungsformen bis zum Ablauf der gesetzlichen Schutzfrist gegen eine einmalige Vergütung abzutreten. Fotografen müssen sich regelmäßig zum Verzicht auf das – nach dem Gesetz nicht abtretbare – Urheberpersönlichkeitsrecht auf Namensnennung verpflichten. Auch bei Film- und Multimediawerken ist es üblich, dass alle Rechte der beteiligten Künstler an den Produzenten abgetreten werden. Das Gesetzesrecht wird somit durch Formularverträge außer Kraft gesetzt. Die IG Medien schilderte die aktuelle Lage der Autoren in einer Stellungnahme an das Bundesjustizministerium und forderte eine Reform des Urhebervertragsrechts, um Autoren gegen die Aushöhlung desselben zu schützen, ihnen eine angemessene Vergütung für jede Nutzung gesetzlich zuzusichern, die bestehenden Verzichts- und Abtretungsverbote z.B. der Urheberpersönlichkeitsrechte klarzustellen sowie die Vertragslaufzeiten zu begrenzen.<@3 hoch fliess>16<@$p> In Anerkennung einer längst überfälligen gesetzlichen Regelung des Urhebervertragsrechts erarbeiteten fünf namhafte deutsche Rechtsgelehrte einen Entwurf zur Änderung des Urhebergesetzes, den sie im Mai 2000 der Bundesjustizministerin übergaben.<@3 hoch fliess>17<@$p> In der Begründung weist dieser so genannte Professorenentwurf auf das eklatante Missverhältnis der Einkommenssituation der Urheber zu der volkswirtschaftlichen Bedeutung ihrer Arbeit hin.<@3 hoch fliess>18<@1 fliess normal> <@$p>»Die Gründe dafür liegen [...] zu einem Teil in der häufig unangemessenen Vergütung ihrer Leistungen auf der Grundlage ihrer Verträge mit den Verwertern.«<@3 hoch fliess>19<@$p> Die IG Medien hat zusammen mit anderen Gewerkschaften, Organisationen und Berufsverbänden im Kultur- und Medienbereich die »Initiative <@1 fliess normal>für die Reform des Urhebervertragsrechts« ins Leben gerufen, um dem Professorenentwurf N<@$p>achdruck zu verleihen. Sie sieht eine reelle Chance, dass das Urhebervertragsrecht noch in der laufenden Legislaturperiode verabschiedet wird.<@3 hoch fliess>20<@$p><t-1>Ist der Vertrag mit der Autorin geschlossen, gibt der Verlag die Herstellung der vereinbarten Zahl von Werkstücken (Bücher, Zeitschriften, CDs) in Auftrag, die über den Handel an den »Endkunden« vertrieben werden. Der Konsument erwirbt keineswegs das Werk selbst, sondern nur ein Werkstück mit bestimmten Nutzungsrechten daran. Diese Verbreitung geschieht entweder über das Erstverkaufsrecht von Copyright/Urheberrecht oder über Nutzungslizenzen. Im ersten Fall muss eine Endkundin, um ein Werkstück und damit Zugang zum enthaltenen geistigen Eigentum zu erwerben, nicht in eine Pauschallizenz oder gar in einen individuell ausgehandelten Vertrag mit dem Verleger einwilligen. Büchern ist kein seitenlanger Text vorangestellt, der spezifiziert, was die Leserin damit machen darf und was nicht. Es findet sich allein der Vermerk »Copyright © [Jahr] [Rechteinhaber] Alle Rechte vorbehalten.« Oft folgt noch ein Hinweis, dass die Vervielfältigung, Übersetzung, Verwendung in elektronischen Systemen usw. ohne schriftliche Zustimmung des Verlags strafbar ist, doch bei beiden handelt es sich nur um einen Hinweis auf die ohnehin bestehende Rechtslage. Und die wird vom Urheberrecht/ Copyright bestimmt, ohne von zusätzlichen Auflagen eingeschränkt zu werden.<t$>Beim Vertrieb über Nutzungslizenzen handelt es sich um eine Innovation, die gelegentlich auch schon in den analogen Medien versucht wurde. So findet sich auf alten <@1 fliess kursiv>Victrola<@$p>-Schallplattenhüllen die Auflage, dass der Käufer die Aufnahme nur auf einem einzigen <@1 fliess kursiv>Victrola<@$p>-Gerät benutzen und sie nicht weiterverkaufen dürfe. Auch Buchverleger versuchten, einen Wiederverkauf ihrer Produkte an Auflagen zu binden. Ein in der US-Copyright-Geschichte wichtiger Rechtsstreit war Bobb-Merril <\n>gegen Straus.<@3 hoch fliess>21<@$p> Der Verleger Bobb-Merrill verklagte den Buchhändler Straus, weil dieser gegen die Lizenzauflage verstoßen hatte, die das Recht des Wiederverkaufs an die Bedingung knüpfte, einen Mindestpreis von einem Dollar pro Exemplar nicht zu unterschreiten. Das Urteil des US-Verfassungsgerichts von 1908 zugunsten von Straus trug zur Etablierung der Rechtserschöpfungs- oder Erstverkaufsdoktrin bei, derzufolge Verleger nach dem ersten Verkauf ihrer Werkstücke kein Recht haben, den weiteren Verkauf zu kontrollieren (vgl. <@6 Caps>Samuelson<@$p>, 1998). Was die Gerichte für diese Mediengeneration unterbanden, brachte erst die Softwareindustrie zur vollen Blüte. Als Ende der 70er-Jahre eine eigenständige Softwareindustrie aufkam, wurden Vertriebsstücke zu<\h>nächst genauso angeboten wie Bücher oder Schallplatten. Doch kurz darauf wurden ihnen immer elaboriertere Lizenzen mitgegeben. Wenn heute ein Kunde im Einzelhandel ein Computerprogramm erwirbt, schließen die beiden Parteien einen Kaufvertrag ab. Wenn er zuhause die Schachtel öffnet oder spätestens, wenn er beginnt, die Software zu installieren, wird er damit konfrontiert, eine Lizenz zu akzeptieren. Er muss <\n><t-1>also in einen zweiten Vertrag einwilligen, diesmal mit dem Softwareverlag,<t$> nicht für das Werkstück – das besitzt er ja bereits –, sondern für die Nutzung des darin enthaltenen Werkes. Beim unkörperlichen Vertrieb von Software über das Internet entfällt der erste Schritt. Auf die gleiche Weise werden zunehmend auch andere digitale Werkarten lizenziert. In diesen Lizenzen muss sich der Nutzer häufig verpflichten, zusätzliche Bedingungen zu erfüllen und auf Rechte zu verzichten – z. B. das Programm zu dekompilieren –, die ihm nach Copyright- und Urheberrecht zustehen. Lizenzen, wie die konventionelle Softwareindustrie sie verwendet, verbieten die Nutzung durch Dritte, das Kopieren, die Weiterverbreitung und die Modifikation. Sie lizenziert binäre, also ausschließlich ausführbare, nicht aber veränderbare Versionen.<@3 hoch fliess>22<@$p> Wer nachlesen möchte, wie Unternehmen von Sega über Microsoft und AT&T bis Caldera ihr geistiges Eigentum untereinander lizenzieren, wird im Internet unter »T<@1 fliess kursiv>ech Deals: Intellectual Property Licenses<@$p>«<@3 hoch fliess>23<@$p> reiche Beute finden. Microsofts <@1 fliess kursiv>End User License Agreement<@$p> (EULA) ist ein Beispiel für solche Lizenzen. Microsoft verkündete zum 1. Januar 2000 eine Innovation in seiner Lizenzpolitik, derzufolge künftige Versionen von MS-Windows nicht mehr frei reinstallierbar sind.<@3 hoch fliess>24<@$p> <@1 fliess kursiv>Original Equimpment Manufacturers<@$p> (OEMs), wie Compaq, Toshiba oder Dell, die Computersysteme mit vorinstallierter Software verkaufen, dürfen seither keine vollwertigen Windows-CDs mit ihrer Hardware mehr ausliefern. Vielmehr dürfen sie neben dem <@1 fliess kursiv>Disk-Image<@$p> des Betriebssystems auf der Festplatte (eine 1<\!q>:<\!q>1-Kopie der gepackten Installationsdateien) ihren Kunden nur eine »<@1 fliess kursiv>Recovery CD<@$p>« geben. Diese wiederum ist über eine Kennung im <@4 Pfeil (Umschalt/Alt #)>’<@$p> BIOS nur auf diesem individuellen Computer verwendbar. Dieses <@1 fliess kursiv>BIOS Lock <@$p>soll Software-»Piraterie« verhindern.<@3 hoch fliess>25<@$p> Es verhindert aber auch völlig legale Nutzungen. Tauscht der Kunde z.B. die Grundplatine seines Rechners mit dem BIOS-ROM aus, so verliert er vollständig die Möglichkeit, von seinen erworbenen Nutzungsrechten an der Software Gebrauch zu machen. Er kann sie auch nicht auf seinem PC deinstallieren, um sie auf seinem Laptop neu zu installieren. Nach der Erstverkaufsdoktrin von Copyright und Urheberrecht darf der rechtmäßige Besitzer eines Buches, einer Musik-CD usw. dieses Werkstück weiterverkaufen. Doch das <@1 fliess kursiv>BIOS Lock<@$p> macht es unmöglich, dass jemand, der GNU/Linux auf dem betreffenden Rechner installiert, einen Teil der Investitionen in die Microsoft-Software zurückgewinnt, indem er sie verkauft. Ein Markt für gebrauchte Software wird so unterbunden. Was rechtens ist und bleiben sollte, macht Microsofts Strategie technisch unmöglich.Bis 1999 die großen OEMs begannen, auch Rechner mit vorinstalliertem Linux anzubieten, gab es für einen Käufer nur einen Weg, an Microsoft vorbeizukommen: die Lizenz zu verweigern. Wer bei »Akzeptieren?« auf »Nein« klickt, dem wird die Nutzung der MS-Software verweigert und mitgeteilt, dass er sein Windows gegen Rückerstattung des Kaufpreises zu seinem Händler zurückbringen kann.<@3 hoch fliess>26<@$p> Doch auch diese Option war eine rein hypothetische, bis Ende 1998 ein australischer <\n>GNU/Linux-Nutzer nach hartnäckigen und langwierigen Bemühungen, bei denen sich Microsoft, OEM und Händler gegenseitig die Verantwortung zuschoben, tatsächlich erstmals eine Rückerstattung erwirken konnte. Das alles hat mit Copyright/Urheberrecht wenig zu tun. Beide geben dem Konsumenten das Recht, Sicherheitskopien anzufertigen, Software zu dekompilieren, um Fehler zu beheben und interoperable Programme zu erstellen, und ein nicht mehr benötigtes Programm an Dritte weiterzuverkaufen – Rechte, die ihm diese Lizenzen verweigern. Genaugenommen ist ein Urheberrechtsanspruch nicht einmal erforderlich. Eine Firma könnte selbst gemeinfreies Material unter einer restriktiven Lizenz verkaufen, solange ihre Kunden bereit sind, sie zu akzeptieren. Bei den genannten Mechanismen wird das Urheberrecht mit seiner lästigen Balance zwischen Rechteinhaber und Öffentlichkeit gleichsam links liegen gelassen, während die möglichen Nutzungen von Lizenz und Technologie regiert werden. Anders als bei der Rechteübertragung zwischen Urheber und Verlag sind die Vertragsparteien bei diesen Massenmarktlizenzen einander nicht bekannt und bekunden ihre Zustimmung nicht durch ihre Unterschrift. Ob durch das Öffnen der Schutzhülle (<@1 fliess kursiv>Shrink-wrap License<@$p>) oder das Anklicken eines »Akzeptieren«-Knopfes <@1 fliess normal>(<@1 fliess kursiv>Click-through License<@$p>) tat<\h>sächlich ein gültiger Vertrag zustande kommt, war in den USA lange und ist in Deutschland noch immer umstritten. Die Reform der US-amerikanischen Entsprechung des Allgemeinen Geschäftsbedingungs(AGB)-Gesetzes von 1999 hat diese Klasse von Verträgen unterdessen im Wesentlichen legalisiert. @2  ZÜ 3:Der Uniform Computer Information Transactions Act (UCITA) <*J*h"mehr"><t0f"FFScala">Lizenzen waren individuell zwischen Firmen ausgehandelte und unterzeichnete Verträge, bis mit dem PC ein anonymer Massenmarkt für Software aufkam. Für diesen Bereich entwickelten die »Inhaltsbesitzer« vereinfachte anonyme Lizenzierungsverfahren. Auch die Lizenzen der frei<\h>en Softw<t1>are sehen nach demselben Mechanismus vor, dass der Nutzer durch Verbreitung oder Veränderung des Programms seine Einwilligung in die Lizenzbedingungen anzeigt. Doch viele amerikanische Richter weigern sich bislang, die <x@1 fliess kursiv><t1>Shrink-wrap <@$p><t1f"FFScala">Lizenzen durchzusetzen. Mit der Bezahlung der Ware im Laden, so die Argumentation, sei ein Kaufvertrag zustande gekommen. Die Lizenz, die der Käufer erst zur Kenntnis nehmen kann, wenn er die Packung öffnet, sei ein Versuch, die <\n>Natur der Transaktion durch zusätzliche Bedingungen nachträglich zu verändern. Diesen geänderten Vertragsbedingungen müsse der Käufer separat zustimmen, und dafür reiche ein Mausklick nicht aus. Siepmann schreibt für die deutsche Rechtslage: »AGB auf Schutzhüllen von Datenträgern (so genannte ›Shrink-Wrap-Agreements‹) haben aus vertragsrechtlicher Sicht im Allgemeinen keine Gültigkeit, da diese erst nach Vertragsschluss zur Kenntnis genommen werden können. Sie können jedoch urheberrechtlich von Bedeutung sein« (<x@6 Caps><t1>Siepmann<@$p><t1f"FFScala">, 1999, Abs. 53).<t0> @1 fliess mit:Diese Rechtsunsicherheit sollte im Zuge der Revision des US-amerikanischen <@1 fliess kursiv>Uniform Commercial Code<@$p> (UCC), dem Äquivalent zu den deutschen AGB, beseitigt werden. Zur Begründung heißt es: »Da die Nation sich von einer Ökonomie, die sich um Waren- und Dienstleistungstransaktionen dreht, hin zu einer Informations-Ökonomie entwickelt, ist der Bedarf an konsistenten und berechenbaren Rechtsnormen drastisch angestiegen, auf die sich die Verträge stützen, die dieser Ökonomie zu Grunde liegen. Ein Mangel an Einheitlichkeit und Klarheit der Rechtsnormen, die diese Transaktionen bestimmen, ruft Unsicherheit, Unvorhersehbarkeit und hohe Transaktionskosten hervor.«<@3 hoch fliess>27<@$p>Die Reform des UCC wurde gemeinsam vom <@1 fliess kursiv>American Law Institute <@$p>(ALI) und der <@1 fliess kursiv>National Conference of Commissioners on Uniform State Laws <@$p>(NCCUSL) betrieben, da Vertragsrecht in den USA Ländersache ist. Anfangs waren die Bestimmungen zu Computerprogrammen als Artikel 2b des UCC geplant, wobei das Gesetz ansonsten den Handel mit materiellen Gütern behandelt. Mitte 1999 gaben die Beteiligten bekannt, dass <\n>die Regeln für Transaktionen von computergestützter Information in einem eigenständige Rahmengesetz, dem <@1 fliess kursiv>Uniform Computer Information Transactions Act <@$p>(UCITA)<@3 hoch fliess>28 <@$p>geregelt werden, das derzeit in den einzelnen US-Bundesländern umgesetzt wird.<@3 hoch fliess>29<@$p>Der UCITA legalisiert <@1 fliess kursiv>Shrink-Wrap-<@$p> (Ziff. 209) und Online-Lizenzen (Ziff. 211) für die Nutzung von »Computerinformation« (nicht nur Programme, sondern jede Art elektronischer Inhalte, die von einem Computer verarbeitet werden können, einschließlich der dazu gehörigen Dokumentation – Ziff. 102.10), sofern der Lizenznehmer die Möglichkeit hat, die Vertragsbedingungen zur Kenntnis zu nehmen, bevor er seine Zustimmung manifestieren muss. Daneben regelt der UCITA den Zugang zu Online-Informationen für eine bestimmte Zeitspanne (Ziff. 611).Besonders umstritten ist der Passus, der es Softwareherstellern erlaubt, Mechanismen zur <@1 fliess kursiv>Electronic Self-Help Repossession <@$p>in ihre Produkte einzubauen, die im Falle eines Vertragsbruchs durch den Lizenznehmer ausgelöst werden können (Ziff. 816). Mit »Wiederaneignung durch elektronische Selbsthilfe« ist gemeint, dass das Unternehmen bei einem (tatsächlichen oder vermeintlichen) Verstoß des Lizenznehmers, ohne ein Gericht anzurufen, die Lizenz widerrufen und 15 Tage nach einer Vorwarnung<@3 hoch fliess>30<@$p> mit elektronischen Mitteln,<@3 hoch fliess>31<@$p> z.B. über das Internet, die Programmversion des Kunden deaktivieren oder löschen kann. Die »Selbsthilfe« der Industrie ist an einige Bedingungen gebunden (der Abschnitt ist auch nicht mit »<@1 fliess kursiv>Self-Help<@$p>«, sondern »<@1 fliess kursiv>Limitations on Electronic Self-Help<@$p>« betitelt), doch grundsätzlich problematisch an dieser »Wieder<\h>aneignung« bleibt, dass das Rechtsgut der geschützten Privatsphäre (der Festplatte des Nutzers) im Interesse des Rechts von Copyright-Eigentümern, die Nutzung ihrer Werke zu kontrollieren, eingeschränkt wird. Auch ein Verbot auf den Wiederverkauf von Massenmarktlizenzen, sofern es deutlich kenntlich gemacht wird, legalisiert der UCITA (Ziff. 503.4). Tatsächlich liefert Ziffer 503.1(b) (»Das vertragliche Interesse einer Partei darf übertragen werden, es sei denn, die Übertragung würde materiell das Eigentum der anderen Partei oder die Wahrscheinlichkeit oder Erwartung, Gegenleistungen zu erhalten, beeinträchtigen«) das Argument dafür, jeglichen Second-Hand-Markt für digitales Wissen zu unterbinden. Befürworter des UCITA sehen ihn als einen Fortschritt im Konsumentenschutz. Zwar gewährt er (den Inhaltsanbietern) weitgehende Vertragsfreiheit, aber er schreibt auch einen Minimalsatz von Rechten fest, auf deren Verzicht keine (für den Konsumenten nicht verhandelbare) Lizenz die Vertragsparteien festschreiben kann (Ziff. 113), darunter explizite und implizite Garantieansprüche (Teil 4), z.B. die – wenn auch mehrfach eingeschränkte – Garantie, dass die gelieferte Information das hält, was der Anbieter in Werbung oder Vorführungen versprochen hat. Interessanterweise betont Carol Kunze auf der nicht offiziellen <\h>Website <@1 fliess kursiv>Ucitaonline.com<@3 hoch fliess>32<@$p> unter dem Titel »Myths about UCITA«, dass er gerade nicht verhindere, dass Hersteller alle Garantieansprüche aus<\h>schlie<\h>ßen. Das ist tatsächlich übliche Praxis in der Branche. Die Rechtslage ändere sich nicht. Software wird »so wie sie ist« verkauft, ohne Ansprüche der Kundin darauf, dass sie so ist, wie es ihr der Hersteller versprochen hat. Die Belehrung ist direkt an die GNU/Linux-Anhänger adressiert, die »das traurigste Beispiel« für häufige Missverständnisse des UCITA abgeben würden.<@3 hoch fliess>33<@$p> Ihre Ablehnung begründe sich darin, dass die GNU/Linux-Anhänger gesagt bekommen hätten, dass der UCITA den Lizenzgebern erlaube, sich von allen Garantieansprüchen freizusprechen, was zur Auslieferung von defekten Produkten führen würde. Die absurde Missinterpretation der Kritik der freien Softwarewelt benutzt Kunze, um sie zu <t-2>widerlegen. Diese, sagt sie – zu Recht –, habe gerade kein Interesse an recht<\h>l<t$>ichen Garantieansprüchen. Es sei ja gerade die Möglichkeit des Garantieausschlusses, die den Aufstieg der freien Software hervorgebracht habe [sic!]. Auch die verbreitete Fehlersuchtechnik der Betaversionen würde verschwinden, wollte man eine Garantieverpflichtung vorschreiben.<@3 hoch fliess>34<@$p><t-1>Die juristische Untermauerung für diese vertrackte Logik lieferte Robert Gomulkiewicz in der <x@1 fliess kursiv><t-1>Houston Law Review <@$p><t-1>unter dem Titel »Wie das Copyleft Lizenzrechte verwendet, um die Open Source-Revolution zum Erfolg zu führen, und seine Bedeutung für Artikel 2B« (<x@6 Caps><t-1>Gomulkiewicz<@$p><t-1>, 1999). Der Artikel beginnt mit einem Debian GNU/Linux-Zitat: »Um frei zu bleiben, muss Software urheberrechtlich geschützt und lizenziert werden«, und liefert dann einen kundigen Überblick über die Lizenzmodelle der freien Software. Gomulkiewicz weist pikanterweise nach, dass einige zentrale Kritikpunkte an 2B UCC respektive jetzt UCITA auch für die freie Software unerlässlich sind. Pikant, da Gomulkiewicz Vorsitzender der damals noch UCC-2B-Arbeitsgruppe der <x@1 fliess kursiv><t-1>Business Software Alliance<@$p><t-1> sowie leitender Unternehmensanwalt von Microsoft war. Sein zentraler Punkt ist, dass es sich bei freien Lizenzen um »standardisierte, nicht ausgehandelte ›friss oder stirb‹-Lizenzen« handelt, deren Rechtmäßigkeit der UCITA ja gerade absichern soll. Auch am Punkt der Garantiepflicht treffen sich die Interessen von Industrie und Freien. Beide sind gleichermaßen »nicht bereit, das Risiko einer mehrere Millionen Dollar teuren Sammelklage einzugehen« und möchten die Möglichkeit von Softwareentwicklern bewahren, »das Risiko frei zu verteilen«. Seine Conclusio zur Zukunft der Open Source-Software ist die Folgende: »Die Lizenzierung wird im Zentrum ihres Erfolges oder Misserfolges stehen. Artikel 2B sollte einen Vertragsrechtsrahmen zur Verfügung stellen, der es Revolutionären, wie den Open Source-Hackern, ermöglicht, erfolgreich zu sein.«<t$>Dass die freie Software der Softwareindustrie, gegen deren Schlie<\h>ßungsmechanismen sie sich gegründet hat, unfreiwillige Schützenhilfe leistet, ist ein Treppenwitz der Geschichte. Tatsächlich würde die freie Software jeglichen Schutz verlieren, wenn Massenmarktlizenzen für ungültig erklärt würden, und im Bankrott enden, wenn sie gesetzlich verpflichtet wäre, eine Garantieleistungsinfrastruktur zu unterhalten. Würde es einmal zu einer gerichtlichen Überprüfung der <@1 fliess kursiv>GNU General Public Licence<@$p> (GPL) kommen (vgl. <@6 Caps>Powell<@$p>, 6/2000) und ginge es dabei auch um die Frage der grundsätzlichen Gültigkeit von Massenmarktlizenzen, könnte – nächste Stufe des Treppenwitzes – die <@1 fliess kursiv>Free Software Foundation<@$p> (FSF) Rückendeckung von Microsoft und den UCITA-Befürwortern bekommen – falls Microsoft nicht selbst der Gegner im Gerichtssaal wäre. Schwer wiegende Konsequenzen für die freie Software und ihre Lizenzen sind vom UCITA auf den ersten Blick nicht zu erwarten. Auch sie fallen unter den Schutz der Vertragsfreiheit. Gomulkiewicz nennt die »Lückenfüller-Regel« des UCITA, die bei Abwesenheit von Vereinbarungen zwischen den Vertragsparteien wirksam werden. Auch das, was die freien Lizenzen nicht explizit festlegen, würde durch Standardregeln aufgefüllt. Doch zum einen darf man annehmen, dass die Softwareindustrie ihr Bestes dafür gegeben hat, dass diese für Informationsanbieter möglichst harmlos ausfallen werden, zum anderen kommen Kriterien wie »Angemessenheit« und »redliche Verkehrsübung« zur Geltung, die dem freien Softwaremodell – mit über 15 Jahren Praxis und millionenfacher Verbreitung – ein schwer zu bestreitendes »Gewohnheitsrecht« verleihen. Generell ist jedoch damit zu rechnen, dass durch den UCITA das Klima, in dem wir Wissen schaffen und austauschen, rauher wird. Hat man früher ein Buch gekauft, wenn man die darin enthaltene Information haben wollte, so kauft man heute eine Lizenz – die handelbare Konkretisierung von Wissen: »Wenn Information jemals frei sein wollte, muss sie es sich anders überlegt haben, denn nach UCC 2B scheint Information die Absicht zu haben,<t-1> lizensiert zu werden« (<x@6 Caps><t-1>Samuelson<@$p><t-1>, 1998). Der schwer wiegendste Aspekt <t$>des UCITA ist, dass er den Trend einen gewaltigen Schritt vorantreibt, Vertragsrecht über Urheberrecht dominieren zu lassen (und damit US-Landesrecht über Bundesrecht<\!q><@3 hoch fliess>35<@$p>). Zwar heißt es in Ziffer 105 UCITA, dass Bundesgesetze Vorrang haben vor den UCITAs der Länder sowie natürlich vor allen mit ihnen kompatiblen Lizenzen, doch ausgerechnet für das Copyright scheint dies nicht zu gelten. Das Wort »Copyright« taucht in dem länglichen Gesetzestext nur viermal peripher auf, obgleich der UCITA an zahlreichen Stellen in den Geltungsbereich des Copyright-Rechts hineinragt. Während z.B. ein Copyright nach Ablauf einer Frist verfällt, d.h. der Eigentumsstatus des Wissens sich verändert, kann nach dem UCITA ein Lizenzvertrag den Kunden auf alle Zeiten binden. Die Aufgaben der Bibliotheken, wie Zugänglichmachung und Erhaltung von Wissen, können vertraglich und technisch unmöglich gemacht werden.<@3 hoch fliess>36<@$p> Was das Coypright nach der <@1 fliess kursiv>Fair Use<@$p>-Doktrin erlaubt, kann eine Lizenz verbieten. Anbieter von Information können ihre Kunden und auch ihre Autoren zwingen, auf Rechte zu verzichten, die sie nach dem Copyright-Gesetz haben. Wem’s nicht gefällt, der kann ja woanders kaufen oder verkaufen. Der Markt wird’s schon regeln.Samuelson zieht das Fazit, dass es bei der Aushebelung des Copyright-Rechts durch Massenmarktlizenzen darum gehe, »ob Copyright-Inhaber beides auf einmal haben können.« Das Copyright schreibt mit Ve<t-2>rfassungsmandat eine Balance zwischen den Rechten der Autoren und der Öffentlichkeit vor. Diese Balance sei Angriffen durch die überbreite Copyright-Gesetzgebung der jüngsten Zeit, durch einen verstärkten Schutz für Datenbanken und nun durch Vertragsrecht ausgesetzt: »Wir sollten sie damit nicht durchkommen lassen. Wenn die Verleger die Rechte haben wollen, die das Copyright ihnen gewährt, müssen sie auch die Verantwortung übernehmen, die diese Rechte mit sich bringen« (<x@6 Caps><t-2>Samuelson<@$p><t-2>, 1998).<t$> Wenn dieses Beispiel in Deutschland Schule machen sollte, würde legalisiert, was die Anbieter von digitalem Wissen heute ohnehin bereits ihren Kunden abverlangen: dass diese auf ihre Rechte zur Dekompilierung, zur Erstellung einer Sicherungskopie, zum Weiterverkauf usw. verzichten. Ein Ausschluss des Verleihrechts würde den Auftrag der öffentlichen Bibliotheken, Wissen bereit zu stellen, unmöglich machen. So warnt die Copyright-Beauftragte der <@1 fliess normal>Bundesvereinigung Deutscher Bibliotheksverbände,<@$p> Beger: »Das bedeutet, dass bei zunehmender Verbreitung über Lizenzverträge die Ausnahmetatbestände in den Urheberrechtsgesetzen ihre Bedeutung verlieren, soweit sie nicht zwingend anzuwenden und durch Vertrag nicht auszuschließen sind. Deshalb muss es Ziel aller Bemühungen im Rahmen von Gesetzgebungsverfahren sein, zwingende Normen zu den Ausnahmetatbeständen zu erreichen, die den ungehinderten Zugang zu Informationen für jedermann auch im digitalen Umfeld in Übereinstimmung mit dem Drei-Stufen-Test gemäß Artikel 9, Absatz 2 RBÜ gewährleisten.«<@3 hoch fliess>37<@$p>Während Urheberrecht und Copyright theoretisch darauf zielen, das Verhältnis zwischen Autorinnen und Öffentlichkeit zu regeln, ist es tatsächlich die Rechteindustrie, die mit Hilfe von Verträgen einerseits den Autorinnen und andererseits der Öffentlichkeit die Nutzungsbedingungen diktiert. Der Professorenentwurf könnte im ersten Teil der Rechtetransaktion Abhilfe schaffen. Der Öffentlichkeit stehen jedoch zusätzlich zu den Lizenzverträgen auch noch technische Systeme zu ihrer Einhaltung bevor. »Dabei handelt es sich um elektronische Verträge, deren Einhaltung von den ›Informationsbehältern‹ selbst erzwungen wird.« (<@6 Caps>Stefik<@$p>)@2  ZÜ 2:Code statt Recht: Rechtekontrollsysteme@1 fliess mit:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Dieselben technischen Mittel, die die globale Nutzung der Netze ermög<\h>lichen, werden auch die globale Kontrolle dieser Netze ermöglichen.«<\!q><@3 hoch fliess>38<@$p>@1 fliess ohne:<*h"mehr">Werke existieren heute in digitaler Form in einer auf Code beruhenden Umgebung. Da ihre Existenzbedingungen und die Modi ihrer Übertragung, Speicherung und Veränderung programmiert und programmierbar sind, müsste es doch eigentlich möglich sein, jedem Werk eine »intelligente«, aktive Copyright-Information mitzugeben, die seine Verwendung regelt. Grundsätzlich ist das Problem dasselbe wie bei digitalem Geld: Die beteiligten Parteien müssen eindeutig identifiziert und die betroffenen Zeichenketten kryptografisch zu Unikaten gemacht werden. Ein Werk könnte – eine geeignete Infrastruktur vorausgesetzt – selbst »wissen«, von wem, wo, wie oft, in welcher Weise und zu welchem Preis es konsumiert werden darf. Ein Dokument könnte »selbsttätig« seinen Rechteinhaber über seinen Verbleib unterrichten. Software oder Daten könnten sich beim Eintreffen bestimmter Voraussetzungen (Ablauf oder Bruch der Nutzungslizenz) gar »selbst« zerstören. @1 fliess mit:Genau eine solche umfassende Infrastruktur zu errichten ist derzeit das Hauptprojekt der Rechteindustrie. Unter Namen wie <@1 fliess kursiv>IP-Management Systems<@$p>, <@1 fliess kursiv>Automatic Rights Management<@$p>, od<@1 fliess normal>er Trusted Systems we<@$p>rden <\n>E-Commerce-Lösungen für Texte, Bilder, Bewegtbilder, Musik, Multimedia, Interaktionen (z.B. Online-Games) und Computerprogramme entwickelt, die einen Grad an Kontrolle über Werknutzungen erlauben sollen, wie er nie zuvor in der Geschichte des Urheberrechts vorstellbar war. Eine generische Bezeichnung hat sich noch nicht durchgesetzt. Häufig werden sie <@1 fliess kursiv>Digital Rights Managment-Systeme <@$p>(DRM) genannt, eine doppelt irreführende Bezeichnung, da es bei diesen Systemen nicht um Verwaltung oder Geschäftsführung geht und sie außerdem auch analoge <\n>Medien wie Fotokopien erfassen. Daher wird hier wird durchgängig die Bezeichnung <@1 fliess kursiv>Rights Control Systems<@$p> (Rechtekontrollsysteme – RCS) verwendet. Eine Vorreiterrolle bei ihrer Etablierung spielt die Musikindustrie. Für das Netz hat das Audio-Codierungsformat MP3 dabei die größte Sichtbarkeit und damit die größte Regulierungsdringlichkeit erlangt. Die Filmindustrie entwickelte ähnliche Aktivitäten für den Offline-Datenspeicher DVD. Für gestaltete Textdokumente hat sich das <@1 fliess kursiv>Portable Document Format<@$p> <@4 Pfeil (Umschalt/Alt #)>’<@$p> (PDF) als Standard etabliert, für das ebenfalls entsprechende kryptografische »Kapseln« entwickelt werden. Die vermeintliche Bedrohung, die für die Datenherren in der freien Zirkulation von Wissen liegt, begreifen sie als Gelegenheit zu Errichtung von flächendeckenden Kontrollsystemen.In diesem Abschnitt werden eine Reihe solcher RCS-Technologien vorgestellt. Mark Stefik vom Xerox PARC ist nicht nur einer ihrer Vordenker, sondern spielt bei ihrer konzeptuellen Entwicklung und techni<t-1>schen Implementierung eine zentrale Rolle. Um den gesamten weltweiten<t$> Wissensraum zu erfassen, sind zu allererst Nummerierungsstandards erforderlich. Die zentrale Technologie, um Werke gegen nicht autorisierte Nutzungen zu schützen, ist die Kryptografie. Eine Fülle von öffentlichen Forschungsprojekten, Firmen und Industriekonsortien sind in diesem Bereich aktiv. Wenn sowohl die Schutzsysteme versagen als auch eine Handhabe gegen Internet-Rechner mit nicht mehr geschützten Werken unmöglich ist, soll, nach den Vorstellungen des deutschen Verbandes <\n>der internationalen Musikindustrie, ein elektronischer Schutzwall um Deutschland wenigstens verhindern, dass Bundesbürger sich diese Werke beschaffen. Da, wie an allen behandelten Beispielen aufgezeigt wird, technische Maßnahmen untauglich sind, digitale Werke zuverlässig zu schützen, greifen die Gesetzgeber flankierend ein, um über die ohnehin bestehenden Verbotsrechte hinaus die Schutztechnologien zu schützen. Der Abschnitt schließt mit einer kritischen Diskussion und Überlegungen zu den Auswirkungen von RCSs auf die Wissensordnung. @2  ZÜ 3:Mark Stefik: Trusted Systems@1 fliess ohne:Eine Art abstrakten Generalplan für RCSs gab Mark Stefik in seinem Ende 1994 geschriebenen Aufsatz »Letting Loose the Light« vor (<@6 Caps>Stefik<@$p>, 1996). Stefik ist ein führender Wissenschaftler im Labor für Informationswissenschaften und -technologie und heute Leiter der neuen <@1 fliess kursiv>Secure Document Systems<@$p>-Gruppe am <@1 fliess kursiv>Xerox Palo Alto Research Center<@$p> (PARC). In seinem Aufsatz greift er Barlows Herausforderung vom »Tod des Copyright« und die damals allgemein verbreitete Ansicht auf, dass digitale Informationsbehälter inhärent durchlässig seien, und setzt dem ein Szenario für eine Infrastruktur aus »<@1 fliess kursiv>Trusted Systems<@$p>« entgegen.<@3 hoch fliess>39<@$p> Digitale Werke, einmal im Internet veröffentlicht, würden sich – freigesetzt wie der Geist aus der Flasche – verbreiten, ohne dass Autoren und Verlage die Möglichkeit hätten, eine Kompensation zu erhalten. Aus diesem Grund würden Autoren und Verlage keine hochwertigen Informationen ins Netz stellen. Eine Lösung dieses Problems sei also auch im Interesse der Informationskonsumenten. Wie in der magischen Logik nur das Schwert die Wunde, die es schlug, heilen kann, so ist auch hier die Lösung eines technologischen Problems technologisch: »Richtig entworfene Digital<\h>sys<\h>teme können stärkere und flexiblere Instrumente für den Handel mit Publikationen sein als jedes andere Medium. Der scheinbare Konflikt zwischen digitaler Veröffentlichung und Vermarktung ist lediglich eine Folge davon, wie Computersysteme bis heute entworfen worden sind« (<@6 Caps>Stefik<@$p>, 1996, S.<\!q>6).@1 fliess mit:Steffik hält es für das Problem des bisherigen Computerdesigns, dass es sich um Allzweckrechner mit Allzweckbetriebssystemen und All<\h>zweckprogrammen handelt. <@4 Pfeil (Umschalt/Alt #)>’<@$p> Turings Universalmaschine möchte er ersetzen durch Spezialmaschinen mit dem alleinigen Zweck, Urheberrechte zu wahren. Eine Hierarchie von <@1 fliess kursiv>Trusted Systems<@$p> würden als eine Art Verkaufsautomaten für digitale Werke jeder Art fungieren, die die Werkstücke außerdem mit einer Liste erlaubter und verbotener Nutzungsmöglichkeiten ausstatten. <@1 fliess kursiv>Trusted Systems<@$p> meint nicht etwa ein wechselseitiges Vertrauensverhältnis zwischen Anbieter und Käufer von Information, das z.B. die Datenschutzbedürfnisse der Kunden berücksichtigen würde. Vertrauen sollen sie bei der Rechteindustrie wecken, indem sie deren differenzierte Schutzbedürfnisse »unfehlbar« implementieren und zugleich allen Informationsnutzern betrügerische Absichten unterstellt.<@3 hoch fliess>40<@$p><@1 fliess kursiv>Trusted Systems<@$p> sind <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Blackboxes<@$p>, die den Content und seine Nutzungsumgebung enthalten. Stefik nennt sie auch »Repositorien«. Auf der Client-Seite denkt er dabei an persönliche Unterhaltungsgeräte zum Abspielen von Musik oder Filmen, Video-Game-Konsolen oder kreditkartengroße Geräte. Filmdarstellungsgeräte könnten zwischen Heim-, Kino- und Sendegebrauch unterscheiden. Rundfunksender und -empfänger könnten mit unterschiedlichen Rechten ausgestattet sein. Sie alle enthalten Computer, aber auch ein PC als solcher kann ein Repositorium sein.<@3 hoch fliess>41<@$p> Gegen dessen universelle Verwendung wird er durch eine Kombination von Hard- und Software geschützt. Ein verschweißtes Gehäuse  verhindert nach Stefiks Vorstellung auf der niedrigsten Sicherheitsstufe ein physikalisches Eindringen in ein Repositorium. Auf einer höheren Stufe würde ein Sensor den Versuch wahrnehmen und Schlüsseldaten löschen. Ein <@1 fliess kursiv>Trusted System<@$p> der höchsten Stufe würde um Hilfe telefonieren, einen Alarm auslösen und sich selbst zerstören.Kryptografie und Zertifizierung würden Information und Software im Repositiorium und auf dem Transport über offene Netze vor unerwünschten Zugriffen schützen. Versteckte Identifikationsinformationen würden es erlauben, unrechtmäßige Kopien ausfindig zu machen und auf das Repositorium zurückzuführen, das sie in Umlauf gebracht hat. Stefik sieht ferner eine Definitionssprache für Nutzungsrechte vor, eine <@1 fliess kursiv>Copyright Markup Language<@$p>, ähnlich <@4 Pfeil (Umschalt/Alt #)>’<@$p> HTML für <@4 Pfeil (Umschalt/Alt #)>’<@$p> Hypertext. Marker sagen, was ein Kunde mit einem Werk oder einem seiner Bestandteile machen darf und was er dafür bezahlen muss. Marker können nicht entfernt werden. Die Nutzungsrechte, die dieser elektronisch implementierte Vertrag festschreibt, betreffen den Transport der Werke, ihre Darstellung, ihre Verwendung in abgeleiteten Werken und das Anfertigen von Sicherungskopien.Bei den Rechten zur Darstellung (<@1 fliess kursiv>rendering<@$p>) unterscheidet Stefik das Abspielen (von Musik, Film, Game, Text) und das Drucken. Das Abspielen erlaubt natürlich die Rezeption des Werkes. Damit ist es möglich, vom Bildschirm oder Lautsprecher analoge Kopien anzufertigen. Dies ist nicht zu verhindern, aber für die Rechteinhaber zu verschmerzen, da diese von einer deutlich niedrigeren Qualität sind als das digitale Werkstück. Problematisch aus ihrer Sicht ist das Drucken: »Der Begriff ›Drucken‹ bedeutet im digitalen Bereich, die Kopie eines medialen Werkes außerhalb der Gebrauchsrechtekontrolle zu erstellen, sei es auf Papier oder indem man sie in eine Datei auf einem externen Speichergerät schreibt.« Hier müssten somit die stärksten Schutzmechanismen greifen. Ein Transferrecht würde es gestatten, ein digitales Werk, genauso wie ein ausgelesenes Taschenbuch, an einen Freund weiterzugeben. A und B verbinden dazu ihre Repositorien, A überträgt das Werk (mit »Bewegen«, nicht mit »Kopieren«), so dass es nun bei B, aber nicht mehr bei A vorliegt. A könnte B das Werk auch nur für eine Woche ausleihen. In dem Fall wäre es immer noch bei A vorhanden, doch könnte er erst nach Ablauf der Leihfrist und nachdem es automatisch bei B gelöscht und bei A freigeschaltet worden ist, wieder darauf zugreifen.<@3 hoch fliess>42<@$p> Mit diesem Mechanismus wäre auch ein Leihverkehr von öffentlichen Bibliotheken denkbar. Ausgeliehene Werkexemplare würden im Bibliotheksrepositorium temporär deaktiviert. Ihre »Rückgabe« bestünde darin, dass sie beim Leser gelöscht und in der Bibliothek wieder aktiviert würden. Stefiks Ziel dabei ist es, die Zahl der im Umlauf befindlichen Kopien identisch zu halten.Auch eine private Vervielfältigung ist vorstellbar, wenn A ein Musikstück selber behalten, aber es auch an B verschenken möchte oder wenn B so begeistert von As Berichten über ein neues Computergame ist, dass sie eine eigene Kopie haben möchte. In diesem Fall würde ein mit den entsprechenden Rechten versehenes Werkstück die Abbuchung der Gebühr für diese Kopie entweder bei A oder B veranlassen. In der Möglichkeit dieser »Konsumenten gestützten Distribution« sieht Stefik einen besonderen Vorteil seines Systems.<@3 hoch fliess>43<@$p> In der Tat muss die Vorstellung, dass jeder Konsument eines Werkes dessen potenzieller Mikro-PR-Agent und -Verkäufer ist, verlockend für eine Industrie sein, der so die Arbeit abgenommen wird.Jedes Repositorium wäre mit einem Kredit-Server ausgestattet, der verschiedene Zahlungsmodalitäten anbietet. Um eine Kaufentscheidung zu fördern, könnten kurze Ausschnitte aus dem Werk kostenlos dargestellt werden (<@1 fliess kursiv>pre-listening, pre-viewing<@$p>). Einzelne Nutzungsrechte würden zu spezifischen Konditionen (Darstellung für eine Stunde, fünf Jahre, zeitlich unbegrenzt) verkauft werden. Die Nutzung von Werken, die laufend aktualisiert werden, wie Enzyklopädien, könnten pro Zugriff (<@1 fliess kursiv>pay-for-play<@$p>) oder als Subskription mit einer bestimmten Nutzungsdauer <\n>pro Monat erworben werden. Tageszeitungen könnten unterschiedliche Preise für die einmalige Darstellung und für das Recht der Archivierung verlangen. Um alle heute an der Wertschöpfungskette beteiligten Parteien auch in einer digitalen Ökonomie profitieren zu lassen, schlägt Stefik ineinander geschachtelte Nutzungsrechte vor. Der Autor eines Romans legt die Rechte und Gebühren fest, zu denen er sein Werk veröffentlicht. Dazu kapselt er das Werk in eine erste kryptografische Hülle (<@1 fliess kursiv>shell<@$p>). Sein Verleger fügt eine weitere Hülle hinzu, in der er den Preis für seine Leistungen (Werbung etc.) festlegt. Auch der Distributor und der Buchhändler umgeben das Werk mit ihren <@1 fliess kursiv>Shells<@$p>. Wenn schließlich ein Kunde eine Kopie des Werkes kauft, öffnet sein Repositorium eine Hülle nach der nächsten, wie bei einer russischen Puppe, interpretiert die Nutzungsbedingungen und veranlasst den Kredit-Server, jeder Partei den entsprechenden Betrag zu überweisen.<@3 hoch fliess>44<@$p>Ein dritter Komplex von Nutzungsrechten neben Darstellung und Transfer/Kopie umfasst die Zweitverwertung und die Erstellung von abgeleiteten Werken. Eine College-Professorin, die eine Textsammlung für einen Kurs zusammenstellt, kann, sofern die Ausgangswerke dies zulassen, von einem »Extraktionsrecht« Gebrauch machen. Ist das »Original« mit einem »Editierrecht« versehen, kann sie in dessen Grenzen Änderungen vornehmen und die Ausschnitte dank eines »Einbettungsrechtes« in ihre Sammlung aufnehmen. Die Kompilation kann sie mit einer zusätzlichen Hülle umgeben, in der sie ihre eigenen Nutzungsbedingungen spezifiziert. Kauft nun ein Student das Kursmaterial, werden den Rechteinhabern jedes Bestandteils und der Professorin Gebühren gutgeschrieben. Bislang setzte die Erstellung einer solchen Kompilation das aufwändige Einholen aller einzelnen Rechte voraus, weshalb ihre Erteilung nach der <@1 fliess kursiv>Fair Use<@$p>-Doktrin implizit unterstellt wurde und ohne Gebührenzahlungen erfolgte. Ein automatisiertes RCS dagegen würde, so Stefik, die kommerz<t-2>ielle Wiederverwendung von Werken erheblich fördern. Digitale »Tickets« <t$>sollen, nach dem Vorbild einer Eintrittskarte oder einer Fahrkarte, die einmalige Nutzung eines Werkes gestatten. Sonderangebote oder Werbegeschenke könnten über Tickets realisiert werden. Software würde mit einer Anzahl Tickets ausgeliefert, die es erlauben, Upgrades zu bestellen. <t1>Ein besonders brisantes Konstrukt sind die digitalen »Lizenzen«, die sich Stefik als eine Art Führerschein vorstellt. Es geht also nicht um eine Lizenz, die einem Werkstück anhaftet, sondern um die Zertifizierung eines Informationsnutzers, die es ihm erlaubt, bestimmte, vom Werk vorgesehene Nutzungen vorzunehmen. Als Beispiele nennt er Lizenzen für Bibliothekare, Bibliotheksnutzer, Lehrer, Schüler, Arme.<x@3 hoch fliess><t1>45<@$p><t1> Auf dieselbe Weise sollen auch Händler und Distributoren lizenziert werden. Würde ein Händler versuchen, ein Computerspiel zu verkaufen, bei der Abfrage seiner Lizenz jedoch nicht dazu autorisiert werden, würde sein Repositorium den Transfer in das des Kunden verweigern. An anderer Stelle spricht Stefik von »Identitätszertifikaten«, die das Alter eines Kunden abrufbar und damit einen automatisierten Jugendschutz möglich machen.<t$>Für diese Zertifizierung benötigt Stefik einen letzten Baustein in seiner umfassenden Kontrollinfrastruktur, eine autoritative Institution, ein Master-Repositorium der Hochsicherheitsstufe, das er als <@1 fliess kursiv>Digital Property Trust<@$p> (DPT) bezeichnet. Der DPT müsste die Interessen der verschiedenen beteiligten Parteien ausbalancieren und die »Gesundheit« des digitalen Verlagswesens sichern. Vor allem aber müssten die mächtigsten Marktteilnehmer dahinter stehen: »Der DPT würde die Vertretung und Unterstützung der mächtigsten Technologieanbieter und Verleger benötigen, damit seine Entscheidungen die erforderliche Durchsetzungskraft bekommen.«<@3 hoch fliess>46<@$p> Der DPT soll eine gemeinsame operationale Terminologie von Nutzungsrechten etablieren, die Sicherheit von <@1 fliess kursiv>Trusted Systems<@$p> zertifizieren und unterhalten sowie den Handel mit digitalen Werken fördern. Er würde digitale Zertifikate ausgeben, die sicherstellen, dass bestimmte Plattformen und Software das Konzept der Nutzungsrechte wahren und erzwingen. Bei ihm würden die öffentlichen Schlüssel jedes einzelnen<@1 fliess kursiv> Trusted Systems <@$p>hinterlegt. Die Kosten für den Betrieb des DPT sollen aus Gebühren für Repositoriumstransaktionen und für erneuerbare Lizenzen für Plattformen und Software bestritten werden. Jedes Repositorium müsste sich bei jeder Transaktion mit einem anderen durch ein Zertifikat von einem Master-Repositorium identifizieren. Wird bekannt, dass die Sicherheit eines Repositoriums kompromittiert wurde, weigerten sich andere Repositorien, mit ihm weitere Transaktionen durchzuführen.Die Chance, eine RCS-Infrastruktur flächendeckend durchzusetzen, s<t-1>ieht Stefik vor allem dort, wo neue Technologiegenerationen wie das digi<\h>ta<t$>le Fernsehen eingeführt werden. Kontrollierte Räume, wie Buchhandlungen, in denen man sich Dokumente ausdrucken lassen kann, und <\n>geschlossene Plattformen, wie Video-Game-Konsolen, die keine Kompatibilität mit Allzweckbetriebssystemen wahren müssen, erscheinen ihm ebenfalls besonders geeignet. Drei Jahre später schreibt Stefik rückblickend, 1994 sei die Vorstellung von digitalen Rechten und<@1 fliess kursiv> Trusted Systems <@$p>erschütternd und unerwartet gewesen, heute (1997) jedoch habe sich die Situation sehr gewandelt. Erstens seien technologische Ansätze für RCSs verfügbar, zweitens begännen die an digitalem Eigentum interessierten Parteien ihre Positionen zu artikulieren. Kollektiv verstehe die Gesellschaft die Verschiebung jedoch noch immer nicht. »Die Ideen fordern den gesunden Menschenverstand im Hinblick auf Computer und Information heraus. Sie müssen sich langsam setzen« (<@6 Caps>Stefik<@$p>, 1997a).Seine eigenen Konzepte haben sich gegenüber »Letting Loose the Light« nicht wesentlich verändert. Die Frage der Grenzenlosigkeit des Internet sieht er durch die Identifizierung und Zertifizierung von Informationskonsumenten als gelöst an. Da auch deren Mobilität zunimmt, fügt er die Idee hinzu, Laptops in einem Land zu registrieren, so wie heute bereits Schiffe unter einer Nationalflagge fahren, gleich, wo auf der Welt sie sich befinden: »Digitale Zertifikate können praktikabel eingesetzt werden, um in den Cyberspace Repräsentationen von Grenzen und Identitäten einzuführen, was eine Kontrolle praktisch durchführbar macht.«Ein weiteres Problem wurde an ihn herangetragen: das der Sicherungskopie, die einen unerlässlichen und de jure zugestandenen Schutz dagegen bietet, dass Datenträger unlesbar werden. Stefiks Antwort darauf ist es, weitere Rechte für <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Backup<@$p> und <@1 fliess kursiv>Restore<@$p> einzuführen. Diesen Rechten können Bedingungen und Gebühren anhängen, z.B. könnte ein <@1 fliess kursiv>Trusted System<@$p> beim Verleger um Erlaubnis bitten, bevor es die Sicherungskopie wiederherstellt. »Dies ist eine weitere Stelle, an der der Wettbewerb zu einem de facto Recht führen kann, Back-up-Kopien zu erstellen und wiederherzustellen« (ebd.). Eine weitere Neuerung ist die Idee von <@1 fliess kursiv>Trusted Printers<@$p>: »<@1 fliess kursiv>Trusted Printers<@$p> (vertrauenswürdige Drucker) kombinieren vier Elemente: Druckrechte, kryptografische Online-Distribution, automatische Abrechnung von Kopien sowie digitale Wasserzeichen zur Kennzeichnung der gedruckten Kopien.« Letztere sollen Pixelmuster (Wasserzeichen) mit Copyright-Informationen in die Druckflächen eines Ausdrucks einfügen, die von einem<@1 fliess kursiv> Trusted <@$p>Kopierer oder Scanner erkannt und ausgewertet werden können.Stefik diskutiert auch die Frage, was mit Werken in<@1 fliess kursiv> Trusted Systems <@$p>geschieht, wenn ihre Schutzfrist abläuft. Ein Verfallsdatum des Schutzes in das Werkstück einzuschreiben, scheitert daran, dass weder der Tod eines (oder gar mehrerer) Autoren noch Änderungen in der nationalen Gesetzgebung während der Laufzeit vorherzusehen sind. Unterschiede in den nationalen Schutzfristen könnten dazu führen, dass ein Werk in einem Land bereits gemeinfrei, in anderen noch geschützt ist. Stefiks Lösungsvorschlag ist, öffentliche Einrichtungen wie die <@1 fliess kursiv>Library of Congress<@$p> damit zu betrauen, den Zeitpunkt festzustellen und dann eine RCS-freie <@1 fliess kursiv>Public Domain<@$p>-Version des Werkes bereitzustellen. Die RCS-gekapselten Werkstücke blieben davon unberührt, sie könnten nach Stefiks Vorstellung weiterhin kostenpflichtig angeboten werden. Stefik mahnt: »<@1 fliess kursiv>Trusted Systems<@$p> verschieben die Balance und verleihen den Verlegern mehr Macht.« Um nun die andere Seite der Balance, die öffentlichen Interessen z.B. von Bibliothekaren und »Konsumenten«, wieder zu stärken, sieht er dedizierte soziale Institutionen vor und nennt den <@1 fliess kursiv>Digital Property Trust<@$p> als ein Beispiel dafür. Vertreter der Öffentlichkeit sollen in den Entscheidungsgremien des DPT sitzen. Durch die Effizienz, die die Automatisierung des E-Commerce mit digitalen Werke mit sich bringe, sei es vernünftig, die »seltenen Fälle«, in denen ein öffentliches Interesse den kommerziellen Interessen gegenübersteht, nach menschlichem Ermessen zu entscheiden. Im Falle einer <@1 fliess kursiv>Fair Use<@$p>-Verteidigung könnte der DPT ein Schlichtungsverfahren anbieten. Für besondere Nutzergruppen, wie Bibliothekare, Forscher und Lehrer könnten <\h>besondere Lizenzen eine vergünstigte oder kostenfreie Nutzung ermög<\h>lichen.<@3 hoch fliess>47 <@$p>Da Stefik hierin gleich wieder die Gefahr einer Verschiebung der Balance zugunsten der Öffentlichkeit sieht, schlägt er vor, das Risiko dieser »weniger kontrollierten« Nutzungen mit Hilfe einer Versicherung auszugleichen. Jeder Schritt in der Kette der Transaktionen beinhalte eigene Risiken. Auch hier solle der DPT einspringen und mit Hilfe einer weiteren Gebühr für jede Transaktion eine Versicherung abschließen. Der DPT spielt eine zentrale Rolle in Stefiks Überlegungen, doch stehe eine organisierte Bewegung zu seiner Formierung noch aus. Im Gegenteil arbeiteten verschiedene Unternehmen an Patent-Portfolios, um ihre eigenen Rechtekontroll-Technologien zu schützen. Dies könne zu Marktverwirrung und Patentstreitigkeiten führen. Daran, dass sich ein Markt für<@1 fliess kursiv> Trusted Systems <@$p>entwickeln wird, bestehe kein Zweifel, die Geschwindigkeit und Reichweite dieser Entwicklung hinge jedoch davon ab, ob die Wettbewerber zusammenarbeiteten, um Standards zu fördern. In einem Aufsatz aus demselben Jahr spricht Stefik die Kosten für den Endverbraucher an: »Zuerst würden die Sicherheitsmerkmale in einem Drucker oder einem tragbaren digitalen Lesegerät gebündelt. Dies wäre mit Zusatzkosten für die Verbraucher verbunden, da diese die Möglichkeit hätten, auf höherwertiges Material zuzugreifen. Die Kosten würden schließlich sinken, wenn sich die Technologie weithin durchsetzt« (<@6 Caps>Stefik<@$p>, 1997b). Wiederum ist das Argument dafür, dass »Konsumenten« sich auf ein Arrangement einlassen sollten, »in dem sie weniger als die absolute Kontrolle über Gerätschaften und Daten in ihrem Besitz haben«, dass andernfalls die Rechteindustrie hochwertige Information nicht in digitaler Form verfügbar machen würde. Hier spielt Stefik erstmals explizit seinen technologischen Ansatz gegen den der Gesetzgebung aus: »In einigen Fallen könnte dieser technologische Ansatz, Autoren und Verleger zu schützen, die Notwendigkeit von massiven gesetzlichen Regeln vermeiden, die den digitalen Veröffentlichungsmarkt ersticken könnten.«Ebenfalls zum ersten Mal taucht der Begriff »<@1 fliess kursiv>Privacy«<@$p> auf, in einem bezeichnenden Zusammenhang: »Die Verleger müssen Maßnahmen ergreifen, um die Privatsphäre der Verbraucher, die <@1 fliess kursiv> Trusted Systems <@$p>nutzen, zu schützen, obwohl dieselbe Technologie, die die Eigentumsrechte der Verleger schützt, auch die persönlichen Angaben über die Verbraucher schützen könnte« (ebd.).Stefiks Visionen wurden hier so ausführlich dargestellt, weil sie im Gegensatz zu den meisten anderen Publikationen in diesem Bereich eine vollständige Infrastruktur durchdenken und die zu Grunde liegende Vorgehensweise deutlich machen. Er setzt mit seinem Modell nicht direkt beim Copyright-Recht an, sondern bei den Vertragsbeziehungen, die technologisch modelliert werden. Jede beteiligte Vertragspartei muss identifizierbar gemacht und jedes einzelne Verbreitungsstücke eines Werkes muss digital individualisiert werden. Vorbei die Zeiten, in denen man – ohne seinen Ausweis vorzulegen – eine Zeitschrift am Kiosk kaufen und seine Mitbewohner an deren Lektüre teilhaben lassen konnte. Wenn man sich einmal auf eine RCS-Infrastruktur eingelassen hat, wird jede potenzielle Durchlässigkeit durch das Hinzufügen eines weiteren<@1 fliess kursiv> Trusted Systems <@$p>geschlossen. Für alle Fragen, die die gesamte Infrastruktur betreffen (Standardisierung, Registrierung, Lizenzierung, Schlichtung), wird eine zentrale Instanz, der DPT eingesetzt. Alle Bedürfnisse, die der Markt nicht bedient – der öffentliche »Rest« –, werden an staatliche Einrichtungen wie die Kongressbibliothek delegiert. <*h"mehr">Heute sind viele der Elemente von Stefiks Systementwurf Wirklichkeit. Grundlage für die automatisierte Rechtekontrolle ist eine standardisierte Kennung für alle relevanten Elemente. Bisher verwendeten Verwertungsgesellschaften, Verleger, Produzenten und Rundfunkunternehmen jeweils ihre eigenen Nummerierungsschemata. Hier engagiert sich seit dem Aufstieg des Internet vor allem der internationale Dachverband der Verwertungsgesellschaften CISAC (<@1 fliess kursiv>Confédération Internationale des Sociétés d’Auteurs et Compositeurs<@$p>). Seit 1994 entwickelt und implementiert er das <@1 fliess kursiv>Common Information System <@$p>(CIS),<@3 hoch fliess>48 <@$p>das sicherstellen soll, dass die CISAC-Gesellschaften und ihre Partner weltweit standardisierte Nummerierungssysteme verwenden. Das CIS soll Berechtigte, Werke, Verträge, Aufnahmen, Tonträger und audiovisuelle Produktionen identifizieren und in einheitlichen Formaten über Online-Datenbanken weltweit zugänglich machen, die als gemeinsame Referenz für Rechteindustrie, Verwerter und Autoren dienen. Außerdem arbeitet das CIS-Gremium mit der Rechteindustrie zusammen, um Identifikatoren in die digitalen Objekte selbst einzubetten und Lösungen für ihre automatisierte Verfolgung und Überwachung zu entwickeln. Dazu beteiligt es sich an benachbarten Initiativen, wie dem EU-Projekt IMPRIMATUR (<@1 fliess kursiv>Intellectual Multimedia Property RIghts Model And Terminology for Universal Reference<@$p>)<@3 hoch fliess>49<@$p> und dem entstehenden MPEG-4 ISO-Standard für die Codierung, den Schutz und die Rechtekontrolle von audiovisuellen Informationen (mit Hilfe eines <@1 fliess kursiv>Intellectual Property Identification Data Set<@1 fliess normal>, <@$p>Wasserzeichen und Kryptografie). Einige der Rechtkontrollsysteme, die im Wesentlichen Stefiks Plan auf der technologischen Höhe der Zeit formulieren, sollen im Folgenden angesprochen werden.<*h"Standard">@2  ZÜ 3:Kryptografie@1 fliess ohne:Kryptografie ist die zentrale Technologie, um Inhalte bei der Übertragung, ob unkörperlich übers Netz oder auf voraufgezeichneten Medien, über die gesamte Kette vom Hersteller über den Vertrieb bis zum Endkunden gegen unautorisierte Nutzungen zu schützen und Inhalte, die <\n>einem RCS entkommen sind, ausfindig zu machen und auf ihren Eigen<\h>tümer zurückzuführen. Verschiedene Forschungseinrichtungen, Unternehmen und Industriekonsortien arbeiten an Lösungen für eine kryptografische Infrastruktur. @1 fliess mit:Eines der umfassenderen Rahmenkonzepte dafür haben Anfang 2000 die vier Unternehmen IBM, Intel, Matsushita und Toshiba (4C) mit der <@1 fliess kursiv>Content Protection System Architecture<@$p> (CPSA) vorgelegt (Intel u.a., 2/2000). Sie beschreiben deren Ziel so: »Die wachsende Zahl von Content-Schutzsystemen macht deutlich, dass eine übergreifende Systemarchitektur erforderlich ist, um sicherzustellen, dass die Einzelteile ein zusammenhängendes  Ganzes bilden.« Die 4C arbeiten eng mit der <@1 fliess kursiv>Secure Digital Music Initiative<@$p> (SDMI) zusammen. Der Rahmen schreibt den Schutz von analogen und digitalen Audio- und Video-Inhalten vor, die auf physikalischen Medien sowie elektronisch vertrieben werden. Neben den Elektronikgeräteherstellern richtet sich das Papier an Inhalteanbieter und Sendeunternehmen. Die CPSA legt elf »Axiome« fest, die die drei kritischen Aspekte eines Rechtekontrollsystems absichern sollen – Content-Management-Information (CMI), Zugang und Aufnahme – und an die sich konforme Geräte (z.B. Videorekorder, Fernseher, Set-Top-Boxen oder CD-Brenner für PCs) halten müssen. In der CMI legt der Eigentümer fest, auf welche Weisen seine Inhalte genutzt werden können. Sie sind gewissermaßen eine maschinenlesbare Form des Lizenzvertrages. Sie wird in einem Dateivorspann den Content<\h>daten vorangestellt. Sie kann zusätzlich auch in Form eines »Wasserzeichens« (s.u.) in die Daten selbst integriert werden. So kann auch bei einer Datei, deren <@4 Pfeil (Umschalt/Alt #)>’<@$p> Header entfernt oder manipuliert wurde, die CMI ausgelesen und vom jeweiligen Gerät befolgt werden. Die Axiome zur Zugangskontrolle beginnen: »Der gesamte CPSA-Content auf voraufgezeichneten Datenträgern ist kryptiert.« Kryptografie sei der Schlüsselaspekt der Architektur, »es ist der ›Angelhaken‹, der die Benutzer zwingt, die Vorschriften des <@1 fliess kursiv>Content Protection System<@$p> zu beachten.« Alle Kopien, sofern die CMI-Einstellungen sie erlauben, und <\h>alle Übertragungen an Ausgabegeräte müssen ebenfalls kryptiert sein. Die Axiome zu Aufnahmegeräten fordern, dass diese die <@1 fliess kursiv>Copy Control Information<@$p> (CCI) lesen, interpretieren und, falls zugestanden, beim Kopieren die CCI der Kopien entsprechend den Vorgaben setzen. Erlaubt ein Content z.B., dass eine Sicherungskopie angefertigt wird, so setzt ein konformes Gerät die CCI dieser Kopie so, dass von ihr keine weiteren <\n>Kopien erstellt werden können.<@3 hoch fliess>50<@$p> @2  ZÜ 3:Widerrufung »illegaler Geräte«@1 fliess ohne:Wie Stefiks Architektur benötigen diese Systeme eine zentrale Instanz, die die technischen Standards pflegt, ihre industrieweite Einhaltung durchsetzt, Lizenzen vergibt und kryptografische Schlüssel verwaltet. Eine solche Institution betreiben seit September 1998 die 5C (<@1 fliess kursiv>5 Companies: <@$p>Hitachi, Intel, Matsushita, Sony und Toshiba) unter dem Namen <@1 fliess kursiv>Digi<\h>tal Transmission Licensing Administration<@$p> (DTLA).<@3 hoch fliess>51<@$p> Die DTLA zertifiziert <\n>direkt Gerätehersteller, Betreiber von Content-Servern und Rundfunk<\h>stationen. Indirekt, nämlich mit Hilfe von <@1 fliess kursiv>System Renewability Messages<@$p> (SRMs), zertifiziert sie immer aufs Neue auch jedes einzelne Endgerät. Dieser Mechanismus zur Entziehung der Lizenz eines Gerätes, das z.B. zur Umgehung des Schutzsystems manipuliert wurde, ist besonders bemerkenswer<@1 fliess normal>t. »Systemerneuerbarkeit s<@$p>tellt die langfristige Integrität des Systems sicher und ermöglicht die Widerrufung nicht autorisierter Geräte.«<@3 hoch fliess>52<@$p>@1 fliess mit:Die SRM enthält neben einem Datum eine aktuelle <@1 fliess kursiv>Certificate Revocation List<@$p> (CRL). In dieser Liste stehen einzelne oder ganze Blöcke von Kennungen von Geräten, die nicht die Anforderungen der DTLA erfüllen. Es ist die Rede von »illegalen Geräten« und von »für die Umgehung des Schutzes verwendete, kompromittierte Geräte«. Anträge auf Widerrufung von Geräten können offenbar Regierung, Gerätehersteller und Content-Anbieter an die DTLA richten (<@6 Caps>Hitachi u.a<@$p>., 1998, S. 12, 13).Bei der Produktion von Geräten, Datenträgern (z.B. einer DVD) und Rundfunksendungen erhalten diese eine zu dem Zeitpunkt gültige SRM. Treten nun zwei Geräte, z.B. ein Abspiel- und ein Aufzeichnungsgerät, miteinander in Kommunikation, tauschen sie als erstes mit Hilfe des ersten Schlüssels ihre Zertifikate aus und verifizieren wechselseitig ihre Signaturen. Dabei wird anhand der SRM überprüft, ob das andere Gerät nicht »widerrufen« worden ist. Dann werden die zugestandenen Nutzungen der Daten (Darstellen, Kopieren, usw.) gelesen. Sind all diese Hürden genommen, wird ein zweiter Schlüssel für den Austausch der eigentlichen Content-Daten errechnet, der schließlich beginnen kann. Auf die gleiche Weise wird die Widerrufungsliste übertragen, wenn eine DVD abgespielt, eine entsprechend codierte Sendung per Kabel-TV empfangen oder ein Video-Stream aus dem Internet auf dem PC angeschaut wird. Ist die SRM des Content neuer als die des Geräts, so speichert das Gerät sie und gibt sie seinerseits weiter, wenn es mit anderen Geräten Daten austauscht. Trifft ein »legitimiertes« Gerät nun auf eines, dessen Nummer in der neuesten Widerruffungsliste enthalten ist, bricht es die Verbindung ab.Durch die wechselseitige Verifizierung, mit der jeder Datenaustausch beginnt, pflanzt sich die »schwarze Liste« durch das gesamte Netzwerk der vorhandenen Geräte fort, bis alle »illegalen Geräte«, alle Sicherheitslücken im Copyright-Schutz vollständig isoliert sind. Ein des Vertragsbruches verdächtiger PC würde nun von einer <@4 Pfeil (Umschalt/Alt #)>’<@$p> Set-Top-Box keine Sendungen mehr erhalten, ein digitaler Fernseher keine Signale von ihm mehr darstellen. Ob das »illegale Gerät« in seiner Funktionstüchtigkeit noch weitergehend eingeschränkt wird, was es nach seiner »Entlarvung« mit nicht geschütztem Inhalt machen wird oder ob gar eine Botschaft <\n>an die Zentrale ausgelöst wird, die es einer Copyright-Polizei erlauben würde, »illegale Geräte« ausfindig zu machen, geht aus der spärlichen veröffentlichten Information der DTLA nicht hervor.<@3 hoch fliess>53<@$p> Die Pläne haben zu einiger Unruhe in der Branche geführt, so dass sich die DTLA im Januar 1999 zu einiger »Klarstellung« gezwungen sah. Darin wird versichert, dass die Widerrufung eines Gerätes nur aufgrund eines »geregelten Verfahrens«, nur nach Aufforderung dazu autorisierter Behörden und mit einem Einspruchsrecht der betroffenen Hersteller verhängt werde. Sie werde nur in spezifischen Fällen eingesetzt, in denen Schlüssel kompromittiert, z.B. aus lizenzierten Geräten extrahiert und in andere Geräte kopiert wurden. Keineswegs werde sie sich gegen eine ganze Produkt<\h>linie eines Herstellers richten (<@6 Caps>DTLA<@$p>, 1999, S. 1 f.).Die System-»Auffrischung« durch Widerrufungslisten kann man sich als eine Art laufende virale Verbreitung von Existenzentzügen an <\n>Mediengeräte im Dienste der Verwertungsindustrie vorstellen. Eine zentrale Stelle setzt Codes in Umlauf, die durch das Netz der Heimelektronikgeräte diffundieren und alle »illegalen Gerät« auf ihrem Weg lahmlegen. Ein wahrer »Sieg der Botschaft über das Medium«. So wird jedes Mediengerät im Wohnzimmer ein Erfüllungsgehilfe der Rechteindus<\h>trie. Eines der berühmtesten Beispiele für die Regulierung mit Hilfe von Architektur sind die Brücken, die der New Yorker Stadtplaner Robert Moses in den 1920ern auf Long Island baute. Zu niedrig für die öffentlichen Doppeldeckerbusse, bildeten sie einen effektiven Filter, mit dem die Armen, vor allem Schwarze, vom Naherholungsgebiet der Wohlhabenden fern gehalten wurden. Schützte dort die gebaute Umwelt die Erhohlungsinteressen der Reichen, so schützt hier die digital »gebaute« Umwelt die Verwertungsinteressen der Datenherren. HiFi- und Videogeräte werden zu Copyright-Polizisten, die sich ohne Zutun ihrer Besitzer gegenseitig überwachen.@2  ZÜ 3:<\c>Content Scrambling System (CSS)@1 fliess ohne:CSS<@3 hoch fliess>54<@$p> ist ein Verfahren zum Schutz von voraufgezeichneten DVD-Video<\h>inhalten. Es wurde hauptsächlich von Matsushita und Toshiba entwickelt und wird von der <@1 fliess kursiv>DVD Copy Control Association<@$p> (CCA), einem gemein<\h>nützigen Unternehmen der Film- und der Konsumelektronikindustrie, lizenziert.<@3 hoch fliess>55<@$p> Die Lizenz selbst ist gebührenfrei, es wird jedoch eine Verwaltungsgebühr von 5<\!q>000 Dollar im Jahr erhoben.<@3 hoch fliess>56<@$p> @1 fliess mit:CSS beruht auf einem einmaligen Master-Schlüssel. Dieser wird wieder<\h>um mit einem Zugangsschlüssel verschlossen. Die lizenzierten Hersteller von DVD-Playern, gleich ob Hard- oder Software, erhalten von der CCA Zugangsschlüssel aus einem Satz von etwa 400 Stück. Jede DVD enthält 400 Kopien des Master-Schlüssel, kryptiert mit jedem der 400 Zugangsschlüssel. Das CSS-Modul des Players versucht nun, die DVD mit seinem Schlüssel zu öffnen. Gelingt es, gibt die DVD den <\h>Master-Schlüssel zusammen mit den für die DVD und die einzelnen Titel (z.B. eine Videosequenz) spezifischen Schlüsseln frei und die Entschlüsselung der eigentlichen Daten beginnt. Schließlich müssen diese noch dekodiert werden (DVD-Video verwendet MPEG-2-Kompression), bevor sie endlich dargestellt werden. Eine Gerätelizenz kann widerrufen werden, indem der entsprechende Schlüssel von allen neu produzierten DVDs entfernt wird. CSS verhindert also keineswegs die Erstellung von bitgetreuen Kopien einer DVD, sondern das Abspielen von DVDs auf Geräten, die nicht von der CCA autorisiert sind. Außer an einem nicht autorisierten Player kann das Abspielen auch daran scheitern, dass man sich im falschen Teil der Welt aufhält. Die <\n><@1 fliess kursiv>Regional Playback Control<@$p> (RPC) ist eine Besonderheit von CSS, die sich aus den spezifischen Vermarktungsmechanismen von Kinofilmen begründet.<@3 hoch fliess>57<@$p> Neue Filme werden zuerst in den Kinos gezeigt, danach im <@1 fliess kursiv>Pay-per-View<@$p>-Fernsehen, und schließlich werden sie als Kauf- oder Verleihvideo und im Fernsehen angeboten. Da es mit heutiger Celluloid-Vervielfältigungstechnologie nicht möglich sei, so die <@1 fliess kursiv>Motion Picture Association of America<@$p> (<@4 Pfeil (Umschalt/Alt #)>’<@$p> MPAA),<@3 hoch fliess>58<@$p> alle Erstaufführungskinos der Welt gleichzeitig mit Kopien zu versorgen, läuft z.B. ein neuer Hollywood-Film zuerst in den USA, einige Monate später in Europa und Japan und weitere Monate später in Lateinamerika an. Zu diesem Zeitpunkt werden in den USA bereits Video- und eben DVD-Kopien für den Privatmarkt verkauft, die, wenn sie nach Lateinamerika gelangen, den Kinomarkt untergraben würden. Die Studios verkaufen Vertriebsrechte an verschiedene Distributoren in diesen Regionen und wollen ihnen einen exklusiven Markt garantieren. Um die Kinos zu schützen, so die Filmindustrie, erlaubt es die RPC, DVDs in einer von acht Weltregionen ausschließlich auf Geräten dieser Region abzuspielen. Ohne diese Regionensperre, heißt es, müssten alle Konsumenten auf die DVD-Release warten, bis die Kinoverwertung des Films auf der ganzen Welt abgeschlossen ist. Bei allen Bemühungen der Industrie, die genaue Funktionsweise von CSS geheimzuhalten, muss die Technologie doch in jedem von Millionen von Geräten und in noch mehr DVDs ausgeliefert werden. Und dort scheint das Know-How nicht besonders gut gesichert zu sein. Auch vorher schon war so genannte <@1 fliess kursiv>Ripper<@$p>-Software verfügbar, die die Daten abfängt, wenn sie zur Darstellung entschlüsselt und dekodiert sind, und dann erlaubt, sie ungeschützt zu speichern. Auch Informationen zur Umgehung der Regionalsperre finden sich auf verschiedenen Internet-Sites. Schließlich verbreitete sich im Oktober 1999 das Program<@1 fliess normal>m »DeCSS« im Int<@$p>ernet, mit dem sich CSS umgehen lässt.<@3 hoch fliess>59<@$p> DeCSS »rät« innerhalb von einigen Sekunden die 400 Zugangsschlüssel. Verwendet wurde es, um DVD-Laufwerke unter GNU/Linux nutzbar zu machen, <\n>eine Plattform, die bis dahin wegen ihrer Quelloffenheit von der CSS-Lizenzierung ausgeschlossen war.<@3 hoch fliess>60<@$p> <t1>Bei DeCSS handelt es sich somit um ein <x@4 Pfeil (Umschalt/Alt #)><t1>’<@$p><t1> <x@1 fliess kursiv><t1>Reverse Engineering<@$p><t1> zur Herstellung von Interoperabilität, das in den meisten Rechtsordnungen legal ist (z.B. §<\!q>69e UrhG). Mit der Herstellung von nicht autorisierten Kopien hat es nichts zu tun, da CSS diese, wie gesagt, gar nicht verhindert. Selbst wenn private Kopien die Hauptsorge der Industrie wären, so sind diese und damit ihr möglicher Marktschaden auf »natürliche« Weise begrenzt. Angesichts der Datenmassen (4,7 GB auf einer einseitigen und 17 GB auf einer doppeseitigen DVD) ist es äußerst unwahrscheinlich, dass sich viele Menschen den Inhalt einer DVD aus dem Internet herunterladen. Ebenso unwahrscheinlich ist es, dass er auf selbst gebrannten DVDs weiterverbreitet wird, da ein Rohling etwa 40 bis 50 Dollar kostet, während kommerziell bespielte DVDs für 15 bis 30 Dollar angeboten werden<t$>. Solcher praktischen Überlegungen ungeachtet verklagten die DVD CCA (wegen Verstoßes gegen den Geschäftsgeheimnisschutz) und die MPAA (wegen Verstoßes gegen das Umgehungsverbot von Rechtekontrollsystemen nach dem neuen US-Copyright-Gesetz) die Betreiber von Websites, die DeCSS anboten. In einer ersten Entscheidung im Dezember 1999 lehnte es ein kalifornisches Gericht ab, eine einstweilige Verfügung gegen die Site-Betreiber zu verhängen. Doch im Januar 2000 revidierte der Richter sein Urteil. Auch ein Gericht in New York gab der MPAA-Klage gegen drei Websites statt. In Norwegen durchsuchte die Polizei die Wohnung eines mutmaßlich an der Entwicklung von DeCSS <\h>beteiligten damals 15-Jährigen und beschlagnahmte dessen Computerausrüstung sowie dessen Mobiltelefon. Im August 2000 entschied ein Bezirksgericht in New York, dass ein weiterer Webseitenbetreiber DeCSS nicht mehr zugänglich machen und auch nicht auf andere Sites linken darf, die es weiterhin anbieten. Die juristischen Angriffe gegen diejenigen, die DeCSS zugänglich machen, löste eine Welle der Empörung aus. Rechtsgelehrte und Informatiker schalteten sich mit Expertisen gegen die fachlich unsinnigen Vorwürfe ein. Die Kosten der Verteidigung werden von der <@1 fliess kursiv>Electronic Frontier Foundation<@$p> übernommen, die durch die Klage das Grundrecht auf freie Meinungsäußerung gefährdet sieht.<@3 hoch fliess>61<@$p> Unterstützung kommt auch von <@1 fliess kursiv>Berkman Center for Internet and Society<@$p> der Harvard Law School, das eine Gefahr darin sieht, dass digitalisierte Inhalte auf diese Weise effektiv dem öffentlichen Diskurs entzogen werden.<@3 hoch fliess>62<@$p> Im Mai 2001 ging das New Yorker Verfahren in die zweite Berufungsinstanz. Die Rektorin des Rechtsinsituts der Stanford Universität sprach sich darin für Redefreiheit und den Schutz der <@1 fliess kursiv>Public Domain<@$p> aus.<@3 hoch fliess>63<@$p>Weitere Initiativen wie die <@1 fliess kursiv>Global Internet Liberty Campaign<@3 hoch fliess>64 <@$p>und <@1 fliess kursiv>OpenDVD<@$p> engagieren sich gegen CSS. <@1 fliess kursiv>OpenDVD<@$p> wurde von Soft- und Hardwareingenieuren gegründet, die neue und bessere Wege suchen, um DVD industrieweit einzusetzen. Auch sie wollen die Rechte der Urheberrechtsinhaber schützen, aber es auch Konsumenten ermöglichen, eigene DVD-Videos herzustellen und von ihren <@1 fliess kursiv>Fair Use<@$p>-Rechten Gebrauch zu machen. Zu diesem Zwecke bildete sich das <@1 fliess kursiv>Linux Video and DVD Project<@$p> (LiViD), das im Februar 2001 das <@1 fliess kursiv>Open Media System<@$p> vorstellte, mit dem DVDs nun endlich auch unter dem Betriebssystem GNU/Linux abgespielt werden können.<@3 hoch fliess>65<@$p>CSS ist ein gutes Beispiel dafür, wie die verschiedenen Regularien <\n>ineinandergreifen, um die Interessen der Rechteindustrie zu schützen. Der Content ist ohnehin durch Copyright und darüber hinaus technisch durch das <@1 fliess kursiv>Content Scrambling<@$p>-System geschützt. Die CSS-Technologie selbst ist durch Geschäftsgeheimnis- und Patentrechte geschützt. Die Lizenzvorschriften legen den autorisierten Nutzern vertragsrechtlich weitgehende Restriktionen auf. Und schließlich untersagt das Verbot von Umgehungstechnologie für technische Schutzmaßnahmen in den neues<\h>ten Copyright- und Urhebergesetzen generell die Herstellung, den Besitz und die Verbreitung von DeCSS und ähnlichen Technologien. Das ganze System ist somit fünffach abgesichert. Bezahlen muss dafür die Kundin, ohne für den höheren Preis auch nur eine Minute Videoinformation mehr zu erhalten. CSS ist auch ein gutes Beispiel dafür, dass die technologischen, juristischen und politischen Auseinandersetzungen um die Rechtekontrolle in der digitalen Wissensgesellschaft noch ganz am Anfang stehen. @2  ZÜ 3:Wasserzeichen@1 fliess ohne:Von Geldscheinen und edlem Briefpapier sind uns Wasserzeichen vertraut.<@3 hoch fliess>66<@$p> In der Digitalwelt bezeichnet der Begriff eine weitere kryptografische Schutztechnik neben der Zugangskontrolle durch die Verschlüsselung der Inhalte und der Authentifikation durch digitale Signaturen. Steganografie nennt sich der Zweig der Kryptografie, der sich mit dem Verstecken von Informationen in Bildern oder Klängen befasst. Während eine gewöhnliche verschlüsselte Botschaft als solche erkennbar ist, kann ein Angreifer hier auf den ersten Blick nicht einmal feststellen, ob ein bestimmtes Bild geheime Information enthält. @1 fliess mit:Auch digitale Wasserzeichen<@3 hoch fliess>67<@$p> sind – meist mit dem bloßen Auge oder Ohr nicht wahrnehmbare – Markierungen in Audio-, Bild-, Text- oder Videodaten. Doch hier geht es nicht darum, auf diesem Weg einem Adressaten für Dritte nicht erkennbar Nachrichten zukommen zu lassen. In vielen Fällen kann sich jeder mit frei verfügbaren Werkzeugen die enthaltene Information anzeigen lassen. Vielmehr sollen, da Datei-Header oder andere »Umhüllungen« des Inhalts vergleichsweise leicht entfernt oder manipuliert werden können, Copyright-Informationen mit Wasserzeichen derart in die Inhaltsdaten selbst eingebettet werden, dass sie nicht wieder entfernt werden können, ohne diese Inhalte zu zerstören. Ein Werk und seine Nutzungsbedingungen sollen untrennbar miteinander verkoppelt werden. Folglich integrieren viele Wasserzeichensysteme den Kodierungsalgorithmus direkt in die Hardware von Digitalkameras und Scannern, so dass Bilder schon während der Aufnahme markiert werden.<@3 hoch fliess>68<@$p><t-1>Alle bereits beschriebenen Techniken von Kopiergenerationskontrolle<t$> bis Regionalsperre können in Wasserzeichen eingebettet und von entsprechend ausgestatteten Abspiel-, Lese- und Aufzeichungsgeräten gelesen und befolgt werden. Wasserzeichen lassen sich auch mit Verschlüsselung kombinieren. Gelingt es einem Angreifer, eine gesicherte Datei zu entschlüsseln, sie aus dem Rechtekontrollsystem zu entnehmen und Kopien davon zu erstellen, bleibt die Copyright-Information im Wasserzeichen dennoch erhalten. Sie könnte dann als technischer Nachweis eines Missbrauchs dienen. Einige Verfahren erlauben es, an jeder Station des Vertriebsweges vom Aufnahmestudio bis hin zum Endkunden jeweils eigene Codes einzubetten, so dass sich an nicht autorisierten Kopien able<t-1>sen lässt, an welcher Stelle sie dem Kontrollsystem entnommen wurden.<x@3 hoch fliess><t-1>69<@$p>Zudem können spezialisierte Suchmaschinen das Internet nach mit Wasserzeichen markierten Dateien durchforsten.<@3 hoch fliess>70<@$p> Andere Hersteller richten sich mit ihren Technologien besonders an den Rundfunk. Auch hier gibt es passende Überwachungsstationen, die markierte Radio- und Fernsehsendungen registrieren und z.B. automatische Playlists oder Nachweise darüber erstellen, wie häufig ein Werbeclip gesendet wurde.<@3 hoch fliess>71<@$p> <t-2>Die Firma Blue Spike<x@3 hoch fliess><t-2>72<@$p><t-2> wirbt für ihre so genannten »forensischen« Wasser<\h>z<t$>eichen mit dem Versprechen: »Ihr Content kann fast zuhause anrufen«. Eine Variante der Wasserzeichen sind die so genannten Fingerabdrücke. Erwirbt ein Kunde in einem E-Commerce-System ein Musikstück, so kodiert das System seine persönliche Kennung in die MP3-Datei, die er erhält. Tauchen Kopien dieser Datei im Internet auf, lässt sich anhand des Fingerabdrucks feststellen, wer sein Werkstück unautorisiert in Umlauf gebracht hat. Auch ein markiertes Foto, das ohne Lizenz in einer Zeitschrift abgedruckt wird, kann auf diese Weise auf den vertragsbrüchigen Lizenznehmer zurückgeführt werden. Die <@1 fliess kursiv>Fingerprints<@$p> in Wasserzeichen sind aus datenschutzrechtlichen Gründen kryptiert, so dass nur der Rechteinhaber oder der E-Commerce-Anbieter, der es ursprünglich verschlüsselt hat, diese Information wieder auslesen kann.<@3 hoch fliess>73<@1 fliess normal> <@$p>Fingerprints können ferner den Urhebernachweis und die Integrität von Dokumenten im elektronischen Geschäftsverkehr sichern. Wer den geheimen Schlüssel besitzt, um den Fingerabdruck eines Dokuments zu dekodieren, muss auch derjenige sein, der es ursprünglich verschlüsselt hat. Wird ein Dokument verändert, zeigt sich bei der Entschlüsselung des Fingerabdrucks eine Diskrepanz der Prüfsummen, und die Manipulation wird offenkundig. Wasserzeichen werden gelegentlich auch sichtbar eingebracht, um z.B. von einer unautorisierten Weiterverbreitung abzuschrecken, in der Regel wird der rechtmäßige Besitzer jedoch nicht an einer Verfremdung oder einem Qualitätsverlust des erworbenen Dokuments interessiert sein. Eine Anforderung ist also, dass Wasserzeichen zwar für Maschinen lesbar, aber für die menschliche Wahrnehmung nicht erkennbar sind. <t-2>Dazu wird im Rauschanteil von Bild- oder Klangdaten ein zusätzliches Rauschen eingebracht, eben das Wasserzeichen. Bei Bildern kann es sich um minimale Veränderungen der Helligkeit einzelner Bildpunkte handeln. In einfarbigen Flächen sind solche Veränderungen leicht erkennbar. Frühe Wasserzeichen-Technologien (wie sie z.B. von <x@1 fliess kursiv><t-2>Divx <@$p><t-2>verwendet wurde) erzeugten noch störende »Regentropfen«-Muster und andere Artefakte. Heutige Verfahren analysieren das jeweilige Bild und bringen die Markierungen bevorzugt in den detailreichsten Bereichen ein. Da Wasserzeichen im Rauschanteil der Pixel oder Klangdaten enthalten sind, können Vektorgrafiken ebensowenig markiert werden wie Computerprogramme.<t$> Die zweite zentrale Anforderung an Wasserzeichensysteme ist, dass sie gegenüber Manipulationen »robust« sind. Damit die Markierung auch dann noch feststellbar bleibt, wenn nur ein Bildausschnitt oder eine kurze Sequenz aus einem Video entnommen wird, verteilen die Systeme die meist nur einige Bits lange Botschaft redundant in vielen Kopien über den gesamten Content. Die Markierung soll auch bei Formatkonvertierungen (z.B. <@4 Pfeil (Umschalt/Alt #)>’<@$p> JPEG in <@4 Pfeil (Umschalt/Alt #)>’<@$p> GIF oder Farbbild in Graustufen) oder bei Kompression erhalten bleiben, bei der ja gerade die nicht wahrnehmbaren Rauschanteile entfernt werden. Sie soll Manipulationen wie »Rotieren«, »Verschmieren« oder »Schärfen« von Bildern überstehen. Und selbst bei einer analogen Ausgabe (als Video, Audio oder auf einem Drucker) soll das Wasserzeichen auslesbar bleiben, so dass z.B. ein entsprechend ausgestatteter Fotokopierer sich weigern würde, den Ausdruck eines geschützten Textes oder Bildes zu kopieren, oder ein aus dem Radio auf Analog-Cassette aufgezeichnetes Musikstück weiterhin geschützt bliebe. Eine praktische Anforderung ist schließlich, dass das Wasserzeichen in akzeptabler Geschwindigkeit eingebracht und ausgelesen werden können muss, bei zeitbasierten Medien, wie Musik und Video, also in Echtzeit oder schneller. Ein Standard für Wasserzeichen ist ebensowenig in Sicht wie für andere Kontrolltechnologien. Konsortien der Musikindustrie (die <@1 fliess kursiv>Secure Digital Music Initiative<@$p> – SDMI<@3 hoch fliess>74<@$p>) und der Filmindustrie (die <@1 fliess kursiv>Copy Protection Technical Working Group<@$p> der DVD-Industrie) evaluieren die verfügbaren Produkte. So entschied die SDMI Ende 1999, dass die Wasserzeichen-Technologien der Firma Verance<@1 fliess normal> <@$p>zum Industriestandard für den Schutz von DVD-Audio werden soll. An konforme Produkte vergibt die SDMI dann das Warenzeichen DMAT (<@1 fliess kursiv>Digital Music Access Technology<@$p>). Ein öffentliches Forum für die Standardisierung von Codierungsverfahren ist die ISO. Wie bereits geschildert, wird die <@4 Pfeil (Umschalt/Alt #)>’<@$p> MPEG-Familie von Video- und Audiokompressionstechnologien in der nächsten Generation von vornherein Rechtekontrollmechanismen enthalten. Die Entsprechnung zu MPEG für die Kompression von fotografischen, grafischen und Textbildern ist die <@1 fliess kursiv>Joint Photographic Experts Group<@$p> (JPEG).<@3 hoch fliess>75<@$p> Der gegenwärtige JPEG-Standard ist über zehn Jahre alt. Derzeit arbeitet das Gremium an einem Nachfolger, der ebenfalls bereits im Standard eine Wasserzeichenmarkierung vorsieht. Nach den Erfahrungen mit dem <@1 fliess kursiv>Content Scrambling System<@$p> wird man fragen, wie zuverlässig und »robust« die verschiedenen Wasserzeichent<t-1>echnologien sind. Bei all den jahrelangen Bemühungen in akademischen <t$>und Industrielaboren, in europäischen Forschungsprojekten, einzelnen Unternehmen, Konsortien und internationalen Standardisierungsorganisationen sind die Ergebnisse überraschend schwach. Neben anderen ist es vor allem ein Informatiker von der Cambridge-Universität, der die Verwundbarkeit von bislang noch jeder Wasserzeichentechnologie aufgezeigt hat. Fabien Petitcolas<@3 hoch fliess>76<@$p> entwickelte ein generisches Werkzeug namens <@1 fliess kursiv>StirMark<@$p>, mit dem sich die Robustheit von Bildmarkierungs- und anderen steganografischen Techniken testen lässt. In seiner einfachsten Version simuliert StirMark eine Neuabtastung des Bildes, wie sie bei <\n>einem Ausdruck, gefolgt von einem Einlesen durch einen Scanner geschieht. Dazu werden kleine geometrische Verzerrungen eingeführt; das Bild wird geringfügig gestreckt, verschoben oder gedreht und dann neu gesampelt. Alle Eingriffe bewegen sich unterhalb der Wahrnehmungsschwelle. Die Mehrzahl der kommerziell angebotenen Produkte überleben <@1 fliess kursiv>StirMark<@$p> schon in der Standardeinstellung nicht.@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Unsere Erfahrungen bei Angriffen auf existierende Markierungssys<\h>teme hat uns überzeugt, dass jedes System, das versucht, allen anerkannten Anforderungen für Wasserzeichen gerecht zu werden, versagen würde: Wird die Robustheit als Bedingung erfüllt, würde die Bandbreite ungenügend sein. [...] Unsere eher allgemeine Schlussfolgerung aus dieser Arbeit ist, dass das ›Markierungsproblem‹ zu sehr verallgemeinert worden ist; es gibt nicht ein ›Markierungsproblem‹, sondern eine ganze Reihe Problemkonstellationen. Wir glauben nicht, dass man eine allgemeingültige Lösung finden kann. Man wird sich immer entscheiden müssen, insbesondere zwischen Bandbreite und Robustheit. Solche Abwägungen sind entscheidend für den Entwurf eines spezifischen Sys<\h>tems<t-1>« <x@1 fliess normal><t-1>(<x@6 Caps><t-1>Petitcolas<\!q>/<\!q>Anderson<\!q>/<\!q>Kuhn<x@1 fliess normal><t-1>, 1998, S. 233).<t$>@1 fliess mit:@1 fliess ohne:Seither entwickeln Petitcolas und seine Kollegen <@1 fliess kursiv>StirMark<@$p> zu einem Vergleichstest (<@1 fliess kursiv>Benchmark),<@$p> um die Qualität von Wasserzeichensystemen zu testen.<@3 hoch fliess>77<@$p>@1 fliess mit:@2  ZÜ 3:Elektronische Bücher: E-Books in verschlüsseltem PDF@1 fliess ohne:Adobes 1993 eingeführtes <@1 fliess kursiv>Portable Document Format <@$p>(PDF) ist ein heute weitverbreitetes Format, um gestaltete Textdokumente im Internet zugänglich zu machen.<@3 hoch fliess>78<@$p> Die PDF-Kodierung selbst erlaubt es bereits, den Zugang zu einem Text mit einem Passwort zu schützen und einzelne Operationen wie Drucken, Verändern, Auswahl und Kopieren einzelner Passagen sowie das Hinzufügen von Anmerkungen zu unterbinden. Neben Adobe bieten andere Firmen wie SoftLock und Glassbook zusätzliche Rechtekontrolltechnologie für PDF-Dateien. Diesen beiden bot sich eine spektakuläre Marketing-Gelegenheit, als der Großverlag Simon & Schus<\h>ter<@3 hoch fliess>79<@$p> ihnen den Auftrag gab, eine Kurzgeschichte des Starautors Stephen King auf ihren E-Commerce-Servern zu verbreiten. @1 fliess mit:SoftLock<@3 hoch fliess>80<@$p> ist bereits seit 1994 im Bereich des verschlüsselten Vertriebs von Inhalten aktiv.<@3 hoch fliess>81<@$p> Erst im September 1999 war die Firma eine Partnerschaft mit Adobe eingegangen, um ihre patentierte Technologie für das PDF-Format einzusetzen, und erst im Dezember 1999 hatte sie ein E-Book-Angebot gestartet. SoftLock ist ein so genannter <@1 fliess kursiv>E-Market <\n>Maker<@$p>, der die Wertschöpfungskette im <@1 fliess kursiv>Busines-to-Business<@$p>-Bereich »re-intermediiert«, sich also als neuer Mittler zwischen andere Firmen schaltet. Sein Hauptprodukt ist <@1 fliess normal>E-Merchandising,<@$p> ein System, das Content-Vermarktung, Copyright-Schutz, Zahlungsabwicklung über Kreditkarten und Distribution integriert. Verleger von PDF-Inhalten und On<\h>line-Buchverkäufer stellen ihren Content auf dem Server von SoftLock ein und legen darauf einen Link von ihrer eigenen Website aus. Kunden des Händlers bemerken nicht einmal, dass sie dessen Site verlassen. Der Leser verwendet den üblichen »AcrobatReader«, ein Leseprogramm für PDF-Dateien. Verleger können für die Freischaltung der Betrachter- und der Druckfunktion separate Gebühren erheben. »Wenn Ihre Firma Inhalte anmeldet, entscheiden Sie über Preis, Kategorie, Freigabedatum und Dauer. Sie haben die vollständige Kontrolle darüber, wo Ihr Content verwendet werden könnte.«Der SoftLock-Client installiert sich im Plugin-Verzeichnis des AcrobatReader und – um den Download-Prozess zu kontrollieren – von Net<\h>scape. Die Entschlüsselung erfolgt während des Herunterladens, bei dem das Dokument an die vorgefundene Version des AcrobatReaders gekoppelt wird. Der Käufer kann es also nicht auf einem anderen Rechner, z.B. seinem Laptop öffnen. Das Dokument liegt nur im Reader unverschlüsselt vor, bei Speicherung auf der Festplatte oder Weitergabe an Dritte ist es kryptiert. Eines der Features der Firma ist das so genannte virale Marketing. Ein Abschnitt der verschlüsselten Datei ist als Leseprobe freigeschaltet. Verschickt eine begeisterte Leserin einen gekauften Text an einen Freund, so findet dieser am Ende einen Link, von dem aus er den Schlüssel bestellen kann, um den Rest des Textes zu öffnen.Noch Anfang 2000 richtete sich SoftLocks Business-Plan vor allem auf <@1 fliess kursiv>knowledge workers<@$p>, Angestellte, die für ihre Firmen technische Dokumentationen oder Berichte erstellen und sich so für jeden Abruf durch ihre Arbeit bezahlen lassen können. Doch dann wandte sich der CEO von Simon & Schuster an die Firma und fragte, ob SoftLock den Bestseller-Autor <\h>hosten wolle. <t-2>Der zweite Distributor, Glassbook,<x@3 hoch fliess><t-2>82<@$p><t-2> bietet E-Book-Verlagen und -Händ<\h>le<t$>rn ganz ähnliche Dienstleistungen an. Es verwendet keinen zentralen Server, sondern lizenziert den <@1 fliess kursiv>Glassbook Content Server<@$p> an seine Kunden.<@3 hoch fliess>83<@$p> Der Server nimmt die Kryptierung, die Zuweisung der Nutzungsrechte und die Authentifikation der Transaktionen vor. Als Client produziert Glassbook einen eigenen Reader, mit dem der Leser Markierungen und Anmerkungen im Text anbringen kann. Als Kryptografietechnologie verwendet Glassbook das <@1 fliess kursiv>Electronic Book Exchange <@$p>(EBX)-Format,<@3 hoch fliess>84<@$p> das die Firma zusammen mit anderen in der E-Book-Branche entwickelte. Bei EBS liegt der Schlüssel, der den Content öffnet, im ausgelieferten Dokument vor, ist aber selbst mit dem <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Public-Private-Key<@$p>-Verfahren verschlüsselt. Den symmetrischen Content-Schlüssel kryptiert der Server mit dem öffentlichen Schlüssel des Käufers, der ihn mit seinem privaten  Schlüssel wieder lesbar machen kann. Am 14.<\!q>März 2000 erschien Stephen Kings Kurzgeschichte »Riding the Bullet« exklusiv im Netz, am ersten Tag bei vielen Online-Buchläden kostenlos, danach zum Preis von 2,50 Dollar. SoftLock bearbeitete allein am ersten Tag mehr als 100<\!q>000 Bestellungen. Große Online-Buchhändler wie <@1 fliess normal>Amazon.com und Barnes & Noble wä<@$p>hlten die Glassbook-Technologie, um Kings Geschichte zu vermarkten. Aufmerksamkeit war allen Beteiligten sicher – allerdings auch für die unrühmlichen Folgen: Zwei Tage nach dem Verkaufsstart verbreiteten sich zunächst <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Screen-Shots<@$p> des Textes in Internet-Newsgroups. Noch einen Tag später folgte die ungesicherte PDF-Datei selbst. Diese Sicherheitslücke sorgte für einige Verlegenheit unter E-Book-Verlegern und Distributoren. Wie der Hack genau erfolgte, ist unklar. Anhand der unterschiedlichen Titelblätter in den Versionen von SoftLock und Glassbook wurde deutlich, dass Letztere das Opfer des Angriffs waren. Der Präsident der Firma hielt es für unwahrscheinlich, dass das Verschlüsselungssystem gebrochen worden sei. Er und Vertreter von Adobe sagten jedoch, dass sie künftig längere Schlüssel einsetzen wollten. Aufgrund des bis Januar 2000 bestehenden Exportverbots der USA für starke Kryptografieprodukte ist nur ein 40-Bit langer Schlüssel zum Einsatz gekommen. 40-Bit, so der Adobe-Sprecher, seien ausreichend, um das Kopieren durch die meisten Nutzer zu verhindern, aber wahrscheinlich nicht genug, um einen entschlossenen Hacker abzuwehren. Wahrscheinlicher ist, dass der Hacker den Content-Schlüssel aus einer mit Applikationen gelinkten Bibliothek, einer so genannten .dll-Dateien extrahiert hat. Tatsächlich stellte ein anonymer Internet-Nutzer auf einer kostenlosen Webseite Anweisungen bereit, wie ein Glassbook-EBX-Dokument zu entschlüsseln ist und lieferte gleich zwei Plugin-Programme dazu, eines, das den Schlüssel aus einer bestimmten Link-Bibliothek im Glassbook-Verzeichnis des Rechners zieht und eines, um eine ungeschützte Kopie der PDF-Datei zu speichern. Wiederum wurde die Gefahr für Sicherungssysteme beschworen, die von Universalcomputern ausgehe. Größeren Schutz böten dedizierte Hardwaresysteme, wie spezielle E-Book-Lesegeräte (vgl. <@6 Caps>Sanders<\!q>/<\!q>Roush<@$p>, 3/2000).<@1 fliess normal>Verlag und Distributoren forderten die Sites, auf denen der Text verfügbar war, auf, ihn zu entfernen – doch der Geist war einmal aus der Flasche, es gab keinen Weg zurück. Simon & Schuster ließ sich durch den Reinfall nicht vom E-Publishing abhalten. Seither werden auch S&S Bücher von Mary Higgins Clark, Shirley McLaine und CBS bei SoftLock angeboten.<@$p>@2  ZÜ 3:Firewall um Deutschland: Rights Protection System (RPS)@1 fliess ohne:Die Bemühungen der Rechteindustrie, die gesamte Medienwelt nach ihren Bedürfnissen neu zu erfinden, sind vom Anspruch her vermessen und in der Durchführung zum Scheitern verurteilt. Dessen ungeachtet setzte der Bundesverband der <@1 fliess normal>Phonographischen Industrie<@$p> (BPI/Deutsche IFPI)<@3 hoch fliess>85 <@$p>noch eins drauf, als er im September 1999 das <@1 fliess kursiv>Rights Protection System<@3 hoch fliess>86<@$p> vorstellte. Soll Kryptografie verhindern, dass Werke auf eine andere, als die vom Rechteinhaber autorisierte Weise genutzt werden, sollen Wasserzeichen dazu dienen, Werke ausfindig zu machen, die einem »sicheren« Kryptosystem entnommen und unverschlüsselt ins Netz gegeben wurden, so wählt der BPI einen dritten Weg. Er durchsucht das Netz nach Musikstücken, die ohne Genehmigung der betreffenden Plattenfirmen angeboten werden. Befindet sich der Server in Deutschland, fordert er den Betreiber auf, die Dateien zu entfernen. Nach dem Teledienstegesetz ist der Provider dann für fremde Inhalte verantwortlich, wenn er von einem Urheberrechtsverstoß in Kenntnis gesetzt wird und technisch in der Lage ist, die Nutzung zu verhindern (<@1 fliess kursiv>Notice-and-Take-<\h>down<@$p>). Auf diese Weise wurden bis Anfang 2001 mehr als 2<\!q>000 deutsche Websites geschlossen (vgl. BPI, 2001). Befindet sich der Server jedoch im Ausland, etwa in Osteuropa oder Südamerika, so hat der BPI diese rechtliche Handhabe in vielen Fällen nicht. Wenn der BPI schon nicht gegen die »illegalen« Dateien selbst vorgehen kann, möchte er zumindest verhindern, dass deutsche Internetnutzer darauf zugreifen können. Das vorgeschlagene RPS soll dazu den gesamten grenzüberschreitenden Datenverkehr überwachen und die als illegal identifizierten Dateien außen vor halten. @1 fliess mit:<t-0.5>Das geplante System gleicht technisch den Filterungsverfahren, die zum Jugendschutz gegen Pornografie<x@3 hoch fliess><t-0.5>87<@$p><t-0.5> verwendet werden: Eine von Hand zusammengestellte Liste von <x@4 Pfeil (Umschalt/Alt #)><t-0.5>’<@$p><t-0.5> URLs (Internetadressen) kann durch den Filter nicht mehr aufgerufen werden. Jugendschutzfilter werden üblicherweise von Eltern auf dem Rechner ihrer Kinder installiert. Die URL-Liste abonnieren sie bei einem Anbieter, der sie, ähnlich wie bei Viren-Scannern, in regelmäßigen Abständen über das Netz aktualisiert. Filter werden aber auch in den <x@1 fliess kursiv><t-0.5>Firewalls<@$p><t-0.5> (»Brandschutzmauern«) von Unternehmen eingesetzt, um zu verhindern, dass Arbeitnehmer während der Arbeitszeit auf Unterhaltungsinformation zugreifen, oder von Universitäten, um Ressourcen schluckende Dienste wie Online-Spiele zu unterbinden. Schließlich sind unrühmliche Beispiele bekannt, bei denen Länder wie China oder Singapur Filter in den Gateways zwischen Ländern eingesetzt haben, um ihren Bürgern Informationen von Dissidenten im Ausland unzugänglich zu machen. Filter können also eine nützliche Aufgabe erfüllen, wenn sie freiwillig an einem Endpunkt des Netzes verwendet werden. An zentralen Verkehrsknotenpunkten installiert, bewirken sie eine Bevormundung und Zensur für ein große Zahl von Nutzern. <t$>Genau das beabsichtigt der BPI. Alle deutschen Provider, die über eine Verbind<t-1>ung ins Ausland verfügen (nach IFPI-Angaben nicht mehr als 50 bis 70), <t$>sollen RPS-Server aufstellen. Der gesamte Datenverkehr würde dort zunächst zwischengespeichert, um ihn zu filtern. Die URL-Negativliste mit den bekannten, vermeintlich gegen das Urheberrecht verstoßenden Adressen soll bis zu stündlich auf dem Laufenden gehalten, von den Rechteinhabern bestückt und nach Möglichkeit unter der Ägide von offizieller staatlicher Seite, wie etwa den Zollbehörden, verwaltet werden (vgl. <@6 Caps>Bortloff<@$p>, 1999). Fände das RPS im durchfließenden Datenverkehr eine URL aus der Liste, würde sie den Zugang sperren. Die deutsche IFPI sieht dies als eine »virtuelle Grenzbeschlagnahme« an. Nach Abschluss des laufenden Feldversuchs will die IFPI alle Internetprovider mit Border-Gateway-Routern ansprechen, damit sie dieses System installieren. Dabei wies ihr Justiziar darauf hin, dass die Provider hierzu gesetzlich verpflichtet seien, da mit dem RPS eine solche Filte<\h>rung »technisch möglich und zumutbar« sei (vgl. <@6 Caps>Schulzki-Haddouti<@$p>, 2/2000).Es ist technisch relativ einfach möglich, ein solches System zu umgehen, würden Anbieter und Nutzer von nicht autorisierten Inhalten eine Verschlüsselung verwenden, die Daten per E-Mail verschicken, die URLs von Zugängen aus dem Ausland aufrufen oder ihre Server in kurzen Abständen wechseln, wie bei Anbietern von Pornografie üblich.Das eigentliche Problem liegt in den politischen Implikationen. Die Vorstellung, dass eine Industrievereinigung im Interesse ihrer Mitglieder die Funktionalität einer öffentlichen Infrastruktur für alle rund 20 Millionen Internetnutzer in Deutschland beschneiden will, ist skandalös. Die Industrie ist sich der Brisanz eines solchen Systems bewusst: »Die Gefahr, in die Zensurecke gedrängt zu werden, ist groß«, fürchtet ein Sprecher des BPI (vgl. <@6 Caps>Krempl<@$p>, 9/1999). <@1 fliess kursiv>»Rights Protection System«<@$p> ist ein doppelt irreführender Name. Er verschleiert, dass es sich keineswegs um ein Rechtekontrollsystem im hier behandelten komplexen Sinne handelt, sondern um eine Filterung. Vermutlich soll damit eine Assoziation zu der Mitte der 90er-Jahre heftig geführten Debatte über Filterung und zu ihrem Ergebnis vermieden werden: dass nämlich Filterung auf dem Rechner des Endanwenders und <\n>unter dessen Kontrolle ein gutes Hilfsmittel für die informationelle Selbstbestimmung, jede Filterung an einer vorgelagerten Stufe (beim Internetprovider oder Gateway) aber abzulehnen sei.<@3 hoch fliess>88<@$p> Zum anderen ist er irreführend, weil das System weder Inhalte vor Urheberrechtsverstößen schützt, noch gegen Sites vorgeht, die Inhalte anbieten, die die Rechteindustrie nicht zu schützen in der Lage war. Vielmehr soll all den Millionen von Internetnutzern in Deutschland der Zugang zu einer von der Musik<\h>industrie bestimmten (und natürlich geheimgehaltenen) Liste von URLs verwehrt werden. Damit nicht genug, will die Musikindustrie die Lösung ihres Partikularproblems auch noch als »nationales Schutzsystem« verkaufen, das, einmal etabliert, auch gleich den Zoll- und Strafverfolgungsbehörden sowie dem Fiskus dienen soll. Der IFPI-Justiziar Nils Bortloff behauptete, dass das RPS helfe, das nationale Recht im Internet umzusetzen. Es eigne sich nicht nur zum Schutz des Urheberrechts, sondern könne auch gegen den Vertrieb illegaler Produkte oder rechtswidrigen Materials eingesetzt werden (vgl. <@6 Caps>Bortloff<@$p>, 1999, S. 117.). Die Musikindustrie <@1 fliess kursiv>– <@1 fliess normal>zweifellos groß, aber hier mit deutlichen Anflügen von Größenwahn<@1 fliess kursiv> –<@$p> maßt sich an, im Alleingang <\h>eine technische Lösung für Probleme einzuführen, die bislang in die Kompetenz von öffentlichen, demokratisch gewählten Instanzen wie der Bundesregierung oder der Europäischen Kommission fielen. Diese haben jedoch bislang aus gutem Grund keine Schritte unternommen, nationale Grenzen im Cyberspace nachzubauen.Es ist zu vermuten, dass die IFPI hier ihre deutsche Sektion vorgeschickt hat, um einen Testballon zu starten. In den USA hätte die ansonsten keineswegs zurückhaltende RIAA einen solchen Vorstoß wohl kaum gewagt, da sie sich eines allgemeinen Aufschreies der Internetwelt hätte gewiss sein können. In Deutschland dagegen haben die RPS-Pläne, bis auf die Artikel einiger aufmerksamer Fachjournalisten, keinerlei Reaktion ausgelöst. Es scheint als würde die Welt nach den Interessen der Rechteindustrie neu geordnet. Nicht nur die Endgeräte und die Kommunikationsnetze werden neu gestaltet, das Wissen in kryptografische Um- schläge gehüllt und Kontrollmechanismen in das Wissen selbst eingebettet, die Leser und Hörer registriert und bis in ihre Festplatten hinein überwacht, nun wollen die Datenherren auch noch elektronische Schutzwälle um die Nationen errichten.@2  ZÜ 3:Rechtliche Absicherung von Rechtekontrollsystemen (RCS)@1 fliess ohne:<*h"mehr"><t-1>O<t$>b CSS, Wasserzeichen oder PDF-Kryptierung, bislang ist noch jedes Schutzsystem gebrochen worden. Daher drängte die Rechteindustrie darauf, die technischen Schutzsysteme zusätzlich rechtlich zu schützen. Den Rahmen geben wiederum die WIPO-Verträge von 1996 ab. In Ar<\h>tikel 11 des <@1 fliess kursiv>WIPO Copyright Treaty<@$p> verpflichten sich die Unterzeichnerstaaten, »adäquaten gesetzlichen Schutz und effektive Rechtsmittel gegen die Umgehung von wirksamen (<@1 fliess kursiv>effective<@$p>) technischen Maßnahmen bereitzustellen«, die von Autoren verwendet werden, um ihre Rechte zu schützen.<@3 hoch fliess>89<@$p> Artikel 12 erläutert, dass damit das Entfernen oder Verändern von elektronischen Rechtemanagementinformationen von Werkkopien und das Verbreiten der dadurch ungeschützten Werke gemeint ist.<@3 hoch fliess>90<@$p> <t-1>@1 fliess mit:In der Erläuterung zu Artikel 12 heißt es, dass die Vertragsparteien sich nicht auf ihn berufen dürfen, um Rechtekontrollsysteme zu implementieren, die die Ausübung der von der Berner Konvention oder des WCT zugestanden Rechten unterbinden, »die den freien Warenverkehr untersagen oder die Ausübung eines unter diesen Vertrag fallenden Rechtes behindern.«<@3 hoch fliess>91<@$p> Damit sind die Schrankenbestimmungen angesprochen, die bei der Umsetzung der Verträge in nationales Recht die größten Schwierigkeiten bereiten.Verschiedene WIPO-Mitgliedsländer haben diese Vorgaben inzwischen in nationales Recht umgesetzt, Großbritannien z.B. im Artikel 296 des <@1 fliess kursiv>Copyright, Designs and Patents Act.<@3 hoch fliess>92<@$p> In den USA hatte bereits der <@1 fliess kursiv>Audio Home Recording Act <@$p>von 1992 die Herstellung, Einfuhr und Verbreitung von Geräten, die nicht über ein S<@1 fliess kursiv>erial Copy Management System<@$p> oder ein vergleichbares System verfügen, sowie Geräte, deren primärer Zweck es ist, solche Kopierschutzmechanismen zu umgehen, zu entfernen oder zu deaktivieren, unter Strafe gestellt (§§<\!q>1002 (a) und 1002 (c) U.S.C.). Allerdings bezog sich der AHRA ausschließlich auf Geräte für digitale Audioaufnahmen. Der <@1 fliess kursiv>Digital Millenium Copyright Act<@$p> (DMCA) schuf daraus in Umsetzung der WIPO-Verträge ein komplexes Regelwerk gegen die <@1 fliess kursiv>Circumvention of Copyright Protection Systems<@$p> (§§ 1201–1205 U.S.C.), das im Oktober 2000 in Kraft trat. Auch hier bezieht sich das Verbot auf Technologien, die »primär« für eine Umgehung entworfen worden sind, außer der Umgehung nur begrenzten kommerziellen Nutzen haben oder für den Zweck der Umgehung vermarktet werden. Allzweckcomputer, die »gefährlichste Waffe« eines Hackers, sind damit ausgeschlossen. Ausdrücklich ausgeschlossen ist ebenfalls, dass das Umgehungsverbot die Schranken einschließlich des <@1 fliess kursiv>Fair Use<@$p> sowie die Meinungsäußerungs- und Pressefreiheit beeinträchtigt. Weitere Ausnahmen gelten für öffentliche Bibliotheken und für Strafverfolgungsbehörden. Das Recht zum <@1 fliess kursiv>Reverse Engineering <@$p>zur Herstellung von interoperablen Software<\h>werken bleibt unberührt. Selbst eine »Kryptografie-Forschung« ist erlaubt, sofern sie dazu dient, den Erkenntnisstand in diesem Feld voranzutreiben. Schließlich ist eine Umgehung erlaubt, wenn die technologische Maßnahme personenbezogene Daten über das Online-Verhalten des Nutzers sammelt und weitergibt, sofern die Umgehung dazu dient, diese Eigenschaft des Systems zu identifizieren und auszuschalten. Das Strafmaß bei Zuwiderhandlung beträgt bis zu fünf Jahren Gefängnis und 500<\!q>000 Dollar Strafe (§<\!q>1204). Darüber hinaus können zivilrechtlich Schadensersatzansprüche geltend gemacht werden (§ 1205).Die Autorenrechtstradition tat sich mit der Erfassung der Rechtekontrolle schwerer. Nach langwierigen Verhandlungen erließ das Europäische Parlament im Mai 2001 die EU-Richtlinie zur Harmonisierung bestimmter Aspekte des Urheberrechts und verwandter Schutzrechte.<@3 hoch fliess>93<@$p> Auch sie dreht sich zentral um den Schutz von technischen Maßnahmen für die Rechtekontrolle. Während die Einführung eines solchen Rechtsschutzes den Mitgliedsstaaten verpflichtend auferlegt wird, formuliert die Richtlinie Schrankenregelungen für Privatkopien, Bibliotheken, Unterricht, Wissenschaft und Presse nur als Kannvorschrift.<@3 hoch fliess>94<@$p> Verpflichtend ist hier wiederum, dass Schranken nur »in bestimmten Sonderfällen angewandt werden, in denen die normale Verwertung des Werks oder sonstigen Schutzgegenstands nicht beeinträchtigt wird und die berechtigten Interessen des Rechtsinhabers nicht ungebührlich verletzt werden.« Falls die Richtlinie sich um eine Balance der Interessen der Rechteinhaber und der Öffentlichkeit bemüht hat, ist sie gründlich gescheitert. Der parteiische Geist spricht aus der Erwägung 39: »Entsprechende Ausnahmen oder Beschränkungen sollten weder den Einsatz technischer Maßnahmen noch deren Durchsetzung im Falle einer Umgehung dieser Maßnahmen behindern.« M.a.W., setzt ein Anbieter technische Maßnahmen ein, entfallen Ausnahmerechte, die die Nutzer andernfalls hätten. Oder ganz unmissverständlich in Erwägung 51: »Der Rechtsschutz technischer Maßnahmen gilt unbeschadet des in Artikel 5 zum Ausdruck kommenden Gesichtspunkts des Allgemeininteresses.« Eine Vorlage, die das öffentliche Interesse betont, hätte gerade umgekehrt formuliert, dass technische Maßnahmen die Wahrnehmung der Ausnahmen nicht behindern dürfe. Solchen möglichen Ausnahmen auch tatsächlich nutzbar zu machen, wird zunächst den Rechtsinhabern aufgetragen. Erst wenn diese nicht innerhalb einer angemessenen Frist freiwillige Maßnahmen dazu ergreifen, sind die Mitgliedsstaaten verpflichtet sicherzustellen, dass Nutzer von ihren Schrankenrechten Gebrauch machen können (Art. 6 (4)). Die EU-Richtlinie muss nun bis Dezember 2002 in den Mitgliedsstaaten in nationales Recht umgesetzt werden.Auch wenn die Motive begreiflich sind, muss einem nüchternen Beobachter diese Konstruktion als ein juristischer <@1 fliess kursiv>Overkill<@$p> erscheinen. Zunächst sind Werke durch Copyright und Urhebergesetz rechtlich geschützt, was eine Warnung vorab und eine Handhabe bei ihrer Verletzung darstellt. Wie bei der Regel »Du sollst nicht in anderer Leute Haus einbrechen«, ist es verständlich, dass der Besitzer sich nicht allein auf die gesetzlichen Mittel verlässt, sondern zusätzlich ein Schloss anbringt. Nun sind die handelsüblichen Schlösser so schlecht konstruiert, dass sie regelmäßig geknackt werden. Also haben die Content-Besitzer Gesetze erwirkt, die eigens die Schlösser schützen. Zwar sind diese Schlösser bereits, wie jede andere Software, durch Urheberrecht/Copyright und, wie die meisten anderen Technologien, durch Patente gegen Eingriffe geschützt, doch damit nicht genug, wurde ein <@1 fliess kursiv>sui generis<@1 fliess normal>-<@$p>Gesetz für die Klasse von Rechtekontrolltechnologien geschaffen. Im Extremfall ist es also vorstellbar, dass sich eine Person, die ein RCS gehackt hat, um ein enthaltenes Werk unautorisiert zu nutzen, nach Urheber/Copyright-Recht in Bezug auf das Werk und auf das RCS, nach Patentrecht in Bezug auf das RCS sowie nach dem Umgehungsverbot strafbar macht und zusätzlich zu den strafrechtlichen Folgen auch noch mit einem zivilrechtlichen Haftungsanspruch rechnen muss. Im Falle von DeCSS ist es somit vorstellbar, dass die Filmindustrie den Schaden und entgangenen Profit für sämtliche auf DVD veröffentlichten Filme geltend macht. @2  ZÜ 2:Für eine informationelle Nachhaltigkeit@1 fliess mit:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Das Netz behandelt Zensur als eine Funktionsstörung und umgeht sie« <@1 fliess normal>(<@6 Caps>John Gilmore<@1 fliess normal>) @1 fliess mit:@1 fliess ohne:Der renommierte Verfassungsrechtler und <@1 fliess kursiv>Cyberlaw<@$p>-Spezialist Law<\h>rence Lessig von der Harvard Law School<@3 hoch fliess>95<@$p> unterscheidet vier Arten der Verhaltenssteuerung: Recht, soziale Normen, Märkte und Architektur. Sie wirken auf verschiedene Weisen und über unterschiedliche Sanktionen. Eine Regulierung des Rauchens beispielsweise kann über Gesetze geschehen, die den Verkauf von Zigaretten an Minderjährige oder das Rauchen an bestimmten Orten verbieten. Soziale Normen führen in den USA auch an den wenigen Orten, an denen das Rauchen noch nicht untersagt ist, zu einer Ächtung von Rauchern. Die Hersteller können über den Marktpreis und der Staat über die Tabaksteuer das Rauchen regulieren. Schließlich können Technologien wie Sucht erzeugende Zusätze oder rauchlose Zigaretten das Rauchverhalten beeinflussen. Die »Netto-Regulierung« ist die Summe der vier Effekte, der vier <@1 fliess kursiv>modalities of constraint<@$p>. Sie können sich ergänzen oder konkurrieren (vgl. <@6 Caps>Lessig<@$p>, 1999, S. 90 ff). Eine Regulierung mit Hilfe von Architektur ist die am wenigsten offenkundige, aber deshalb nicht weniger wirksame Modalität. Zu Lessigs Beispielen aus der materiellen Welt gehören die stadtplanerischen Veränderungen von Paris durch Haussmann Mitte des 19. Jahrhunderts: Viele der schmalen Straßen, die von den Revolutionären einfach barrikadiert werden konnten, wurden durch Boulevards ersetzt, um es Aufständischen unmöglich zu machen, die Kontrolle über die Stadt zu ergreifen (ebd., S.<\!q>92).@1 fliess mit:Auch der Cyberspace wird durch diese vier Modalitäten reguliert. Die  <@4 Pfeil (Umschalt/Alt #)>’ <@1 fliess kursiv>Netiquette<@$p>, die Verhaltensregeln für das Internet, besagen z.B., dass man nicht dieselbe E-Mail an Hunderte von Newsgroups schickt. Weder Gesetze noch Technologien können dies verhindern, ein Schwall von bösen Antworten wären dem Absender jedoch gewiss. Der Zugang zum Netz selbst und zu bestimmten Informationsangeboten hat seinen Preis, durch den das Verhalten von potenziellen Nutzern gesteuert wird. Der Gesetzgeber überträgt einerseits bestehende Regelungen auf den neuen Raum und schafft andererseits spezifische Gesetze, die den Eigenheiten der digitalen Umgebung gerecht werden sollen. Kinderpornografie, Verleumdung oder Urheberrechtsverstöße sind online ebenso strafbar wie offline. Das Verbot von Umgehungstechnologien für RCSs ist spezifisch auf digitale Medien ausgerichtet. Die »Architektur« des digitalen Raumes ist der Code, aus dem er besteht. Angefangen von Betriebssystemen und Netzwerkprotokollen wie TCP/IP, über Passwortabfragen und <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Cookies<@$p>, bis zu Rechtekontrollinfrastrukturen gibt es dort nichts, was nicht mit bestimmten Absichten geschaffen worden wäre. Die vier Modalitäten unterscheiden sich nach ihrer Reichweite, ihrer Formbarkeit, den Akteuren, die sie betreiben, und nach den Werten, die sie hochhalten oder ignorieren. Betrachten wir nun nach dem bislang Gesagten die Frage des Urheberrechts in der digitalen Wissensordnung in Hinsicht auf diese vier Regulierungsmechanismen. Die sozialen Normen sind kein besonders guter Garant für den Schutz des Urheberrechts. Das Kopieren und Weitergeben von Informationen ist ein natürliches Element des kulturellen Austausches. Weithin mangelt es an einem Unrechtsbewusstsein, Verletzungen werden allenfalls als Kavaliersdelikt angesehen. Gegen etwas angehen zu wollen, was die Mehrzahl der Weltbevölkerung als ihr gutes Recht ansieht, ist zum Scheitern verurteilt. Die Aufklärungskampagnen der Rechteindustrie belegen nur ihre Vergeblichkeit. Der Markt ist dort ein effektiver Mechanismus zur Durchsetzung von Urheberrechten, wo, wie derzeit bei DVDs, vorbespielte Medien billiger angeboten werden als Rohlinge. Doch dies ist die Ausnahme. In den allermeisten Fällen wird ein ökonomisch rational abwägender Nutzer sich, wenn er die Wahl hat, ein digitales Werk zu kaufen oder es gegen <\h>nominelle Kosten zu kopieren, für Letzteres entscheiden. Ziel der Rechteindus<\h>trie ist es daher, diese Kopierkosten in die Höhe zu treiben. Dazu bemühen sie sich, die Architektur des digitalen Wissensraumes und die flankierende Rechtslage zu verändern. Die Architektur dieses Raumes aus Computer und Internet ist daraufhin entworfen, alles zu berechnen, zu speichern und zu übertragen, was in digitaler Form codiert werden kann. Es besteht ein grundlegender und unauflösbarer Interessenkonflikt zwischen der Computerwelt, für die es selbstverständlich ist, dass Computer vom Anwender frei programmierbare Universalmaschinen sind, und der Welt der alten Medien, die Computer als Geräte betrachten, über die sie »Content« an »Konsumenten« ausliefern. Die »Konvergenz«, die seit einigen Jahren angekündigt wird, stellt sich die Rechteindustrie so vor, dass Computer zu Set-Top-Boxen werden und das Internet zu einer Art Kabelfernsehnetz mit einer großen Bandbreite, um audiovisuelle Datenströme zum Käufer zu pumpen, und einem minimalen Rückkanal, über den dieser seine Auswahl aus einem gegebenen Angebot kundtun und seine Kreditkartennummer übermitteln kann. Das Vorstellungsvermögen der Datenherren und des Rechts wird von den überlieferten Copyright-Praktiken geleitet: »Das Ideal scheint zu sein, Computer zu zwingen, dass sich digitale Daten wie physische Gegenstände, z.B. CDs, DVDs und Bücher verhalten. Die alten Medien werden dieses Verhalten akzeptieren, da sie daran gewöhnt sind, und das Rechtssystem wird es durchsetzen, weil es genau das ist, für dessen Durchsetzung das bestehende Recht und die bisherige Rechtsprechung entworfen wurden.« (B.<\!q><@6 Caps>Bell<@$p>, 6/2000).<t-2>Warum aber sollten Anwender einen Computer mit eingeschränkter Funktionalität und zudem einem höheren Preis akzeptieren, wenn sie eine Universalmaschine haben können? Noch sind keine Vorschläge <\n>(öffentlich) geäußert worden, Universalcomputer gesetzlich zu verbieten. Die Strategie der Rechteindustrie läuft über den Markt und besteht aus zwei Schritten. In einem »Selbstregulierungsprozess« der Rechte-, Soft- und Hardwareindustrien werden Standards etabliert und ihre Einhaltung durch alle Beteiligten durchgesetzt. Die Konsumenten werden dann gelockt, diese Techologien zu akzeptieren, weil neue attraktive Inhalte nur noch in Formaten veröffentlicht werden, die diese Technologien vo<\h>raussetzen. <t$>In den Darstellungen zu Rechtekontrollsystemen (RCS) wird man nirgends einen Anspruch der Rechteindustrie auf maximalen Profit finden. Vielmehr seien sie »vor allem« zum Vorteil der Konsumenten (Intel u.a., 2000, S.<\!q>6). Da dies auf den ersten Blick widersinnig erscheint – jede, die wählen kann, ob sie eine Information in einer digitalen Zwangsjacke und gegen einen Preis erhält oder frei und gratis, wird sich für Letzteres entscheiden –, läuft das Argument über den Umweg des Verlegerinteresses. Die Rechteindustrie werde ihre Schätze nur digital verfügbar machen, wenn sie ihre Investitionen amortisieren und einen Gewinn erzielen kann. Da bislang im Internet kein Mechanismus verfügbar war, der dies garantierte, gebe es dort keinen Inhalt von hoher Qualität. Die zahlreichen wissenschaftlichen und technischen Werke, die der vorliegende Band mit ihren Webadressen zitiert, belegen zwar das Gegenteil, doch dessen vollkommen ungeachtet unterstellen die RCS-Befürworter, dass man dafür weiterhin in eine Buchhandlung oder ein Musikgeschäft gehen müsse. Da nun aber Leser und Hörer gerne die Vorteile der weltweiten 24 Stunden x 7 Tage-Verfügbarkeit von Inhalten im Internet nutzen möchten, seien ihre Interessen identisch mit denen der Rechteindustrie. Die zentrale Rechtfertigung für RCSs lautet in der pathetischen Formulierung von Stefik: »Es würden mehr digitale Werke geschaffen, weil mehr potenzielle Autoren die Möglichkeit hätten, durch die Erschaffung solcher Werke ihren Lebensunterhalt zu verdienen. Das ist der Haupt<\h>effekt – <@1 fliess normal>das Licht freizusetzen<@$p>.«<@3 hoch fliess>96<@$p> Dabei wird außerdem eine direkte Korrelation zwischen der Investitionshöhe für die Produktion eines Werkes, zwischen dessen Verkaufswert sowie der erforderlichen Sicherung gegen dessen ertragslose Verbreitung unterstellt. Dies mag für Hollywood-Produktionen zutreffen, wo berechenbar mit jeden zehn Millionen Dollar mehr an Investitionen die Chancen auf einen Kassenschlager wachsen. Das gegenteilige Extrem einer Wissensproduktion bildet die freie Software, die nicht nur gratis, sondern auch ohne Restriktionen für das Kopieren und Modifizieren zirkuliert und deren hohe Qualität dennoch – oder genauer: gerade deshalb – unbestritten ist. Dazwischen liegt ein weites Spektrum von Wissenskreationen, bei denen die Korrelation zwischen dem Bedürfnis von Werkschaffenden, ihren Lebensunterhalt zu verdienen und der Qualität ihrer Werke nur sehr locker ist. Ganz davon zu schweigen, dass die Autoren an den Erträgen, die die Verwerter erzielen, ohnehin nur marginal oder gar nicht beteiligt sind. Problematisch an der Darstellung der RCS-Vertreter ist ferner, dass sie jeden Rezeptionsvorgang von verwertbarem Wissen jeglicher Art als »Konsum« präsentieren. Zwar wird gelegentlich die Klasse von Inhalten genannt, die diese Entwicklung vorantreibt – »<@1 fliess kursiv>commercial entertainment content<@$p>« (Intel u.a., 2000, S.<\!q>14) – es ist jedoch abzusehen, dass keine Form von Wissen davon verschont bleiben wird, wenn eine solche umfassende Kontrollinfrastruktur einmal etabliert ist. Schon heute werden <t-1>wirtschaftsrelevante Expertisen unter restriktiven Konditionen verkauft.<x@3 hoch fliess><t-1>97<@$p><t-1> <t$>Auch in der Hochschul- und Fortbildung werden digitale Lehrmaterialien zu Waren werden.<@3 hoch fliess>98<@$p> Selbst gemeinfreies Wissen wird in die Zwangsjacke der RCSe geraten, wie das Beispiel Corbis zeigt.<@3 hoch fliess>99<@$p> Für das Privileg, Fernsehsendungen wie »Die Simpsons«, die »X-Akte« oder auch die <@1 fliess kursiv>Encyclopaedia Britannica<@$p> im Netz abrufen zu können, wird der Konsument, so Stefiks Logik, bereitwillig sein Geburtsdatum, seine Einkommenverhältnisse, seine Kreditkarteninformationen in eine zentrale Datenbank eingeben, auf die die<@1 fliess kursiv> Trusted Systems <@$p>der Datenherren vor jeder Transaktion zugreifen. Trotz aller Rhetorik, dass RCSs »vor allem« den Interessen der »Konsumenten« dienten, ist kaum damit zu rechnen, dass diesen irgendein Mitspracherecht bei ihrer Etablierung eingeräumt wird.Die technologische »Selbsthilfe« der Wirtschaft findet ihre Grenzen in der Technologie selbst. Wie bereits an einigen Beispielen gezeigt, ist noch jede Schutztechnologie gebrochen worden:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Das Knacken von Microsofts <@1 fliess normal>Windows Media Audio<@$p> (WMA)-Format zeigt einige Probleme auf, die die alten Medien bei dem Bestreben, ihre Urheberrechte zu schützen, erwarten. Anstatt zu versuchen, herauszufinden, wie WMA codiert ist, sichert ›unfuck.exe‹ nur die Ausgabe des Decoders bevor diese an die Soundkarte geschickt wird. Für jedes Audio-Format muss der Medien-Player die Daten zu unverschlüsselten, unkomprimierten, digitalen Tönen decodieren, bevor er sie an die Soundkarte schickt, so dass diese Methode bei jedem proprietären Decoder-Programm funktioniert. [...] Die technische Realität ist, dass jeder auf Software basierende Kopierschutz nur ›Sicherheit durch Undurchsichtigkeit‹ bedeutet und anfällig für Reverse-Engineering ist. Jeder Player, der verbreitet genug ist, dass sich der Aufwand lohnt, wird dem Reverse-Engineering unterzogen« <@1 fliess normal>(<@6 Caps>B. Bell<@1 fliess normal>, 6/2000).@1 fliess mit:@1 fliess ohne:Bruce Bells präzise Analyse zeigt auf, dass hardwaregestützte Systeme, wenn auch nicht prinzipiell unverwundbar, so doch erheblich aufwendiger zu brechen oder zu umgehen sind: @1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Um überhaupt erfolgreich zu sein, müssen die Trusted-Client-Computer zuerst als kleine, günstige, integrierte Systeme auftreten, wie z.B. konvergierte tragbare Mobiltelefon-, PDA- und Medien-Abspielgeräte oder Spielekonsolen, deren Funktionen nach und nach erweitert werden, bis sie schließlich diejenigen eines Einsteiger-Desktop-Computers umfassen. Diese Geräte neigen ohnehin dazu, aus proprietärer Soft- und Hardware zu bestehen, und sie sind so billig, dass sie es sich erlauben können, verkrüppelt zu sein. <*t(25.512,0,"1  ")>	Vertikal integrierte Konglomerate, wie Sony, die überall ihre Finger im Spiel haben, werden wahrscheinlich die ersten sein, die versuchen, Trusted-Client-Computer zu vermarkten. Obwohl die Playstation nur zum Abspielen von Spielen und CDs geeignet ist, wird die nächste Genera<\h>tion von Spielekonsolen Internetzugang und andere Leistungsmerkmale aufweisen, die wir von Universalcomputern gewohnt sind. Sony ist in einer guten Position, um sich die ganze Vertriebskette anzueignen, von   der Musik- und Filmproduktion, über Abspielgeräte im Konsumelektroniksektor und Computer bis zur physischen Datenhaltung. <*t(25.512,0,"1  ")>	Auch der Elektronikgigant Intel ist gut positioniert, um die Trusted-Client-Hardware einzuführen. Intel kann den Prozessor, die Grafikfunktionen und unterstützende Hardware auf einem Chip integrieren, diesen auf eine Karte stecken und günstig verkaufen« <@1 fliess normal>(ebd.).<@$p><*t(25.512,0,"1  ")>@1 fliess ohne:Wo Moral, Markt und Architektur versagen, mobilisiert die Rechteindustrie die Regularien des Gesetzes. Wo technisch versierte Menschen im Wettlauf mit den RCS-Konstrukteuren die Nase vorn haben, droht ihnen das Umgehungsverbot der aktuellen Digital-Gesetze mit Gefängnis und Geldstrafen. Die Verurteilungen der amerikanischen Website-Betreiber, die DeCSS zugänglich gemacht hatten oder gegen die Firma Napster sind Präzedenzfälle, von denen eine Signalwirkung für den Stellenwert des herkömmlichen Urheberrechts im Internet ausgeht.@1 fliess mit:Eine Zeitlang schien es, als sei es der »natürliche Drang« der Information im Netz, frei zu sein. Die Rechteindustrie beschwor den Untergang der Welt oder doch zumindest den der Rechteindustrie herauf und warnte, dass die Autoren leiden würden, weil sie keine Vergütung für die Früchte ihrer Arbeit erhielten, ebenso wie die Öffentlichkeit, weil sie kein<t-2>e neuen Werke von hoher Qualität mehr angeboten bekäme. Je mächtiger <t$>der Feind (»Terroristen«, »Drogenhändler«, »organisiertes Verbrechen«, »Daten-Piraten«), desto massiver der Gegenschlag. Mit dem Ergebnis, dass es heute scheint, als seien die Verwertungsrechte an Informationen besser geschützt als je zuvor seit ihrer Erfindung. Der Information werden elektronische Fußfesseln angelegt. An der langen Leine ihrer Eigen<\h>tümer wird sie gleichsam voltigiert und kann jederzeit zurückgezogen werden. Das Recht steht Gewehr bei Fuß, um jeden, der versucht, diese elektronische Leine zu durchtrennen, mit drakonischen Strafen zu belegen. Selbst dort, wo es die Intention des Gesetzgebers war, die Rechte der Eigentümer zu beschränken, beschneiden die Eigentümer diese Beschränkungen, indem sie die Leser und Hörer verpflichten, freiwillig <\h>darauf zu verzichten. Es fällt nicht schwer, sich dystopische Visionen einfallen zu lassen, in denen sich Menschen strafbar machen, wenn sie einen Freund ein Buch in ihrem Besitz lesen lassen oder wenn sie einen <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Debugger<@$p> besitzen – die gefährlichste »Waffe« im Software-»Krieg«.<@3 hoch fliess>100<@$p> Tatsächlich arbeiten massive Kräfte daran, solche allumfassenden – um nicht zu sagen »totalitären« – Szenarios Wirklichkeit werden zu lassen. All dies geschieht im Gestus der Selbstregulierung der Industrie, auf dass der Staat sie nicht reguliere, und unter Hochhaltung des »freien Marktes«: Wenn die Kundschaft eine gegebene Vemarktungsstrategie für Wissen nicht akzeptiert, wird es Anbieter geben, so die Argumentation, die andere Lösungen versuchen – die Kunden werden mit den Füßen darüber abstimmen. Diese Logik scheitert daran, dass Werke nicht ausstauschbar sind. Wenn ich die Musik einer bestimmten Band hören möchte, die exklusiv bei einer Plattenfirma unter Vertrag ist, kann ich nicht zu einem anderen Anbieter gehen, um »etwas ähnliches« zu hören. Sie scheitert vor allem dann, wenn es keine Alternativen mehr gibt, weil sich die gesamte Industrie auf einen einzigen Standard geeinigt hat. Bruce Bell schreibt treffend: »Überall, wo es Konkurrenz gibt, werden die <@1 fliess kursiv>Trusted-Client-Systems<@$p> im Nachteil sein« <t-1>(<x@6 Caps><t-1>B. Bell<@$p><t-1> 6/2000). An dieser Stelle setzt die »Selbstregulierung« des Marktes e<t$>in: Um den Nachteil der RCSs zu beseitigen, muss die Konkurrenz, die vor nicht eingeschränkten Geräten ausgeht, beseitigt werden.Was ist nun aber verwerflich daran, dass Eigentümer ihr Eigentum schützen? Wer wollte es einem Wohnungsinhaber verdenken, dass er seine Haustür abschließt? Die Antwort darauf besteht aus zwei Teilen: Zum einen unterscheiden sich materielle und Immaterialgüter grund<\h>sätzlich voneinander. Zum anderen verschiebt sich mit dem Wechsel der Regulierung durch Recht hin zur Regulierung durch Code die Instanz, die über Konflikte entscheidet und somit auch die Werte, die einer Entscheidung zugrunde gelegt werden: »In diesem Modell sind <@1 fliess kursiv>Trusted Sys<\h>tems<@$p> eine Alternative zum Schutz von geistigen Eigentumsrechten – eine private Alternative zum Gesetz. Sie müssen nicht der einzige Mechanismus sein; es gibt keinen Grund, nicht sowohl das Gesetz als auch die <@1 fliess kursiv>Trusted Systems<@$p> zu verwenden. Nichtsdestoweniger erledigt der Code im Effekt die Aufgabe, die sonst vom Gesetz geregelt wird. Durch den Code wird der gesetzliche Schutz viel effektiver ausgeübt, als vom Gesetz selber« (<@6 Caps>Lessig<@$p>, 1999, S. 130).Ein Urheberrechtsstreit in der analogen Welt wird von einer Richterin entschieden, die eine Güterabwägung zwischen dem Schutz des geistigen Eigentums und seinen Schranken im Interesse der Öffentlichkeit vornimmt. Die Rechteindustrie dagegen erzwingt mit Hilfe der Architektur des digitalen Wissensraumes Regularien, die naturgemäß ihre eigenen, also partikulare Interessen vertreten: »Die Interessen des Staates sind allgemein, nicht partikular. Es gibt Gründe, Rechte zu schaffen, wenn diese einem gemeinsamen Ziel und nicht einem speziellen Ziel dienen« (ebd., S.<\!q>131). Natürlich gehen in die Erzielung einer Balance auch die berechtigten Interessen der Urheber ein, doch wenn Urheberrechte durch Technologie und privatrechtliche Lizenzverträge geschützt werden, gibt es umgekehrt nichts, was eine vergleichbare Balance erfordern würde. Verträge sind, bei aller Freiheit, in einen rechtlichen Rahmen eingebunden. Sittenwidrige oder dem Urheberrecht widersprechende Bestimmungen können zur Entscheidung vor Gerichte gebracht werden. Aber welche Instanz, fragt Lessig, können wir gegen sittenwidrige Technologie anrufen? Er erinnert an die Grundlagen des Urheberschutzes:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Geistige Eigentumsrechte sind ein Monopol, das der Staat den Erzeugern von geistigem Eigentum im Austausch für ihre Erzeugung von geis<\h>tigem Eigentum gewährt. Nach einer begrenzten Zeit geht das Produkt ihrer Arbeit in das Eigentum der Öffentlichkeit über, die damit machen kann was sie möchte. Das <@1 fliess normal>ist <@$p>Kommunismus, und zwar im Kern unseres verfassungsrechtlichen Schutzes für geistiges Eigentum. Dieses ›Eigentum‹ ist kein Eigentum im gewöhnlichen Sinne. [...] In das geistige Eigentumsrecht sind Grenzen eingebaut, die die Macht der Autorin beschränken, die Nutzung der von ihr geschaffenen Ideen zu kontrollieren. [...] Zusammengenommen geben diese Regeln dem Urheber ein bedeutendes Maß an Kontrolle über die Nutzung seiner Werke, aber niemals eine perfekte Kontrolle. Sie geben der Öffentlichkeit einen gewissen Zugang, aber nicht einen vollständigen Zugang. Sie sind auf eine Balance hin entworfen, aber unterscheiden sich von dem Interessenausgleich, den das Recht für gewöhnliches Eigentum schafft. Sie sind von der Verfassung so eingerichtet worden, dass sie dazu beitragen, eine geistige und kulturelle Allmende aufzubauen« <@1 fliess normal>(ebd., S. 134 f.).@1 fliess mit:@1 fliess ohne:Ein wichtiges Element dieser kulturellen Allmende ist das <@1 fliess kursiv>Fair Use<@$p>-Konzept. Sein Stellenwert im US-amerikanischen Copyright-Recht ist nicht unumstritten. Bei den kodifizierten Schrankenbestimmungen des Autorenrechts kann kein Zweifel bestehen, dass es sich um positive Rechte handelt, doch in der amerikanischen Debatte stehen sich zwei Positionen gegenüber. Die eine Fraktion sieht den <@1 fliess kursiv>Fair Use <@$p>als ein technologisches Residuum, das durch RCSs überflüssig wird. Die andere erachtet ihn als eine positives (<@1 fliess kursiv>affirmative<@$p>) Recht, das auch in einer RCS-Umgebung gewahrt bleiben muss.@1 fliess mit:RCS-Apologeten sehen die Rechtfertigung für die <@1 fliess kursiv>Fair Use<@$p>-Doktrin in einem Marktversagen. Da es unter herkömmlichen Bedingungen zu aufwändig war, die Rechteinhaber von Texten, die z.B. ein Hochschullehrer in einer Kurssammlung mit einer Kopierauflage von 30 Exemplaren aufnehmen möchte, ausfindig zu machen, Nutzungsverträge mit ihnen auszuhandeln und Gebühren abzurechnen, gestand die <@1 fliess kursiv>Fair Use<@$p>-Doktrin ihre Verwendung frei und gratis zu. Solange die Kosten für den Erwerb unverhältnismäßig hoch waren und die Nutzung den kommerziellen Wert der Vorlage nicht beeinträchtigte, konnte man von einer »fairen Verwendung« ausgehen. Durch ein RCS entfällt dieser Aufwand. In einer geeigneten technologischen Infrastruktur können der Professor oder seine Studierenden mit einem Mausklick ein Nutzungsrecht erwerben und den Rechteinhabern einen Obolus zukommen lassen. Diese Auffassung spricht aus Stefiks Definition: »Die <@1 fliess kursiv>Fair Use<@$p>-Doktrin findet Anwendung, wenn jemand durch den Besitz eines Werkes die Möglichkeit hat, es in einer Art und Weise zu nutzen, die dessen Schöpfer als unfair empfindet. [...] Digitale Rechtekontrollsysteme könnten jedoch Transaktionen selbst gegen einen nominellen Geldbetrag ermöglichen und verändern so die Konfrontationsfrage, ob eine Nutzung gebührenpflichtig oder kostenlos ist, zu der praktischen Frage, wieviel soll sie kosten?« (<@6 Caps>Stefik<@$p>, 1996, S. 29<\!q>f.). Die Verfügbarkeit und der Preis von Rechten, so argumentiert diese Fraktion, sei nicht länger eine Frage der rechtlichen Interpretation, sondern des Wettbewerbs auf einem »gesunden Markt« (<@6 Caps>Stefik<@$p>, 1997a).Der wort- (und Fußnoten-) gewaltigste Vertreter dieses Lagers ist Tom W. Bell. Seine zentrale Aussage ist: Wo <@1 fliess kursiv>Fared Use<@$p>, also die Verwendung gegen Gebühr praktikabel ist, wird <@1 fliess kursiv>Fair Use<@$p> überflüssig (<@6 Caps>T. Bell<@$p>, 1998). Bell, ein Vertreter des amerikanischen »libertären« Lagers, Student von Richard Posner, Gewinner der »Hayek Fellowship« und affiliiert mit dem Cato Institut,<@3 hoch fliess>101<@$p> tritt für eine Welt jenseits des Copyright ein, in der Lizenzen und Technologie allein den Markt der Informationen regulieren. Die Gesetzgeber sollten sich nicht in den noch offenen Prozess der Etablierung von RCSs einmischen, vielmehr sollten sie den »Verbrauchern und Anbietern von Informationen erlauben, das Copyright-Recht zu verlassen und zum Vertragsrecht zu wechseln«. RCSs seien attraktiv für Informationsanbieter, da sie die Durchsetzung ihrer Rechte »sauber und effizient« bewerkstelligten, ohne auf ungewisse und kostspielige Rechtsstreitigkeiten zurückgreifen zu müssen. Bell greift auch die Argumente der Gegenfraktion auf, denen zufolge <@1 fliess kursiv>Fair Use<@$p> ein integraler Teil des verfassungsgemäßen <@1 fliess kursiv>quid pro quo<@$p> ist. Auch Akademiker, Künstler und Journalisten, die geschützte Werke verwenden, seien durch die verschwommenen Grenzen des <@1 fliess kursiv>Fair Use<@$p> – von dem Bell nur als »Verteidigung« gegen Copyright-Verstöße spricht – von möglichen Prozessen bedroht: »ARM [<@1 fliess kursiv>Automated Rights Management,<@$p> RCSs] schafft klare Verhältnisse, die denen unmittelbar zugute kommen, die urheberrechtlich geschützte Werke zweitnutzen möchten – und durch sie ihrem Publikum –, indem es Sicherheit vor Copyright-Rechtsstreitigkeiten erzeugt.« Dabei bezieht er sich auf Stefik, der schreibt, dass RCSs Sonderkonditionen für Bibliothekare, Forscher und Lehrer vorsehen könnten – was aber bislang eine rein hypothetische Annahme ist. Auch andere Mitglieder der Öffentlichkeit würden profitieren. Durch den besseren Schutz stiege der Wert von Werken, weshalb ihr Preis fallen würde. Der Preisdruck des Wettbewerbs mache diese wundersame Logik möglich: »Die Erlöse, die das ARM den Copyright-Eigentümern bringt, würden somit an die Verbraucher in Form von reduzierten Zugangsgebühren weitergegeben.« Auch dies ist eine hypothetische Annahme, die von der Erfahrung mit CDs widerlegt wird. Auch hier behaupteten die Hersteller anfangs, ihr Preis werde fallen, sobald sich die Herstellungstechnologie amortisiert habe, da CDs kostengünstiger herzustellen sind als Vinylplatten. Das Gegenteil ist der Fall. Im Verlauf der Argumentation wird deutlich, dass selbst Bell einen Rest an <@1 fliess kursiv>Fair Use<@$p>-Anwendungen nicht abstreiten kann: »Verbraucher würden natürlich weiterhin das Recht haben, die Zahlung für Nutzungen zu verweigern, die die Voraussetzungen für einen <@1 fliess kursiv>Fair Use<@$p> erfüllen. Aber sie werden zweifellos die Aussicht begrüßen, die Rechtsunsicherheit auszuräumen, und die Zahlung via ARM nicht als Belastung empfinden.« In seinem grenzenlosen Vertrauen in den freien Markt fordert er, dass erwachsene Menschen in gegenseitigem Einverständis frei sein sollten, auf eine Weise Informationen zu geben und zu erhalten, wie sie es für richtig halten. Dazu sollte der Gesetzgeber Informationsanbietern (wohlgemerkt: nicht Autoren, die er in seinem langen Aufsatz kein einziges Mal erwähnt) die Option geben, zwischen Copyright-Recht und dem Vertragsrecht zu wählen. Im zweiten Falle würden sie permanent auf die Vorteile des Copyright verzichten und ihren Schutz ausschließlich auf technische Lösungen und Lizenzen stützen. Gegen Ende seiner Argumentation fragt Bell, was geschähe, wenn ein <@1 fliess kursiv>Worst-case<@$p>-Szenario einträfe:@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Verträge über eine individuelle Vergütungsabrechnung entwickeln sich dahin, dass die traditionelle Bedeutung, die das Copyright dem <\h>öffentlichen Interesse zumisst, in digitalen Intermedien gänzlich unrepräsentiert bleibt. Die angemessene Reaktion wäre es nicht, solche Vereinbarungen für ungültig zu erklären, und somit private Vertragsparteien dafür zu bestrafen, dass sie ganz selbstverständlich ihre privaten Interessen verfolgen. Die angemessene Reaktion wäre es stattdessen, ihnen die öffentlichen Vorteile des Copyright zu entziehen. Informationsanbieter, die Verträge dem Copyright vorziehen, müssten sich auf ARM verlassen, um ihre Waren zu schützen, die davon abgesehen gemeinfrei würden. Diejenigen, die das Copyright vorziehen, müssten auf übermäßig restriktive Verträge verzichten.«<@1 fliess normal>@1 fliess mit:@1 fliess ohne:Bell nimmt das Quid-pro-quo des Copyright ernst, wenn er argumentiert, dass die Verwertungsindustrie nicht nur seinen Eigentums- und Investitionsschutz beanspruchen könne, ohne den öffentlichen Verpflichtungen des Copyright nachzukommen. So folgerichtig ein Herausspringen aus dem Copyright-Recht erscheint, so unwahrscheinlich ist es in in der Praxis. Schutztechnologie ist unwirksam ohne ein flankierendes Umgehungsverbot, das aber gerade im Copyright verankert ist. Die Rechteindustrie würde also mit dem Copyright auch auf das gerade neu geschaffene Instrument zur Verhinderung von Angriffen auf ihre technische Lösung verzichten. Gegen ISPs oder die direkten Anbieter von Kopien hätte sie keine Handhabe mehr, da die Werke ja nun nicht mehr ihr Eigentum, sondern gemeinfrei wären. Als Sanktion zur Kontrolle ihrer Werke bliebe ihr nur, wegen Vertragsbruch zu klagen, doch Dritte sind vertraglich nicht gebunden. Bells Vorschlag käme einer Selbstenteignung der Rechteindustrie gleich. @1 fliess mit:Das viel wahrscheinlichere Szenario, dass die Rechteindustrie den Schutz des Copyright in Anspruch nimmt sowie Lizenzverträge <@1 fliess kursiv>und<@$p> technische Sys<\h>teme einsetzt, unterschlägt Tom Bells Rhetorik. Damit das öffentliche Interesse nicht »gänzlich unrepräsentiert« bleibt, halten Andere drastische Mittel für angebracht. Zu denen, die in <@1 fliess kursiv>Fair Use<@$p> ein positives Recht sehen, das nicht einfach verschwindet, wenn Technologie es zu umgehen erlaubt, gehört die Rechtsgelehrte Julie Cohen. Sie erinnert wie Lessig daran, dass nicht jede unautorisierte Nutzung eines geschützten Werkes unrechtmäßig ist. Sie stellt fest, dass – selbst unter der unwahrscheinlichen Annahme, dass die Rechteindustrie Schutzsysteme entwerfe, die öffentliche Interessen berücksichtigen – diese einige Handlungen verhindern werden, die das Copyright-Recht erlaubt. Dies führt die Juristin zu der Frage nach der »rechtmäßigen Manipulation« von Rechtekonktrollsystemen und den Folgen eines rechtlichen Umgehungsverbots: »Nehmen wir einmal eine Person, die das CMS <@1 fliess kursiv>[Copyright Management Sys<\h>tem]<@$p> manipuliert, um eine Nutzung zu ermöglichen, die sie für fair hält. Ihre Handlungen geschehen wissentlich, sie hat aber keine Absicht, ein Recht zu verletzen; tatsächlich beabsichtigt sie entschieden, kein Recht zu verletzen. Sollte sich ihre Ansicht bezüglich des <@1 fliess kursiv>Fair Use <@$p>als falsch herausstellen, wird sie für die Rechtsverletzung verantwortlich gemacht. Sie auch wegen unerlaubter Manipulation haftbar zu machen, scheint sowohl unfair als auch unnötig« (<@6 Caps>Cohen<@$p>, 1997). Die Manipulation von RCSs dürfe nicht strafbar gemacht werden, wenn die Handlung, die die Manipulation ermöglicht, rechtens sei: »Man kann den Copyright-Inhabern nicht verbieten, den Zugang zu ihren Werken schwieriger zu machen; es sollte ihnen aber nicht gestattet werden, andere davon abzuhalten, ihre technologischen Barrieren zu umgehen. Ansonsten würde der bloße Akt, ein Werk in einem CMS zu verschlüsseln, auf wundersame Weise dem Verkäufer größere Rechte gegenüber der breiten Öffentlichkeit geben als das Copyright zulässt.«Diese Überlegung, die als das »Cohen-Theorem« in die Debatte eingegangen ist, leitet sich nicht nur aus den Bibliotheks- und <@1 fliess kursiv>Fair Use<@$p>-Schranken ab, sondern vor allem auch aus Datenschutzerwägungen. Was Menschen lesen, hören und ansehen, enthüllt mehr noch als andere Formen von Konsumverhalten ihre Ansichten, Überzeugugen und Geschmäcker. Daher leitet Cohen aus der Meinungsfreiheit des <@1 fliess kursiv>First Amendment<@$p> ein Recht ab, anonym zu lesen (vgl. <@6 Caps>Cohen<@$p>, 1996). Da RCSs zum Teil hochdetaillierte Nutzungsprofile erstellen, folgt aus dem »Recht, anonym zu lesen« ein Recht, die Systeme zu hacken, nicht um Copyright-verstöße zu begehen, sondern um in Selbsthilfe den Schutz der eigenen Privatsphäre zu sichern (vgl. <@6 Caps>Cohen<@$p>, 1997).Das Umgehungsverbot ist in den USA mit dem DMCA von 1998 Gesetz geworden. Das Cohen-Theorem verliert damit aber keineswegs an Bedeutung, zumal diese Entscheidung in der deutschen Urheberrechtsrevision noch aussteht. Es bleibt zu hoffen, dass das Autorenrecht mit seiner solideren Tradition von Schrankenbestimmungen auch bei der Regulierung der Rechtekontrollsysteme andere Wege gehen wird. Der Drei-Stufen-Test der RBÜ und der WIPO-Verträge, der fordert, dass die Schrankenbestimmungen nicht mit der »normalen Verwertung« eines Werkes konfligieren und den legitimen Interessen der Autoren keine »unzumutbaren Nachteile« bringen, erscheint angesichts einer RCS-Infrastruktur, die die Verwertung jedes kleinsten Bestandteils einer Nutzung »normalisiert«, auf jeden Fall unzureichend. Ebenso bleibt zu hoffen, dass die solidere Datenschutztradition in Europa in die Beschlussfindung eingehen wird. RCSs richten <@1 fliess kursiv>Blackboxes <@$p>auf der Festplatte eines Anwenders ein, über die dieser keinerlei Kontrolle hat. Schon heute kontaktieren Client-Programme, wie die von <@1 fliess kursiv>RealNetworks<@$p> oder <@1 fliess kursiv>Glassbook<@$p> ihre Herstellerfirmen, wenn sie eine TCP/IP-Verbindung wahrnehmen, mutmaßlich, um zu überprüfen, ob es <@1 fliess kursiv>Updates <@$p>gibt. Sie tun dies, ohne den Besitzer des PCs darüber zu unterrichten, geschweige denn, um Erlaubnis zu fragen. Wenn der Anwender keine zusätzliche Software benutzt, um seine ein- und ausgehenden Internetverbindugen zu überwachen, wird er diese Eigenaktivitäten seiner Pro<\h>gramme nicht einmal bemerken. Als bekannt wurde, dass Microsofts <@1 fliess kursiv>Internet-Explorer<@$p> während der Installation die Festplatte scannt, um sein Durchsuchungsprotokoll bei der ersten sich bietenden Internetverbindung an den Konzern zu schicken, ging ein Aufschrei durch die Computerwelt. Inzwischen sind die Programme »intelligenter« und ihre Kommunikationsformen undurchsichtiger geworden. Selbst wenn der Anwender von einer Kontaktaufnahme seines Clients weiß, ist es ja gerade Sinn eines RCS, dass es den Inhalt der Kommunikation geheim hält. Da der Markt die urheberrechtlich vorgeschriebenen Informationsfreiheiten nicht von sich aus wahren wird, ist es am Gesetzgeber, dafür zu sorgen, dass die Grundwerte von Freiheit und ungehinderter offener Kooperation, von Schutz der Privatsphäre, von Redefreiheit, Anonymität und universellem Zugang in die Architektur der digitalen Wissensordnung eingehen. <t-1>Nicht nur für das Individuum, sondern auch für den Wissensbestand gehen absehbare Gefahren von RCSs aus. Wird Wissen zunehmend nur noch in Rechtekontrollsystemen eingekapselt, ohne dass parallel gedruckte Versionen publiziert würden, fällt es aus dem öffentlichen Biblio<\h>thekssystem heraus. Einsicht und Ausleihe sind ebenso erschwert oder ausgeschlossen, wie die Aufgaben der Bibliothekare, Wissen für künftige Generationen zu konservieren. Bei allem, was wir über die kurze Geschichte des Computers wissen, müssen wir damit rechnen, dass in fünf oder zehn Jahren die Technologie, die erforderlich ist, um ein Werk zu lesen, nicht länger verfügbar und damit die darin eingekapselten geistigen Güter unzugänglich sein werden. Hardware, Software und Protokolle entwickeln sich weiter und machen ältere Versionen inkompatibel. Datenträger haben eine begrenzte Lebenszeit. Firmen gehen bankrott, mit ihnen geraten ihre proprietären Technologien unwiderruflich in Vergessenheit. <t$>@1 fliesskursiv Zitat:<*t(25.512,0,"1  ")>»Wegen der begrenzten technischen Gebrauchsdauer einzelner Kaufstücke digitaler Medien bedeutet dies de facto, dass es insoweit keine privaten [und öffentlichen, VG] Archive mehr geben wird. [...] Nur der Rechteinhaber hat das Archivmonopol, da er nach Belieben die Inhalte digital umkopieren und somit über einen langen Zeitraum archivieren kann. Was der Rechteinhaber nicht mehr verkaufen will (oder <@1 fliess normal>darf!,<@$p> der Staat könnte sich hier auch noch einmischen), ist nach spätestens ein bis zwei Jahrzehnten für den Normalverbraucher einfach nicht mehr verfügbar. Spätestens dann, wenn auch wichtige Literatur hauptsächlich digital vermarktet wird, werden wir damit ein großes kulturpolitisches Problem haben. Leute, schmeißt eure Hardcopy-Bücher noch nicht weg, vielleicht werden wir sie noch einmal brauchen« <@1 fliess normal>(<@6 Caps>Horns<@1 fliess normal>, 2000).@1 fliess mit:@1 fliess ohne:Die Lebensdauer von Computerprogrammen mag begrenzt sein, doch auch hier haben Nutzer, die ihre in den 80er-Jahren mit <@1 fliess kursiv>dBase<@$p> dem Programm angelegten Datenbanken nicht in regelmäßigen Abständen auf neue Formate migriert haben, heute ihre Arbeit von Jahren verloren. Wie ließe es sich rechtfertigen, dass ich ein digitales Buch, eine Fotosammlung, eine Enzyklopädie, die ich heute kaufe, in zehn Jahren nicht mehr benutzen kann, selbst wenn der Rechteinhaber nicht explizit ein Verfallsdatum eingebaut hat? Die einzige Möglichkeit, dieses Wissen zu erhalten, ist es, die Dateien in regelmäßigen Abständen in die aktuellen physikalischen und logischen Formate zu konvertieren. Genau das wird aber mit Kopierschutzmechanismen unterbunden. @1 fliess mit:In Bezug auf die natürliche Umwelt hat unsere Generation schmerzhaft die Bedeutung der <@1 fliess kursiv>Nachhaltigkeit<@$p> gelernt. Parallel dazu stellte sich <\n>eine technologische Innovation in der Papierproduktion am Ende des 19.<\!q>Jahrhunderts als Zeitbombe für die gedruckten Wissensbestände heraus. Selbst mit den größten Anstrengungen wird es nur möglich sein, einen Bruchteil der Bücher, Noten, Karten, Handschriften eines ganzen Jahrhunderts vor dem Säurefraß zu retten. Hätte unsere Wissenskultur etwas aus diesen Erfahrungen gelernt, würde sie heute nicht sehenden Auges in die nächste Katastrophe rennen. Statt der Kontrolltechnologien, die auf einen kurzfristigen Investitionschutz von Partikularinteressen ausgelegt sind, würde eine Wissenspolitik im Interesse der Allgemeinheit, das auch kommende Generationen einschließt, die Verwendung von offenen Technologien vorschreiben. Nur so ist eine Nachhaltigkeit zu gewährleisten, die die Lebensdauer einer Startup-Firma überschreitet. Eine andere Art von Technologie, eine andere Art von lizenzvertraglicher Regelung und – wenn man so will – eine andere Art von Selbstregulierung, die in der digitalen Wissensordnung eine wesentlich nachhaltigere intellektuelle und kulturelle Informationslandschaft schaffen hilft, wird im zweiten Teil des Bandes besprochen. <\c>@0  Über 50%:Teil 2@0  Über 100%:Die Wissens-Allmende@1 fliess mit:@1 fliess ohne:Nachdem wir uns im ersten Teil mit der Geschichte, Struktur, der Denkweise und den aktuellen Trends des geistigen Eigentums beschäftigt <\n>haben, wird es im zweiten Teil um die Alternative zum Privatbesitz an geistigen Gütern gehen.@1 fliess mit:@2  ZÜ 1:Geschichte@1 fliess mit:@2  ZÜ 2:»Wissenskommunismus« der Wissenschaften@1 fliess ohne:Seit den Athenern gehört es zur Universität, dass das von ihr erzeugte und durch sie weitergegebene Wissen, anders als das in geschlossenen und gar geheimen Forschungsstellen der Staaten oder der Industrien üblich ist, ohne den Schutz von Patenten und Copyright zirkulieren können muss. Die im Hochmittelalter entstandenen europäischen Universitäten bildeten einen Medienverbund aus Verarbeitung des gesprochenen Wortes zu handschriftlichen Büchern, Speicherung in Bibliotheken und Übertragung von Texten in einem eigenen Universitätspostsystem. In der frühen Neuzeit übernahmen dank Gutenbergs Erfindung Verlage die Produktion von Büchern, entstehende Territorial- und später Nationalstaaten beanspruchten das Postmonopol. Die Informationsverarbeitung, so Friedrich Kittler, wurde einer Hardware übertragen, die in den geschlossenen Kreisen der militärischen Nachrichtentechniken entstand. Die Computersoftware dagegen sei eine Schöpfung der Universität gewesen. Die universale Turing-Maschine stammte als Konzept und als Software aus einer akademischen Dissertation: »Ganz entsprechend stammt die noch immer herrschende von-Neumann-Architektur von einem, der es vom Göttinger mathematischen Privatdozenten schließlich zum Chefberater des Pentagon brachte. Auf diesem Weg zur Macht hat das Wissen, das in Computer und ihre Algorithmen versenkt ist, einmal mehr jene Schließung erfahren, die einst bei der Übernahme der Universitäten durch die Territorialstaaten drohte.«<@3 hoch fliess>1<@$p>@1 fliess mit:Solcher realen Vereinnahmungen zum Trotz entwirft die Gelehrtenrepublik des 19. Jahrhunderts eine akademische Wissenschaftsverfassung, die auf der Freiheit von Lehre und Forschung beruht. Konstitutiv für diese klassische Wissensordnung humboldtscher Prägung und fortgeschrieben in der Forschungsgemeinschaft des letzten Jahrhunderts durch Autoren wie Weber, Popper, Merton, Spinner usw. sind vier große Abkopplungen:@1 fliess ohne:<f"BulletsNstuff">p<f$>  <\i>Die Trennung von Erkenntnis und Eigentum: Forschungsergebnisse müssen veröffentlicht werden, um sie in einem <f"FFScala-Italic">Peer Review<f$>-Prozess überprüfen, replizieren, kritisieren und fortschreiben zu können. Das ist es, was Robert Merton mit dem »Wissenskommunismus« der Wissenschaften meinte,<@3 hoch fliess>2 <@$p><f"BulletsNstuff">p<f$>  <\i>die Trennung von Ideen und Interessen,<f"BulletsNstuff">p<f$>  <\i>die Trennung von Theorie und Praxis,<f"BulletsNstuff">p<f$>  <\i>die Trennung von Wissenschaft und Staat: Lehre und Forschung folgen keinen externen Anweisungen. Das heißt nicht, dass sie nicht öffentlich finanziert werden dürften, ganz im Gegenteil. Tatsächlich wurde die Grundlagenforschung für die neue Ordnung digitaler Medien, also der Computer und Datennetze, mit öffentlichen Mitteln betrieben (<@6 Caps>Spinner<@$p>, 1994, S. 15 f). @1 fliess mit:@1 fliess ohne:Der für die freie Software wesentliche Punkt ist die »Abkopplung der <t-2>Ideenwirtschaft von der normalen Güterwirtschaft« (ebd<f"FFScala-Caps">.<f$>, S.<\!q>91).<t$> Mit seiner Veröffentlichung wird das Wissen zum Gemeingut der Forschungsgemeinschaft. Es kann von Kollegen frei nachvollzogen, überprüft und weiterentwickelt werden und in der Lehre frei der Reproduktion der Wissensträger in der nächsten Generation dienen. Durch diese fruchtbaren Bedingungen im »Sondermilieu« der Wissenschaften können die parallelen, kollektiven Bemühungen Ergebnisse hervorbringen, die kein Einzelner und kein einzelnes Team produzieren könnten. Die einzelne Wissenschaftlerin erhält im Wissenskommunismus als Anerkennung für die von ihr erarbeiteten Erkenntnisse keine Geldzahlungen – um von dieser Notwendigkeit freigestellt zu sein, alimentiert sie der Staat –, sondern ein symbolisches Entgelt in Form von fachlicher Reputation, wie sie sich z. B. an der Zahl der Einträge im<f"FFScala-Italic"> <f"ZapfDingbats">’<f$> <f"FFScala-Italic">Citation Index<f$> ablesen lässt. Statt eines Monopolverwertungsrechts, wie es das Patentsystem für Erfindungen von industriellem Wert gewährt, steht hier das Recht auf Namensnennung im Vordergrund.@1 fliess mit:Die Wissensordnung dieses Sondermilieus strahlt über ihren eigentlichen Geltungsbereich hinaus auf seine Umwelt in der modernen, demokratischen Gesellschaft aus, mit der zusammen sie entstanden ist: @1 fliesskursiv Zitat:»Der Wissenstransfer in das gesellschaftliche Umfeld konnte unter güns<\h>tigen Bedingungen (Rechtsstaat, Demokratie, liberale Öffentlichkeit) wesentliche Bestandteile dieser Wissensordnung in die ›Wissensverfassung‹ der Gesellschaft einfließen lassen. Die freie wissenschaftliche Forschung, Lehre, Veröffentlichung findet so ihre Ergänzung in der ›Freien Meinung‹ des Bürgers<\!q><+f"Univers-Condensed">3<$f$> und verwandter Wissensfreiheiten, wie in unserem Grundgesetz verankert. So spannt sich der Bogen der ordnungspolitischen Leitvorstellungen, mit Abstrichen auch der positiven Regulierungen und praktischen Realisierungen, vom Wissenskommunismus der Forschungsgemeinschaft bis zur informationellen Grundversorgung in der Informationsgesellschaft und dem geforderten weltweiten freien Informationsfluss ...«<\!q><+f"Univers-Condensed">4<$f$>@1 fliess mit:@2  ZÜ 2:Internet@1 fliess mit:@1 fliess ohne:Das Internet ist mediengeschichtlich eine Anomalie. Übliche Modelle der Medien- wie der Technikgenese allgemein laufen vom Labor über die Entwicklung hin zur Anwendungsreife bis zur gesellschaftlichen Implementierung entweder als staatliche Militär- oder Verwaltungskommunikation, als wirtschaftliches Kontroll- und Steuerungsinstrument oder als Massenprodukt der Individualkommunikation oder Massenmedien. Anders hingegen im Falle von akademischen Datennetzen. Hier gab es in den ersten Jahren keine Trennung zwischen Erfindern, Entwicklern und Anwendern. @1 fliess mit:Die Informatik hat im Netz nicht nur ihren Forschungsgegenstand, sondern zugleich ihr Kommunikations- und Publikationsmedium. Es ist gleichzeitig Infrastruktur und Entwicklungsumgebung, die von innen heraus ausgebaut wird. Innovationen werden von den Entwickler-Anwendern in der Betaversion, d.h. ohne Garantie und auf eigene Gefahr, in die Runde geworfen, von den Kollegen getestet und weiterentwickelt. Darüber hinaus stellt sie den anderen, zunehmend computerisierten Wissenschaften die gleiche Infrastruktur zur Verfügung. Der Zugang zu Rechenressourcen, der Austausch innerhalb einer weltweiten Community von Fachkollegen, das zur Diskussion Stellen von <f"FFScala-Italic">Pre-Prints<f$>, die Veröffentlichung von Konferenzreferaten und Datenbanken im Internet – all dies gehört seit den 80er-Jahren zu den täglichen Praktiken in der Physik und Astronomie, der Informatik selbst und zunehmend auch in den <t-2>»weicheren« Wissenschaften. Schließlich ist das Weiterreichen der Grund<\h><\h>w<t$>erkzeuge an die Studierenden Teil der wissenschaftlichen Lehre. Da das Netz, anders als die meisten Laborgeräte, keinen eng definierten Anwendungsbereich hat, sondern eben Medium ist, kommen hier auch studentische private und Freizeitkulturen auf – eine brisante Mischung aus Hightech und Hobbyismus, <f"FFScala-Italic">Science<f$> und <f"FFScala-Italic">Science Fiction<f$>, Hackern und Hippies.Die Geschichte des Internet lässt sich grob in drei Phasen einteilen: in der Frühphase ab Mitte der 60er-Jahre werden die Grundlagen gelegt, die Technologie demonstriert und zur Anwendungsfähigkeit entwickelt. Zeitgleich mit dem Wechsel von der militärischen zur akademischen Forschungsförderung Ende der 70er begann das Wachstum und die internationale Ausbreitung des Internet. In dieser Zeit gedieh das, was gemeinhin mit der »wilden Phase« des ursprünglichen Internet assoziiert wird: eine Tauschökonomie für Software und Information, eine graswurzel-basierte Selbstorganisation, emergierende Communities und der Hacker-Geist, der jede Schließung, jede Beschränkung des Zugangs und des freien Informationsflusses zu umgehen weiß. 1990 wurde das  <f"ZapfDingbats">’<\!q><$f$>ARPA<\h>NET abgeschaltet, es begann die kommerzielle Phase des Internet. @2  ZÜ 3:Frühphase@1 fliess ohne:In den späten 50er-Jahren leitete J.C.R. Licklider eine Forschungsgruppe beim US-Rüstungslieferanten Bolt, Beranek and Newman (BBN), die auf einer PDP-1, einem Großrechner der Firma Digital Equipment Corporation (DEC), eines der ersten <f"FFScala-Italic">Time-Sharing<f$>-Systeme bauten. Computerhersteller und die meisten Vertreter des Informatik-Establishments waren der Ansicht, dass <f"FFScala-Italic">Time-Sharing<f$> eine ineffiziente Verwendung von Computerressourcen darstelle und nicht weiter verfolgt werden solle. Lickliders Argument war umgekehrt, dass Rechner für eine Echtzeit-Interaktion – für »kooperatives Denken mit einem Menschen« – zu schnell und zu kostspielig seien, weshalb sie ihre Zeit zwischen vielen Nutzern aufteilen müssten. Licklider war auch der Architekt des MAC-Projektes (<f"FFScala-Italic">Multiple-Access Computer<f$> oder <f"FFScala-Italic">Machine-Aided Cognition<f$> oder <f"FFScala-Italic">Man And Computer<f$>) am Massachusetts Institute of Technology (MIT). 1962 wechselte er von BBN zur <f"FFScala-Italic">Advanced Research Projects Agency <f$>(ARPA) des US-Verteidigungsministeriums, wo er Leiter des <f"FFScala-Italic">Command and Control <\h>Re<\h><\h><\h>search<f$> wurde, das er sogleich in <f"FFScala-Italic">Information Processing Techniques Office <f$>(IPTO) umbenannte.<+f"Univers-Condensed">5<$f$>@1 fliess mit:Seine Erfahrungen mit <f"FFScala-Italic">Time-Sharing<f$>-Systemen erlaubten es ihm, eine Neudefinition vom Computer als Rechenmaschine zum Computer als Kommunikationsgerät vorzunehmen. Als Leiter des ARPA-Forschungsbereiches war er nun in die Lage versetzt, diesen Paradigmenwechsel in der Netzplanung zur Wirkung zu bringen:@1 fliesskursiv Zitat:»Das ARPA-Leitmotiv ist, dass die Möglichkeiten, die der Computer als Kommunikationsmedium zwischen Menschen bietet, die historischen Anfänge des Computers als einer Rechenmaschine in den Schatten stellen. [...] Lick war einer der ersten, die den Gemeinschaftsgeist wahrnahmen, der unter den Nutzern des ersten Time-Sharing-Systems entstand. Indem er auf das Gemeinschaftsphänomen hinwies, das zum Teil durch den gemeinsamen Zugriff auf Ressourcen in einem Time-Sharing-System aufkam, machte Lick es leicht, sich eine Verbindung zwischen den Gemeinschaften vorzustellen, die Verknüpfung von interaktiven Online-Gemeinschaften von Menschen ...«<\!q><+f"Univers-Condensed">6<$f$>@1 fliess mit:@1 fliess ohne:Zeitgleich findet ein telekommunikationstechnischer Paradigmenwechsel von leitungsorientierten zu paketvermittelten Konzepten statt. Er geht auf parallele Arbeiten von Paul Baran an der RAND Corporation<+f"Univers-Condensed">7<$f$> (der erste <f"FFScala-Italic">Think Tank<f$>, 1946 von der U.S. Air Force gegründet) und von Donald Watts Davies am <f"FFScala-Italic">National Physical Laboratory<f$> in Middlesex, England, zurück. Die Zerlegung von Kommunikationen in kleine Datenpakete, die, mit Ziel- und Absenderadresse versehen, »autonom« ihren Weg durch das Netzwerk finden, war Voraussetzung für die verteilte, dezentrale Architektur des Internet. Sie war auch der Punkt, an dem die Geister der Computer- und der Telekommunikationswelt sich schieden. @1 fliess mit:<*h"mehr">Die Telefonbetreiber der Zeit waren durchaus an Datenkommunikation sowie an der Paketvermittlung interessiert, nachdem nachgewiesen worden war, dass diese Technik nicht nur überhaupt machbar war, sondern dass sie die vorhandene Bandbreite viel wirtschaftlicher nutzte als die Leitungsvermittlung, doch die vorrangigen Designkriterien der nationalen Monopole waren flächendeckende Netzsicherheit, Dienstequalität und Abrechenbarkeit. Diese sahen sie nur durch ein zentral gesteuertes Netz mit dedizierter Leitungsnutzung für jede einzelne Kommunikation gewährleistet. Die Telekommunikationsunternehmen vor allem in England, Italien, Deutschland und Japan unterlegten daher den unberechenbaren Paketflüssen eine »virtuelle Kanalstruktur«. Auch in diesem Sys<\h>tem werden Pakete verschiedener Verbindungen auf derselben physikalischen Leitung ineinandergefädelt, aber nur bis zu einer Obergrenze, bis zu der die Kapazität für jede einzelne Verbindung gewährleistet werden kann. Außerdem ist dieses Netz nicht verteilt, sondern über zentrale Vermittlungsstellen geschaltet. Die Spezifikationen dieses Dienstes wurden im Rahmen der Internationalen Telekommunikations-Union (ITU) verhandelt und 1976 unter der Bezeichnung »X.25« standardisiert. Die Bundespost bot ihn unter dem Namen »Datex-P« an. Damit ist der Gegensatz aufgespannt zwischen einem rhizomatischen Netz, das aus einem militärischen Kalkül heraus von einzelnen Knoten dezentral wuchert, und einer hierarchischen, baumförmigen Struktur, die zentral geplant und verwaltet wird.<*h"Standard">Doch zurück zum Internet. Die ARPA-Forschungsabteilung unter Licklider schrieb die verschiedenen Bestandteile des neuen Netzes aus. Das <f"FFScala-Italic">Stanford Research Institute<f$> (<f"ZapfDingbats">’<f$> SRI) erhielt den Auftrag, die Spezifikationen für das neue Netz zu schreiben. Im Dezember 1968 legte das SRI den Bericht »A Study of Computer Network Design Parameters« vor. Zur selben Zeit arbeitete Doug Engelbart und seine Gruppe am SRI bereits an computergestützten Techniken zur Förderung von menschlicher Interaktion. Daher wurde entschieden, dass das SRI der geeignete Ort sei, ein <f"FFScala-Italic">Network Information Center<f$> <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’ <@$p>NIC) für das ARPAnet einzurichten. Die <f"ZapfDingbats">’<f$><\!q>DARPA-Ausschreibung für ein <f"FFScala-Italic">Network Measurement Center<f$> ging an die University of California in Los Angeles <x@1 fliess normal><f"ScalaSans">(<@$p><f"ZapfDingbats">’<f$>UCLA), wo Leonard Kleinrock arbeitete, der seine Doktorarbeit über Warteschlangentheorie geschrieben hatte. Ebenfalls im UCLA-Team arbeiteten damals Vinton G. Cerf, Jon Postel und Steve Crocker.<t-1>Den Zuschlag für die Entwicklung der Paketvermittlungstechnologien, genauer eines <f"FFScala-Italic">Interface Message Processors <f$>(<f"ZapfDingbats">’<f$>IMP), erhielt <x@1 fliess normal><t-1>B<@$p><t-1>BN. Dort arbeitete u.a. Robert Kahn, der vom <f"ZapfDingbats">’<f$> MIT gekommen war und auf den ein Großteil der Architektur des Internet zurückgeht. Die IMPs – Vorläufer der heutigen <f"ZapfDingbats">’<f$> <f"FFScala-Italic">Router <f$>– hatten die Aufgabe, die niedrigste Verbindungsschicht zwischen den über Telefonleitungen vernetzten Rechnern (<@4 Pfeil (Umschalt/Alt #)>’<@$p><t-1f"ZapfDingbats"><\!q><$f"FFScala-Italic">Hosts<f$>) herzustellen. Die ersten IMPs wurden im Mai 1969 ausgeliefert. <t$>Der Startschuss zum Internet fiel im Herbst 1969, als die ersten vier Großrechner in der UCLA, im SRI, der University of California in Santa Barbara (UCSB) und der University of Utah miteinader verbunden wurden.<+f"Univers-Condensed">8<$f$> Bereits ein halbes Jahr vorher war das erste von Tausenden von <f"FFScala-Italic">Request for Comments<f$>-Dokumenten (<f"ZapfDingbats">’<f$> RFCs)<+f"Univers-Condensed">9<$f$> erschienen, die die technischen Standards des Internet spezifizieren. Diese Standards werden nicht im Duktus eines Gesetzes erlassen, sondern als freundliche Bitte um Kommentierung. Steve Crocker, Autor des ersten RFC, begründete diese Form damit, dass die Beteiligten nur Doktoranden ohne jede Autorität waren. Sie mussten daher einen Weg finden, ihre Arbeit zu dokumentieren, ohne dass es schien, als wollten sie irgendjemandem etwas aufoktroyieren, in einer Form, die offen für Kommentare war. RFCs können von jedem erstellt werden. Sie sind als Diskussionspapiere gedacht, mit dem erklärten Ziel, die Autorität des Geschriebenen zu brechen.<+f"Univers-Condensed">10<$f$> Neben den meist technischen Texten werden auch die Philosophie (z. B. RFC 1718), die Geschichte (RFC 2235) und die Kultur des Netzes aufgezeichnet und zuweilen sogar gedichtet (RFC 1121). Die freie Verfügbarkeit der Spezifikationen und der dazugehörigen Referenzimplementationen waren ein Schlüsselfaktor bei der Entwicklung des Internet. Aus dem ersten RFC ging ein Jahr später das <f"FFScala-Italic">Network Control Protocol <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’<@$p><f"FFScala-Italic"> <f$>NCP) hervor, ein Satz von Programmen für die Host-Host-Verbindung, das erste ARPANET-Protokoll. 1971 bestand das Netz aus 14 Knoten und wuchs um einen pro Monat.<+f"Univers-Condensed">11<$f$> Nach Fertigstellung des NCP und Implementierung für die verschiedenen Architekturen entstanden jetzt die höheren Dienste Telnet (RFC 318) und <f"ZapfDingbats">’<f$> FTP (File Transfer Protocol, RFC 454). Ray Tomlinson von BBN modifizierte ein E-Mail-Server-Programm für das ARPANET und erfand die »user<\@>host«-Konvention. Larry Roberts schrieb hierfür einen <f"ZapfDingbats">’<f$><\!q><$f"FFScala-Italic">Mail-Client<f$>. Das Netzwerk konnte sich sehen lassen. Es war Zeit für eine erste öffentliche Demonstration, die 1972 auf der <f"FFScala-Italic">International Conference on Computer Communications<f$> in Washington stattfand. Im Keller des Konferenzhotels wurde ein Paketvermittlungsrechner und ein <f"FFScala-Italic">Terminal Inter<\h>face Processor<f$> (TIP) installiert, der anders als ein IMP den Input von mehreren Hosts oder Terminals verarbeiten konnte. Angeschlossen waren 40 Maschinen in den ganzen USA. Zu den Demonstrationen gehörten interaktive Schachspiele und die Simulation eines Lufverkehrskontrollsy<\h>s<\h>tems. Berühmt wurde die Unterhaltung zwischen ELIZA, Joseph Weizenbaums künstlich-intelligentem Psychiater am MIT, und PARRY, einem paranoiden Programm von Kenneth Colby an der Stanford Universität. Teilnehmer aus England, Frankreich, Italien und Schweden waren dabei. Vertreter von AT&T besuchten die Konferenz, verließen sie jedoch in tiefer Verwirrung. Im selben Jahr starteten Projekte für radio- und satellitengestützte Paketvernetzung, Letztere mit Instituten in Norwegen und England. Bob Metcalfe umriss in seiner Doktorarbeit an der Harvard Universität das Konzept für ein <f"FFScala-Italic">Local Area Network <f$>(<f"ZapfDingbats">’<f"FFScala-Italic"> <f$>LAN) mit multiplen Zugangskanälen, das er »Ethernet« nannte. Am Xerox PARC entwickelte er das Konzept weiter, bevor er später 3COM gründete. ARPANET, SATNET und das Radionetz hatten verschiedene Schnittstellen, Paketgrößen, Kennzeichnungen und Übertragungsraten, was es schwierig machte, sie untereinander zu verbinden. Bob Kahn, der von BBN an die DARPA ging, und Vint Cerf, der jetzt an der Stanford Universität unterrichtete, begannen, ein Protokoll zu entwickeln, um verschiedene Netze miteinander zu verbinden. Im Herbst 1973 stellten sie auf einem Treffen der <f"FFScala-Italic">International Network Working Group<f$> in England den ersten Entwurf zum <f"FFScala-Italic">Transmission Control Protocol <@1 fliess normal>(<@$p><f"ZapfDingbats">’ <f$>TCP) vor. Im Jahr darauf wurde TCP zeitgleich an der Stanford Uni, bei BBN und dem University College London (Peter Kirstein) implementiert. »Somit waren die Bemühungen, die Internet-Protokolle zu entwickeln, von Anfang an  international« (Cerf, 1993). Es folgten vier Iterationen des TCP-Protokoll<\h>satzes. Die letzte erschien 1978.1974 startete BBN »Telenet«, den ersten öffentlichen paketvermittelten Datenkommunikationsdienst, eine kommerzielle Version des ARPANET. Aufgrund der DARPA-Förderung besaß BBN kein exklusives Recht am Quellcode für die IMPs und <f"ZapfDingbats">’<f$> TIPs. Andere neue Netzwerkunternehmen forderten BBN auf, ihn freizugeben. BBN sträubte sich zunächst, da der Code ständig verändert würde, gab ihn jedoch 1975 frei.@2  ZÜ 3:Wachstum@1 fliess ohne:Mit der Forschungsförderung für die Implementierung von TCP hatte die DARPA ihre initiale Mission erfüllt. 1975 wurde die Verantwortung für das ARPANET an die <f"FFScala-Italic">Defense Communications Agency<f$> (später umbenannt in <f"FFScala-Italic">Defense Information Systems Agency<f$>) übertragen. BBN blieb der Auftragnehmer für den Betrieb des Netzes, doch militärische Sicherheitsinteressen wurden jetzt wichtiger. Zusätzlich zur DARPA förderte auch die <f"FFScala-Italic">National Science Foundation<f$> <@1 fliess normal>(<@$p><f"ZapfDingbats">’ <f$>NSF) die Forschung in Informatik und Netzwerken an rund 120 US-amerikanischen Universitäten. Weitere Einrichtungen, wie das Energieministerium und die NASA starteten eigene Netzwerke. Anfang 1975 verfügte das ARPANET über 61 Knoten<k-10>.<+k$f"Univers-Condensed">12 <$f$>Die erste <f"FFScala-Italic">Mailinglist<f$> wurde eingerichtet. Zusammen mit den RFCs werden Mailinglisten zum wichtigsten Mittel der offenen Kooperation der technischen Community. In der beliebtesten Liste dieser Zeit diskutierte man jedoch über Sciencefiction. Der <f"FFScala-Italic">Jargon File<f$>, ein Wörterbuch der Hacker-Kultur, zusammengestellt von Raphael Finkel, wurde zum ersten Mal publiziert, natürlich im Netz. @1 fliess mit:<*h"mehr">UUCP (<f"FFScala-Italic">Unix to Unix Copy<f$>) wurde 1976 an den AT&T Bell Labs entwickelt und als Teil der Unix Version 7 verbreitet. Einrichtungen, die sich keine Standleitung leisten konnten, ermöglichte es UUCP, über Wählerverbindungen auf normalen Telefonleitungen Daten mit Rechnern am ARPANET auszutauschen. Das neue netzwerkverbindende TCP wurde im Juli 1977 erstmals in einem aufwändigen Versuchsaufbau demonstriert. Die Übertragungsstrecke begann mit einem mobilen Paketsender in einem fahrenden Auto auf dem San Francisco <f"FFScala-Italic">Bayshore Freeway<f$>, lief zu einem Gateway bei BBN, über das ARPANET, über eine Punkt-zu-Punkt-Satellitenverbindung nach Norwegen, von dort via Landleitung nach London, zurück über das <f"FFScala-Italic">Atlantic Packet Satellite Network<f$> (SATNET) ins AR<\h>PANET und schließlich zum Informatikinstitut der University of Southern California: »Was wir also simulierten, war jemand auf einem mobilen Kriegsschauplatz, der sich über ein kontinentales Netz bewegt, dann über ein interkontinentales Satellitennetz und dann zurück in ein Leitungsnetz und zum Haupt<\h>rechenzentrum des nationalen Hauptquartiers geht. Da das Verteidigungsministerium dafür bezahlte, haben wir nach Beispielen gesucht, die für ein militärisches Szenario interessant sein könnten« (<f"FFScala-Caps">Cerf<f$>, 1993).<*h"Standard">Seit Mitte der 70er-Jahre wurden Experimente zur paketvermittelten Sprachübertragung durchgeführt. TCP ist auf zuverlässige Übertragung ausgelegt. Pakete, die verloren gehen, werden erneut geschickt. Im Falle von Sprachübertragung ist jedoch der Verlust einiger Pakete weniger nachteilig als eine Verzögerung. Aus diesen Überlegungen heraus wurden 1978 TCP und IP getrennt. IP spezifiziert das <f"FFScala-Italic">User Datagram Protocol <@1 fliess normal>(<@$p><f"ZapfDingbats">’ <f$>UDP), das noch heute zur Sprachübertragung verwendet wird (vgl. ebd.). Damit wird 1978 das ARPANET-Experiment offiziell beendet. Im Abschlussbericht heißt es: »Dieses ARPA-Programm hat nichts weniger als eine Revolution in der Computertechnologie hervorgebracht und war eines der erfolgreichsten Projekte, das die ARPA je durchgeführt hat. Das volle Ausmaß des technischen Wandels, der von diesem Projekt ausgeht, wird sich vielleicht erst in vielen Jahren ermessen lassen.<t-1>«<+f"Univers-Condensed">13<$f$> Einer der Pioniere erinnert sich an die entscheidenden Faktoren:<t$>@1 fliesskursiv Zitat:»Für mich war die Teilnahme an der Entwicklung des ARPANET und der Internetprotokolle sehr aufregend. Ein entscheidender Grund dafür, dass es funktionierte, ist meiner Meinung nach, dass es viele sehr kluge Menschen gab, die alle mehr oder weniger in dieselbe Richtung arbeiteten, angeführt von einigen sehr weisen Menschen in der Förderungsbehörde. Das Ergebnis war, dass eine Gemeinschaft von Netzwerkforschern entstand, die fest daran glaubte, dass unter Forschern Zusammenarbeit mächtiger ist als Konkurrenz. Ich glaube nicht, dass ein anderes Modell uns dahin gebracht hätte, wo wir heute stehen.«<+f"Univers-Condensed">14<$f$>@1 fliess mit:@2  ZÜ 3:Institutionalisierung@1 fliess ohne:Um die Vision eines freien und offenen Netzes fortzuführen, richtete Vint Cerf 1978 noch vom DARPA aus das <f"FFScala-Italic">Internet Configuration Control Board<f$> (ICCB) unter Vorsitz von Dave Clark am MIT ein. 1983 trat das <f"FFScala-Italic">Internet Activities Board<f$> <@1 fliess normal>(<@$p><f"ZapfDingbats">’ <f$>IAB) (nach der Gründung der Internet Society umbenannt in <f"FFScala-Italic">Internet Architecture Board<f$>) an die Stelle des ICCB. @1 fliess mit:Für die eigentliche Entwicklungsarbeit bildeten sich 1986 unter dem IAB die <f"FFScala-Italic">Internet Engineering Task Force <@1 fliess normal>(<@$p><f"ZapfDingbats">’ <f$>IETF)<+f"Univers-Condensed">15<$f$> und die <f"FFScala-Italic">Internet Re<\h>search Task Force<f$> (IRTF). Anders als staatliche Standardisierungsgremien oder Industriekonsortien ist die IETF – »nach Gesetz und Gewohnheitsrecht« – ein offenes Forum. Mitglied kann jeder werden, indem er eine der etwa 100 aufgabenorientierten Mailinglisten subskribiert und sich an den Diskussionen beteiligt. »Theoretisch erhält ein Student, der ein technisch fundiertes Anliegen in Bezug auf ein Protokoll anspricht, dieselbe sorgfältige Beachtung, oder mehr, als jemand von einem Multi-Milliarden-Dollar-Unternehmen, der sich Sorgen über die Auswirkungen auf seine ausgelieferten Systeme macht« (Alvestrand, 1996, S. 61). <t-1>Alle Arbeit – mit Ausnahme des Sekretariats – ist unbezahlt und freiwillig.<t$>Die Entwicklungsarbeit innerhalb der IETF gehorcht einem begrenzten Anspruch. Die Ergebnisse müssen ein anstehendes Problem möglichst direkt und, gemäß einer Hacker-Ästhetik von Eleganz, möglichst einfach und kompakt lösen. Sie müssen mit den bestehenden Strukturen zusammenarbeiten und Anschlüsse für mögliche Erweiterungen vorsehen. Da es keine scharf umrissene Mitgliedschaft gibt, werden Entscheidungen nicht durch Abstimmungen getroffen. Das Credo der IETF lautet: »Wir wollen keine Könige, Präsidenten und Wahlen. Wir glauben an einen groben Konsens und  an ablauffähigen Code.«<+f"Univers-Condensed">16 <$f$>Wenn sich ein interessantes Problem und genügend Freiwillige finden, wird diskutiert, ein ablauffähiger Code auch für alternative Lösungsansätze geschrieben und solange getestet, bis sich ein Konsens herausbildet. Wenn dies nicht geschieht, das Verfahren auf unlösbare Probleme stößt oder die Beteiligten das Interesse verlieren, kann ein Standard auch vor seiner Verabschiedung stecken bleiben. Standards oder Code werden in jeder Phase der Entwicklung im bewährten RFC-Format für jeden Interessierten zugänglich veröffentlicht. Dies führt dazu, dass sie frühzeitig von einer Vielzahl von Anwendern unter den unterschiedlichsten Bedingungen getes<\h>tet werden und diese breiten Erfahrungen in den Entwicklungsprozess eingehen, bevor ein Standard offiziell freigegeben wird. Die Standards sind offen und frei verfügbar. Anders als im ISO-Prozess können von den an der Standardisierung Beteiligten keine Patente erworben werden, und anders als die ISO finanziert sich die IETF nicht aus dem Verkauf der Dokumentation von Standards. Der kontinuierlichen Weiterentwicklung dieses Wissens steht somit nichts im Wege. Eine weitere Internet-Institution ist das CERT. 1988 legte der außer Kontrolle geratene »Morris-Wurm«, ein ausgerechnet vom Sohn eines führenden Kryptografieexperten losgelassenes virusartiges Programm, 6<\!q>000 der inzwischen 60<\!q>000 Hosts am Internet lahm.<+f"Univers-Condensed">17<$f$> Daraufhin bildet die DARPA das <f"FFScala-Italic">Computer Emergency Response Team<f$> (CERT), um auf zukünftige Zwischenfällen dieser Art reagieren zu können. Die 1990 von Mitch Kapor gegründete <f"FFScala-Italic">Electronic Frontier Foundation <@1 fliess normal>(<@$p><f"ZapfDingbats">’ <f$>EFF) ist keine Internet-Institution im engeren Sinne, doch als Öffentlichkeits- und Lobbyingvereinigung zur Wahrung der Bürgerrechte im Netz hat sie in den USA eine bedeutende Rolle gespielt.Als Dachorganisation für alle Internetinteressierten und für die bestehenden Gremien wie IAB und IETF gründeten u.a. Vint Cerf und Bob Kahn 1992 die <f"FFScala-Italic">Internet Society<f$> (ISOC).<+f"Univers-Condensed">18 <$f$>Im Jahr darauf etablierte die NSF das InterNIC (<f"FFScala-Italic">Network Information Center<f$>), das bestimmte Dienste in seinem Aufgabenbereich an Dritte ausschrieb, nämlich Directory- und Datenbankdienste an AT&T, Registrierungsdienste an Network Solutions Inc. und Informationsdienste an General Atomics/CERFnet.@2  ZÜ 3:Netzwerkforschung<t0f"FFScala">@1 fliess ohne:Auf Initiative von Larry Landweber erarbeiteten Vertreter verschiedener Universitäten (darunter Peter Denning und Dave Farber) die Idee eines Informatik-Forschungsnetzes (CSNET). Ein Förderungsantrag an die NSF wurde zunächst als zu kostspielig abgelehnt. Auf einen überarbeiteten Antrag hin bewilligte die NSF 1980 dann fünf Millionen Dollar über einen Zeitraum von fünf Jahren. Das Protokoll, das die verschiedenen Subnetze des CSNET verbindet, ist TCP/IP. 1982 wurde beschlossen, dass alle Systeme auf dem ARPANET von NCP auf TCP/IP übergehen sollen – obgleich davon nur einige hundert Computer und ein Dutzend Netze betroffen waren, keine einfache Operation (vgl. RFC 801). @1 fliess mit:CSNET und ARPANET wurden 1983 verbunden, doch US-amerikanische Wissenschaftler klagten, dass die Supercomputer des Landes nicht zugänglich seien. Astrophysiker mussten nach Deutschland reisen, um einen in den USA hergestellten Supercomputer verwenden zu können. Im Juli 1983 gab daraufhin eine NSF-Arbeitsgruppe einen Plan für ein <f"FFScala-Italic">National Computing Environment for Academic Research<f$> heraus. Die Supercomputer-Krise führte zur Verabschiedung eines Etats von 200 Millionen Dollar für die Einrichtung von Supercomputer-Zentren an verschiedene Universitäten und des NSFnet. Das NSFnet startete 1986 mit einem landesweiten 56 Kbps-Backbone, der bald auf 1,5 Mbps- und 1989 auf 44,7 Mbps-Leitungen ausgebaut wurde. Zu der Zeit planten Bob Kahn und Vint Cerf bereits ein Versuchsnetz mit 6 Gigabit pro Sekunde. Um den NSFnet-Backbone herum entwickelte sich eine ganze Reihe NSF-geförderter regionaler Netzwerke. Von Anfang 1986 bis Ende 1987 stieg die Gesamtzahl der Netzwerke am Internet von 2<\!q>000 auf beinah 30<\!q>000.@2  ZÜ 3:Neue Architekturen, Protokolle und Dienste@1 fliess ohne:Neben TCP/IP wurden weiterhin proprietäre Protokolle eingesetzt, aber es entstanden auch neue offene Protokolle. Das wichtigste darunter ist das <f"ZapfDingbats">’<f$> BITNET (<f"FFScala-Italic">Because It’s Time NETwork<f$>), das 1981 als ein kooperatives Netzwerk an der City University of New York startete und die erste Verbindung an die Yale Universität legte. Zu den Eigentümlichkeiten von BITNET gehört z. B., dass es die Dateiübertragung per E-Mail realisiert. 1987 überschritt die weltweite Zahl der BITNET-Hosts 1<\!q>000.@1 fliess mit:TCP/IP wurde zum <f"FFScala-Italic">de facto<f$> Standard, doch die Anerkennung als offizieller Standard blieb ihm verwehrt. Ein Irrweg in der Netzwerkentwicklung begann, als die <f"FFScala-Italic">International Standards Organization<f$> (ISO) ab 1982 ein Referenzmodell für einen eigenen verbindungsorientierten Internetzwerk-Standard namens <f"FFScala-Italic">Open Systems Interconnection<f$> <@1 fliess normal>(<@$p><f"ZapfDingbats">’ <f$>OSI) entwickelte. Im Gegensatz zum horizontalen Prozess der Internetcommunity beruht das Standardisierungsverfahren der ISO auf einem vertikalen, mehrschichtigen Prozess aus Vorschlägen, Ausarbeitungen und Abstimmungen, der zwischen den nationalen Standardisierungsorganisationen, den Arbeitsgruppen und schließlich dem Plenum der ISO hin- und hergeht. Dabei sollen alle Interessen berücksichtigt werden. Der Standard soll in einem theoretischen Sinne vollständig sein. Er soll zugleich rückwärts kompatibel und abstrakt genug sein, um zukünftige Entwicklungen nicht zu verbauen. Durch die begrenzte Zirkulation in den am Verfahren beteiligten Institutionen werden Standards auch nur begrenzt getestet, bevor sie verabschiedet werden. Ist ein Standard endlich verabschiedet, ist er von der Technologie oft genug schon überholt. OSI hat sich nie sehr weit von den Papierkonzepten in den praktischen Computereinsatz hinein entwickelt und gilt heute als gescheitert. Bis in die 90er-Jahre hinein dekretierten die Forschungs- und Technologiebehörden vieler Länder, darunter Deutschland und Japan, jedoch, dass OSI das offizielle und damit das einzige Netzwerkprotokoll sei, in das Forschungsmittel fließen. Selbst die US-Regierung schrieb noch 1988 vor, dass alle Rechner, die für den Einsatz in staatlichen Stellen angekauft werden, OSI unterstützen müssen und erklärte TCP/IP zu einer »Übergangslösung«.1983 beschloss das US-Verteidigungsministerium, das Netz in ein <\n>öffentliches ARPANET und das vertrauliche <f"ZapfDingbats">’<f$> MILNET aufzuteilen. Nur 45 der 113 Host-Rechner blieben im ARPANET übrig. Die Zahl der an diese Hosts angeschlossenen Rechner war natürlich viel größer, vor allem durch den Übergang von <f"FFScala-Italic">Time-Sharing<f$>-Großrechnern hin zu Workstations in einem <f"FFScala-Italic">Ethernet<f$>-LAN. Jon Postel wies den einzelnen miteinander verbundenen Netzen erst Nummern zu, dann entwickelte er zusammen mit Paul Mockapetris und Craig Partridge das <f"FFScala-Italic">Domain Name System <f"ZapfDingbats">’<f"FFScala-Italic"><\!q><$f$>(DNS),<+f"Univers-Condensed">19<$f$> mit einem ersten Name-Server an der University of Wisconsin, der Namen in Nummern übersetzt. Gleichzeitig empfahl er das heute übliche »user<\@>host.domain«-Adressierungsschema. Das neue Adressensystem institutionalisierte sich 1988 mit der <f"FFScala-Italic">Internet Assigned Numbers Authority<f$> <@1 fliess normal>(<@$p><f"ZapfDingbats">’ <f$>IANA), deren Direktor Postel wurde.<t-2h97>1981 begann Bill Joy an der Berkeley Universität mit einem Forschungsauftrag der DARPA, die TCP/IP-Protokolle in die dort gepflegte freie Version des Betriebssystems Unix zu integrieren. Sie wurden im August 1983 in der <f"ZapfDingbats">’<f$> BSD (<f"FFScala-Italic">Berkeley Systems Distribution<f$>)-Unix-Version 4.2 veröffentlicht. Die Betriebssysteme von Computer und Netz waren verschmolzen. Nicht zuletzt deshalb begannen viele Computerunternehmen, wie z. B. das von Joy mitgegründete Sun Microsystems, BSD zur Basis ihrer Workstations zu machen. Die freie Software 4.2BSD verbreitete sich rasch. Tausende von Entwicklern in der ganzen Welt übernahmen es und legten so die Grundlage für das heutige globale Internet. 1977 waren mit dem Tandy TRS-80 und dem Commodore Pet die ersten Computer für den Privatgebrauch auf den Markt gekommen, Steve Wozniak und Steve Jobs kündigten den Apple II an. Der IBM-PC folgte 1981 und kurz darauf die ersten IBM PC-Clones. Durch die billigen <\h>Kleinstrechner und ihre Fähigkeit, per Modem zu kommunizieren, betrat eine neue Generation von Nutzerkulturen die Informatik- und Netzwelt. <t$h$>Die Integration von TCP/IP und lokalen <f"FFScala-Italic">Ethernets<f$> trieb die Ausbreitung des Internet voran.<+f"Univers-Condensed">20<$f$> <f"FFScala-Italic">Ethernet<f$>-Karten wurden auch für PCs verfügbar. Anfang der 80er-Jahre entwickelten Studenten von Professor David Clark am MIT den ersten TCP/IP-Stapel (<f"FFScala-Italic">Stack<f$>) für MS-DOS. Der Quellcode für PC/IP und einige einfache Netzapplikationen verbreiteten sich rasch und inspirierte viele andere, den PC für das Internet zu erschließen. Da das Betriebssystem <f"ZapfDingbats">’<f$> DOS nicht multitasking-fähig ist, konnte PC/IP nur eine einzige Verbindung (ein <f"FFScala-Italic">Socket<f$>) unterstützen. Für einige Anwendungen (wie Telnet) stellt die Beschränkung kein Problem dar, FTP dagegen benötigt zwei Verbidungen gleichzeitig, einen Kontroll- und einen Datenkanal. Phil Karn, damals bei den Bell Labs beschäftigt, begann 1985 einen neuen TCP/IP-Stack zu schreiben, bei dem er Multitasking innerhalb der Applikation realisierte – ein waghalsiger Trick, der aber funktionierte. Für CP/M entwickelt, portierte Karn den Code bald auf DOS und, da er ein leidenschaftlicher Amateurfunker war, überarbeitete er ihn außerdem für die Verwendung über <f"FFScala-Italic">Packet<f$>-Radio. Unter dem Namen seines Rufzeichens KA9Q<+f"Univers-Condensed">21<$f$> gab er den Code für nicht kommerzielle Verwendung frei (vgl. <f"FFScala-Caps">Baker<f$>, 1998). 1979 entstand das USENET, das zu einem internetweiten schwarzen Brett werden sollte. Steve Bellovin schrieb dazu einige Kommandozeilen-Skripte, die es einem Rechner erlauben, über UUCP Nachrichten auf einem anderen Rechner abzurufen. Technisch ist das <f"ZapfDingbats">’<f$> USENET ein frühes Beispiel für Client-Server-Architekturen. Sozial bildet es einen öffentlichen Raum, in dem jeder lesen und schreiben kann, zu Themen, die so ziemlich alles unter der Sonne umfassen.<+f"Univers-Condensed">22<$f$> Eine andere Form von kooperativem sozialem Raum, der zusätzlich synchrone Kommunikation ermöglicht, sind <f"FFScala-Italic">Multi-User Dungeons<f$> <@1 fliess normal>(<@$p><f"ZapfDingbats">’ <f$>MUDs). Angelehnt an Tolkiens <f"FFScala-Italic">Dungeons and Dragons<f$>-Motive erlauben es diese Welten mehreren Spielern, gemeinsam durch rein textbasierte Räume zu ziehen, Drachen zu töten, Puzzle zu lösen und miteinander zu plaudern. Als Spielumgebungen entstanden, fanden MUDs später auch für Bildungs- und Diskussionszwecke Verwendung. Das erste von ihnen, das MUD1, schrieben ebenfalls 1979 Richard Bartle und Roy Trubshaw an der University of Essex. 1988 kommt mit dem <f"FFScala-Italic">Internet Relay Chat<f$> <@1 fliess normal>(<@$p><f"ZapfDingbats">’ <f$>IRC) von Jarkko Oikarinen ein weiteres synchrones Kommunikationsformat hinzu.Parallel zum Internet kamen lokale Diskussionsforen, <f"FFScala-Italic">Bulletin Board Systems<f$> <@1 fliess normal>(<@$p><f"ZapfDingbats">’ <f$>BBS) auf, zunächst als allein stehende PCs mit einer oder mehrere<t-1>n Einwahlverbindungen. Mit Hilfe von Telefonleitungen und X.25 vernetzten sich auch diese Kleinrechner, z. B. zum »FidoNet«, 1983 von Tom Jennings entwickelt. 1985 gründet Stewart Brand das legendäre BBS <f"FFScala-Italic">Whole Earth ’Lectronic Link<f$> (WELL) in San Francisco. Kommerzielle BBSs wie Comp<t$>uServe und AOL folgten. Auch diese separaten Netze richteten Ende der 80er-Jahre Gateways zum Internet ein, über die sie seither E-Mail und News <t-2>austauschen können (Fidonet z. B. 1988, MCIMail und Compuserve 1989).<t$> Um auch Menschen außerhalb der Universitäten den Zugang zum Internet zu ermöglichen, entstanden eine Reihe von so genannten Freenets. Das erste, das »Cleveland Freenet«, wurde 1986 von der <f"FFScala-Italic">Society for Public Access Computing<f$> (SoPAC) in Betrieb genommen.Die Masse der im Internet verfügbaren Informationen wurde immer unüberschaubarer. Der Bedarf nach Navigations- und Suchwerkzeugen führte zu neuen Entwicklungen an verschiedenen Forschungseinrichtungen. Am <f"ZapfDingbats">’<f$> CERN stellte Tim Berners-Lee 1989 Überlegungen zu <\h>einem verteilten Hypertext-Netz an, aus dem das <f"FFScala-Italic">World-Wide<\!q>Web <f"ZapfDingbats">’<f"FFScala-Italic"><\!q><$f$>(WWW) geworden ist. Ähnliche Verknüpfungen bieten die im folgenden Jahr gestarteten Dienste »Archie« (von Peter Deutsch, Alan Emtage und Bill Heelan, McGill University) und »Hytelnet« (von Peter Scott, University of Saskatchewan). 1991 kamen <f"FFScala-Italic">Wide Area Information Servers <f$>(WAIS, von Brewster Kahle, <f"FFScala-Italic">Thinking Machines Corporation<f$>) und »Gopher« (von Paul Lindner und Mark P. McCahill, University of Minnesota) hinzu. Die erste Version von Berners-Lees WWW wurde freigegeben. Im Jahr darauf entstand am <f"FFScala-Italic">National Center for Supercomputing Applications<f$> <@1 fliess normal>(<@$p><f"ZapfDingbats">’ <f$>NCSA) der erste Web-Browser »Mosaic«. Ebenfalls 1992 veröffentlich die University of Nevada »Veronica«, ein Suchwerkzeug für den »Gopher«-Raum. Im selben Jahr startete der Bibliothekar Rick Gates die »Internet Hunt«, ein Suchspiel nach Informationen, bei dem auch diejenigen aus den veröffentlichten Lösungsstrategien lernen konnten, die sich nicht selber beteiligten. 1990 wurde die erste fernbedienbare Maschine ans Netz gehängt, der »Internet Toaster« von John Romkey. Bald folgten Getränkeautomaten, Kaffeemaschinen und eine Fülle von Webkameras. Die offene Architektur des Internet macht es jedoch möglich, jede Kommunikation an allen Knoten zwischen Sender und Empfänger abzuhören. Die Antwort auf dieses Problem lautet Kryptogragfie, doch die galt als militärisch-staatliches Geheimwissen. Das erste für Normalsterbliche zugängliche Kryptografie-Werkzeug war <f"ZapfDingbats">’<f$> PGP (<f"FFScala-Italic">Pretty Good Privacy<f$>), 1991 von Philip Zimmerman freigegeben. Neben Texten fanden sich auch schon in den 80ern Bilder und Audio<\h>dateien im Netz, doch ihre Integration hatte mit dem WWW gerade erst begonnen. Die ersten regelmäßigen »Radiosendungen« im Netz waren die Audiodateien von Interviews mit Netzpionieren, die Carl Malamud ab 1993 unter dem Namen <f"FFScala-Italic">Internet Talk Radio <f$>ins Netz stellte. Weiter ging der »Multimedia-Backbone« <@1 fliess normal>(<@$p><f"ZapfDingbats">’ <f$>MBONE), über den 1992 die ersten Audio- und Video-Multicasts ausgestrahlt wurden. Anfangs konnten sich daran nur wenige Labors mit einer sehr hohen Bandbreite beteiligen, doch bald wurden die hier entwickelten Werkzeuge auch für den Hausgebrauch weiterentwickelt. Das Programm »CUSeeMe« (ein Wortspiel auf »I see you seeing me«) bot Video-Conferencing für den PC. Das Streaming-Format »RealAudio« (1995) machte es möglich, Klanginformationen in Echtzeit im Netz abzurufen. Multimediale Inhalte können mit <f"ZapfDingbats">’<f$><\!q>MIME <f"FFScala-Italic">(Multimedia Internet Mail Extensions <f$>– RFC 1437) seit 1993 auch in E-Mails verschickt werden.@2  ZÜ 3:Internationalisierung@1 fliess ohne:<t1>In Europa gab es Anfang der 80er-Jahre bereits erste auf Wählverbindungen und UUCP basierende Netze, wie z. B. das 1982 etablierte »Eunet« (<f"FFScala-Italic">European Unix Network<f$>) mit Knoten in Holland, Dänemark, Schweden und England. In Deutschland kannte man das Internet höchstens <t2>aus dem Kino (z.B. »War Games«), wie sich einer der deutschen Internetpioniere, Claus Kalle vom Rechenzentrum der Universität Köln, erinnert.<+f"Univers-Condensed">23 <$f$>Großrechner kommunizierten über das teure »Datex-P«. Das erste Rechnernetz, das über einen E-Mail-Link in die USA und dort über ein Gateway ins Internet verfügte, war das 1984 gestartete <t$f"ZapfDingbats">’<t2f$><\!q>EARN (<f"FFScala-Italic">European Academic Research Network<f$>). Natürlich wurde auch bald mit TCP/IP experimentiert – die RFCs, die man sich per E-Mail über EARN beschaffen konnte, machten neugierig – doch das Klima war für TCP/IP nicht günstig. Als 1985 der Verein zur Förderung eines Deutschen Forschungsnetzes e.V. (DFN-Verein) gegründet wurde, vertrat er ausschließlich die offizielle OSI-Linie. »In Deutschland und Europa war man damals vollkommen davon überzeugt und förderte auch politisch und finanziell, dass die Protokolle der OSI-Welt in Kürze weit verfügbar und stabil implementiert seien, und damit eine Basis für die herstellerunabhängige Vernetzung existieren würde.«<+f"Univers-Condensed">24<$f$> Die ersten Verbindungen von Rechnern außerhalb der USA liefen über UUCP. 1984 wurde z. B. das <t$f"ZapfDingbats">’<t2f$> JUNET (<f"FFScala-Italic">Japan Unix Network<f$>) establiert, und eine erste Botschaft von »Kremvax« sorgte für Aufregung, da seither auch die UdSSR an das USENET angeschlossen war.<t1> <t$>@1 fliess mit:Die Initiative für ein IP-Netz in Deutschland ging 1988 von der Universität Dortmund aus. Es hatte im Rahmen des europaweiten »InterEUnet«-Verbundes eine Anbindung erst über »Datex-P«, dann über eine Standleitung nach Amsterdam und von dort aus an das US-amerikanische Internet. Die Informatik-Rechnerbetriebsgruppe (IRB) der Universität Dortmund betrieb einen anonym zugänglichen FTP-Server. »Besonders förderlich war es, mit den recht frischen Kopien der GNU- und anderer Public-Domain-Pakete (emacs, gcc, ISODE usw.) zu arbeiten. Auch war auf diesem Wege erstmalig Zugang zu Netnews und Internet-Mail möglich, so dass man sich auf dem Laufenden halten konnte.«<+f"Univers-Condensed">25 <\n><$f$>Eine ähnliche Initiative gab es am Informatik-Lehrstuhl von Professor Zorn an der Universität Karlsruhe, die zum Aufbau des XLINK (eXtended Lokales Informatik Netz Karlsruhe) führte, das ebenfalls eine Verbindung in die USA zum New Yorker NYSERnet <@1 fliess kursiv>(New York State Education and Research Network) <@$p>anbot. Das OSI-Regime des DFN lockerte sich nach und nach. Das X.25-basierte Wissenschaftsnetz (WiN) sollte gleich von seinem Start an auch TCP/IP-Hosts unterstützen.<+f"Univers-Condensed">26 <$f$>Die europäischen Netzanbieter schlossen sich 1989 auf Initiative von Rob Blokzijl am <f"FFScala-Italic">National Institute for Nuclear Physics and High-Energy Physics<f$> in Amsterdam zum <x@1 fliess normal><f"ZapfDingbats">’<@$p> RIPE <f"FFScala-Italic">(Reseaux IP Européens)<f$> zusammen, um die administrative und technische Koordination für ein paneuropäisches IP-Netzwerk zu gewährleisten. Zur Konsolidierung der bereits existierenden europäischen IP-Netze begannen 1991 <\n>einige Netzbetreiber, eine europäische IP-Backbone-Struktur namens <f"ZapfDingbats">’<f$><\!q>EBONE zu planen und aufzubauen. 1992 begannen auch Initiativen wie Individual Network e.V. (IN) mit dem Aufbau alternativer Verfahren und Strukturen zur Bereitstellung von IP-Diensten. Auch das IN nahm im Weiteren aktiv an der Gestaltung der deutschen IP-Landschaft teil. Nicht zuletzt die Netnews-Verteilung wäre ohne die IN-Mitarbeit nur schleppend vorangekommen. Der Zuwachs der internationalen IP-Konnektivität lässt sich an der Anmeldung von Länder-Domains ablesen. 1988 kamen Kanada, Dänemark, Finland, Frankreich, Island, Norwegen und Schweden dazu. Im November 1989 sind insgesamt 160 000 Hosts am Internet. Australien, Deutschland, Israel, Italien, Japan, Mexiko, Holland, Neuseeland und Großbritannien schließen sich an. 1990 kommen Argentinien, Österreich, Belgien, Brasilien, Chile, Griechenland, Indien, Irland, Südkorea, Spanien und die Schweiz dazu. 1991 sind es Kroatien, die Tschechische Republik, Hong Kong, Ungarn, Polen, Portugal, Singapur, Südafrika, Taiwan und Tunesien. 1992 überschreitet die Zahl der Hosts die Eine-Million-Marke. Immer kleinere Länder und Territorien wie Zypern, die Antarktis, Kuwait und Luxemburg melden Länder-Domains an. 1997 kommen noch eine Reihe von Inselnationen und Protektorate hinzu, so dass heute die gesamte Weltkarte auf den Adressraum des Internet abgebildet ist.@2  ZÜ 3:Kommerzialisierung@1 fliess ohne:Das Entstehen eines kommerziellen Marktes für Internetanbieter anzuregen und zu fördern, war eines der Ziele der NSFnet-Initiative. Zu den ersten Nutznießern gehörten Firmen wie Performance Systems International (PSI), <@1 fliess kursiv>Advanced Network and Systems<@$p> (ANS – von IBM, MERIT und MCI gegründet), Sprintlink und CERFNet von General Atomics, das auch das <@1 fliess kursiv>San Diego Supercomputer Center<@$p> betrieb. Die kommerziellen ISPs sollten Ende der 80er den Erhalt und Ausbau des Internet von den Universitäten und Forschungbehörden übernehmen. Dadurch entstand auch ein bedeutender Markt für internetbasierte Produkte. Len Bozack, ein Stanford-Student, gründete Cisco Systems. Andere, wie 3COM, Proteon, Banyan, Wellfleet und Bridge gingen ebenfalls in den Router-Markt. Die erste Internet-Industriemesse, die »Interop« in San Jose 1988, zog 50 Aussteller und 5 000 Besucher an. @1 fliess mit:1991 hob die NSF das bis dahin bestehende Werbeverbot (die <f"FFScala-Italic">acceptable use policy<f$>) in der öffentlichen Netzinfrastruktur auf. Damit war der Weg frei dafür, dass sich sich General Atomics (CERFnet), PSI (PSInet) und UUNET Technologies, Inc. (AlterNet) in Kalifornien zum ersten <f"ZapfDingbats">’<f$><\!q>CIX <f"FFScala-Italic">(Commercial Internet Exchange)<f$> zusammenschlossen, um den uneingeschränkten Verkehr zwischen den kommerziellen Netzen zu organisieren. Auch in Deutschland begann Anfang der 90er die Privatisierung der universitären Infrastruktur. Das Drittmittelprojekt »Eunet« der Informatik-Rechnerbetriebsgruppe der Uni Dortmund wurde Ende 1992 zur GmbH. Im Jahr darauf wurde auch das XLINK-Projekt an der Uni Karlsruhe zur Tochter der NTG, ihrerseits Tochter von Bull.<+f"Univers-Condensed">27<$f$>@2  ZÜ 3:Wende ab 1990@1 fliess ohne:<t-1>Ein Wendepunkt lässt sich am Übergang von den 80er- zu den 90er-Jahren ausmachen. Das ARPANet wird 1990 offiziell abgeschaltet. Die NSF verlagert die Netzwerkförderung von einer direkten Finanzierung der akademischen Backbone-Infrastruktur hin zur Bereitstellung von Etats, mit denen die Universitäten sich Konnektivität von kommerziellen Anbietern einkaufen. Mit der schwindenden Rolle der NSF im Internet endete auch die Verbindlichkeit der <f"FFScala-Italic">Acceptable Use Policy<f$>. Zunächst behutsam, dann in einem unaufhörlichen Strom setzten die Werbebotschaften im Netz ein. Die im CIX zusammengeschalteten Netzanbieter vermarkteten das Internet als Businessplattform. Über die Gateways der kommerziellen BBSe kamen Nutzerkulturen, die es gewohnt waren, für Informationen zu bezahlen und ihrerseits die Kanäle hemmungslos für gewerbliche Zwecke zu verwenden. Einen berüchtigten Konflikt löste die Anwaltsfirma Canter & Siegel aus Arizona aus, als sie 1994 Massen-E-Mails <x@1 fliess normal><t-1>(<@$p><t-1f"ZapfDingbats">’ <f"FFScala-Italic">Spam<f$>) zur Bewerbung ihrer Green-Card-Lotteriedienste ins Internet schickte. <t-2>Die Netzbewohner reagierten heftig und unterbanden diesen Missbrauch, indem sie ihrerseits die Firma massenhaft mit E-Mails eindeckten.<t-1>@1 fliess mit:Ab 1990 wurden gezielte Anstrengungen unternommen, kommerzielle und nicht kommerzielle Informationsdiensteanbieter ins Netz zu holen. Unter den ersten befanden sich Dow Jones, Telebase, Dialog, CARL (die <@1 fliess kursiv>Colorado Alliance of Research Libraries)<@$p> und die <f"FFScala-Italic">National Library of Medicine.<f$> 1991 trat das WWW seinen Siegeszug an. Mehr als 100 Länder waren an das Internet angeschlossen, mit über 600<\!q>000 Hosts und fast 5<\!q>000 einzelnen Netzen. Im Januar 1993 waren es schon über 1,3 Millionen Rechner und über 10<\!q>000 Netzwerke. Der damalige US-Präsident Bill Clinton und Vize Al Gore gaben im Februar 1993 unmittelbar nach ihrem Amtsantritt auf einem <f"FFScala-Italic">Town <\h>Meeting <f$>im Silicon Valley eine Erklärung über ihre Technologiepolitik ab, in der das Internet bereits eine zentrale Rolle spielte. Damit lösten sie eine Art Vorbeben aus, in einer geopolitischen Situation, in der die USA sich in einer Wirtschaftskrise befanden, Europa im Aufschwung und Japan an der Weltspitze. Die eigentliche Schockwelle ging über die Welt hinweg, als Al Gore am 15. September des Jahres die <f"FFScala-Italic">National Information Infrastructure Agenda for Action<f$> verkündete, in der er Netzwerke nicht nur selbst zu einer Multi-Milliarden-Dollar-Industrie, sondern zu einer Grundlageninfrastruktur für Wirtschaft, Bildung, Wissenschaft und Kultur erklärte.<+f"Univers-Condensed">28<$f$> Das Bewusstsein, in einem Schlüsseltechnologiesektor <\n>hinter den USA herzuhinken, löste allerorten hektisches Treiben aus. <t-1>Spätestens damit begann die kommerzielle Erschließung und die Massen<\h>b<t$>esiedlung des Internet. <t2>Für die neuen Generationen von Nutzern gibt es nur eine Information, die frei und möglichst weit zirkulieren soll: Werbung. Alle andere Information ist für sie Ware. Um nun in diesem promiskuitiven Milieu <\h>eine Information (z.<\!q>B. Börsendaten, Lehrmaterial, Musikstücke) derjenigen und nur derjenigen zugänglich zu machen, die dafür bezahlt hat, mussten in das Internet zusätzliche, aufwendige Schutzmechanismen, Zonen mit Zugangskontrollen und kryptografisch abgesicherte Rechtekontrollsysteme<f"FFScala-Italic"> <f$>eingezogen werden. Die Rechteindustrie (Bertelsmann, Sony, Time-Warner usw.) arbeitet seit etwa 1994 nach Kräften daran, ihre Waren über das Netz verkaufbar zu machen und technisch abzusichern. Nichts demonstrierte die neue Qualität des Internet besser, als die erste Cyber-Bank »First Virtual«, die 1994 ihren Betrieb aufnahm. <t$>Microsoft (MS) hatte das Internet zunächst verschlafen. Bill Gates erwähnte in der Erstausgabe seines 1995 erschienen Buches »The Road Ahead« das Internet mit keinem Wort. Kurz darauf schwenkte er den Ozeanriesen Microsoft auf Internet-Kurs. Noch im selben Jahr erschien die erste Version des Web-Browsers »MS Internet Explorer«. Nachdem die Kopplung von Hard- und Software gebrochen war, löste das Web die Verbindung von jeweils spezifischer Software und Information auf. Microsoft Network (MSN) war dagegen ein Versuch, erneut eine solche Kopplung zu legen: ein geschlossenes Format, in dem Firmen kosten<\h>pflichtige Informationen und Dienstleistungen anbieten konnten – sofern sie eine Startgebühr von 50<\!q>000 Dollar und einen Anteil aller Einnahmen an MS zahlten. Es handelte sich um eine verspätete Imitation der geschlossenen BBSe wie Compuserve oder AOL, die bereits durch das WWW überholt waren, das es jedem erlaubte, gebührenfrei Informationen anzubieten.Domain-Namen waren bislang nichts als eine Mnemotechnik gewesen, die die darunter liegenden numerischen IP-Adressen handhabbarer machten. Durch den Einzug großer Unternehmen mit ihren geschützten Warenzeichen wurden sie zu einem aggressiv umstrittenen Territorium. Der erste prominente Streit darüber, ob Domain-Namen geistiges Eigentum sind, war der von MTV Networks gegen Adam Curry. Etwa im Mai 1993 hatte Curry, ein MTV-Video-Jockey, auf eigene Faust und Kosten ein Informationsangebot unter »mtv.com« gestartet. In Gesprächen mit führenden Angestellten von MTVN und deren Mutterfirma Viacom New Media hieß es, MTV habe kein Interesse am Internet, hindere Curry aber auch nicht an seinen Aktivitäten. Also baute Curry sein Informationsangebot weiter aus, u.a. mit einem schwarzen Brett, auf dem sich Musiker und Vertreter der Musikindustrie miteinander unterhielten. In den von ihm moderierten Fernsehprogrammen wurden E-Mail-Adressen wie »popquiz<\@>mtv.com« eingeblendet. Im Januar 1994 forderte MTVN Curry förmlich auf, die Verwendung von »mtv.com« einzustellen. Dennoch verwiesen MTV-Sendungen weiterhin auf diese Adresse, und ein führender Angestellter bat Curry im Februar, bestimmte Informationen in seiner Site aufzunehmen. Inzwischen hatten MTVN und AOL einen Vertrag abgeschlossen, um einen kostenpflichtigen Dienst anzubieten, der u.a. ein schwarzes Brett für Musikprofis beinhalten sollte, das dem von Curry auffällig glich. MTVN verklagte Curry u.a. wegen des Verstoßes gegen Trademark-Ansprüche auf Freigabe der Domain »mtv. com«. Currys Versuche, den Streit gütlich beizulegen, scheiterten. Er kündigte. Letztlich kam es doch zu einer außergerichtlichen Einigung, bei der Curry die Domain an MTV aufgab.<+f"Univers-Condensed">29 <$f$>Die Situation war typisch für die Zeit um 1993-94: Große Unternehmen, auch aus der Medienbranche, ignorierten oder unterschätzten die Bedeutung des Internet, während innovative Einzelpersonen durch ihr persönliches Engagement populäre und kostenlose Informationsangebote aufbauten, nur um zusehen zu müssen, wie ihnen die Früchte ihrer Arbeit mit Hilfe des Rechtssystems abgesprochen wurden. Nachdem in zahlreichen Urteilen entschieden war, dass Domain-Namen dem Warenzeichenregime unterliegen, setzte ein reger Handel ein. CNET beispielsweise kaufte 1996 die URL »tv.com« für 15<\!q>000 Dollar. »business.com« wurde 1997 für 150<\!q><\!q>000 Dollar verkauft und zwei Jahre später für bereits 7,5 Millionen Dollar weiterverkauft. Bis 1995 war die kommerzielle Backbone-Infrastruktur in den USA soweit errichtet und untereinander verschaltet, dass der NSFNET-Backbone-Dienst eingestellt werden konnte.<+f"Univers-Condensed">30 <$f$>Im selben Jahr gingen eine Reihe von Internetunternehmen an die Börse, am spektakulärsten das auf der NCSA-Browser-Technologie errichtete Netscape mit dem drittgrößten Nasdaq-IPO-Wert aller Zeiten.Im Gefolge der Wirtschaft hielten auch die Rechtsanwälte Einzug ins Internet. Als Teil der Verrechtlichung unternahm auch der Gesetzgeber Schritte zur Regulierung. 1996 wurde in den USA der umstrittene <f"FFScala-Italic">Communications Decency Act<f$> (CDA) verabschiedet, der den Gebrauch von »unanständigen« Wörtern im Internet verbietet. Einige Monate später verhängte ein Gericht eine einstweilige Verfügung gegen die Anwendung dieses Gesetzes. 1997 erklärte das höchste US-Gericht den CDA für verfassungswidrig. Dennoch wurde in dieser Zeit der vermeintlich rechtsfreie Raum des Internet in die gesetzlichen Regularien von Gebieten wie der Kryptografie über das Urheberrecht bis zu den Allgemeinen Geschäftsbedingungen einbezogen. <*h"mehr">In vielen Ländern greifen Gerichte und staatliche Behörden in den <\n>Cyberspace ein. China verlangt, dass ISPs und Nutzer sich bei der Polizei registrieren. Ein deutsches Gericht entschied, dass Compuserve den Zugang zu Newsgroups, die sich im weitesten Sinne mit Sexualität beschäftigen, unterbinden muss. Da Compuserve sein weltweites Informationsangebot in seiner Zentrale in Ohio vorrätig hält und es technisch nicht nach einzelnen Länder differenzieren konnte, schaltete es die Newsgroups für alle Nutzer ab, was eine vor allem amerikanische Protest- und Boykottwelle gegen Deutschland auslöste. Saudi Arabien beschränkt den <\n>Zugang zum Internet auf Universitäten und Krankenhäuser. Singapur verpflichtet Anbieter politischer und religiöser Inhalte, sich staatlich regi<t-1>s<\h>trieren zu lassen. Neuseeland klassifiziert Computerdisketten als »Publi<\h>k<t$>ationen«, die zensiert und beschlagnahmt werden können. Amerikanische Telekommunikationsunternehmen nahmen Anstoß an Internet-Telefoniediensten und forderten das Parlament auf, die Technologie zu verbieten. <*h"Standard">Auch die Selbstorganisation der technischen Entwicklung der Internetgrundlagen veränderte ihren Charakter. Saßen in den jährlichen Treffen von IETF-Arbeitsgruppen Mitte der 80er-Jahre höchstens 100 Personen, sind es jetzt nicht selten 2<\!q>000 bis 3<\!q>000. Entsprechend sind sie kein kollektives Brainstorming mehr, sondern dicht gedrängte Abfolgen von Präsentationen. Die eigentliche Arbeit findet immer häufiger in kleinen geschlossenen Gruppen, den Design-Teams, statt. Während die mehr als 20 Jahre alte Technologie des Internet erstaunlich stabil skaliert, stoßen die Communitystrukturen an ihre Grenzen. Auch die Zusammensetzung der Arbeitsgruppen veränderte sich: »Seit den späten 80er-Jahren hat sich der Anteil akademischer Mitglieder in der IETF stetig verringert – und das nicht nur, weil die Zahl der Unternehmen immer mehr anstieg, sondern auch, weil immer mehr Gründungsmitglieder in die Wirtschaft wechselten.«<+f"Univers-Condensed">31<$f$> Das kollektive Streben nach der besten Lösung für das Internet als Ganzes, so Jeanette Hofmann, drohe, von den Interessen konkurrierender Unternehmen unterlaufen zu werden, die ihre jeweiligen Produkte durchsetzen wollten. Schließlich führten die schiere Größe, die nachrückende Generation von Ingenieuren und das Gewicht der gewachsenen Struktur dazu, dass die Standardentwicklung dazu neigt, konservativer und mittelmäßiger zu werden. Hofmanns Fazit: Die IETF sei auf dem besten Weg, eine Standardisierungsorganisation wie jede andere zu werden: »Das Internet und seine Gemeinde sind in der Normalität angekommen. Irgendwann werden sich die Väter unter der wachsenden Zahl gleichberechtigter Mitglieder verloren haben – und mit ihnen ein Teil der Ideen und Prinzipien, die die Entstehung des Internet umgaben.«<+f"Univers-Condensed">32<$f$>@2  ZÜ 3:The Beginning of the Great Conversation@1 fliess ohne:Welche Bedeutung hat nun die hier geschilderte Entwicklung des Internet für die Wissensordnung digitaler Medien allgemein und für die freie Software im Besonderen? Das Netz der Netze ist offen, verteilt, dezentral und heterogen. Im Gegensatz zum zentralistischen Telefonsystem beruht es auf lokalen <@1 fliess kursiv>Peering<@$p>-Abkommen zwischen geografisch benachbarten Netzen. Das Internet ist offen, weil es von Beginn an auf den verschiedensten Rechnerarchitekturen implementiert wurde, weil es auf jede Netzwerk-Technologie (Radio, Satelliten, ISDN, Frame Relay, ATM usw.) aufsetzen kann,<@3 hoch fliess>33<@$p> und weil es seinerseits andere Protokolle (AppleTalk, Novell IPX, DECNet usw.) gekapselt transportieren kann. Offen ist es auch von Beginn an für internationale Zusammenarbeit. Offen ist es schließlich, weil die öffentliche Förderung – durch die US-Wissenschaftsbehörden ARPA und NSF und mit Verzögerung auch durch die entsprechenden Behörden anderer Länder – eine proprietäre Schließung der Forschungsergebnisse verhinderte. Der antikommerzielle Geist in dieser öffentlichen Infrastruktur der Wissenschaftsgemeinde wurde in der <@1 fliess kursiv>Acceptable Use Policy <@$p>der NSF kodifiziert, die bis Ende der 80er-Jahre jegliche kommerzielle Nutzung, wie Werbung oder Verkauf von Waren und Dienstleistungen im Internet, untersagte. Es ging beim Internet um Grundlagenforschung, in einem Gebiet, in dem viele Leute kooperieren mussten, durchgeführt an Universitäten und in seinem ersten Jahrzehnt noch ohne militä<\h>rische<@3 hoch fliess>34 <@$p>und wirtschaftliche Anwendung. Aus all diesen Faktoren erwuchs eine kooperative Community, die das Netz trägt und die von ihm getragen wird. @1 fliess mit:Aus dieser Zeit und aus diesem Wissensmilieu stammt der Grundsatz der Wissensordnung digitaler Medien, den einige Unverdrossene auch heute im Zeitalter der <@1 fliess kursiv>Dot-com-<@1 fliess normal>Wirtschaft<@1 fliess kursiv> <@$p>aufrecht erhalten: Information will frei sein. Die besondere Qualität dieser Umgebung ergibt sich aus der Zweiweg-Kommunikationsstruktur und der Archivfunktion des Netzes. Die schlichte Tatsache, dass jeder Teilnehmer Einzelne oder Gruppen beliebiger Größe von anderen Teilnehmern ansprechen kann, und dass viele der öffentlichen Äußerungen nachlesbar und referenzierbar bleiben, führt zu dem, was John Perry Barlow als »das Ende der Rundfunkmedien und den Beginn des Großen Gesprächs« (<@1 fliess kursiv>The Great Conversation<@$p>) bezeichnet hat. »Wenn es mit einemmal möglich wird, Ideen weithin zu verbreiten, ohne sie erst durch eine zentral gesteuerte und hochkapitalisierte industrielle Maschinerie schleusen zu müssen – ob es sich um den Komplex der Buchindustrie oder 50 000 Watt-Sender handelt – ... wird die Freiheit der Meinungsäußerung nicht länger nur jenen gehören, die Druckerschwärze tonnenweise oder Sendeleistung nach Kilowatt kaufen. Niemand wird sich in seinen öffentlichen Äußerungen mehr auf Ideen beschränken müssen, die den Medienfürsten und ihren Inserenten genehm sind.« (Barlow, 1996).Kommunikationsmittel, die in vergleichbarer Form nur großen Unternehmen und gut besoldeten staatlichen Stellen verfügbar waren (z. B. Video-Conferencing), erlauben es jetzt Individuen, ad hoc oder kontinuierlich zusammenzuarbeiten. Daraus ergeben sich für einzelne Gruppen wie für das Internet insgesamt Strukturcharakteristika, die Helmut Spinner für die viel versprechendste nicht technische Neuerung der aktuellen Wissensordnung hält: »Es sind die weiten, aber trotzdem flachen Informationsnetze, die erstmals in der Geschichte das traditionelle ›Organisationsgesetz<B>‹<$> durchbrechen, demzufolge Größenwachstum unvermeidlich mit Abschließung nach außen und Hierarchiebildung im Innern verbunden ist, vom Sportverein über den Wirtschaftsbetrieb bis zum Großreich« (<@6 Caps>Spinner<@$p>, 1998, S. 9). Das Internet entwickelt sich selbst im Modus der offenen Kooperation und wird zur Voraussetzung für eine Wissensentwicklung durch Tausende auf der ganzen Welt verteilter Individuen, ohne Management- und andere Overhead-Kosten, in einer direkten Rückkopplungsschleife mit den Anwendern. Damit ist es auch die <\n>essenzielle Bedingung für das Phänomen der freien Software.@2  ZÜ 2:Geschichte der Softwareentwicklung@1 fliess ohne:Der Wandel der Wissensordnung im 20. Jahrhundert wird durch drei Dynamiken charakterisiert: »Technisierung des Wissens, Kommerzialisierung der Wissensdienste, Globalisierung der Rahmenbedingungen unter dem Ordnungsregime der Wirtschaftsordnung« (<@6 Caps>Spinner<@$p>, 1998, S. 34). Eine zentrale Rolle darin spielt der Computer: »Als Buchdruck und Nationalstaat die Medientechniken der mittelalterlichen Universität schluckten, blieben die Inhalte des Wissens ziemlich unberührt. Die Speicherung und Übertragung wurden zwar privatisiert oder verstaatlicht, aber die eigentliche Verarbeitung des Wissens lief weiter in jenem schönen alten Rückkopplungskreis aus Auge, Ohr und Schreibhand. Genau das hat die Computerrevolution verändert. Universale Turing-Maschinen machen auch und gerade die Verarbeitung des Wissens technisch reproduzierbar.«<@3 hoch fliess>35<@$p>@1 fliess mit:In den frühen Tagen des Computers war alle Software quelloffen und frei. Auch in der kommerziellen Computerwelt waren damals die Quellen verfügbar, einfach weil Software noch keinen eigenständigen Markt darstellte. Die Hardwarehersteller lieferten sie gleichsam als Gebrauchsanweisung zu ihren Rechnern dazu, alles weitere schrieben die Anwender selbst. Die Computerunternehmen hatten es also mit programmierkompetenten Nutzern zu tun und förderten deren Selbstorganisation und gegenseitige Unterstützung in <@1 fliess kursiv>User-Groups<@$p> wie IBMs SHARE<@3 hoch fliess>36 <@$p>und DECs DECUS. Auch in Zeitschriften wie in der Algorithmen-Rubrik der <@1 fliess kursiv>Communications of the ACM<@$p> oder in Amateurfunkzeitschriften <\h>zirkulierte uneingeschränkter Quellcode. Entwickler bauten auf den wachsenden Bestand der vorhandenen Software, verbesserten sie, entwickelten sie weiter und gaben ihre Ergebnisse wieder zurück an die Nutzergemeinschaft. Bezahlt wurden sie, ob in Universität oder Firma, für das Programmieren, nicht für die Programme. Auch an den Universitäten war es bis in die 70er-Jahre üblich, die dort entwickelte Software frei zu verbreiten. In den 60er-Jahren dominierten Hardware-Firmen wie IBM, DEC, Hewlett-Packard und Data General die Computerindustrie. Eine Unterscheidung in Hard- und Softwareunternehmen existierte noch nicht. Beim Marktführer IBM konnten Kunden Hardware nur in einem Paket kaufen, das Software, Peripheriegeräte, Wartung und Schulung einschloss. Kleinere Wettbewerber hatten kaum eine Chance, diese Leistungen anzubieten. 1969 begann IBM diese Bündelung aufzugeben. Die <\n>offizielle Firmengeschichte von IBM bezeichnet diesen Schritt euphemistisch als »Innovationen im Marketing«.<@3 hoch fliess>37<@$p> Tatsächlich erfolgte er unter dem Druck des gerade vom US-Justizministerium eingeleiteten Kartellverfahrens.<@3 hoch fliess>38<@$p> Die »freiwillige« Entkopplung (<@1 fliess kursiv>Unbundling<@$p>) beinhaltete, dass ein Teil der Software als separate Produkte angeboten wurde. Eine große Zahl von Programmen stand jedoch frei zur Verfügung, da IBM sie als gemeinfrei (in der <@1 fliess kursiv>Public Domain<@$p>) erachtete. Da sie aus der Nutzergemeinschaft stammten, hätte das Unternehmen auch kaum ein Urheberrecht darauf beanspruchen können. Dieser Schritt zog weitere Kritik nach sich, da natürlich niemand eine Softwarefirma dafür bezahlen würde, die gleichen Programme zu schreiben, die es von IBM bereits kostenlos gab.<@3 hoch fliess>39<@$p><t-1>Trotz der Konkurrenz durch freie Software, schuf diese Entkopplung die Voraussetzung für eine eigenständige Softwareindustrie, auch wenn die in den 70ern neu entstandenen Softwarefirmen meist noch Satelliten von Hardwareherstellern waren. Software war erstmals zu einer Ware geworden. Eine berüchtigte Episode ist der »<x@1 fliess kursiv><t-1>Open Letter to Fellow Hobbyists<@$p><t-1>« von 1976.<x@3 hoch fliess><t-1>40<@$p><t-1> Bill Gates beklagte darin, dass die meisten seiner Fellows ihre Software »stehlen« würden, was verhindere, dass gute Software geschrieben werden könne. Der neue Zeitgeist war umso bemerkenswerter, als Gates kurz vorher beinahe aus der Harvard Universität geflogen wäre, weil er die öffentlich finanzierten Ressourcen missbraucht hatte, um proprietäre, kommerzielle Software zu schreiben. Nachdem er gezwungen worden war, seine Software in die <x@1 fliess kursiv><t-1>Public Domain<@$p><t-1> zu stellen, verließ er Harvard und gründete Microsoft – und damit das Zeitalter von Software aus der Schachtel. Den überwiegenden Teil der Software stellen auch heute noch maßgefertigte Programme dar, die in den EDV-Abteilungen oder bei externen Dienstleistern geschrieben werden. So entstanden Tausende von nahezu identischen Buchhaltungs-, Abrechnungs- und Datenbanksystemen, immer aufs Neue. Zu Beginn des Zeitalters von Software als Massenware aus dem Regal stand eine weitere folgenreiche Entscheidung von IBM. <t$>Anfang der 80er-Jahre brachte das Unternehmen den IBM-PC heraus und nahm ihn gleichzeitig nicht ernst. Erst nach einigen Überzeugungsversuchen und Provokationen (Mitarbeiter warfen IBM vor, nicht in der Lage zu sein, einen so kleinen Computer zu bauen) gab die Firmenleitung ihre Skepsis auf und der Entwicklergruppe um Don Estridge in Boca Raton den Auftrag, einen <@1 fliess kursiv>Personal Computer<@$p> zu entwickeln. Statt wie bislang alle Komponenten im eigenen Haus zu fertigen, kaufte IBM die Bestandteile wie Prozessor und Betriebssystem von außen zu. Die<@4 Pfeil (Umschalt/Alt #)> ’<\!q><@$p>CPU kam von Intel (8088), das Betriebssystem DOS von einer 32-köpfigen Firma namens Microsoft. Einen tief greifenden Wandel der Computerlandschaft bewirkte ferner IBMs Entscheidung, die Spezifikation der Hardwarearchitektur des IBM-PC zu veröffentlichen. Ob es ebenfalls an der Verkennung des PC lag, an einem Zufall, einer subversiven Aktion oder an einer glücklichen Fügung des Schick<\h>sals,<@3 hoch fliess>41 <@$p>auf jeden Fall hat IBM mit der Veröffentlichung sich selbst Konkurrenz und den Computer zur Massenware gemacht. Ein Industriestandard, unabhängig von einem einzelnen Unternehmen, war gesetzt. Eine Fülle neuer Hardwarehersteller aus West und Fern<\h>ost unterboten sich mit ihren Preisen für IBM-PC-<@1 fliess kursiv>Clones.<@$p> Leute wie Michael Dell begriffen, dass es nicht mehr vorrangig um die Erfindung neuer Rechnerarchitekturen ging, sondern um Verkauf, Marketing und Distribution von etwas, das zu einem Gebrauchsgegenstand geworden war (vgl. <@6 Caps>Dell/Fredman<@$p>, 1999). Computer wurden billig genug für den Privatgebrauch. Die Branche zollte ihrem ehemaligen Führer Ehre, indem noch jahrelang Intel-Rechner aller Hersteller dieser Welt als »IBM-kompatibel<B>«<$> bezeichnet wurden. Der eine große Gewinner an diesem Wendepunkt war Intel, da jeder PC mit <t-4>der offenen, IBM-kompatiblen Architektur einen seiner Prozessoren enthielt.<t$>Der andere große Gewinner war Microsoft. Wie so oft spielte dabei der Zufall eine Rolle. Das am weitesten verbreitete Betriebssystem für die damalige Generation von 8-Bit-Rechnern war <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>CP/M<@$p>. 1973 hatte Gary Kildall begonnen, einen PL-1 Compiler auf dem Intel-8080-Prozessor zu implementieren und einige kleinere Routinen zum <@1 fliess kursiv>Control Program for Microcomputers<@$p> (CP/M) zusammenzufassen (vgl. <@6 Caps>Fritsch<@$p>, 1992). 1975 wurde CP/M erstmals von Digital Research (DR) auf dem Markt angeboten. Es wurde weltweit schätzungsweise 500 000 mal installiert und bot ein umfangreiches Spektrum an Anwendungsprogrammen diverser Softwarefirmen, darunter das am weitesten verbreitete Textverarbeitungssys<\h>tem »WordStar«.<@3 hoch fliess>42<@$p> Als sich IBM 1980 nach einem Betriebssystem für den PC umsah, war es nahe liegend, dass sie sich an Digital Research wandten. Das Millionengeschäft mit IBM ging jedoch an Microsoft, das damals vor allem für das auf fast jedem Mikrocomputer verfügbare MS-BASIC bekannt war, weil Kildall einer Anekdote nach am entscheidenden Tag mit seinem Privatflugzeug unterwegs und damit nicht erreichbar war (vgl. <@6 Caps>Fritsch<@$p>, 1992). Wie viele seiner Produkte hatte Microsoft MS-DOS nicht selbst entwickelt, sondern von einer Firma namens Seattle Computer gekauft. Es handelte sich um eine abgespeckte Version von CP/M, von der Digital Research noch dazu behauptete, dass Seattle Computer den Quellcode gestohlen habe. Die Industrie war sich einig, dass CP/M ein weit überlegenes Betriebssystem war, doch mit der Unterstützung durch IBM, das sicherstellte, dass alle Programme, die anfänglich für den IBM-PC ausgeliefert wurden, nur mit MS-DOS, nicht aber mit DRs CP/M kompatibel waren, dominierte MS-DOS in kürzester Zeit den Markt. »Microsoft hatte bereits die Kunst gemeistert, Druck auf Hardware<\h>geschäfte auszuüben sowie Anwendungssoftware und Betriebssystem in einer sich wechselseitig verstärkenden Strategie zu koppeln, um Rivalen auszuschalten, selbst wenn diese bessere Produkte anzubieten hatten« (<@6 Caps>Newman<@$p>, 1997). Im neuen Zeitalter des Desktop-Computers bildete die Kontrolle über das Betriebssystem den Schlüssel zur Etablierung eines Imperiums, wie IBM schmerzvoll feststellen musste. Microsoft erbte gleichsam IBMs Monopol, das zeitgleich zu Ende ging.Digital Research versuchte noch gut zehn Jahre, durch Innovationen mit MS zu konkurrieren. Während die Kundschaft MS vorwarf, wenig für die Weiterentwicklung von MS-DOS zu tun, waren spätestens 1984 CP/M-Versionen <@1 fliess kursiv>multitasking- <@1 fliess normal>und<@1 fliess kursiv> multiuser<@$p>-fähig. Als Digital Researchs <t-2>CP/M-Weiterentwicklung DR-DOS einen deutlichen Marktanteil im Desk<\h>to<t$>p-Bereich errang, verlautbarte MS 1990, dass das <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Release <@$p>von MS-DOS 5.0 unmittelbar bevorstehe. Der Nachfolger des 1986 erschienen MS-DOS 3.3 sollte alle Features enthalten, die Anwender an DR-DOS schätzten. Es dauert noch mehr als ein Jahr, bis MS-DOS 5.0 tatsächlich auf den Markt kam, doch bremste allein die Ankündigung den Absatz von DR-DOS. Als es schließlich erschien, hatte sich MS ein neues Marketingverfahren einfallen lassen, das DR und alle anderen PC-Betriebssystemhersteller endgültig aus dem Markt drängte.<@3 hoch fliess>43<@$p> MS verlangte von allen Hardwareverkäufern, die MS-DOS auf einigen ihren Rechnern installieren wollten, dass sie eine Lizenzgebühr für sämtliche Computer an MS bezahlten, auch wenn sie einige davon mit anderen Betriebssystemen auslieferten (vgl. <@6 Caps>Newman<@$p>, 1997).<*h"mehr">Die Möglichkeiten zur Integration erweiterten sich mit MS-Windows, der grafischen Benutzeroberfläche über DOS. Hauptkonkurrent hier war der Mac von Apple, der diese neue Art der Computerbedienung am Markt eingeführt hatte, aber auch Window-Umgebungen auf DOS, wie Quarterdecks <@1 fliess kursiv>Desqview<@$p> und DRs <@1 fliess kursiv>Graphics Environment Manager<@$p> (<@6 Caps>Gem<@$p>). 1983 führte MS seinen <@1 fliess kursiv>Interface Manager<@$p> erstmals öffentlich vor, ausgeliefert wurde er als Windows 1.0 erst zwei Jahre später. Im Monat der Auslieferung unterzeichneten Apple und MS eine Vereinbarung über MSs Verwendung von Apples Copyrights auf das Grafik-Display des Mac. Knapp drei Jahre später verklagt Apple MS wegen Urheberrechtsverletzung in Bezug auf Windows 2.03. Und noch ein Jahr später wird Apple seinerseits verklagt. Xerox behauptete, die grafische Benutzeroberfläche von Lisa und Mac sei eine Kopie des Interface von Xerox’ Star-System. Auf der Intel-Plattform war es gleichsam ein Heimspiel für MS. Es verwendete nicht veröffentliches Wissen über die Arbeitsweise von MS-DOS und MS-Windows, um Wettbewerber auszu<\h>schließen. So gaben frühe Versionen von MS-Windows falsche Fehlermeldungen aus, die suggerierten, dass es inkompatibel zu DR-DOS sei. Selbst mit dem ehemaligen Bündnispartner IBM nahm MS es auf. 1987 brachten MS und IBM die 1.0-Version des gemeinsam entwickelten <@1 fliess kursiv>multitasking<@$p>-fähigen grafischen Betriebssystems OS/2 auf den Markt, das für höherwertige PCs an die Stelle von DOS/Windows treten sollte. 1990 stellen MS und IBM die gemeinsame Arbeit an Betriebssystemen ein. MS konzentrierte sich auf das im selben Jahr vorgelegte Windows 3.0. IBM versuchte noch einige Jahre, OS/2 gegenüber MS-Betriebssystemen zu plazieren.<@3 hoch fliess>44 <@$p>MS änderte 1991 den Namen des Projekts OS/2 v3.0 in »Windows NT«. <*h"Standard">Auf der Grundlage seiner Betriebssysteme DOS, Windows<@3 hoch fliess>45 <@$p>und NT schuf MS neue Formen von <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Bundling.<@$p> Es konnte seine Applikationen wie Word, Excel, SQL, PowerPoint<@3 hoch fliess>46<@$p> und Quicken<@3 hoch fliess>47<@$p> als Standards auf Intel-Rechnern durchsetzen. Kombinationen dieser Anwendungen brachte MS in den Software-Suiten MS-Works (in der ersten DOS-Version 1987 erschienen) und MS-Office auf den Markt, die zumindest suggerierten, zuverlässig untereinander Daten austauschen zu können. Mit Hilfe von proprietären Dateiformaten verhinderte MS den Datenaustausch mit Programmen anderer Hersteller. Zwar schuf die neue Plattform auch Chancen für andere Softwareanbieter, doch bis WordPerfect, Lotus oder Corel ihre Produkte auf die jeweils neueste Version von Windows angepasst hatten, hatte MS den Markt bereits in der Tasche. Ganze Kategorien von Softwarewerkzeugen, wie Dateimanager und Kompressionsprogramme, die zuvor separat angeboten worden waren, integrierte MS in Windows und trocknete so effektiv den Markt für alternative Produkte aus. Diese Strategie der Integration in Windows setzte MS mit Software für Sprach- und Handschrifterkennung und dem Web-Browser »MS-Internet Explorer« fort. Durch ein System von Lehrmaterialien, Schulung, Zertifizierung, Entwicklerwerkzeugen, abgestufter Freigabe von programmierrelevanten Informationen sowie Sonderkonditionen für Universitäten sicherte sich MS die Kontrolle über die Welt der Entwickler, sowohl für den Privat- wie für den Geschäftsbereich.<@3 hoch fliess>48<@$p> Ausgehend von Betriebssystemen und Applikationen erweitert MS sein Imperium u<@1 fliess normal>m Content (z.<\!q>B. m<@$p>it »Encarta«, der ersten Multimedia-Enzyklopädie für den Computer, der Allianz mit dem US-amerikanischen Rundfunknetzwerk NBC und seinen Investitionen in den Hollywoodfilm- und musikproduzenten Dreamworks), um Finanz- und Handelsdienstleistungen (durch Allianzen mit Banken, den Online-Verkauf von Flugtickets, Autos, Nachrichten und Unterhaltung) und um Netzkonnektivität (mit Microsoft Network, dem Aufkauf von WebTV, den Investitionen in die Kabelunternehmen Comcast und US West und Bill Gates’ <\h>Milliardenprojekt eines Satellitennetzes namens Teledesic). Die monopolistischen Praktiken von MS waren immer wieder Gegenstand von Kartellverfahren u.a. des US-Justizministeriums und der Europäischen Kommission. Der Machtwechsel von IBM zu Microsoft stellte eine Verschiebung von Hardware zu Software dar. Hardware wurde zur Massenware, Software wurde zum ersten Mal überhaupt zu einer eigenständigen und zugleich zu einer Massenware. Seither kaufen Privatanwender ebenso wie Firmen ihre Software in eingeschweißten Packungen von der Stange. Im nächsten Schritt, den nun seinerseits MS zu verschlafen schien, eröffnete das Internet ein neues Marktsegment, in dem MS sich neue Gelegenheiten für vertikale Integration und das <@1 fliess kursiv>Bundling<@$p> erschloss. Der Verleger Tim O’Reilly erzählt immer wieder gern die Geschichte von einer computerlosen Freundin, die ihm eines Tages sagte, sie wolle einen Computer kaufen, um bei Amazon online kaufen zu können. Online-Buchbestellung sei eine »Killer-Applikation«, die Leute dazu bringt, sich einen Rechner zuzulegen, ähnlich wie es Tabellenkalkulationsprogramme in den frühen 80er-Jahren waren. O’Reilly begann nach diesem Erlebnis, Amazon und Yahoo als Applikationen zu begreifen, nicht als Websites: »Eine Website war etwas, das man ›publizierte‹, und alles drehte sich um Dokumente, die online gestellt wurden. Aber Applikationen drehen sich um Dinge, die Menschen tun.«<@3 hoch fliess>49<@$p> Er bezeichnet sie nicht als Software-Applikationen, sondern als <@1 fliess normal>Infoware<@1 fliess kursiv>-<@1 fliess normal>Applikationen.<@$p> Natürlich beruht auch Infoware auf Soft- und Hardware, doch der Schwerpunkt liegt nicht auf der Nutzeroberfläche oder der Funktionalität, sondern auf den Inhalten. Das alte Paradigma sei: wenig Information eingebettet in viel Software (in MS Word z. B. ein bisschen Hilfeinformation in einem <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Pull-Down-Menü<@$p>). Nach dem neuen Paradigma ist in vergleichsweise wenig Software (z. B. einigen Perl-Skripten) viel Information eingebettet (z. B. eine riesige Datenbanken, aus der dynamisch Webseiten generiert werden<@3 hoch fliess>50<@$p>). Die herkömmliche aufgedunsene <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Bloatware <@$p>bietet ihren Kunden immer mehr Menüs für ihr Geld. Eine <@1 fliess kursiv>Infoware<@$p>-Anwendung im Sinne O’Reillys manifestiert sich durch nicht mehr als etwa einen »What’s related«-Knopf im Browser oder in der Suchmaschine. Dahinter stehen jedoch aufwändige semantische Modelle und Verknüpfungen, die beständig erneuert werden. »Hier ist Software ein Prozess eher als ein Produkt.«<@3 hoch fliess>51<@$p>Für das neue Infoware-Paradigma steht <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>HTML<@$p> als einfaches Format mit geringer Einstiegshürde durch einen »<@1 fliess kursiv>View Source<@$p>«-Button (Ansicht/Seitenquelltext), der es jedem erlaubt, von den Webseiten anderer zu lernen. Man muss kein Programmierer sein, um Webseiten zu bauen. Daher betrachtet O’Reilly HTML als eine neue Schicht über der Software, die es einer Flut von Menschen ermöglicht, zu Informationsanbietern zu werden. Denselben Effekt wie die Quelloffenheit von HTML haben Skripte in Perl, Python oder Tcl, die sich in der Regel ebenfalls jeder herunterladen, studieren, modifizieren und für eigene Zwecke verwenden kann. Beide sind charakteristisch für den Geist der freien Software-Gemeinde. Umgekehrt sei Microsofts ActiveX bei dem Versuch gescheitert, das Web in ein Softwareprodukt zurückzuverwandeln.<@3 hoch fliess>52 <@$p>Im Internet haben wir heute die Protokolle der unteren Schicht wie TCP/IP, eine mittlere Schicht wie HTTP und darüber XML-basierte Datenaustauschprotokolle. Kritisch für die Zukunft wird sein, wer diese letzte Schicht kontrolliert. Vor dem Hintergrund dieser Entwicklungen der vergangenen 20 Jahre ist das Auftauchen der freien Software umso verblüffender. Doch auch unter proprietären Anbietern setzt sich die Erkenntnis wieder durch, dass Software keinen Produktmarkt darstellt, sondern einen Dienstleistungsmarkt. So erzielen Firmen wie IBM und DEC einen überwiegenden Teil ihrer Einnahmen heute durch Dienstleistungen. Freie Software bietet die ideale Grundlage für eine maßgeschneiderte Anpassung und für die Wiederverwendung von Modulen und deren Weiterentwicklung, ohne das Rad neu erfinden zu müssen. Ohne wirklich selbst akademisch zu sein, beerbt sie die Wissenschaftstradition des freien Austausches.<@3 hoch fliess>53 <@$p>Um dem Phänomen näher zu kommen, wird im Folgenden die von der PC-Welt getrennte Entwicklungslinie der Software für Großrechner und Workstations geschildert, die sich ab Anfang der 90er durch PC-Unixe mit der hier nacherzählten Software-Linie zu überschneiden beginnt. @2  ZÜ 3:Betriebssysteme@1 fliess ohne:Betriebssysteme sind eine Art Mnemotechnik. Im Prinzip könnte ein Programmierer bei jedem Projekt mit der nackten Maschine beginnen und seine eigenen Anweisungen für die Ansteuerung der Datenträger, die Struktur der Dateien oder das Aufleuchten von Pixeln auf dem Schirm schreiben. Genau das geschah auch, bis IBM 1962 einen Satz dieser immer wieder benötigten Routinen zusammenstellte und unter der Bezeichnung »OS/360« veröffentlichte – das erste Betriebssystem war in der Welt. Ein Betriebssystem enthält hardwarenahe Basisoperationen, auf die jedes Anwendungsprogramm zurückgreift. Es stellt also eine infrastrukturelle Schicht über einer Hardware (z. B. der IBM 360er-Serie) dar, auf der ein Programmierer aufsetzt, um eine Statistik- oder Textverarbeitungssoftware zu schreiben. Statt sich um Hardwaremanipulation kümmern zu müssen, kann er sich auf die Symbolmanipulation konzentrieren. Da sein eigener Code an zahlreichen Stellen mit der zu Grunde liegenden Betriebssystemschicht interagiert, muss er wissen, wie diese funktioniert. »Daher ist ein proprietäres, geschlossenes, geheimes Betriebssystem ein Wider<\h>spruch in sich. Es läuft dem eigentlichen Sinn eines Betriebssystems zuwider« (<@6 Caps>Stephenson<@$p>, 1999). @1 fliess mit:Mit dem Aufkommen eines Massenmarktes für <@1 fliess kursiv>Personal Computers <@$p>trat neben die Programmierer eine neue Gruppe von reinen Anwendern. Die beiden Gruppen haben eine sehr unterschiedliche Sicht auf Computer und Software. Betriebssysteme sind die Arbeitsumgebungen der Programmierer, die daher zugänglich, flexibel und offen sein müssen. Auch für Anwender ist Software eine Arbeitsumgebung, doch Gegenstand ihrer Arbeit ist nicht die Software selbst, sondern Texte, Bilder oder Musik. Das Betriebssystem läuft für sie möglichst unsichtbar im Hintergrund. Es ist die Voraussetzung für die Ausführung von Applikationen. Nur wenn etwas schief geht, tritt es in den Vordergrund. @1 fliesskursiv Zitat:»Es liegt in der Natur von Betriebssystemen, dass es keinen Sinn macht, dass sie von einer bestimmten Firma entwickelt werden, deren Eigentum sie dann sind. Es ist sowieso ein undankbarer Job. Applikationen schaffen Möglichkeiten für Millionen von leichtgläubigen Nutzern, wohingegen Betriebssysteme Tausenden von missmutigen Codern Beschränkungen auferlegen. Daher werden die Betriebssystemhersteller für immer auf der Hassliste aller stehen, die in der Hightech-Welt etwas zählen. Applikationen werden von Leuten benutzt, deren großes Problem es ist, alle ihre Funktionen zu begreifen, wohingegen auf Betriebssystemen Programmierer hacken, die von deren Beschränkungen genervt sind« <f"FFScala">(ebd.).@1 fliess mit:@2  ZÜ 3:Unix@1 fliess ohne:Da Unix – das Betriebssystem und die Kultur seiner Entwickler und Anwender – im Folgenden eine zentrale Rolle spielen wird, sei hier kurz auf die Zufälle und Wendungen seiner Geschichte eingegangen. @1 fliess mit:<t-1>Unix war eine Reaktion auf das ab 1964 gemeinsam vom MIT, General Electric und AT&T entwickelte Betriebssystem »Multics«<x@3 hoch fliess><t-1>54<@$p><t-1>. Nachdem der allumfassende Anspruch des Multics-Projekts gescheitert war, zog sich AT&T 1969 aus der Kooperation zurück. Als Ken Thompson von den AT&T Bell Laboratories daraufhin einen neuen Versuch startete, setzte er auf seinen Erfahrungen mit Multics auf, wählte aber einen entgegengesetzten Design-Ansatz. Statt alles für alle zu sein, sollte Unix aus kleinen, einfachen, effizienten Werkzeugen bestehen, die miteinander kombiniert werden können, um komplexere Aufgaben zu lösen. Es sollte auf verschiedenen Plattformen einsetzbar sein, da AT&T Rechner verschiedener Hersteller im Einsatz hatte. Und es sollte <I>Time-Sharing<$> unterstützen, also eine interaktive Computernutzung, statt, wie damals üblich, eine Verarbeitung von Lochkartenstapeln, die Tage dauern konnte. Der <I>Time-Sharing<$>-Betrieb, bei dem mehrere Nutzer gleichzeitig in nahezu Echtzeit auf einem Großrechner arbeiten können, wurde Anfang der 60er-Jahre erstmals im Betriebssystem CTSS (<I>Compatible Time-Sharing System<$>) des MIT implementiert und im Multics-Projekt weiterentwickelt. Dennis Ritchie, der Co-Autor von Unix, schrieb: »Wir wollten nicht nur ein gutes Programmierumfeld bewahren, in dem programmiert wird, sondern ein Sys<\h>tem, um das herum sich eine Zusammengehörigkeit bilden kann. Aus <\h>Erfahrung wussten wir, dass das Wesen der gemeinschaftlichen Computernutzung, wie sie fernnutzbare <x@1 fliess kursiv><t-1>Time-Shared<@$p><t-1>-Computer bieten, nicht darin liegt, Programme in einen Terminal einzutippen, statt in einen Kartenlocher, sondern darin, eine dichte Kommunikation unter den Beteiligten zu ermuntern« (<x@6 Caps><t-1>Hauben/ Hauben<@$p><t-1>, Kapitel 9).<t$>Um eine Portierbarkeit auf andere Rechner zu erreichen, wurde Unix 1971 in der Programmiersprache C neu geschrieben. Damit führte es das Konzept der <@1 fliess kursiv>Source Code<@$p>-Kompatibilität ein. Der Objektcode eines Programms lässt sich nicht von einem Unix-System auf ein anderes übertragen, doch der Quellcode kann auf dem Zielsystem zu einer ablauffähigen Version kompiliert werden.<@3 hoch fliess>55 <@$p>Im selben Jahr machte AT&T Unix offiziell zum internen Standardbetriebssystem.Als staatlich reguliertem Telefonmonopol war es AT&T untersagt, sich in anderen Wirtschaftsbereichen zu engagieren. Es war also aus kartellrechtlichen Gründen nicht in der Lage, Unix regulär zu vermarkten. Stattdessen gaben die Bell Labs den Unix-Quellcode zum Selbstkos<\h>tenpreis von etwa 50 Dollar, ähnlich den heutigen GNU/Linux-Distributionen, an Universitäten ab. Die Kompaktheit, Modularität und Zugänglichkeit durch C ermunterte viele Benutzer, eigene Entwicklungen durchzuführen, so dass Unix schnell einen hohen Reifegrad erreichte. »Dies ist deshalb bemerkenswert, da kein Entwicklungsauftrag hinter diesem Prozess stand und die starke Verbreitung von Unix nicht auf den Vertrieb oder die Werbung eines Herstellers, sondern primär auf das Benutzerinteresse zurückzuführen ist« (<@6 Caps>Gulbin/Obermayr<@$p>, 1995, S. 7).Um die Wartungsarbeiten auf entfernten AT&T-Rechnern zu erleichtern, wurde an den Bell Labs ein Verfahren zur automatischen Herstellung von Telefonverbindungen und Übertragung von Software entwickelt. <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>UUCP<@$p> (<@1 fliess kursiv>Unix to Unix Copy<@$p>) wurde als Teil der Unix Version 7 (1978) ausgeliefert. Da AT&T für ihr Unix keinerlei Support anbot, blieb den Nutzern gar nichts anderes übrig, als sich zu einer Community zusammenzufinden. Diese kommunizierte bereits mit Hilfe von Newslettern und Konferenzen, doch mit UUCP bekam sie ein Medium innerhalb der Unix-Umgebung selbst zur Verfügung, noch bevor die meisten Universitäten Zugang zum ARPANET hatten. Dateiübertragung, E-Mail und Netnews, die bereits vorher zur Bildung von lokalen Communities um jeweils einen interaktiven, <@1 fliess kursiv>Multiuser-Time-Sharing<@$p>-Rechner geführt hatten, konnten jetzt eine weltweite Community potenziell aller Unix-Nutzer unterstützen. Elektronische »schwarze Bretter« dienten bereits Dutzenden oder Hunderten von Nutzern einer einzelnen z.<\!q>B. PDP-11 (ein damals weitverbreiteter Großrechner der Digital Equipment Corporation (DEC)) zum Austausch nicht nur über computerbezogene Themen. 1979 lud Thompson einen Doktoranten an der Duke Universität und ebenfalls leidenschaftlichen Computerschachspieler, Tom Truscott, zu einem Sommerjob in die Bell Labs ein. Nachdem Truscott aus dem Unix-Himmel an die Duke Universität zurückgekehrt war, begann er im Herbst des Jahres, die Usenetsoftware zu schreiben. Durch den <@1 fliess kursiv>Polling<@$p>-Mechanismus von UUCP erlaubt sie einem Rechner, zu einer festgelegten gebührengünstigen Tageszeit einen anderen Rechner anzurufen und die neuesten Nachrichten auszutauschen. Nach einigen solcher Verbindungen waren alle neuen Nachrichten auf allen an das Usenet angeschlossenen Rechnern<@3 hoch fliess>56<@$p> angekommen. Bald wurde an der Berkeley Universität ein <@1 fliess kursiv>Gateway<@$p> zu den Mailinglisten des ARPANET eingerichtet. In kürzester Zeit hatten Universitäten, Unternehmen (DEC, Microsoft, Intel usw.) sowie Forschungseinrichtungen Zugang zum Usenet.<@3 hoch fliess>57<@$p>In den Newsgroups wurde über Schach, Sciencefiction und das allgegenwärtige »Weltnetz« der Zukunft debattiert, vor allem aber war das Usenet das Medium, in dem die Mitglieder der Unix-Community sich gegenseitig unterstützten. Hier baten Nutzer um Hilfe und boten ihre Erfahrungen und ihre Programme an, so dass andere darauf aufbauen konnten, ohne das Rad immer wieder neu erfinden zu müssen: @1 fliesskursiv Zitat:»Oft schauen sich die Leute den Code der anderen an, kommentieren  ihn persönlich oder durch Interuser-Kommunikation und verwenden Teile davon für ihre eigenen Zwecke. Die Vorstellung von Programmierteams und egolosem Programmieren passt gut in die <t-1>Unix-Philosophie, da sie das (Mit)Teilen dem Alleingang vorziehen <\h>lässt. ... Der Code, den die Leute sehen, dann aufnehmen und imitieren, ist gewöhnlich gut strukturiert. Durch Nachahmung und sofortiges Feed<\h>back erlernen sie das Kodieren ebenso, wie sie ihre Muttersprache lernen.«<x@3 hoch fliess><t-1>58<@$p>@1 fliess mit:@2  ZÜ 3:Berkeley Unix@1 fliess ohne:Die Bell-Forscher um Thompson waren sehr bereitwillig, ihre Wissen in den Newsgroups mit der Unix-Community zu teilen. Gelegentlich verbreiteten sie auch Datenbänder mit <@1 fliess kursiv>Fixes<@$p>, Verbesserungen und Zusätzen, doch ihre Hauptaufgabe war es, Unix für den Gebrauch innerhalb von AT&T weiterzuentwickeln. Zum Zentrum der akademischen Forschung dagegen wurde Thompsons Alma Mater, die Universität von Kalifornien in Berkeley, die 1974 ihr erstes Unix auf einer neuen PDP-11 installiert hatte. Da es vergleichsweise leicht zu studieren und zu lehren war und noch klein genug, um von einem Einzelnen überschaut zu werden,<@3 hoch fliess>59 <@$p>wurde es unter den Studenten schnell populär. Einige ihrer Programme, wie die Verbesserungen am Pascal-Compiler und der Editor »ex«, waren sehr gefragt, so dass ein Doktorant namens Bill Joy 1977 die erste <@1 fliess kursiv>Berkeley Software Distribution <@$p>(BSD) zusammenstellte, von der im Laufe des <t-3>Jahres etwa 30 freie Kopien verschickt wurden (vgl. McKusick, 1999, S. 33 ff.). <t$>@1 fliess mit:<t-1>Joy entwickelte »ex« weiter zu »vi«, integrierte die Reaktionen auf das Pascal-System und stellte sie in 2.11BSD (1978) zu einer vollständigen Unix-Distribution zusammen. Berkeley steuerte selbst zahlreiche weitere Innovationen und Portierungen auf neue Hardware bei und übernahm die Sammlung der Unix-Erweiterungen aus der akademischen Community, während parallel dazu AT&T mit wachsendem Nachdruck auf <\h>stabile kommerzielle <x@1 fliess kursiv><t-1>Releases<@$p><t-1> an der eigenen Version von Unix weiterarbeitete.<t$>In dieser Zeit suchte das ARPANET-Projekt nach einer Möglichkeit, die Computerumgebung des Netzes zu vereinheitlichen. Von allen Knotenbetreibern zu verlangen, dass sie dieselben Rechner anschafften, war ausgeschlossen, also entschied man, die Vereinheitlichung auf der Ebene des Betriebssystems vorzunehmen. Durch seine nachgewiesen einfache Portierbarkeit fiel die Wahl auf Unix. 1980 gelang es der Berkeley Universität, einen Forschungsauftrag der ARPA zu erhalten, um die <@1 fliess normal>BSD<@1 fliess kursiv> <@$p>nach den Anforderungen der ARPANET-Gemeinde weiterzuentwickeln. Bill Joy wurde zum Projektleiter.<@3 hoch fliess>60 <@$p>Die Anwälte von AT&T und der Universität erarbeiteten eine Lizenz, mit der alle Seiten leben konnten. Zu den wichtigsten Berkeley-Innovationen dieser Periode gehören ein schnelleres Dateisystem, ein Mechanismus für die Kommunikation zwischen Prozessen, der verteilte Systeme möglich machte, und vor allem die Integration der ARPANET-Protokollfamilie TCP/IP in das BSD-Unix. Die neue stabile und dokumentierte Version wurde im August 1983 als 4.2BSD herausgegeben.@1 fliesskursiv Zitat: »Die Popularität von 4.2BSD war beeindruckend; in achtzehn Monaten wurden mehr als 1 000 Site-Lizenzen ausgestellt. Somit waren mehr Exemplare der 4.2 BSD ausgeliefert worden, als von allen vorherigen Berkeley Software-Distributionen zusammen. Die meisten Unix-Anbieter lieferten ihre Systeme mit einem 4.2BSD aus, statt mit dem kommerziellen Sys<\h>tem V von AT&T. Der Grund dafür war, dass das System V weder Netzwerkfähigkeit noch das Berkeley Fast File System bieten konnte« (McKusick, 1999, S. 38).@1 fliess mit:@1 fliess ohne:Im Jahr darauf wurde AT&T aufgespalten.<@3 hoch fliess>61<@$p> Mit dem Ende des Telefonmonopols konnte das Unternehmen Unix deutlicher als kommerzielles Produkt vermarkten, als dies ohnehin seit einigen Jahren der Fall gewesen war. Zu diesem Zweck gründete es das Tochterunternehmen Unix System Laboratories (USL).<@3 hoch fliess>62 <@$p>Die Firma bot Schulungen, Support, Wartu<t1>ng und Dokumentation an, machte Werbung für Unix und veränderte die Lizenzen. Auch vorher schon hatten andere Hersteller den von AT&T lizenzierten Unix-Code auf bestimmte Hardwareplattformen oder Anwendungsgebiete hin optimiert und ihre eigenen Unix-Versionen vermarktet. Neben AT&Ts und BSDs Unix entstanden weitere untereinander zunehmend inkompatible Verzweigungen der Code-Basis. AT&T hatte versucht, mit System V.0 (1983) die Unix-Welt auf einen einheitlichen Standard festzulegen, und erneut im System V.4 (1990) die divergierenden Hauptlinien in einem einzigen Unix zu integrieren. Zu den heutigen proprietären, kommerziellen Unixen, die meist von USLs Unix V.4 ausgehen, gehören AIX (IBM), HP/UX (Hewlett Packard), SCO-Unix (Santa Cruz Operation), Sinix (Siemens), Sun/OS und Solaris (Sun), Unixware (Novell), Ultrix und OSF/1 (DEC). Nach dem Konvergenzpunkt von V.4 laufen die Entwicklungslinien aufgrund von lizenztechnischen Mechanismen und der vermeintlichen Marktnotwendigkeit, sich durch proprietäre Zusätze von den Konkurrenten zu unterscheiden, wieder auseinander. Die Quellcodekompatibilität geht verloren. Anwendungshersteller müssen Versionen für die einzelnen Unixvarianten anbieten.<t$>@1 fliess mit:<*h"mehr">In der BSD-Welt führte der wachsende Lizenzdruck zu einer Befreiung des Code. Bis 1988 mussten alle Nutzer von BSD, das immer mit sämtlichen Quellen ausgeliefert wurde, gleichzeitig eine AT&T-Quell<\h>codelizenz erwerben – der Preis für eine solche Lizenz stieg nun kontinuierlich. Händler, die BSD-Code verwenden wollten, um TCP/IP-Produkte für den PC-Markt zu entwickeln, baten die Berkeley Universität, die Netzwerkelemente aus BSD separat unter einer freieren Lizenz anzubieten. Da der TCP/IP-Netzwerkcode vollständig an der Berkeley Universität entwickelt worden war, konnte sie ihn zusammen mit den umgebenden Werkzeugen 1989 im »Networking Release 1« als Berkeleys ersten frei weitergebbaren Code veröffentlichen. Die BSD-Lizenz erlaubte es, den Code in Quell- und Binärform, modifziert und unmodifiziert ohne Gebühren frei zu verbreiten, solange er den Urheberrechtsvermerk der Berkeley Universität enthielt. Berkeley verkaufte die Datenbänder für 1 000 Dollar, doch in kürzester Zeit stand der Code auch auf anonymen ftp-Servern zur Verfügung.<*h"Standard">Aufgrund des Erfolgs des »Networking Release 1« wollte Keith Bostic von der Berkeley-Gruppe weitere Bestandteile des BSD-Unix freigeben. Seine Kollegen waren skeptisch, da es bedeutet hätte, Hunderte von Werkzeugen, die riesige C-Bibliothek und den <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Kernel<@$p> auf AT&T-Code hin zu durchforsten und diesen zu ersetzen.@1 fliesskursiv Zitat:»Unverdrossen erfand Bostic das Verfahren einer hochgradig parallelen, netzbasierten Entwicklungsanstrengung. Er warb Leute ein, die die Unix-Dienstprogramme <@1 fliess normal>(Utilities)<@$p> gestützt auf nichts als ihre veröffentlichte Beschreibung von Grund auf neu schrieben. Ihre einzige Entschädigung dafür war, dass ihr Name unter den Berkeley-kontributoren neben dem Namen des Programms, das sie neu geschrieben hatten, aufgelistet wurde. ... Innerhalb von 18 Monaten waren fast alle wichtigen Dienstprogramme und Bibliotheken neu geschrieben« <@1 fliess normal>(<@6 Caps>McKusick<@1 fliess normal>, 1999, S. 42)<@$p><f"FFScala">.@1 fliess ohne:Mike Karels, Bostic und McKusick verbrachten daraufhin die nächsten Monate damit, auch die Kernel-Dateien von AT&T-Codeteilen zu befreien, was ihnen mit Ausnahme von sechs Dateien gelang. Das Ergebnis wurde 1991 als »Networking Release 2« unter derselben Lizenz wie »Release 1« veröffentlicht. Weitere sechs Monate später hatte Bill Jolitz auch die ausstehenden sechs Kernel-Dateien ersetzt. Das jetzt voll funktionstüchtige freie Betriebssystem, kompiliert für den 386er PC, stellte <\n>Jolitz Anfang 1992 unter dem Namen 386/BSD ins Netz.@1 fliess mit:Das Unix auf dem PC wurde begeistert aufgenommen und bald setzte eine Flut von <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Bug Fixes <@$p>und Erweiterungen ein, die Jolitz nicht mehr alleine bewältigen konnte. So bildeten die regesten Nutzer die NetBSD-Gruppe, die das System unterhielt und weiterentwickelte. Auf NetBSD und die Varianten FreeBSD und OpenBSD wird später im Rahmen der freien Softwareprojekte näher eingegangen. Zusätzlich zu den freien Gruppen bildet sich auf Grundlage des »Networking Release 2« die Firma <@1 fliess normal>Berkeley Software Design, Inc.<@1 fliess kursiv> <@$p>(BSDI). Auch sie programmierte die fehlenden sechs Kernel-Dateien nach und verkaufte 1992 ihre kommerziell unterstützte Version einschließlich Quellcode für 995 Dollar. Der Preis für Unix System V von AT&Ts USL inklusive Quellcode lag zu diesem Zeitpunkt bereits bei 100<\!q>000 Dollar. USL strengte daraufhin eine Klage gegen BSDI an, damit diese für ihr Produkt nicht länger den markenrechtlich geschützten Namen Unix verwendete, da es proprietären USL-Code und Handelsgeheimisse enthalte. Nach einem zweijährigen Prozess, in den sich auch die Berkeley Universität mit einer Gegenklage einschaltete und in dessen Verlauf USL von Novell aufgekauft wurde, mussten schließlich drei der 18<\!q>000 Dateien aus dem »Networking Release 2« entfernt und einige andere geringfügig geändert werden (<@6 Caps>McKusick<@$p>, 1999, S. 44 ff.). Die derart gesegnete Version wurde 1994 unter dem Namen 4.4BSD-Lite veröffentlicht. Sie wurde weiter gepflegt, bis die <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Bug Reports<@$p> und Erweiterungen versiegten. Die letzten Änderungen wurden im Juni 1995 als 4.4BSD-Lite, Release 2 vorgelegt. Die Forschungsgruppe an der Berkeley Universität wurde aufgelöst. Nachdem sie fast 20 Jahre lang die freie akademische Entwicklung von Unix getragen hatte, war es Zeit für andere, die Stafette zu übernehmen.@2  ZÜ 3:Das GNU-Projekt@1 fliess ohne:<t-1>Als Richard Stallman 1971 seine akademische Laufbahn am Labor für Künstliche Intelligenz (KI) des MIT aufnahm, gab es noch keine ›unfreie‹ Software, nur autoritärere und freiere Informatikinstitute. Harvard, wo Stallman als Experte für Assemblersprachen, Betriebssysteme und Text<\h>editoren gearbeitet hatte, gehörte zur ersten Kategorie. Auf der Suche nach einer hacker-freundlicheren Atmosphäre wechselte er dann als Systemprogrammierer an das MIT, dessen KI-Labor ein damals bereits legendäres Hackerparadies war, ein Kloster, in dem man lebte, um zu hacken und in dem man hackte, um zu leben. Steven Levys 1984 geschriebener Klassiker »Hackers. Heroes of the Computer Revolution« (<x@6 Caps><t-1>Levy<@$p><t-1>, 1994) verfolgt das Phänomen zurück bis in den Modelleisenbahnclub am MIT der späten 50er. Im Club gab es zwei Fraktionen, eine, die es liebte, Modellhäuser, Landschaften und Replikas historischer Züge zu bauen – heute würde man sie die Interface-Designer nennen. Die andere Fraktion verbrachte die meiste Zeit mit dem Steuerungssystem unter der Platte, mit der Stromversorgung, den Kabeln und elektromagnetischen Relais, die sie von einem Telefonhersteller bekommen hatten. Diese zweite Gruppe strebte nach Höherem, doch der zentrale MIT-Rechner, eine IBM 704, die Lochkartenstapel verarbeitete, war von der Computerpriesterschaft abgeschirmt. Als das MIT 1959 einen der ersten transistor-betriebenen Rechner der Welt bekam, der außerdem mit einem Kathodenstrahlmonitor ausgestattet war, verloren sie bald das Interesse an Modelleisenbahnen. Die TX-0 des Lincoln Labs war ein Zwerg im Vergleich zur 704, doch auf ihr bekam man Zeitabschnitte zugewiesen, in denen man sie exklusiv für sich benutzen konnte. Zum ersten Mal konnte man am Computer sitzen, während dieser ein Programm durchrechnete, und auf der Stelle neue Anweisungen in die Tastatur hacken. Während bislang und auch später beim »strukturierten Programmieren« der größte Teil des Softwareentwurfs abstrakt auf Papier stattfand, war es mit der neuen »interaktiven« Computernutzung möglich, eine Idee in die Tasten zu hacken, das Programm laufen zu lassen, Fehler zu entdecken, die Korrekturen einzugeben und es sofort wieder ablaufen zu lassen. Diese Art der iterativen Ad-hoc-Programmierung trug den Namen »Hacken« (<x@6 Caps><t-1>Levy<@$p><t-1>, 1994, S.<\!q>21 ff.).@1 fliess mit:Was Stallman am KI-Lab mochte, war, dass es »keine künstlichen Hindernisse, Dinge, auf denen bestanden wird, die es den Menschen schwerer machen, ihre Arbeit zu erledigen – Dinge, wie Bürokratie, Sicherheit oder die Weigerung mit anderen Leuten zu teilen«, gab (ebd., S. 416). Dort traf Stallman auf Hackerlegenden wie Richard Greenblatt und Bill Gosper und tauchte in eine Kultur des freien Wissensaustausches ein, eine Oase der konstruktiven Kooperation im allgemeinen Kampf von jedem gegen jeden. @1 fliesskursiv Zitat:»Ich hatte in den 70er-Jahren das Glück, Teil einer Gemeinschaft zu sein, in der die Menschen Software miteinander teilten. Wir entwickelten Software und wann immer jemand ein interessantes Programm geschrieben hatte, wurde es weitergegeben. [...] So arbeitete einer nach dem anderen, um die Software zu verbessern und weiterzuentwickeln. Man konnte in dieser Gemeinschaft immer eine zumindest passive Mitarbeit eines jeden erwarten. Sie mochten zwar nicht bereit sein, ihre Arbeit zu unterbrechen, um stundenlang etwas für dich zu tun, aber das, was sie bereits erledigt hatten, konntest du gerne benutzen.«<@3 hoch fliess>63<@$p>@1 fliess mit:@1 fliess ohne:Neben seiner Arbeit als Systementwickler und am Editor »Emacs« erwarb er gleichzeitig einen <@1 fliess kursiv>Magna cum laude<@$p>-Abschluss in Physik an der Harvard Universität. Emacs, das »Schweizermesser« unter den Editoren, war damals Stallmans bekanntestes Werk. Basierend auf einem Lisp-Dialekt, ist es beliebig konfigurierbar und erweiterbar. Seine weit offene Architektur ermunterte viele, Zusätze und Verbesserungen zu schreiben. Stallman betrieb das Projekt Emacs im selben <@1 fliess kursiv>sharing spirit<@$p>, den er am KI-Lab schätzte. Er gab das Programm frei an jeden weiter, unter der Bedingung, dass alle, die Erweiterungen schrieben, diese mit der Emacs-Community teilten.@1 fliess mit:In den ausgehenden 70ern und frühen 80ern erlebte Stallman jedoch auch den Verfall der Hackerethik und die Auslöschung der Gemeinde am KI-Lab mit. Es begann damit, dass auf den Systemen des MIT-Rechenzentrums Passwörter eingeführt wurden. Als echter Hacker verachtete Stallman Passwörter. Die Rechner, die er am KI-Lab administrierte, hielt er frei davon und auf den anderen führte er einen Feldzug zur Durchsetzung eines »leeren« Passwortes, also der schlichten Betätigung der Eingabetaste, bis schließlich das US-Verteidigungsministerium drohte, das KI-Lab vom ARPAnet abzuhängen. In einem zunehmend wichtigeren Netz ginge es nicht an, dass jedermann, ohne eine Legitimation vorzuweisen, durch diese weit offene Tür spazieren und sich in militärischen Rechnern tummeln könne. Stallman und andere waren jedoch der Ansicht, dass es genau so sein sollte. Doch der Schließungsdruck wurde stärker: Nach und nach verließen die Hacker der ersten und zweiten Generation das MIT, um für Computerfirmen zu arbeiten, eigene zu gründen oder gar zu heiraten.Die nachwachsende Generation, die Stallman jetzt als »Touristen« auf seinen Lab-Rechnern beobachtete, war nicht in die Hackerethik hineingewachsen. Nicht alle sahen offene Systeme als Chance, um Gutes zu tun und zu lernen, um selbst einmal den Titel eines »echten Hackers« zu erwerben. Viele von ihnen sahen auch nichts Verkehrtes in der Idee eines Eigentums an Programmen. Wie ihre Vorgänger schrieben sie aufregende Software, doch immer häufiger tauchte beim Starten ein Copyright-Vermerk auf dem Schirm auf. Gegen diese Form der Schließung des freien Austauschs kämpft Stallman bis heute: »Ich finde nicht, dass Software Eigentum sein sollte«, zitiert Steven Levy ihn im Jahr 1983, »weil mit dieser Praxis die Menschlichkeit im Ganzen sabotiert wird. Sie verhindert, dass die Menschen aus einem Programm den maximalen Nutzen ziehen.«<@3 hoch fliess>64<@$p>Die zunehmende Kommerzialisierung war auch der Grund für das Ende der Hackerkultur am KI-Lab. Seit 1975 hatte Richard Greenblatt <\h>zusammen mit einigen anderen an einem Hackertraum, der LISP-Maschine, gearbeitet, einem Computer, dessen Architektur speziell auf die Anforderungen dieser mächtigsten und flexibelsten, aber auch ressourcenhungrigen Programmiersprache zugeschnitten war. Nach jahrelanger Entwicklungsarbeit hatten die MIT-KI’ler schließlich 32 LISP-Maschinen gebaut. Greenblatt begann seine fortgeschrittene Technologie als Element in Amerikas Kampf mit Japan um die Führung im Bereich der Künstlichen Intelligenz zu sehen. Dieses Potenzial sah er am besten durch ihre Verbreitung durch den kommerziellen Sektor verwirklicht. Greenblatt wollte eine Firma gründen. Aus den Auseinandersetzungen mit seinen Hackerkollegen am MIT und externen Beratern ging erst LISP Machine Incorporated (LMI) sowie ein knappes Jahr später die hochkapitalisierte Firma Symbolics hervor. Die beiden Firmen warben fast alle verbliebenen Hacker vom KI-Lab ab. An die Stelle des <@1 fliess kursiv>sharing spirit <@$p>war eine Konkurrenz um Marktanteile für dasselbe Produkt getreten, mit all ihren kommunikationswidrigen Umständen, wie Vertraulichkeitsvereinbarungen (<@1 fliess kursiv>Nondisclosure Agreements<@$p>, <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>NDA<@$p>) und geschlossener Quellcode.@1 fliesskursiv Zitat:»Sie konnten nicht über das sprechen, was am Wichtigsten war – die Magie, die sie entdeckt und in die Computersysteme eingebracht hatten. Die Magie war nun ein Industriegeheimnis. Dadurch, dass die Mitglieder der puristischen Hackergemeinschaften im Auftrag von Firmen arbeiteten, hatten sie das Schlüsselelement der Hackerethik fallengelassen: den freien Informationsfluss« <@1 fliess normal>(<@6 Caps>Levy<@1 fliess normal>, 1994, S. 424).<@$p>@1 fliess mit:@1 fliess ohne:Als »letzter Überlebender einer toten Kultur«,<@3 hoch fliess>65<@$p> wie er sich selbst bezeichnete, blieb Richard Stallman zurück. Das lebende Beispiel dafür, dass eine »anarchistische und großartige Einrichtung« möglich ist, war ausgelöscht. @1 fliesskursiv Zitat:»Wenn ich Leuten erzählte, dass es möglich ist, auf einem Computer keinerlei Sicherheit zu haben, ohne dass andere ständig deine Dateien löschen, und kein Vorgesetzter dich davon abhält, Dinge auszuprobieren, konnte ich zumindest auf das KI-Labor verweisen und sagen: ›Schau, wir machen das. Komm und nutze unseren Rechner! Sieh selbst!‹ Jetzt kann ich das nicht mehr. Ohne dieses Beispiel wird mir niemand glauben. Eine Zeit lang haben wir ein Vorbild für den Rest der Welt gesetzt. Wo soll ich jetzt, da es das nicht mehr gibt, wieder anfangen?«<@3 hoch fliess>66<@$p>@1 fliess mit:@1 fliess ohne:Steven Levys Buch »Hackers« von 1984 endet auf einer positiven Note: Das Hackerzentrum am MIT war verschwunden, doch sein Geist – so Levys Fazit – habe sich mit dem persönlichen Computer allgemein verbreitet. Millionen von Menschen wurden der Magie ausgesetzt. Die Hacker<\h>ethik war vielleicht nicht mehr so rein wie in den Jahren der »Priesterschaft«, und tatsächlich hatten die Hacker der dritten Generation ihre eigenen Vorstellungen, doch weiterhin bietet jeder PC die Chance, die Magie zu entdecken, die Kreativität anzustacheln und – ein Hacker zu werden.@1 fliess mit:Doch bevor sich Levys Prognose machtvoll beweisen sollte, trat die Computerwelt in eine dunkle Phase ein. Anfang der 80er war fast alle Software proprietär. Den Begriff »Hacker« hatte die Presse inzwischen zu »Computer-Einbrecher« verkehrt. Seit 1981 kann in den USA Software, die bis dato als Algorithmen oder mathematische Formeln und damit als unschützbar angesehen wurde, zum Patent angemeldet werden. DEC stellte seine PDP-10-Serie ein, für die die KI-Lab-Hacker das freie <@1 fliess kursiv>Incompatible Timesharing System<@$p> (ITS) geschrieben hatten – unter den Betriebssystemen der bevorzugte Tummelplatz für Hacker. Die neue Generation von Rechnern, wie die VAX oder der 68020 von Motorola, kamen mit ihren eigenen proprietären Betriebssystemen. Die durch die Aufspaltung von AT&T 1984 einsetzende Privatisierung und Schließung von Unix verhinderte zwar nicht, dass die Hacker an freien Unixvarianten weiterarbeiten konnten, doch sie machte viele der zahllosen Informatiker in der ganzen Welt, die an dem Betriebssystem mitentwickelt hatten, wütend. Als Schlüsselerlebnis nennt Stallman eine Episode Anfang der 80er um einen Xerox-Netzwerkdrucker am MIT. Es kam regelmäßig vor, dass jemand einen Druckauftrag abschickte und einige Zeit später in den Raum ging, wo der Drucker stand, nur um festzustellen, dass sich das Papier gestaut hatte oder ausgegangen war. Stallman wollte nun eine Funktion einfügen, die den Druckerstatus direkt am Arbeitsplatz anzeigte. Er fand auch jemanden bei Xerox, der den Quellcode des Druckertreibers hatte, doch weigerte dieser sich, ihn herauszugeben, da er sich zu Nichtweitergabe verpflichtet hatte (z.<\!q>B. Stallman in Hohmann, 1999). In dieser Erfahrung verdichtete sich der neue Geist der Zeit: Ein praktisches Problem stellt sich. Die Lösung besteht darin, eine bestehende Software um eine Funktion zu erweitern. Früher hätte man den Autor der Software um den Quellcode gebeten und hätte diesen fortgeschrieben – die Technologie wäre für alle Betroffenen nützlicher geworden. Doch jetzt stand vor dem Quellcode und vor einer Kooperation von Programmieren eine Mauer namens »geistiges Eigentum«. Eine Lösung war damit nicht unmöglich geworden, doch die Firma zu bitten, die Funktion zu implementieren und es im nächsten <@1 fliess kursiv>Update <@$p>zu verbreiten, ist langwierig und unsicher, und eine Dekompilierung ist mühsam, zeitraubend und nur in engen Grenzen legal. Stallman fragte sich also, was er tun könne, um erneut die Voraussetzungen für eine Gemeinschaft zu schaffen. Um einen Computer zu betreiben, benötigt man zu allererst ein Betriebssystem. Betriebssysteme waren eines von Stallmans Spezialgebieten. Also startete er 1984 das GNU-Projekt. Das rekursive Akronym steht für »GNU’s not Unix«, doch genau das war sein Ziel: ein Betriebssystem zu schreiben, das funktional äquivalent zu Unix ist, aber keine einzige Zeile von AT&T geschütztem Code enthält und vor allem, das in freier Kooperation weiterentwickelt werden kann, ohne irgendwann dasselbe Schicksal zu erleiden wie Unix. Die Wahl fiel auf Unix und nicht ein anderes Betriebssystem, weil es sich bewährt hatte, weil es portabel war und weil es bereits eine aktive weltweite Unix-Gemeinde gab, die durch seine Kompatibilität leicht zu GNU wechseln konnte. Stallman kündigte seinen Job am MIT, weil seine Arbeit als Angestellter der Universität gehören würde,<@3 hoch fliess>67<@$p> die damit die Vertriebsbedingungen seiner Software bestimmen konnte. Er wollte verhindern, dass er erneut eine Menge Arbeit investierte, nur um hilflos zuzusehen, wie das MIT ein proprietäres Softwarepaket daraus machte. Er rechnet es dem damaligen Leiter des KI-Labs hoch an, dass er ihn einlud, auch nach seiner Kündigung die Einrichtungen des Labors zu benutzen.<@3 hoch fliess>68<@$p>Im September 1983 kündigte Stallman in Unix-Newsgroups sein Projekt einer »neuen Unix-Implementation« an und lud zur Mitarbeit ein.<@3 hoch fliess>69<@$p> Er startete, zunächst noch allein, mit dem GNU C-Compiler (GCC) und seinem Editor GNU Emacs: @1 fliesskursiv Zitat:»So fingen wir an, die Komponenten dieses Systems zu schreiben. Die Struktur dieses Systems besteht aus vielen einzelnen Programmen, die miteinander kommunizieren; und es war dokumentiert, so dass man nachvollziehen konnte, wie die Schnittstellen zwischen diesen Programmen aussahen. Wir konnten nach und nach alle Bestandteile als Ersatz für die entsprechenden Teile eines Unix-Systems schreiben und schließlich testen. Wenn alle Teile ersetzt sind, fügt man diese zusammen und erhält so das vollständige System. Und das taten wir. Es war eine sehr dezentralisierte Vorgehensweise, was zu einer eher amorphen und dezentralisierten Community von Freiwilligen, die überall in der Welt hauptsächlich über E-mail kommunizierten, passte.«<@3 hoch fliess>70<@$p>@1 fliess ohne:Sein ursprüngliches Emacs für die PDP-10 hatte er auf einem anonymen ftp-Server verfügbar gemacht und alternativ Interessierten angeboten, ihm einen frankierten Rückumschlag und ein leeres Datenband zu schicken, auf das er dann die Software spielte. Da Stallman kein Einkommen mehr hatte, bot er – neben der weiterhin kostenlosen ftp-Distribution – jetzt ein Band mit Emacs für 150 Dollar an. Als das Interesse an Emacs wuchs, mussten neue Finanzierungsmöglichkeiten erschlossen werden. Zu diesem Zweck wurde 1985 die gemeinnützige <@1 fliess kursiv>Free Software Foundation <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’ <@$p>FSF) errichtet. Die FSF übernahm die Distribution der Datenbänder, erst für Emacs, dann auch für andere GNU-Software. Die Mittel aus dem Verkauf von Software (in Quellcode und vorkompiliert für bestimmte Plattformen) und Handbüchern sowie Geld- und Sachspenden verwendet die FSF, um Entwickler dafür zu bezahlen, dass sie bestimmte, für eine vollständige Betriebssystemsumgebung notwendige Programme schreiben. @1 fliess mit:<t-1>Im »<x@1 fliess normal><t-1>GNU Manifest«<x@1 fliess kursiv><t-1> <@$p><t-1>(vgl. <x@6 Caps><t-1>Stallman<@$p><t-1>, 1985) – aus dem selben Jahr – begründet Stallman die Philosophie der freien Software auf dem kantschen Imperativ: »Ich denke, dass die goldene Regel vorschreibt: Wenn ich ein Programm gut finde, muss ich es mit anderen Leuten, die es gut finden, teilen« (ebd.). Wer umgekehrt die Nutzungsmöglichkeiten eines Programms einschränkt, um Geld von den Nutzern zu extrahieren, verwende destruktive Mittel. Wenn jeder sich so verhalten würde, würden wir alle durch die wechselseitige Destruktion ärmer werden. Der Wunsch, für seine Kreativität belohnt zu werden, rechtfertige es nicht, der Welt die Ergebnisse dieser Kreativität vorzuenthalten. Stallman nennt hier bereits eine Reihe Möglichkeiten, wie Softwareentwickler und -firmen mit freier Software Geld verdienen können, die in den 80ern erprobt wurden und in den 90ern Schule machten: Vertrieb und Dokumentation; Support in Form von echter Programmierarbeit und »Händchen halten« für unerfahrenere Nutzer; Hardwarehersteller, die für die Portierung eines Betriebssystems auf ihre Rechner bezahlen; Schulung; eine User’s Group, die gemeinsam einen Programmierer beauftragt, gewünschte Zusätze zu schreiben.<x@3 hoch fliess><t-1>71<x@1 fliess normal><t-1> <@$p><t-1>Der wachsenden Tendenz, Information zu horten, hält er einen Begriff von Freundschaft, Gastfreundschaft (<x@1 fliess kursiv><t-1>hospitality<@$p><t-1>) und Nachbarschaftlichkeit entgegen, wie etwa auf den Konferenzen »<x@1 fliess kursiv><t-1>Wizards of OS1<@$p><t-1>«:<t$>@1 fliesskursiv Zitat:<t-2.5h98>»Außerdem sollte es möglich sein, eine Kopie [von einem Programm]    für einen Freund zu erstellen, so dass der Freund ebenfalls davon profitiert. Dies ist nicht nur nützlich, diese Art der Kooperation ist ein fundamentaler Akt von Freundschaft unter Leuten, die Computer benutzen. Der fundamentale Akt von Freundschaft unter denkenden Wesen besteht darin, einander etwas beizubringen und Wissen gemeinsam zu nutzen. […] Jedes Mal, wenn man die Kopie eines Programms weitergibt, ist dies nicht nur ein nützlicher Akt, sondern es hilft die Bande des guten Willens zu verstärken, die die Grundlage der Gesellschaft bilden und diese von der Wildnis unterscheiden. Dieser gute Wille, die Bereitschaft unserem Nächsten zu helfen, wann immer es im Bereich des Möglichen liegt, ist genau das, was die Gesellschaft zusammenhält und was sie lebenswert macht. Jede Politik oder jedes Rechtssystem, das diese Art der Kooperation verurteilt oder verbietet, verseucht die wichtigste Ressource der Gesellschaft. Es ist keine materielle Ressource, aber es ist dennoch eine äußerst wichtige Ressource.«<x@3 hoch fliess><t-2.5h98>72<x@1 fliess normal><t-2.5h98>@1 fliess mit:@1 fliess ohne:Im »<@1 fliess normal>Manifest«<@1 fliess kursiv> <@$p>ist von der Solidarität unter Programmierern die Rede: »Der fundamentale Akt der Freundschaft unter Programmierern liegt in der gemeinsamen Nutzung der Programme.« Es finden sich auch naturrechtliche Argumentationen: »Das teilweise oder komplette Kopieren eines Progammes ist für einen Programmierer genauso natürlich wie das Atmen und genauso produktiv. Und genauso frei müsste es sein.« Fluchtpunkt der Vision ist eine Welt jenseits des Mangels. Bereits heute sei die Menge der notwendigen Arbeit für die Produktivität der Gesellschaft stark zurückgegangen. Dass sich dies nicht in eine größere Freizeit übersetzt, liege vor allem an den nicht produktiven Aktivitäten wie Verwaltung und Konkurrenz: »Freie Software wird diese Vergeudung im Bereich der Softwareproduktion erheblich reduzieren. Wir müssen dies tun, um technische Fortschritte in weniger Arbeit für uns umzuwandeln.«@1 fliess mit:Im Kern der Vision steht ein freies Softwareuniversum: »Das ultimative Ziel besteht darin, freie Software für alles anzubieten, die alles kann, was Menschen mit den Computern machen möchten – so dass proprietäre Software obsolet wird.«<@3 hoch fliess>73<@$p> Das GNU-Projekt ist mehr als nur ein Sammelbecken für diverse freie Programme. Es wird von einer Gesamtvision für ein vollständiges System geleitet. Systematisch wurden alle Bestandteile einer freien Betriebsumgebung in eine <@1 fliess kursiv>Task List <@$p>eingetragen und nach und nach erarbeitet. <*h"mehr">Zentrales Instrument zur Absicherung dieses expandierenden Universums der freien Software ist die Lizenz, unter der es steht. Die Freiheit, die die <@1 fliess kursiv>GNU General Public License <@1 fliess normal>(<@1 fliess kursiv>GPL)<@$p> den Nutzern einer Software gewährt, umfasst (1) den Zugang zum Quellcode, (2) die Freiheit, die Software zu kopieren und weiterzugeben, (3) die Freiheit, das Programm zu ändern und (4) die Freiheit, das veränderte Programm – unter denselben Bedingungen – zu verbreiten. Die vierte Auflage verhindert, dass freie Software privatisiert und ihrer Freiheiten entkleidet wird. Die Philosophie der freien Software schreibt nicht vor, dass die Weiterverbreitung kos<\h>tenlos zu geschehen hat. Für Dienstleistungen wie Zusammenstellung, Produktion und Vertrieb von CD-ROMs, Support, Schulung und Handbüchern ist die Erhebung einer Gebühr ausdrücklich erlaubt, nicht jedoch für die Software selbst. Auf diese vielleicht wichtigste Innovation Richard Stallmans, die GPL, wird im Abschnitt »Lizenzmodelle« eingegangen. <*h"Standard">Mit der GPL von 1989 beginnen die FSF und ihre Advokaten, wie der New Yorker Rechtsprofessor Eben Moglen,<@3 hoch fliess>74<@$p> auf dem Feld von Copyright- und Vertragsrecht zu intervenieren. In anderer Form tut dies auch die <@1 fliess kursiv>League for Software Freedom<@$p>,<@3 hoch fliess>75<@$p> die 1989 anlässlich von Apples <@1 fliess kursiv>Look-and-Feel<@$p>-Verfahren gegen Microsoft von John Gilmore und Richard Stallman gegründet wurde. Die <@1 fliess kursiv>League<@$p> engagiert sich bis heute gegen User-Interface-Copyrights und Softwarepatente. Mit der Zeit konnten immer mehr zu ersetzende Unix-Komponenten von der <@1 fliess kursiv>Task List <@$p>gestrichen werden, der Schwerpunkt verlagerte sich auf Anwendersoftware. Viele der GNU-Komponenten entwickelten ein Eigenleben. Da sie die entsprechenden Komponenten von Unixsystemen ersetzen konnten und nicht selten besser waren als ihre proprietären Gegenstücke, verbreiteten viele sich als Standardwerkzeuge auch unter Verwaltern proprietärer Unixsysteme. 1990 war das GNU-System nahezu vollständig. Die einzige wesentliche Komponente, die noch fehlte, war ein Kernel. Hier war die Wahl auf einen Mikrokernansatz gefallen. Der Microkernel <@1 fliess kursiv>Mach<@$p>, von der Carnegie Mellon University entwickelt und dann von der University of Utah übernommen, sollte als Basis dienen. Darüber sollen die verschiedenen Betriebssystemsfunktionen als eine Sammlung von Servern (<@1 fliess kursiv>a herd of gnus<@$p>; eine Herde Gnus) mit dem Namen HURD implementiert werden, doch das Debugging solcher inter<\h>agierender Server stellte sich als schwieriger heraus, als erwartet. 1991 kam Linus Torvalds dem GNU-Projekt mit dem Linux-Kernel zuvor. Ob nun Linux als letzter Baustein in das GNU-System eingefügt wurde oder die GNU-Systemkomponenten um Torvalds’ Kernel – wie man es auch sehen mag, auf jeden Fall gibt es seither ein vollständiges, leistungsfähiges, freies System mit dem Namen GNU/Linux.GNU ist das erste »bewusste« freie Softwareprojekt. Die Freiheiten, die in der vorangegangenen Hackerkultur unausgesprochene Selbstverständlichkeit waren, wurden nun expliziert und in einem Vertrag zwischen Autoren und Nutzern (der GPL) rechtsverbindlich festgeschrieben. Mit seinen Tools ist GNU außerdem Geburtshelfer aller folgenden Projekte. Schließlich eröffnet es einen Blick weit über die Software hinaus: »Der Kern des GNU-Projekts ist die Vorstellung von freier Software als soziale, ethische, politische Frage. Kurzum: Wie soll die Gesellschaft beschaffen sein, in der wir leben wollen?«<@3 hoch fliess>76<@$p>@2  ZÜ 3:GNU<\!q>/<\!q>Linux<@3 hoch fliess>77<@$p>@1 fliess ohne:Ein Zwischenspiel stellt das 1987 von Andrew Tanenbaum entwickelte <@1 fliess kursiv>Minix <@$p>dar, ein Unix-Derivat für 286er PCs, das der Informatikprofessor an der Universität Amsterdam speziell für Lehrzwecke entworfen hat.<@3 hoch fliess>78<@$p> In der Usenet-Newsgroup comp.os.minix konnten Interessierte seine Entwicklung mitverfolgen. Ende der 80er-Jahre gab es bereits mehr als 50<\!q>000 Minix-Anwender. @1 fliess mit:Darunter befand sich auch der finnische Informatikstudent Linus Torvalds. Er hatte sich einen PC mit dem gerade neu erschienen 386er-Prozessor angeschafft, der erstmals echten Speicherschutz und <@4 Pfeil (Umschalt/Alt #)>’<@$p> Multitasking anbot. Ausgehend von Minix begann er, einen neuen unixartigen Betriebssystemskern zu schreiben, der die neuen Möglichkeiten unterstütze, zuerst in Assembler, dann in C. Auch Torvalds veröffentlichte den Quelltext seiner Arbeit im Netz, tat die Adresse in der Minix-Newsgroup kund und lud zur Mitarbeit ein. Die Werkzeuge aus dem GNU-Projekt (C-Compiler, Linker, Editoren usw.) taten dem Projekt gute Dienste. Linux ist heute das Paradebeispiel einer solchen unwahrscheinlichen Organisationsform, bei der Tausende von Menschen in der ganzen Welt in einer verteilten, offenen, locker gekoppelten Zusammenarbeit ein komplexes Softwareprojekt entwickeln. »Tatsächlich glaube ich, dass Linus’ clevers<\h>ter und erfolgreichster Hack nicht der Entwurf des Linux-Kernels selbst war, sondern vielmehr seine Erfindung des Linux-Entwicklungsmodells« (<@6 Caps>Raymond<@$p>, 1998). Im Januar 1992 lag der bereits stabile Linux-Kernel 0.12 vor. Und da man mit einem Kernel allein wenig anfangen kann, benutzte die wachsende Community dazu das GNU-System. Ebenfalls vom GNU-Projekt übernommen wurde die Copyleft-Lizenz (GPL), die die Freiheit von Linux und aller abgeleiteter Software sichert. Als Basis einer grafischen Benutzeroberfläche wurde von einem benachbarten freien Projekt XFree86 übernommen. Im März 1994 erschien GNU/Linux Version 1.0. Um eine Aufspaltung des Projekts in verschiedene »Geschmacksrichtungen« (das so genannte <@1 fliess kursiv>Code-Forking<@$p>) zu verhindern, etablierte sich unter der weithin akzeptierten Autorität von Torvalds ein System, wonach bestimmte Entwickler für Teilbereiche des Kerns zuständig sind, er aber das letzte Wort darüber hat, was in den Kern aufgenommen wird. Den Projektleitern (<@1 fliess kursiv>Maintainern<@$p>) arbeitet ein große Zahl von Leuten zu, die die Software testen, Fehler melden und beheben und weitere Funktionen hinzufügen. Neben dem <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Core-Team<@$p> für den Kernel<@3 hoch fliess>79<@1 fliess normal> <@$p>bildeten sich Standardisierungsgremien: die <@1 fliess kursiv>Linux File System Standard Group <@$p>(im August 1993 gegründet<@3 hoch fliess>80<@$p>), das <@1 fliess kursiv>Linux Documentation Project<@$p> (LDP)<@3 hoch fliess>81<@$p> und – zur Sicherstellung der Kompatibilität der Betriebsumgebung in verschiedenen Distributionen – die <@1 fliess normal>Linux Standard Base<@$p> (LSB).<@3 hoch fliess>82<@$p> Die Distribution erfolgte zunächst nur übers Netz, spätestens ab 1993 auch auf Disketten und kurz darauf auf CD-ROM. Ab 1993 stellten kommerzielle Anbieter wie SuSE, Caldera und Yggdrasil Distributionen mit dem neuesten Kernel, Tools und Anwendungen zusammen. Ab 1994 erschienen spezialisierte Zeitschriften (wie das <@1 fliess kursiv>Linux Journal<@$p> und das deutschsprachige <@1 fliess kursiv>Linux Magazin<@$p>) und Online-Foren (wie »Linuxtoday« und »Slashdot.org«). Ab 1993 (und inzwischen in der 7. Auflage von 1997) erscheint das »Linux Anwenderhandbuch« unter der GPL, das schnell zum Standardwerk im deutschsprachigen Raum wurde (vgl. <@6 Caps>Hetze<\!q>/<\!q>Hohndel<\!q>/<\!q>Müller<\!q>/<\!q>Kirch<@$p>, 1997). 1995 gab es bereits 500<\!q>000 GNU/Linux-Nutzer. Ein Jahr darauf waren es 1,5 Millionen, 1997 schon 3,5 und 1998 7,5 Millionen. Was als freie Software aus dem GNU-Projekt begann und sich um andere freie Software wie dem Linux-Kernel anreicherte, stellte inzwischen eine Plattform dar, die auch für die Anbieter von Anwendungssoftware interessant wurde. Mehr als 1<\!q>000 Anwendungen stehen heute auf GNU/Linux zur Verfügung, der größte Teil davon kostenlos. Auch kommerzielle Office-Pakete von Applixware und Star Division, Corels WordPerfect oder Datenbanken von Adabas und Oracle wurden auf GNU/Linux portiert. Linuxer können heute zwischen den grafischen Benutzeroberflächen KDE (<@1 fliess kursiv>K Desktop Environment<@$p>) und Gnome (<@1 fliess kursiv>GNU Network Object Model Environment<@$p>) mit jeweils verschiedenen Window-Managern wählen. Die Nutzerfreundlichkeit nimmt zu. Während für <\n>alte Unix-Hasen der normale Weg die Source Code-Installation ist, wird unter GNU/Linux-Software zunehmend als <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Binary<@$p> installiert, wie das in der PC-Welt üblich ist. Auch die häufig zu hörende Kritik, für GNU/Linux gebe es keinen Support, ist gründlich widerlegt. Tausende von engagierten GNU/Linux-Nutzern gewähren Anfängern und Hilfe Suchenden in Newsgroups und Mailinglis<\h>ten Unterstützung und reagieren auf Bugs manchmal schon innerhalb von Stunden mit <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Patches<@$p>. Schon 1997 verlieh die Zeitschrift <@1 fliess kursiv>Infoworld<@$p> der Linux-Gemeinde für diesen einzigartig leistungsfähigen Support den <@1 fliess kursiv>Best Technical Support Award<@$p>. Auch die kommerziellen Distributoren wie SuSE und Red Hat bieten ihren Kunden Unterstützung. Daneben etablierte sich eine wachsende Zahl von Systemhäusern, die Installation, Schulung und Wartung von GNU/Linux-Systemen kommerziell anbieten. Heute gibt es schätzungsweise 30 Millionen Installationen weltweit. GNU/Linux hat inzwischen auch in Wirtschafts- und Verwaltungskreisen Popularität als zuverlässige und kostengünstige Alternative zu Microsoft-NT vor allem im Serverbereich erlangt. Zu GNU/Linuxgroßanwendern gehören Unternehmen wie Edeka, Sixt, Debis und Ikea. Große Computervertreiber begannen ab Mitte 1999, Desktop-Rechner, Workstations und vor allem Server mit vorinstalliertem GNU/Linux auszuliefern. Nach den Hochburgen Europa und den USA erobert Linux auch den pazifisch-asiatischen Raum von Japan über China bis Indien.<@3 hoch fliess>83 <@$p>Linus Torvalds ist ein unangefochtener Star der freien Software. GNU/Linux gelangte durch den Film »<@1 fliess normal>Titanic«<@$p> zu Hollywood-Ruhm, dessen Computergrafiken auf einem GNU/Linux- <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Cluster<@$p> gerechnet wurden. Seit der Linux<\h>-World Expo im März 1999 – eine Art Volljährigkeitsparty für das Projekt – hat Linux auch seine eigene Industriemesse. Sogar in der Kunstwelt fand es Anerkennung, als dem Projekt 1999 der »Prix Ars <\h>Electronica« (in der Kategorie ».net«) verliehen wurde. Somit nahmen sowohl das GNU- wie das Linux-Kernel-Projekt ihren Ausgang in Universitäten. Wie Friedrich Kittler betont, war es eine praktische Kritik an der Schließung des wissenschaftlich frei zirkulierenden Wissens, »... dass am Massachusetts <@1 fliess normal>Institute of Technology<@$p> einige Programmierer der Käuflichkeit widerstanden und dass an der Universität Helsinki ein einsamer Informatikstudent die herbei geredete Angst vor Assemblern und Kaltstarts – denn darum geht es – überwand. So direkt waren offene Quellen und freie Betriebssysteme an die Freiheit der Universität gekoppelt. Schauen Sie wieviel ›edu‹ da drinsteht in den Linux-Kernel-Sources. So direkt hängt aber auch die Zukunft der Universität von diesen freien Quellen ab.«<@3 hoch fliess>84<@$p>@2  ZÜ 3:Von »Free Software« zu »Open Source Software« und zurück@1 fliess ohne:Der Schlüsseltext für die begriffliche Wende in der Debatte ist Eric S. Raymonds Aufsatz »<@1 fliess normal>The Cathedral and the Bazaar«<@1 fliess kursiv> <@$p>(Raymond, 1998). Darin stellt er ein zentral gelenktes Projekt wie einen Kathedralenbau oder Microsoft Windows dem kreativen, selbst organisierenden Gewusel eines Basars oder eben der Linux-Gemeinde gegenüber. In den Textversionen bis zum Februar 1998 sprach Raymond darin von »Free Software«, dann ersetzte er den Ausdruck durch »Open Source Software«.<@3 hoch fliess>85<@$p> »Free« ist nicht nur zweideutig (»Freibier« und »Freie Rede«), sondern offensichtlich war es in <@1 fliess kursiv>The Land of the Free <@$p>zu einem unanständigen, »konfrontationellen«, irgendwie kommunistisch klingenden <@1 fliess kursiv>four-letter word <@$p>geworden. Jedenfalls war es das erklärte Ziel der <@1 fliess kursiv>Open Source Initiative<@$p> (OSI), den mindestens 14 Jahren zuvor von Richard Stallman geprägten Begriff »Free Software«, unter dem sich die Bewegung weit über das GNU-Projekt hinaus gesammelt hatte, durch einen Begriff zu ersetzen, der auch mutmaßlich in den Vorstandsetagen und Aktionärsversammlungen schmackhaft gemacht werden konnte. @1 fliess mit:Der neue Begriff wurde auf dem Gründungstreffen der OSI im <\h>Februar 1998 geprägt. Daran nahmen neben Raymond Vertreter einer <\h>Linux-Firma und der Silicon Valley Linux User Group teil. Christine <\h>Peterson<@3 hoch fliess>86<@$p> (Foresight Institute) erdachte den Begriff »Open Source«. <\h>Anlass für dieses Treffen war Netscapes Ankündigung Ende <\n>Januar jenen Jahres gewesen, den Quellcode seines Browsers offen zu <\h>legen. Netscape hatte Raymond eingeladen, dabei zu helfen: »Wir begriffen, dass Netscapes Ankündigung ein kostbares Zeitfenster geöffnet hatte, in dem es uns endlich gelingen könnte, die Unternehmenswelt dazu zu bringen, sich anzuhören, was wir ihr über die Überlegenheit eines offenen Entwicklungsmodells beizubringen hatten. Wir erkannten, dass es an der Zeit war, die Konfrontationshaltung abzulegen, die in der Vergangenheit mit der ›freien Software‹ in Verbindung gebracht wurde, und die Idee ausschließlich mit den pragmatischen, wirtschaftlichen Argumenten zu verkaufen, die auch Netscape dazu motiviert hatte.«<@3 hoch fliess>87<@$p>Bruce Perens, Autor der <@1 fliess kursiv>Debian Free Software Guidelines <@$p>und der davon abgeleiteten <@1 fliess kursiv>Open Source Definition <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’ <@1 fliess kursiv>OSD<@$p>) (s.u. »Lizenzmodelle«), meldete »Open Source« als Warenzeichen an. Die Website »www.open<\h>source.org« wurde gestartet. Viele kritisierten den Begriff und zogen das etablierte »Free Software« vor, andere innerhalb der lockeren Bewegung folgten der Wende (vgl. <@6 Caps>Perens<@$p>, 1999, S. 174). So lud der O’Reilly Verlag im April 1998, kurz nachdem Netscape den Quelltext seines Navigators freigegeben hatte, noch zu einem <@1 fliess kursiv>Free Software Summit<@$p> ein. Im August des Jahres organisierte der Verlag dann den <@1 fliess kursiv>Open Source Developer Day<@$p>. Die Computer-, aber auch die Finanzpresse griff nach Netscapes Schritt den Begriff »Open Source« auf.Quelloffenheit ist eine zwingende, wenngleich nicht hinreichende Voraussetzung auch von »Freier Software«, doch indem diese neue Fraktion die das Phänomen konstituierende Freiheit in den Hintergrund rückte, öffnete sie einer Softwarelizenzierung die Tore, die mit Freiheit nichts zu tun hat. So ist auch Quellcode, den man einsehen, aber nicht modifizieren darf, »quelloffen«. Oder Softwareunternehmen behalten sich vor, nach firmenstrategischen Gesichtspunkten zu entscheiden, ob sie die Modifikationen von freien Entwicklern aufnehmen oder nicht. Der Versuch der OSI, das Warenzeichen »Open Source« als Gütesiegel für Software durchzusetzen, deren Lizenz der Open Source-Definition genügt, ist gescheitert. Heute führen viele Produkte das Label, die keine Modifikationsfreiheit gewähren – und genau sie ist der Sinn der Quelloffenheit.Hinter der neuen Sprachpolitik standen auch Angriffe gegen die Person Stallmans – eine Art Vatermord: »Für die neue Generation ist Stallman eine Peinlichkeit und ein Hindernis. Er muss um jeden Preis in ein <t-1>Hinterzimmer verbannt werden, bevor er die Investoren verschreckt« (<x@6 Caps><t-1>Leonard<@$p><t-1>,<t$> 8/1998). Andrew Leonard führte in einem Artikel im <@1 fliess kursiv>Salon Magazine <@$p>Stimmen an, die von einem Generationswechsel sprechen und Kritik an Stallmans Stil äußern, seine Projekte zu leiten. Eine »übertriebene Kontrollmentalität« sagen sie ihm nach. Der Unterschied im »Führungsstil« war es auch, der Raymond dazu veranlasste, GNU als ein Kathedralen-Projekt dem Basar-Modell von Linux und anderen Projekten der dritten Generation gegenüberzustellen. Stallman selbst sieht durch diesen Begriffswechsel das Gleichgewicht zwischen Verbreitung und Aufklärung bedroht: »Somit konzentrieren sich die Argumente von ›Open Source‹ auf die Möglichkeit, eine hohe Qualität und eine mächtige Software zu erhalten, vernachlässigen aber die Vorstellungen von Freiheit, <@1 fliess normal>Community<@$p> und Prinzipien« (<@6 Caps>Stallman<@$p>, 1999, S. 69 f.). Die Open Source-Fraktion betone allein die pragmatischen Aspekte, die Nützlichkeit, die Features, die Zuverlässigkeit und Effizienz der Software: »Die Herangehensweise oder die Haltung, die mit Linux verbunden ist, ist eine ganz andere, eher technologisch, wie die Einstellung eines Ingenieurs: Wie können wir mächtige Software herstellen, wie können wir ›erfolgreich‹ sein. Diese Dinge sind nicht schlecht, aber ihnen fehlt das Wichtigste.«<@3 hoch fliess>88<@$p> Entsprechend beharrt das GNU-Projekt auf der Bezeichnung »Free Software«, um zu betonen, dass Freiheit und nicht allein Technologie zählt. »Das ist der wahre Unterschied zwischen freier Software und Open Source. Freie Software verfolgt eine politische Philosophie, Open Source ist eine Entwicklungsmethodologie – und deswegen bin ich stolz, Teil der ›Freien-Software‹ -Bewegung zu sein und ich hoffe, Sie werden es auch sein« (ebd.). Perens, der mit der <@1 fliess kursiv>Open Source Definition<@$p> (s.u. »Lizenzmodelle«) die Messlatte für die Erteilung des Gütesiegels Open Source geschaffen und zusammen mit Raymond die <@1 fliess kursiv>Open Source Initiative <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’ <@1 fliess kursiv>OSI)<@$p> zu seiner Zertifizierung gegründet hatte, wechselte ein Jahr später das politische Lager. Die OSI habe ihre Aufgabe, der Nicht-Hacker-Welt die freie Software nahe zu bringen, erfüllt: »Und jetzt ist es Zeit für die zweite Phase: Jetzt, wo alle Welt zusieht, ist es für uns an der Zeit, Sie über Freie Software aufzuklären. Beachten Sie, ich sagte Freie Software und nicht etwa Open Source« (Perens, 1999a). Er bedauerte, dass Open Source die Bemühungen der FSF überschattet habe – »eine Spaltung der beiden Gruppen hätte sich niemals entwickeln dürfen« – verließ die OSI und wählte wieder die Seite von <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>SPI<@$p> und FSF. Leonards Fazit dazu im <@1 fliess kursiv>Salon Magazine:<@$p> »Wenn er auch ungekämmt  und verschroben sein mag, so verkörpert Stallman doch die Inbrunst und den Glauben, die es wert machen, sich der freien Software zu verschreiben. Wenn die Pragmatiker der Open-Source-Sache ihn opfern, um freie Software ungefährlich fürs Business zu machen, scheint es mir, setzen sie die Seele der Bewegung aufs Spiel« (<@6 Caps>Leonard<@$p> , 8/1998).@1 fliess ohne:<\3>Nachdem das vorangegangene Kapitel die Wurzeln und die Netzwerkumgebung, in der die freie Software lebt, behandelte, werden wir uns nun einige technische, soziale und wirtschaftliche Zusammenhänge der freien Projekte näher ansehen. Nähere Informationen zu den einzelnen Softwareprojekten, auf die sich diese Beobachtungen stützen, folgen im nächsten Kapitel.@1 fliess mit:@2  ZÜ 3:<\c>@2  ZÜ 1:Was ist freie Software, wie entsteht sie, <\h><\n>wer macht sie?@1 fliess mit:@1 fliess ohne:Freie Software stellt eine von der proprietären Software grundlegend verschiedene Welt mit ihrer eigenen Kultur dar. Alle weiteren Merkmale ergeben sich aus ihren vier Grundeigenschaften: (1) die Software darf ohne Einschränkungen benutzt werden, (2) der Quellcode freier Software ist verfügbar; er darf skidiert und aus ihm darf gelernt werden, (3) sie darf ohne Einschränkungen und ohne Zahlungsverpflichtungen kopiert und weitergegeben werden, (4) sie darf verändert und in veränderter Form weitergegeben werden.<@3 hoch fliess>1<@$p> Diese Eigenschaften bilden die idealen Voraussetzungen für eine offene, d.h., nicht auf Arbeitsvertragsverhältnissen beruhende, kooperative Softwareentwicklung und eine weitestgehende Beteiligung der Anwender. @1 fliess mit:Jedes größere Softwareprojekt wird von Gruppen von Entwicklern erstellt. Auch in der Industrie haben heute Teams, die über eine kreative Selbständigkeit verfügen, ihren Code synchronisieren und in regelmäßigen Abständen das gemeinsame Produkt stabilisieren, die hierarchischen Verfahren unter einem Chefprogrammierer weitgehend abgelöst. Beobachter wollen selbst bei Microsoft eine hochskalierte Hackermethodik im Einsatz sehen (vgl. <@6 Caps>Cusumano<\!q>/<\!q>Selby<@$p>, 1997). Die freie Software dagegen hat dank der genannten Eigenschaften und dank des Internet die Zusammenarbeit auf alle ausgeweitet, die sich beteiligen möchten. Da hier weder Leistungslohn noch Geheimhaltungsvorschriften die Teamgrenzen bestimmen, kann das entstehende Wissen nur allen gehören.@2  ZÜ 2:Quellcode und Objektcode@1 fliess mit:@1 fliess ohne:Software stellt eine besondere Klasse von Wissen dar. Es handelt sich um operative Anweisungen, die sich, von Menschen geschrieben, an einen Computer richten. Menschen schreiben diese Anweisungen in einer höheren Programmiersprache, wie Pascal, C, oder C++. Dieser Quelltext wird einem <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Compiler<@$p> übergeben, einem Softwarewerkzeug, das ihn in <\n>eine maschinennähere, für Menschen nicht mehr lesbare Form, den <\n>Objektcode übersetzt. Diese binäre Form des Programms erst veranlasst den Rechner, für den Menschen (meist) sinnvolle Zeichenoperationen auszuführen. @1 fliess mit:Kompilierung ist ein im Wesentlichen irreversibler Prozess, denn aus dem Binärcode lässt sich allenfalls mit großen Anstrengungen der Quellcode rekonstruieren – mit annähernd so viel Mühe, wie die ursprüngliche Codierung gekostet hat.<@3 hoch fliess>2<@$p> Proprietäre Software wird nur in einer für eine bestimmte Prozessorplattform vorkompilierten Form ausgeliefert, ohne Quellen. Das stellt zwar keinen Kopierschutz dar, das Wissen darüber, wie ein solches <@1 fliess kursiv>Blackbox<@$p>-System macht, was es macht, ist jedoch für alle praktischen Belange effektiv geschlossen. Böse Zungen mutmaßen, dass viele Firmen ihre Quellen nicht veröffentlichen, damit niemand sieht, von wie geringer Qualität ihre Software ist, doch der Hauptgrund liegt natürlich im Schutz des geistigen Eigentums. Microsoft wehrte sich gegen die Offenlegung des Quellcode von Windows in einem Rechtsstreit mit Caldera mit der Begründung, es handele sich um eines der wertvolls<\h>ten und geheimsten Stücke geistigen Eigentums auf der Welt.<t-1>Für reine Anwender mag eine <x@1 fliess kursiv><t-1>Blackbox<@$p><t-1> zunächst keine wirkliche Qualitätseinschränkung bedeuten, für Entwickler und Systemadminis<\h>tratoren macht es ihre Arbeitsumgebung jedoch zu einer Welt voller Mauern und verbotener Zonen. Für Anpassung, Integration, Fehlerbehebung und Ergänzung müssen sie Veränderungen an der Software vornehmen, und das ist nur am Quelltext möglich. Freie, quelloffene Software bietet ihnen diese Möglichkeit. Bei proprietärer Software bleibt ihnen nur, auf die Unterstützung durch den Hersteller zu hoffen oder sich durch umständliches und nur in engen Grenzen legales <x@1 fliess kursiv><t-1>Reverse Engineering <@$p><t-1>zu behelfen. <t$>Ein häufig von Anbietern proprietärer Software vorgebrachtes Argument lautet, ihre Software umfasse mehrere Millionen Zeilen Code, weshalb ihre Kunden gar kein Interesse hätten, den Quellcode zu lesen. Im Übrigen verfalle durch einen Eingriff in die Software die Gewährleis<\h>tungsgarantie, und der Support würde ungleich komplexer.<@3 hoch fliess>3 <@$p>Das Gegenargument lautet, dass keine Software 100-prozentig das leistet, was Anwender sich wünschen. Außerdem veraltet Software rasch, wenn sie nicht ständig den sich schnell verändernden Soft- und Hardwaregegebenheiten angepasst wird. Anspruchsvolle Anwender werden also in jedem Falle die Möglichkeit schätzen, in den Quellcode eingreifen oder andere damit beauftragen zu können, die gewünschten Veränderungen für sie vorzunehmen. Selbst wenn keine Änderungen vorgenommen werden sollen, ist es sinnvoll, den Quellcode mitzuliefern, da Interessierte ihn lesen und daraus lernen können. Modifikationen an proprietärer Software sind technisch nur schwer möglich und lizenzrechtlich nicht erlaubt. Die Datenformate, die diese Software schreibt, sind häufig nicht dokumentiert, weshalb die Gefahr besteht, dass ein Generationswechsel oder die Einstellung einer Software, z. B. durch Verkauf oder Konkurs des Herstellers, die über Jahre angesammelten Daten unlesbar und unkonvertierbar macht. Modifikationsfreiheit ist also auch eine Voraussetzung für Investitionssicherheit. Auch die urheberrechtlich geschützten Handbücher dieser Software erlauben keine Anpassung und Fortschreibung, um Änderungen oder die an einzelnen Arbeitsplätzen eingesetzten Varianten zu dokumentieren. Mit der freien und der proprietären Software stehen sich also zwei grundlegend verschiedene Auffassungen über das Wissen gegenüber. Hier der Quelltext als ein in geschlossenen Gruppen, unter Vertraulichkeitsverpflichtung gefertigtes Masterprodukt, das in geschlossener, binärer Form vermarktet und mit Hilfe von Urheberrechten, Patenten, Markenschutz und Kopierschutzmaßnahmen vor Lektüre, Weitergabe und Veränderung geschützt wird. Dort der Quelltext als in einer offenen, nicht gewinnorientierten Zusammenarbeit betriebener Prozess, bei dem eine ablauffähige Version immer nur eine Momentaufnahme darstellt, zu deren Studium, Weitergabe und Modifikation die Lizenzen der freien Software ausdrücklich ermutigen. Hier eine Ware, die dem Konsumenten vom Produzenten verkauft wird, dort ein kollektives Wissen, das allen zur Verfügung steht. Hier konventionelle Wirtschaftspraktiken, die tendenziell immer auf Verdrängung und Marktbeherrschung abzielen. Dort ein freier Wettbewerb um Dienstleistungen mit gleichen Zugangschancen zu den Märkten.@2  ZÜ 2:Wie funktioniert ein Projekt der freien Software?@1 fliess mit:@1 fliess ohne:Es beginnt meist damit, dass jemand ein Problem hat. Das Sprichwort »Notwendigkeit ist die Mutter aller Erfindungen« übersetzt Eric Raymond in eine der Faustregeln der freien Software: »Jedes gute Soft<\h>warewerk beginnt damit, dass ein Entwickler ein ihn persönlich betreffendes Problem angeht« (<@6 Caps>Raymond<@$p>, 1998). So wollte Tim Berners-Lee 1990 die Informationsflut, die am europäischen Hochenergiephysikzentrum <@4 Pfeil (Umschalt/Alt #)>’<@$p><\!q><@1 fliess kursiv>CERN<@$p> produziert wird, in den Griff bekommen und begann, das WWW-Protokoll zu entwickeln (vgl. <@6 Caps>Berners-Lee<@$p>, 1999). Rob McCool am US-amerikanischen Supercomputerzentrum <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>NCSA<@$p> wollte einen Server für dieses Format haben und schrieb den »NCSA-httpd«. David Harris arbeitete an einer Universität in Dunedin, Neuseeland, die 1989 ein Novell NetWare-Netzwerk installierte, nur um festzustellen, dass es kein E-Mailsystem enthielt. Das Budget war aufgebraucht, die kommerziellen E-Mailpakete sehr teuer, also schrieb Harris in seiner Freizeit ein einfaches Programm namens »Pegasus Mail«, das er seither zu einem mächtigen und komfortablen Werkzeug weiterentwickelt hat und weiterhin verschenkt.<@3 hoch fliess>4<@$p> Brent Chapman wurde 1992 zum Administrator von 17 technischen Mailinglisten. Mit der damals verfügbaren Software musste der Listenverwalter jede Subskription und andere Administrationsvorgänge von Hand eintragen, was eine Menge Zeit kostete. Deshalb begann er »Majordomo« zu entwickeln, heute ein immer noch weit verbreiteter Mailinglistenserver (vgl. <@6 Caps>Chapman<@$p>, 1992). Ein finnischer Informatikstudent namens Linus Torvalds wollte ein Unix auf seinem 386er-PC laufen lassen, fand allein Andrew Tanenbaums Kleinst-Unix »Minix«, das seinen Ansprüchen nicht genügte, und so begann er Linux zu entwickeln. In einem Interview mit der Zeitschrift <@1 fliess kursiv>c’t<@$p> zeigt sich Torvalds noch zehn Jahre später verwundert über den Erfolg: @1 fliess mit:@1 fliesskursiv Zitat:»Ich muss sagen, dass ich niemals etwas Vergleichbares erwartet hätte. Nicht einmal annähernd. Ich wollte ein System haben, das ich selbst auf meiner Maschine nutzen konnte, bis es etwas Besseres gäbe. Ich habe es im Internet zur Verfügung gestellt, nur für den Fall, dass jemand anderes an so einem Projekt Interesse hätte. Dass es nun Millionen von Anwendern gefunden hat und im Internet ziemlich bekannt ist, hätte ich mir niemals träumen lassen.«<@3 hoch fliess>5<@$p>@1 fliess mit:@1 fliess ohne:Freie Software entsteht also zunächst nicht auf Anweisung eines Vorgesetzten oder Auftraggebers. Sie ist vielmehr eine eigenmotivierte Tätigkeit, angetrieben von dem Wunsch, ein auf der Hand liegendes Problem bei der Arbeit oder Forschung zu lösen. Eine Ausnahme bildet das GNU-Projekt, das von der Vision eines vollständigen freien Betriebssystems geleitet wurde. @1 fliess mit:All diese Männer der ersten Stunde – es scheint tatsächlich nicht eine einzige Frau unter ihnen zu geben<@3 hoch fliess>6<@$p> – veröffentlichten ihre Projekte frühzeitig, in der Annahme, dass sie nicht die Einzigen sind, die das jeweilige Problem haben, und dass es andere gibt, die ihnen bei der Lösung helfen würden. Meist bildet sich schon bald nach der Erstveröffentlichung um den Initiator eine Gruppe von Mitstreitern, die ihre Kenntnisse und Zeit in das Projekt zu investieren bereit sind und die, wenn die Zahl der Beteiligten wächst, als <@1 fliess normal>Maintainer<@$p> (eine Art Projektleiter) eine koordinierende Verantwortung für Teile des Projekts übernehmen. @2  ZÜ 3:Core-Team und Maintainer@1 fliess ohne:Wenn die Zahl der Mitentwickler und Anwender wächst, bildet diese Gruppe das zentrale Steuerungsgremium des Projekts, das <@1 fliess kursiv>Core-Team.<@$p> Ein solches Team rekrutiert sich meist aus den Leuten, die entweder schon am längsten daran arbeiten oder sehr viel daran gearbeitet haben oder derzeit am aktivsten sind. Beim Apache umfasst es 22 Leute aus sechs Ländern. XFree86 hat ein <@1 fliess normal>Core-Team von elf Personen und eine Community von etwa 600 Entwicklern. Innerhalb des Core-Teams<@1 fliess kursiv> <@$p>werden Entscheidungen über die allgemeine Richtung der Entwicklung gefällt, über Fragen des Designs und interessante Probleme, an denen weitergearbeitet werden soll. Große Projekte werden in funktionale Einheiten, in <@1 fliess kursiv>Packages<@$p> oder Module gegliedert, für die jeweils ein oder mehrere Maintainer zuständig sind. Die Rolle eines <@1 fliess normal>Maintainers<@$p> ist es zunächst, Ansprechpartner für die jeweilige Software zu sein. Häufig handelt es sich um altgediente Coder, die sich eine Reputation erworben haben, als treibende Kraft wirken, die Gemeinschaft koordinieren, zusammenhalten und andere motivieren können. An jedem Einzelprojekt arbeiten mehrere Dutzend bis hundert Entwickler weltweit mit. Änderungen werden an das <@1 fliess normal>Core-Team<@$p> geschickt und von diesem in den <@1 fliess normal>Quellcode<@$p> integriert. @1 fliess mit:Beim Linux-Kernel gibt es kein offizielles Entwicklerteam. Im Laufe der Zeit hat sich meritokratisch eine Gruppe von fünf oder sechs Leuten herausgeschält, die das Vertrauen des zentralen Entwicklers Linus Torvalds genießen.<@3 hoch fliess>7<@$p> Sie sammeln die eintreffenden <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Patches<@$p>, die »Code-Flicken«, sortieren Unsinniges und Unfertiges aus, testen den Rest und geben ihn an Torvalds weiter. Er trifft dann aufgrund der Vorentscheidungen seiner Vertrauten die letzte Entscheidung (vgl. <@6 Caps>Dietrich<@$p>, 1999).@2  ZÜ 3:Die Community@1 fliess ohne:Die neueren Projekte, wie Linux und Apache messen der Entwicklergemeinschaft einen größeren Stellenwert bei, als das ältere GNU-Projekt. Im »<@1 fliess normal>GNU<@$p> Manifest« von 1985 schrieb Stallman: »GNU ... ist ... das Softwaresystem, das ich schreibe, um es frei weiterzugeben... Mehrere andere Freiwillige helfen mir dabei« (<@6 Caps>Stallman<@$p>, 1985). In einem Interview im Januar 1999 antwortete er auf die Frage, wie viele Menschen im Laufe der Jahre an GNU mitgearbeitet haben: »Es gibt keine Möglichkeit, das festzustellen. Ich könnte vielleicht die Anzahl der Einträge in unserer Freiwilligendatei zählen, aber ich weiß nicht, wer am Ende wirklich etwas beigetragen hat. Es sind wahrscheinlich Tausende. Ich weiß weder, wer von ihnen echte Arbeit geleistet hat, noch ist jeder, der echte Arbeit geleistet hat, dort aufgeführt.«<@3 hoch fliess>8<@$p> Bei den jüngeren Projekten wird stärker auf das – im Übrigen auch urheberpersönlichkeitsrechtlich verbriefte – Recht auf Namensnennung geachtet. Der Pflege der Community, die die Basis eines Projektes bildet, wird mehr Aufmerksamkeit gewidmet. Das Debian-Team besteht aus etwa 500 Mitarbeitern weltweit. Bei XFree86 sind es rund 600. Bei den meisten freien Softwareprojekten gibt es keine festgelegte Aufgabenverteilung. Jeder macht das, was ihn interessiert und programmiert oder implementiert das, wozu er Lust hat. @1 fliess mit:Auch der Apache-Webserver wird natürlich nicht allein von den 22 <@1 fliess normal>Core-Team-<@$p>Mitgliedern entwickelt. Es gibt sehr viele Leute, die – regelmäßig, gelegentlich, zum Teil auch nur einmalig – Fehler im Apache beheben. Die Beteiligung fängt bei simplen Fehlerberichten (<@1 fliess kursiv>Bug Reports<@$p>) an und geht über Vorschläge für neue Funktionen (<@1 fliess kursiv>Feature Requests<@$p>) und Ideen zur Weiterentwicklung bis hin zu <@1 fliess kursiv>Patches<@$p> oder größeren Funktionserweiterungen, die von Nutzern erstellt werden, die ebenfalls Spaß am Entwickeln haben und ihren Teil dazu beitragen wollen, den Apache zu verbessern.<@3 hoch fliess>9<@$p> Auch das freie X-Window-System beruht auf einem solchen Spektrum vielfältiger Mitarbeit:@1 fliesskursiv Zitat:»Die Anzahl der neuen Leute, die Interesse zeigen und in so ein Projekt einsteigen, ist absolut verblüffend. Das Interesse ist riesig. ›Entwickler‹ ist natürlich ein bisschen grob gegriffen. Die Zahl der Leute, die mehr als 1000 Zeilen Code im XFree86 drin haben, ist vielleicht zwei Dutzend, mehr sind das nicht. Es gibt jede Menge Leute, die mal einen ein- bis dreizeiligen Bug Fix gemacht haben, aber auch ›nur‹ Tester. Leute, die Dokumentationen schreiben, sind wahnsinnig wertvoll. Denn, das werden viele Free Softwareprojektleiter mir bestätigen können: Coder schreiben keine Dokumentation. Es ist leicht, Leute zu finden, die genialen Code schreiben. Es ist schwierig, Leute zu finden, die bereit sind, diesen genialen Code auch für den Anfänger lesbar zu dokumentieren.«<@3 hoch fliess>10<@$p>@1 fliess mit:@2  ZÜ 3:Entscheidungsfindung: »rough concensus and running code«@1 fliess ohne:Das Credo der Internet-Entwicklergemeinde lautet: »Wir wollen keine Könige, Präsidenten und Wahlen. Wir glauben an einen groben Konsens und an ablauffähigen Code.«<@3 hoch fliess>11<@$p> Die gleiche Philosophie herrscht auch in den meisten freien Softwareprojekten. Auch die <@1 fliess normal>Core-Team-<@$p>Mitglieder sind keine Projektleiter oder Chefs. Lars Eilebrecht über Apache:@1 fliesskursiv Zitat:»Alle Entscheidungen, die getroffen werden, sei es nun welche <@1 fliess normal>Patches<@$p>, welche neuen Funktionalitäten in den Apache eingebaut werden, was in der Zukunft passieren soll und sonstige Entscheidungen werden alle auf Basis eines Mehrheitsbeschlusses getroffen. Das heißt, es wird auf der Mailingliste darüber abgestimmt, ob ein bestimmter <@1 fliess normal>Patch<@$p> eingebunden wird oder nicht. Bei <@1 fliess normal>Patches<@$p>, die eine große Änderung darstellen, ist es typischerweise so, dass sich mindestens drei Mitglieder der Apache-Group [des <@1 fliess normal>Core-Teams<@$p>] damit beschäftigt haben müssen, das heißt, es getestet haben und dafür sein müssen, dass der Patch eingebaut wird. Und es darf keinerlei Gegenstimmen geben. Wenn es sie gibt, dann wird typischerweise das Problem, das jemand damit hat, behoben und, wenn der <@1 fliess normal>Patch <@$p>dann für sinnvoll erachtet wird, wird er irgendwann eingebaut.«<@3 hoch fliess>12<@$p>@1 fliess mit:@1 fliess ohne:Selbstverständlich gibt es auch in diesen Projekten Meinungsverschiedenheiten, doch anders als bei philosophisch-politischen Debatten, die oft auf Abgrenzung und Exklusion von Positionen zielen, scheinen Debatten über technische Fragen eher zu inklusiven Lösungen zu neigen. Wirkliche Zerreißproben hat Dirk Hohndel in den Projekten, in denen er in den letzten acht Jahren beschäftigt war, noch nicht erlebt.<@3 hoch fliess>13<@1 fliess normal> <@$p>Das offen-kooperative Verhältnis ändert sich auch nicht notwendig dadurch, dass Firmenvertreter an Entscheidungsprozessen teilnehmen. Auch im <@1 fliess normal>Core-Team<@$p> des Apache-Projekts gibt es, wie Eilebrecht berichtet, gelegentlich Konflikte.@1 fliesskursiv Zitat:»Dabei ist es aber uninteressant, ob die Person, die damit irgendein Problem hat, von einer Firma ist oder ob sie privat bei der Apache-Group dabei ist. Wir haben zwar vom Prinzip her, ich will nicht sagen Kooperationen, aber Firmen mit in der Apache-Group dabei, in Form eines Vertreters dieser Firma. So ist etwa IBM mit dabei, Siemens und Apple demnächst. Aber sie haben bei Abstimmungen jeweils nur eine Stimme. Im Fall von IBM sind es mittlerweile zwei, aber das ist ein anderes Thema. Wenn aber die meisten anderen Mitglieder etwas dagegen haben, dass bestimmte Änderungen gemacht werden oder bestimmte Entscheidungen nicht gewollt sind, dann haben die keine Chance, das irgendwie durchzusetzen. Dann müssen sie entweder aussteigen oder akzeptieren, dass es nicht gemacht wird.«<@3 hoch fliess>14<@$p>@1 fliess mit:@2  ZÜ 3:Code-Forking @1 fliess ohne:Das eben Gesagte bedeutet nicht, dass Konflikte prinzipiell nicht dazu führen können, dass Fraktionen getrennte Wege gehen. Tatsächlich hat es verschiedentlich Spaltungen von Projekten gegeben, bei denen sich mit der Entwicklergruppe natürlich auch die Code-Basis verzweigt (von engl. <@1 fliess kursiv>fork, <@1 fliess normal>die Gabel)<@$p>. Im schlimmsten Fall bedeutet dies den Tod eines Projekts, oder es entstehen daraus zwei ähnliche Projekte, die um Entwickler und Anwender konkurrieren – die Gesamtbemühungen verdoppeln sich. Im güns<\h>tigsten Fall kann eine Spaltung fruchtbar sein. Die entstehenden Einzelprojekte entwickeln die Software für verschiedene komplementäre Anwendungsschwerpunkte weiter, ergänzen sich und profitieren von Innovationen in den anderen Projekten (so geschehen bei den drei aus BSD-Unix hervorgegangenen Entwicklungslinien FreeBSD, NetBSD und OpenBSD). Die Möglichkeit, sich jederzeit von einem Projekt abzusetzen und es in eine eigene Richtung weiterzutreiben, wird auch als heilsam erachtet. Sie verhindert, dass nicht zu wartende Mammutprogramme entstehen und Personen, die Verantwortung für Programme tragen, sich nicht mehr darum kümmern. Vor allem steuert sie den Prozess: Wenn die Entwicklung am Bedarf von ausreichend vielen vorbei geht, kommt es irgendwann zur Verzweigung.@1 fliess mit:@2  ZÜ 3:Die Werkzeuge@1 fliess ohne:<t1>Die zentralen Kommunikationsmittel für die weltweit ortsverteilte Kooperation sind E-Mail, genauer Mailinglisten sowie Newsgroups. Für Echtzeitkommunikation verwenden einige Projekte auch den <x@1 fliess kursiv><t1>Internet <\n>Relay Chat <x@1 fliess normal><t1>(<x@4 Pfeil (Umschalt/Alt #)><t1>’ <x@1 fliess kursiv><t1>IRC)<@$p><t1>. Die Projekte präsentieren sich und ihre Ressourcen auf Websites. Das zentrale Instrument zur kooperativen Verwaltung des Quellcode sind CVS-Server. Das <x@1 fliess kursiv><t1>Concurrent Versions System <x@1 fliess normal><t1>(<x@1 fliess kursiv><t1>CVS)<@$p><t1> ist ein mächtiges Werkzeug für die Revisionsverwaltung von Softwareprojekten, das es Gruppen von Entwicklern erlaubt, gleichzeitig an denselben Dateien zu arbeiten, sich zu koordinieren und einen Überblick über die Veränderungen zu behalten. CVS ist Standard bei freier Software, aber auch viele Firmen, die proprietäre Software erstellen, setzen es ein. Jeder kann lesend weltweit auf die <x@4 Pfeil (Umschalt/Alt #)><t1>’<@$p><t1> <x@1 fliess kursiv><t1>Quellcodebäume<@$p><t1> zugreifen und sich die aktuellste Version einer Software auf seine lokale Festplatte kopieren. <t$>@1 fliess mit:Wer die Software weiterentwickeln möchte, muss sich registrieren, um für andere Entwickler ansprechbar zu sein. Die Sammlung von Dateien liegt in einem gemeinsamen Verzeichnis, dem <@1 fliess kursiv>Repository<@$p>. Um mit der Arbeit zu beginnen, führt man den <@1 fliess kursiv>Checkout<@$p>-Befehl aus, dem man den Verzeichnispfad oder den Namen des Moduls übergibt, an dem man arbeiten möchte. Das CVS kopiert dann die letzte Fassung der gewünschten Dateien aus dem Repository in einen Verzeichnisbaum auf der lokalen Festplatte. Der Entwickler kann diese Dateien nun mit einem Editor seiner Wahl verändern, sie in eine Output-Datei »bauen« (<@1 fliess kursiv>build<@$p>) und das Ergebnis testen. Ist er mit dem Ergebnis zufrieden, schreibt er es mit dem <@1 fliess kursiv>Commit<@$p>-Befehl in das Repository zurück und macht damit seine Änderungen anderen Entwicklern zugänglich. Wenn andere Entwickler zur selben Zeit dieselben Dateien bearbeiten, werden die verschiedenen neuen Versionen lokal mit dem <@1 fliess kursiv>Update<@$p>-<\h>Befehl verschmolzen. Lässt sich das Ergebnis korrekt bauen und testen, werden die verschmolzenen Dateien gleichfalls in den CVS-Baum zu<\h>rückgeschrieben. Ist das nicht automatisch möglich, müssen sich die beteiligten Entwickler über die Integration ihrer Änderungen untereinander verständigen. Diese als »kopieren-verändern-zusammenfügen« bezeichnete Methode hat den Vorteil, keine Sperrung der Quelldateien zu erfordern, die gerade von einem Entwickler bearbeitet werden. Damit Anwender die neuen Funktionen sofort verwenden können, ohne die gesamte Software neu installieren zu müssen, werden daraus <@1 fliess kursiv>Patches <@$p>erstellt, die sie in ihre bestehende Installation einfügen können. In regelmäßigen Abständen, gewöhnlich wenn bestimmte Meilensteine in der Entwicklung erreicht sind, werden Zweige des Quellbaums eingefroren, um sie für die folgende <@1 fliess kursiv>Release<@$p>, die Freigabe als stabile Version vorzubereiten<k-20>.<@3 hoch fliess>15<@$p>@2  ZÜ 3:Debugging @1 fliess ohne:Software enthält Fehler, die nach einer Motte, die 1945 einen Hardwarekurzschluss verursachte, als <@1 fliess kursiv>Bugs<@$p> bezeichnet werden.<@3 hoch fliess>16<@$p> Einer Informatikerfaustregel zufolge steckt in jedem Programm pro 100 Zeilen <\n>Code ein <@1 fliess kursiv>Bug<@$p>. Sich dieser Tatsache des Lebens öffentlich zu stellen, fällt Softwareunternehmen schwer. Bei ihnen herrscht, was Neal Stephenson eine »institutionelle Unehrlichkeit« nennt. @1 fliess mit:@1 fliesskursiv Zitat:»Kommerzielle Betriebssysteme müssen die gleiche offizielle Haltung gegenüber Fehlern einnehmen, wie sie die kommunistischen Länder gegenüber der Armut hatten. Aus doktrinären Gründen war es nicht möglich zuzugeben, dass Armut in den kommunistischen Ländern ein erns<\h>tes Problem darstellte, weil der Kommunismus sich ja gerade zum Ziel gesetzt hatte, die Armut zu beseitigen. Ebensowenig können kommerzielle Betriebssystemhersteller wie Apple und Microsoft vor aller Welt zugeben, dass ihre Software Fehler enthält und ständig abstürzt. Das wäre so, als würde Disney in einer Presseerklärung bekanntgeben, dass Mickey Mouse ein Schauspieler in einem Kostüm ist... Kommerzielle Betriebssystemanbieter sind als direkte Folge ihrer kommerziellen Ausrichtung dazu gezwungen, die äußerst unredliche Haltung einzunehmen, dass es sich bei Bugs um selten auftretende Abweichungen handelt, meist die Schuld von anderen, und es daher nicht wert sind, genauer über sie zu sprechen« <@1 fliess normal>(<@6 Caps>Stephenson<@1 fliess normal>, 1999). <@$p>@1 fliess mit:@1 fliess ohne:Freie Softwareprojekte dagegen können sich dem Problem offen stellen. Ihr Verfahren, mit <@1 fliess kursiv>Bugs<@$p> umzugehen, stellt einen der wichtigsten Vorteile gegenüber proprietärer Software dar. Die Stabilität dieser Software, d.h. ihr Grad an Fehlerfreiheit, verdankt sich nicht der Genialität ihrer Entwickler, sondern der Tatsache, dass jeder Anwender Fehler an den Pranger stellen kann und die kollektive Intelligenz von Hunderten von Entwicklern meist sehr schnell eine Lösung dafür findet. @1 fliess mit:Wer z. B. in Debian GNU/Linux einem<@1 fliess kursiv> Bug <@$p>begegnet, schickt einen E-Mailbericht darüber an »submit<\@>bugs.debian.org«. Der Bug erhält automatisch eine Nummer (Bug#nnn), sein Eingang wird dem Anwender bestätigt und er wird auf der Mailingliste »debian-bugs-dist« gepostet. Hat der Einsender den Namen des Pakets, in dem der <@1 fliess kursiv>Bug<@$p> aufgetreten ist, angegeben, erhält auch der Maintainer dieses Pakets eine Kopie. Betrifft der Fehler eines der in der Debian-Distribution enthaltenen selbständigen Pakete (XFree86, Apache etc.), erhalten auch die Zuständigen für dieses Projekt eine Kopie. In das Rückantwort-Feld der E-Mail wird die Adresse des Einsenders und »nnn<\@>bugs.debian.org« eingetragen, so dass die Reaktionen darauf an alle Interessierten in der Projekt-Community gehen. Übernimmt der Maintainer oder ein anderer Entwickler die Verantwortung für die Lösung des Problems, wird auch seine Adresse hinzugefügt. Er ordnet den <@1 fliess kursiv>Bug<@$p> einer von sechs Dringlichkeitskategorien (»kritisch«, »schwer wiegend«, »wichtig«, »normal«, »behoben« und »Wunschliste«) zu. Gleichzeitig wird er in die webbasierte, öffentlich einsehbare Datenbank »http://www.debian.org/Bugs« eingetragen. Die Liste ist nach Dringlichkeit und Alter der offenen Fehler sortiert und wird einmal pro Woche auf »debian-bugs-reports« gepostet. In vielen Fällen erhält man binnen 24 Stunden einen <@1 fliess kursiv>Patch<@$p>, der den Fehler beseitigt, oder doch zumindest Ratschläge, wie man ihn umgehen kann. Auch diese Antworten gehen automatisch in die <@1 fliess kursiv>Bug<@$p>-Datenbank ein, so dass andere Nutzer, die mit demselben Problem konfrontiert werden, hier die Lösungen finden.<@3 hoch fliess>17<@$p> Die Stärke der freien Projekte beruht also darauf, dass alle Änderungen an der Software eingesehen werden können, sobald sie in den CVS-Baum eingetragen sind, und dass <@1 fliess kursiv>Releases <@$p>in kurzen Abständen erfolgen. Dadurch können sich Tausende von Nutzern am Auffinden von <@1 fliess kursiv>Bugs<@$p> und Hunderte von Entwicklern an ihrer Lösung beteiligen. Das <@1 fliess kursiv>Debugging <@$p>kann hochgradig parallelisiert werden, ohne dass der positive Effekt durch erhöhten Koordinationsaufwand und steigende Komplexität konterkariert würde. In der Formulierung von Raymond lautet diese Faustregel: »Wenn genügend Augen sich auf sie richten, sind alle Fehler behebbar« (<@6 Caps>Raymond<@$p>, 1998). Gemeint ist nicht etwa, dass nur triviale Fehler auf diese Weise beseitigt werden könnten, sondern dass durch die große Zahl von Beteiligten die Chance steigt, dass einige von ihnen genau in dem frag<\h>lichen Bereich arbeiten und relativ mühelos Lösungen finden können.Frank Rieger weist darauf hin, dass dieses Verfahren nicht nur durch seine Geschwindigkeit den Verfahren in der proprietären Software überlegen ist, sondern auch durch die Gründlichkeit seiner Lösungen:@1 fliesskursiv Zitat:»[D]adurch, dass die Sourcen offen sind, werden oft genug nicht nur die Symptome behoben, wie wir das z. B. bei Windows-NT sehen, wo ein Problem im Internet-Information-Server ist und sie sagen, wir haben dafür jetzt gerade keinen Fix, weil das Problem tief im Kernel verborgen liegt und da können wir jetzt gerade nichts tun, sondern wir geben euch <t1>mal einen Patch, der verhindert, dass die entsprechenden Informationen bis zum Kernel durchdringen – also: Pflaster draufkleben auf die großen Löcher im Wasserrohr. Das passiert halt in der Regel bei Open <t$>Source-Systemen nicht, da werden die Probleme tatsächlich <t-1>behoben.«<x@3 hoch fliess><t-1>18<@$p>@1 fliess mit:@1 fliess ohne:Die Geschlossenheit des proprietären Modells ist nützlich für den Schutz des geistigen Eigentums. Für die Stabilität der Software ist es ein struktureller Nachteil, durch den es nicht mit freier Software konkurrieren kann: @1 fliesskursiv Zitat:»In der Welt der Open Source-Software sind <@1 fliess normal>Fehlerberichte <@$p>eine nützliche Information. Wenn man sie veröffentlicht, erweist man damit den anderen Nutzern einen Dienst und verbessert das Betriebssystem. Sie systematisch zu veröffentlichen, ist so wichtig, dass hochintelligente Leute freiwillig Zeit und Geld investieren, um <@1 fliess normal>Bug<@$p>-Datenbanken zu betreiben. In der Welt der kommerziellen Betriebssysteme ist jedoch das Melden eines <@1 fliess normal>Fehlers <@$p>ein Privileg, für das Sie eine Menge Geld bezahlen müssen. Aber wenn Sie dafür zahlen, folgt daraus, dass der <@1 fliess normal>Bug<@$p>-Report vertraulich behandelt werden muss – sonst könnte ja jeder einen Vorteil aus den von Ihnen bezahlten 95 Dollar ziehen« <@1 fliess normal>(<@6 Caps>Stephenson<@1 fliess normal>, 1999).<@$p><f"FFScala">@1 fliess ohne:Natürlich gibt es auch in der proprietären Softwarewelt Testverfahren und <@1 fliess kursiv>Bug-Reports<@$p>. Nach einer ersten Testphase, die innerhalb des Hauses mit einer begrenzten Gruppe von Mitarbeitern durchgeführt wird, geben Unternehmen üblicherweise <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Beta-Versionen<@$p> heraus. Diese enthalten natürlich keinen Quellcode. Nur das Finden, nicht aber das Beheben von Fehlern wird von den Beta-Testern erwartet. Für diese unentgeltliche Zuarbeit müssen die Tester auch noch bezahlen. Die Beta-Version von Windows 2000 b<t2>eispielsweise verkaufte Microsoft in Deutschland für etwa 200 Mark. Wer <x@1 fliess kursiv><t2>Bug-Reports<@$p><t2> einsendet, erhält als Dank z. B. ein MS-Office-Paket geschenkt und bekommt einen Nachlass beim Kauf der »fertigen« Version. Käufer von Betatest-Versionen sind neben leidenschaft<\h>lichen Anwendern, die sofort alles haben möchten, was neu ist, vor allem Applikationsentwickler, die darauf angewiesen sind, dass ihre Programme mit der neuen Version von Windows zusammenarbeiten. Kommt schließlich die erste »fertige« Version der Software auf den Markt, stellt der Kundendienst eine weitere Quelle für den Rücklauf von Fehlern dar.@1 fliess mit:Die 95 Dollar, von denen Stephenson spricht, sind der Preis, den Kunden bei Microsoft nach dem <@1 fliess kursiv>Pay Per Incident-<@$p>System zu entrichten haben. Bei diesem Kundendienst können gewerbliche Anwender von Microsoft-produkten »Zwischenfälle«, die sie nicht selbst beheben können, melden und erhalten dann innerhalb von 24 Stunden eine Antwort von einem <@1 fliess kursiv>Microsoft Support Professional<@$p>. Seit Stephenson seinen Aufsatz schrieb, ist der Preis auf 195 Dollar für Zwischenfälle, die über das Internet gemeldet werden, und 245 Dollar für solche, die per Telefon durchgegeben werden, gestiegen.<@3 hoch fliess>19<@1 fliess normal> <@$p>Die Fehlerhaftigkeit ihrer Software sehen solche Unternehmen als zusätzliche Einnahmequelle an.@2  ZÜ 3:Die Releases @1 fliess ohne:Durch die beschleunigten Innovationszyklen in der von der wissenschaftlich-technologischen Wissensproduktion vorangetriebenen »nach<\h>industriellen Gesellschaft« (Daniel Bell) werden auch materielle Produkte in immer kürzeren Abständen durch neue Versionen obsolet gemacht. Nirgends ist diese Beschleunigung deutlicher als in der Informations- und Kommunikationstechnologie und hier besonders in der Software. Nach der herkömmlichen Logik kommt eine Software auf den Markt, wenn sie »ausgereift« ist, tatsächlich jedoch, wenn der von der Marketing<\h>abteilung bekannt gegebene und beworbene Termin gekommen ist. Freie Projekte dagegen veröffentlichen Entwicklerversionen frühzeitig und in kurzen Abständen. Offizielle Versionen werden dann freigegeben (<@1 fliess kursiv>re<\h>leased<@$p>), wenn sie den gewünschten Grad an Stabilität erreicht haben. Das Ausmaß der Änderung lässt sich an den Versionsnummern ablesen. <\n>Eine Veränderung von Ver(sion) 2 auf Ver 3 markiert ein fundamental neues Design, während ein Wechsel von Ver 3.3.3.1 zu Ver 3.3.4 einen kleineren Integrationsschritt darstellt. @1 fliess mit:Freie Software wird heute, wie mehrfach angesprochen, nicht als Produkt, sondern als kontinuierlicher Prozess verstanden. Zahlreiche Projekte h<t2>aben bewiesen, dass eine Entwicklergemeinschaft ihre Software über lange Zeit zuverlässig und in hoher Qualität unterstützen und <\n>weiterentwickeln kann. In der amerikanischen Debatte ist von einer »evolutionären« Softwaregenese durch Mutation, Selektion und Vererbung die Rede. Der »Basar« sorge dafür, dass in einem »darwinistischen Selektionsprozess« die bestangepasste und effizie<t$>nteste Software überlebe.<@3 hoch fliess>20<@$p>Abhängig von der Geschwindigkeit, mit der ein Projekt sich verändert, werden täglich oder im Abstand von Wochen oder Monaten neue <@1 fliess kursiv>Releases<@$p> herausgegeben.<@3 hoch fliess>21<@$p> Im Entwickler-Quellcodebaum werden laufend neue <@1 fliess kursiv>Patches<@$p> eingegeben, die über das CVS abrufbar sind. Die jeweils avancierteste Fassung einer Software ist naturgemäß noch nicht sehr stabil und daher nicht für den täglichen Einsatz geeignet. Sind die für eine neue Version gesteckten Ziele erreicht, wird der Source-Baum für weitere Änderungen gesperrt. Erst nachdem alle Änderungen weithin getestet und korrigiert worden sind und zuverlässig mit den vorbestehenden Bestandteilen zusammenarbeiten, wird ein offizielles <@1 fliess kursiv>Release<@$p> veröffentlicht. Im Unterschied zu den auftrags- und termingebundenen Entwicklungsarbeiten für proprietäre Software können bei freier Software die Ideen bis ins Detail ausdiskutiert und optimiert werden. Dirk Hohndel (von XFree86) sieht darin eine der Stärken von freien Softwareprojekten: »Denn ich kann einfach sagen: ›Naja, das funktioniert noch nicht. Ich <\n>release das Ding nicht.‹ Wenn ich jetzt natürlich schon 25 Millionen US-Dollar ausgegeben habe, um den 4. Mai als den großen Release-Tag in der Presse bekannt zu machen, dann sagt mein Marketing-Department mir: ›Das ist mir wurscht. Du released das Ding.‹«<@3 hoch fliess>22<@$p> Mit der zunehmenden Integration freier Software in konventionelle Wirtschaftsprozesse könnte der Druck jedoch wachsen, angekündigte <@1 fliess kursiv>Release<@$p>-Termine einzuhalten, um an anderen Stellen die Planbarkeit zu erhöhen. Als z. B. Linus Torvalds in seiner Keynote-Ansprache auf der <@1 fliess kursiv>LinuxWorld <@$p>im Februar 2000 in New York ankündigte, dass sich die Veröffentlichung des Linux-Kernels 2.4 um ein halbes Jahr verschieben werde, wertete die IT-Fachpresse dies als eine Schwäche.<@3 hoch fliess>23<@$p>@2  ZÜ 3:Institutionalisierung: Stiftungen und  <\n>nicht profitorientierte Unternehmen@1 fliess ohne:Freie Projekte entstehen als formloser Zusammenschluss von Individuen. Da unter den Beteiligten keinerlei finanzielle oder rechtliche Beziehungen bestehen, stellt dies für das Innenverhältnis kein Problem dar. Sobald jedoch die Außenverhältnisse zunehmen, sehen die meisten Projekte es als vorteilhaft, sich eine Rechtsform zu geben. Anlass dazu können Kooperationen mit Firmen sein, die z.<\!q>B. unter einer Vertraulichkeitsvereinbarung Hardwaredokumentation bereitstellen, so dass Treiber für Linux entwickelt werden können, die Beteiligung an Industriekonsortien, die Möglichkeit, Spenden für Infrastruktur (Server) und Öffentlichkeitsarbeit entgegenzunehmen oder die Notwendigkeit, Warenzeichen anzumelden oder die Interessen des Projekts vor Gericht auszufechten. Daher bilden sich meist parallel zu den weiterhin offenen Arbeitsstrukturen formelle Institutionen als Schnittstellen zu Wirtschaft und Rechtssystem. @1 fliess mit:Am augenfälligsten wird die Problematik an einer Episode aus dem April 1998. Damals trat IBM an die Apache-Gruppe mit dem Wunsch heran, deren Server als Kernstück in IBMs neues E-Commerce-Paket »WebSphere« aufzunehmen. Es handelte sich um ein Geschäft mit einem Marktwert von mehreren Millionen Dollar, und IBM war gewillt, die Apache-Entwickler dafür zu bezahlen. Doch sie mussten erstaunt feststellen, dass diese nur aus 20 über die ganze Welt verstreuten Individuen ohne eine Rechtsform bestanden: »Habe ich das wirklich richtig verstanden?«, zitierte <@1 fliess kursiv>Forbes Magazine <@$p>einen der IBM-Anwälte, »wir machen ein Geschäft mit einer ... Website?«<@3 hoch fliess>24<@1 fliess normal> <@$p>Für eine Geldzahlung von IBM hätte es also gar keinen Empfänger gegeben. Die Apache-Gruppe willigte unter der Bedingung in das Geschäft ein, dass der Webserver weiterhin im Quellcode frei verfügbar bleiben müsse. IBM zeigte sich in der Währung der freien Softwareszene erkenntlich: mit einem Hack. IBM-Ingenieure hatten einen Weg gefunden, wie der Apache-Webserver auf Windows-NT schneller laufen kann. Diese Software gaben sie im Quellcode zur freien Verwendung an die Szene weiter. Aufgrund dieser Erfahrung gründete sich aus dem Apac<@1 fliess normal>he-Core-Team di<@$p>e <@1 fliess kursiv>Apache Software Foundation<@$p> (ASF).<@3 hoch fliess>25<@$p> Die ASF ist ein mitgliederbasiertes gemeinnütziges Unternehmen. Individuen, die ihr Engagement für die kooperative freie Softwareentwicklung unter Beweis gestellt haben, können Mitglied werden. Hauptzweck der ASF ist die administrative Unterstützung der freien Entwicklungsprojekte.<@3 hoch fliess>26<@$p>Die erste rechtskräftige Institution entstand 1985 aus dem GNU-Projekt. Die <@1 fliess kursiv>Free Software Foundation <@$p>(FSF)<@3 hoch fliess>27<@$p> kann als gemeinnützige Ein<t-1>richtung steuerlich abzugsfähige Spenden entgegennehmen. Ihre Haupt<\h>e<t$>innahmen stammen jedoch aus dem Verkauf von freier GNU-Software (damals auf Datenbändern, heute auf CD), von freien Handbüchern und von Paraphernalia wie T-Shirts und Anstecker. Die Gewinne werden benutzt, um Entwickler dafür zu bezahlen, dass sie einzelne vorrangige Softwareelemente schreiben. Rechtsschutz (z. B. bei Lizenzstreitigkeiten) und Öffentlichkeitsarbeit sind weitere Aufgaben der FSF.<@3 hoch fliess>28 <@$p>Um diese Ziele diesseits des Atlantiks besser verfolgen zu können und regionaler Ansprechpartner für Politik, Medien und Wirtschaft zu sein, gründete sich  im März 2001 die FSF Europe (http://fsfeurope.org). Die FSF India folgte im Juli des Jahres (http://www.fsf.org.in/). <@1 fliess kursiv>Software in the Public Interest <@$p>(SPI)<@3 hoch fliess>29<@$p> dient als Dachorganisation für verschiedene Projekte, darunter die Debian GNU/Linux-Distribution, Berlin (ein Windowing-System), Gnome, die Linux Standard Base, Open Source.org und Open Hardware.org.<@3 hoch fliess>30<@$p> 1997 gegründet, erhielt SPI Inc. im Juni 1999 den Status der Gemeinnützigkeit. Spenden werden verwendet, um Domain-Namen (z. B. debian.org) zu registrieren, CDs für das Testen neuer Releases zu brennen oder Reisekosten für Treffen und Konferenzen zu bezahlen. Auch Hardwarespenden und Netzressourcen sind immer willkommen. SPI meldet ferner <@1 fliess kursiv>Trademarks <@$p>(TM) and <@1 fliess kursiv>Certification Marks<@$p> (CM) an. SPI ist Eigentümer der CMs »Open Source« und »Open Hardware« und des TM »Debian«, wobei die Verwaltung der Rechte den einzelnen Projekten obliegt. Beim XFree86-Projekt entstand die Notwendigkeit einer Rechtsform daraus, dass das X-Consortium, in dem die industrieweite Standardisierung und Weiterentwicklung des X-Window-Systems verhandelt wird, nur Unternehmen als Mitglieder aufnimmt. Das 1992 gegründete Projekt hatte Ende 1993 die Wahl, unter dem Dach einer Organisation teilzunehmen, die bereits Mitglied des Konsortiums war, oder selbst eine Rechtsform zu etablieren. Nachdem sich ein Anwalt fand, der bereit war, eine Unternehmensgründung <@1 fliess kursiv>pro bono <@$p>vorzunehmen, wählte man den zweiten Weg. Durch das aufwändige Anerkennungsverfahren dauerte es bis Mai 1994, bis das gemeinnützige Unternehmen, die XFree86 Project, Inc.,<@3 hoch fliess>31<@$p> gegründet war. Als wissenschaftliche Organisation dient sie der Forschung, Entwicklung und Implementation des X-Window-Systems und der Verbreitung von Quellcode, Objektcode, Ideen, Informationen und Technologien für die öffentliche Verwendung.<@3 hoch fliess>32<@$p>Die Konstruktion dieser Organisationen ist sorgfältig darauf bedacht, eine Verselbständigung zu verhindern. Ämter werden, ähnlich wie bei NGOs, in der Regel ehrenamtlich wahrgenommen. Die wichtigsten Lizenzen der freien Software sind darauf angelegt, eine proprietäre Schließung der Software zu verhindern. Es ist jedoch noch zu früh, um einschätzen zu können, ob Spannungen zwischen der freien Entwickler- und Nutzerbasis und ihren Institutionen sowie die Herausbildung von bürokratischen Wasserköpfen vermieden werden können. Gerade durch den Erfolg und die derzeitige Übernahme von Aspekten des freien Softwaremodells durch herkömmliche Unternehmen ist die Gefahr nicht von der Hand zu weisen, dass die freie Software ein ähnliches Schicksal erleidet, wie zahlreiche soziale Bewegungen vor ihr.@2  ZÜ 3:Die Motivation: Wer sind die Leute und warum machen die das ..., wenn nicht für Geld?@1 fliesskursiv Zitat:»Jede Entscheidung, die jemand trifft, beruht auf den Wertvorstellungen und Zielen dieser Person. Menschen können viele verschiedene Wertvorstellungen und Ziele haben; Ruhm, Geld, Liebe, Überleben, Spaß und Freiheit sind nur einige der Ziele, die ein guter Mensch haben kann. Wenn das Ziel darin besteht, anderen ebenso wie sich selbst zu helfen, nennen wir das Idealismus. Meine Arbeit an freier Software ist von einem idealistischen Ziel motiviert: Freiheit und Kooperation zu verbreiten. Ich möchte dazu beitragen, dass sich freie Software verbreitet, um proprietäre Software zu ersetzen, die Kooperation verbietet, und damit unsere Gesellschaft zu einer besseren zu machen.« <@1 fliess normal>(<@6 Caps>Richard Stallman<@1 fliess normal>, 1998)@1 fliess mit:@1 fliess ohne:Unsere Gesellschaften kennen wohltätige, karitative und kulturelle Formen des Engagements für eine gute Sache, die nicht auf pekuniären Gewinn zielen. Gemeinnützige Tätigkeiten sind steuerlich begünstigt und sozialwissenschaftlich in Begriffen wie Zivilgesellschaft, Dritter Sektor und NGOs reflektiert. Doch während einer Ärztin, die eine Zeit lang in Myanmar oder Äthiopien arbeitet, allgemeine Anerkennung gezollt wird, ist die erste Reaktion auf unbezahlte Softwareentwickler meist, dass es sich entweder um Studenten handeln muss, die noch üben, oder um Verrückte, da ihre Arbeit in der Wirtschaft äußerst gefragt ist und gut bezahlt würde. @1 fliess mit:»Jedes Geschäft – welcher Art es auch sei – wird besser betrieben, wenn man es um seiner selbst willen als den Folgen zuliebe treibt«, weil nämlich »zuletzt für sich Reiz gewinnt«, was man zunächst aus Nützlichkeitserwägungen begonnen haben mag, und weil »dem Menschen Tätigkeit lieber ist, als Besitz, ... insofern sie Selbsttätigkeit ist«.<@3 hoch fliess>33 <@$p>Diese humboldtsche Erkenntnis über die Motivlage der Menschen bietet einen Schlüssel für das Verständnis der freien Software.<@3 hoch fliess>34<@1 fliess normal> <@$p>Ihre Entwicklung gründet in einem kreativen Akt, der einen Wert an sich darstellt. Ein <@1 fliess kursiv>Learning-by-Doing<@$p> mag noch von Nützlichkeitserwägungen angestoßen wer<\h>den, das Ergebnis des Lernens ist jedoch nicht nur ein Zuwachs an Verständnis des Einzelnen, sondern eine Schöpfung, die man mit anderen teilen kann. Statt also aus dem Besitz und seiner kommerziellen Verwertung einen Vorteil zu ziehen, übergeben die Programmiererinnen und Programmier der freien Software ihr Werk der Öffentlichkeit, auf dass es bewundert, kritisiert, benutzt und weiterentwickelt werde. Die Anerkennung, die ihnen für ihre Arbeit gezollt wird, und die Befriedigung, etwas <t-1>in den großen Pool des Wissens zurückzugeben, spielen sicher eine Rolle. <t$>Die wichtigste Triebkraft vor allen hinzukommenden Entlohnungen ist die kollektive Selbsttätigkeit.<@3 hoch fliess>35<@$p>Entgegen einer verbreiteten Auffassung, dass vor allem Studenten mit viel Freizeit sich für freie Software engagieren, ist das Spektrum erheblich breiter. Bei XFree86 beispielsweise liegt das Altersspektrum der Beteiligten zwischen 12 und 50 Jahren, und sie kommen aus allen Kontinenten der Erde.<@3 hoch fliess>36<@$p> Die Voraussetzung für die Kooperation, in der freie Software entsteht, ist das Internet. Mit seiner zunehmenden Verbreitung wächst somit auch die Zahl derjenigen, die potenziell an solchen Projekten mitarbeiten. Viele sind tatsächlich Studenten – bei XFree etwa ein Drittel, beim GIMP, der von zwei Studenten gestartet wurde, ist es die Mehrzahl. Viele andere haben eine Anstellung als Programmierer, System<\h>architekt oder Systemadministrator, aber auch in nicht computerbezogenen Berufen und entwickeln in ihrer Freizeit an freien Projekten. @1 fliesskursiv Zitat:»Das sind ganz normale Leute, die nach ihrer ganz normalen 60-Stunden-Woche gerne auch noch mal 20, 30 Stunden etwas machen, was Spaß macht. Ich glaube, das entscheidende Kriterium für Leute, die sehr produktiv sind und viel in solchen Projekten arbeiten, ist einfach <t-3>die Faszination am Projekt. Und diese Faszination bedeutet auch sehr oft, d<t$>ass man sich abends um sieben hinsetzt, um noch schnell ein kleines Problem zu lösen und auf die Uhr schaut, wenn es vier Uhr früh ist.«<@3 hoch fliess>37<@$p>@1 fliess mit:@1 fliess ohne:Auch die produktivsten Entwickler bei KDE haben einen ganz normalen Tagesjob, oft auch als Programmierer. Für sie ist es eine Frage der Prioritäten. Der Bundesbürger verbringe im statistischen Mittel, so Kalle Dalheimer, 28 Stunden in der Woche vor dem Fernseher. Er selbst besitze keinen und vergnüge sich in dieser Zeit mit Programmieren.<@3 hoch fliess>38<@$p>@1 fliess mit:Vom Programmieren fasziniert zu sein, ist natürlich eine Grundvoraussetzung für die Beteiligung an freien Projekten. Der Computer als die universelle Maschine bietet ein großes Spektrum möglicher Kreativität. In ihm etwas Nützliches zu schaffen und von ihm ein sofortiges Feed<\h>back zu bekommen, kann etwas sehr Erfüllendes sein. Hinzu kommt, dass die Arbeit in einer offenen, konstruktiven Szene stattfindet, die guten Beiträgen bereitwillig Anerkennung zollt. Die Aufmerksamkeit, das <\h>Image, der Ruhm und die Ehre, die in dieser meritokratischen Wissensordn<t-1>ung zu erlangen sind, werden häufig als Antriebskraft genannt. Lob aus dieser Szene trägt fraglos dazu bei, das Selbstwertgefühl zu erhöhen. Sebastian Hetze weist jedoch darauf hin, dass noch vor allen Eitelkeiten schon Neugier und Lernbegierde eine hinreichende Motivation darstellen: <t$>@1 fliesskursiv Zitat:»Wenn ich etwas dazu tue, dann veröffentliche ich nicht nur für mein Ego, sondern ich setze mich auch der Kritik aus. Und da ist es ganz klar so, dass in der Welt der freien Software ein sehr positives, ein konstruktives Kritikklima herrscht, d.h. ich werde ja für einen Fehler – Software hat <@1 fliess normal>Bugs<@$p> –, den ich in einem Programm gemacht habe, nicht runtergemacht. Es sind ganz viele Leute da, die versuchen, Fehler mit mir gemeinsam zu beheben, d.h. ich lerne. Und das ist eine ganz wichtige Motivation für sehr viele Menschen, die sich einfach auch daran weiterentwickeln wollen. An dieser Stelle sind die Motivationen ganz offenbar so stark, dass sie über das rein Monetäre hinausgehen. Ich muss vielleicht sogar an dieser Stelle diese Ego-Präsentation ein biss<\h>chen zurücknehmen. Es ist bei den großen Softwareprojekten so, dass natürlich oben immer welche im Rampenlicht stehen, aber die vielen hundert Leute, die nichts weiter machen, als mal einen <@1 fliess normal>Bugfix <@$p>oder eine gute Idee reinzugeben, kriegen niemals diese Aufmerksamkeit. Ich habe selber viel <@1 fliess normal>Source Code <@$p>gelesen – das musste ich, um das Linux-Handbuch zu schreiben – und enorm viel gelernt, und ich bin dafür enorm dankbar. Ich lese es einfach nur, um es zu lernen, weil es mich bereichert.<k10>«<@3 hoch fliess>39<@$p>@1 fliess mit:@1 fliess ohne:Die Forscherinnen des sozialwissenschaftlichen Projekts »Kulturraum Internet« am Wissenschaftszentrum Berlin fassen die Beweggründe der freien Entwickler folgendermaßen zusammen:@1 fliesskursiv Zitat:»Eine interessante Frage lautet: Was motiviert die Menschen, etwas     zu einem freien Softwareprojekt beizutragen? Es gibt individuelle und    soziale Motivationen:<@5 Klötzchen (kleines p)>p  <\i><\i><@$p>intellektuelle Herausforderung und Spiel,<@5 Klötzchen (kleines p)>p  <\i><@$p>Kreativität und der Stolz auf etwas selbst Geschaffenes,<@5 Klötzchen (kleines p)><*L>p  <\i><@$p>etwas verwirklichen, das den eigenen Ansprüchen an Stil und Qualität genügt,<@5 Klötzchen (kleines p)>p  <\i><@$p>ein sozialer Kontakt mit Leuten, die dieselben Ideen und Interessen teilen,<@5 Klötzchen (kleines p)>p  <\i><@$p>Ruhm,<@5 Klötzchen (kleines p)>p  <\i><@$p>Förderung der kollektiven Identität« <@1 fliess normal>(<@6 Caps>Helmers/Seidler<@1 fliess normal>, 1995).<@$p>@1 fliess mit:@1 fliess ohne:Wenngleich die Motivation der freien Entwickler nicht auf wirtschaftlichen Profit zielt, zahlt sich die Möglichkeit, sich über ein Projekt zu profilieren, schließlich auch auf dem Arbeitsmarkt aus. Wer sich beim Management eines freien Projektes oder durch seinen Code bewiesen hat, ist ein begehrter Mitarbeiter der Softwareindustrie. Im besten Fall wird er dann dafür bezahlt, im Rahmen seines Arbeitsverhältnisses weiterhin freie Software zu entwickeln. Cygnus z. B. war eines der ersten Unternehmen, die auf die Entwicklung und den Support von GNU-Software setzten. Auftraggeber sind häufig Hardwareunternehmen, die die GNU-Tools (wie den C-Compiler) durch Cygnus auf eine neuen Architektur portieren lassen. Bezahlt wird die Arbeit. Ihre Resultate stehen unter der GPL wieder allen zur Verfügung.<@3 hoch fliess>40<@$p> Auch der Unix-Distributor SCO, das Linux-Kontor oder Be entwickeln freie Software stückchenweise im Kundenauftrag.@1 fliess mit:In zunehmendem Maße haben Leute also auch im Rahmen ihrer Anstellung bei einem Internet Service Provider, einem Hardwarehersteller oder einer dedizierten freien Softwarefirma die Gelegenheit, an freier Software zu arbeiten. Eine Umfrage unter den XFree86-Entwicklern ergab, dass rund zehn Prozent von ihnen (65 Personen) ihren Lebensunterhalt direkt mit freier Software verdienen.<@3 hoch fliess>41<@$p> GNU/Linux-Distributoren wie Red Hat, Caldera oder SuSE bezahlen Angestellte dafür, Software zu entwickeln, die an die freie Softwaregemeinde freigegeben wird. Die Ausweitung der installierten Basis allein – der europäische Marktführer SuSE verkaufte von seinem GNU/Linux-Paket 6.0 innerhalb von zwei Monaten mehr als 100<\!q>000 Stück – gewährleistet Einnahmen aus Support, weiteren Dienstleistungen und kommerziellen Zusatzprodukten. Das kooperative Klima in einer solchen freien Umgebung motiviert auch angestellte Entwickler, länger an einem Projekt zu arbeiten. Cygnus verzeichnet einen Personalwechsel, der nur ein Zehntel dessen anderer Silicon Valley-Firmen beträgt (vgl. <@6 Caps>Tiemann<@$p>, 1999, S. 85). Die Zusammensetzung der Entwicklergemeinschaft ist von Projekt zu Projekt sehr unterschiedlich. Auch nach der Art der Aufgaben unterscheiden sich die Beteiligten. Große Programmierarbeiten, wie Betriebssystemkerne, werden eher von fest angestellten oder in ihrer Freizeit arbeitenden erfahrenen Programmierern übernommen, während die Entwicklung von kleineren Tools und das <@1 fliess kursiv>Bugfixing <@$p>von einer viel größeren und heterogeneren Gruppe getragen wird. @1 fliesskursiv Zitat:»FreeBSD kann sich den Luxus leisten, dass viele der Hauptentwickler fest angestellt sind von größeren Firmen, z. B. von Walnut-Creek CD-ROM, die ja ihren FTP-Server-Betrieb auf FreeBSD fahren. [...] Ein anderer Entwickler sitzt bei der NASA und baut für die gerade einen Parallelrechner auf FreeBSD-Basis auf, und auch das geht alles als <@1 fliess normal>Feedback <@$p>in das Projekt zurück. Soweit mal zum Kernel, wo wir den größten Teil an zentral anfallender Hauptarbeit haben. Bei den <@1 fliess normal>Utilities<@$p> außenrum ist es wirklich ein Heer von Leuten, die Kleinigkeiten machen, so wie ich das hin und wieder auch tue.«<@3 hoch fliess>42<@$p>@1 fliess mit:@1 fliess ohne:Mit der zunehmenden Kommerzialisierung der freien Software nehmen auch die Bemühungen der Unternehmen zu, freie Entwickler anzuwerben. Bemerkenswert ist, dass viele den Verlockungen einer Festanstellung widerstehen. Linux-Entwickler Alan Cox sagte in einem <@1 fliess kursiv>c’t-<@$p>Interview, dass es viele seiner Kollegen vorzögen, ihre Entwicklungsarbeit weiterhin als Hobby zu betreiben. Statt weisungsgebunden zu arbeiten, schätzen sie die Freiheit höher, ihre eigenen Probleme zu wählen und sie in ihrer eigenen Zeit zu bearbeiten. Auch diejenigen, die mit GNU/Linux Geld verdienen, aber gleichzeitig unabhängig bleiben möchten, haben die Gelegenheit dazu: »Es gibt Hersteller, die ein Problem mit ihrer Hardware haben, aber nicht die Möglichkeit, es innerhalb ihrer Firma zu lösen. Solche Auftragsarbeiten werden weltweit vergeben – eine gute Sache für Leute, die nicht im Silicon Valley leben, sondern sonstwo in der Welt« (<@6 Caps>Diedrich<@$p>, 1999).@1 fliess mit:@2  ZÜ 3:Softwarezyklus: Entwickler, Power-User, Endnutzer@1 fliess ohne:Man kann drei große Phasen im Leben eines freien Softwareprojekts unterscheiden. In der ersten Phase zieht es ausschließlich Entwickler an. Die Software ist noch nicht mehr als eine rohe Skizze, ein Entwurf dessen, was es verspricht, zu werden. Jeder kann sich die Software beschaffen, aber nur wer selbst am Quellcode mitarbeiten möchte, wird es tun. In der zweiten Phase hat die Software ein Stabilität erlangt, die sie für <@1 fliess kursiv>Power-User<@$p> (z.<\!q>B. Systemadministratoren) interessant macht, die sie nicht primär weiterentwickeln, sondern einsetzen wollen. Installation, Integration und Wartung erfordern auch jetzt noch weiterreichende Programmierfähigkeiten, doch diese Nutzergruppe erhält dafür eine Funktionalität, Flexibilität und ein Maß an Kontrolle über die Software, die sie zu einer attraktiven Alternative zu proprietärer Software macht. @1 fliess mit:Hat die Software einen Grad an Stabilität erreicht, die sie für den praktischen Einsatz geeignet macht, gibt das Projekt ein offizielles <@1 fliess kursiv>Release<@$p> <\n>heraus. Parallel zu dieser Produktionsversion wird an einer Entwicklerversion weitergearbeitet. Diese Zweiteilung erlaubt einerseits eine kontinuierliche Verbesserung und Erweiterung, an der sich alle Interessierten beteiligen können sowie andererseits das Einfrieren, Durchtesten und Stabilisieren von Momentaufnahmen aus diesem Prozess, die bis zum nächsten Release unverändert für diejenigen bleibt, die die Software in ihrer täglichen Arbeit verwenden wollen. In der dritten Phase erhält die Software Merkmale wie Installationshilfen und Dokumentation, die sie schließlich auch für technisch weniger bedarfte Nutzer zugänglich macht. Dieser Schritt wird oft nicht mehr von den selbst motivierten Teilnehmern des freien Projekts durchgeführt (»Coder schreiben keine Dokumentation«), sondern von Dienstleistungsfirmen, die oft eigens für den Support von freier Software gegründet wurden. Auf dieser Stufe stellt sich auch die Frage einer Integration vom Betriebssystem über die grafische Benutzeroberfläche bis zu den Anwendungen. Zentralistischen Modellen von Unternehmen wie Apple oder Microsoft gelingt dies naturgemäß gut, ebenso einem enger gekoppelten Projektverbund wie GNU. In einer weniger eng gekoppelten Umgebung besteht dagegen ein Bedarf an Abstimmung, ohne dafür jedoch auf zentralisierte Gremien zurück fallen zu wollen. Christian Köhntopp beschreibt die Problematik so: @1 fliesskursiv Zitat:»Wenn man Gesamtinstallationen eines Systems betrachtet, dann kann man sagen, die haben einen gewissen Reichtum oder eine Komplexität. Die ergibt sich einmal aus der Anzahl der <@1 fliess normal>Features,<@$p> die in dem System zur Verfügung stehen und auf der anderen Seite aus der Anzahl der Integrations- oder Kombinationsmöglichkeiten dieser <@1 fliess normal>Features.<@$p> Ein Problem, das ich bei vielen Projekten im Open Source-Bereich sehe, ist eine schlechte oder mangelnde Koordination über solche Projektgrenzen hinweg. Viele solche Open Source-Projekte schaffen es sehr gut, sich selbst zu managen und im Rahmen ihres Projektes auch entsprechende Integrationen zu schaffen, aber über Projektgrenzen hinweg ist das in der Regel deutlich schlechter entwickelt. Bei so großen Projekten wie KDE gibt es innerhalb des Projektes eine vergleichsweise gute Integration. Es war jedoch äußerst schmerzhaft, das KDE-Projekt und das vergleichbare Gnome-Projekt aufeinander abzustimmen. Es gab in der Vergangenheit sehr große Schwierigkeiten, etwa die XFree-Entwicklung und vergleichbare Projekte auf anderer Ebene miteinander abzustimmen. In diese Richtung müssten bessere Kommunikationsmöglichkeiten geschaffen oder unterstützt werden« <@1 fliess normal>(<@6 Caps>Köhntopp<@1 fliess normal>, Fachgespräch 7/1999).@1 fliess mit:@1 fliess ohne:Freie Software stößt auf gewisse Grenzen, wo sie Berührungsflächen mit einer unfreien Umgebung hat. So macht es proprietäre Hardware schwierig, Treiber für GNU/Linux und XFree86 zu schreiben. Jeder neue Drucker, jede Grafik- oder ISDN-Karte erfordert ein aufwändiges <@1 fliess kursiv>Reverse Engineering<@$p>, um sie unter freier Software nutzbar zu machen. Mit der wachsenden Installationsbasis nimmt jedoch auch das Interesse von Hardwareherstellern zu, ihre Produkte der GNU/Linux-Welt zugänglich zu machen, und sie bieten entweder von sich aus Treiber an oder stellen den Projekten unter Vertraulichkeitsbedingungen die erforderliche Dokumentation zur Verfügung. @1 fliess mit:Unfreie Bibliotheken stellen eine weitere Bedrohung dar. Sie locken Programmierer mit attraktiven Features in die Falle der Unfreiheit. Die Problematik stellte sich zum ersten Mal in den 80er-Jahren mit dem proprietären »Motif-Tookit«, auf den das X-Window System aufsetzte. Das freie XFree86 ersetzte »Motif« 1997 durch das freie »LessTif«. Eine ähnliche Problematik ergab sich ab 1996, als der Grafik-Desktop KDE die proprietäre GUI <@1 fliess kursiv>Toolkit Library<@$p> namens »Qt« der norwegischen Firma Troll Tech AS verwendete. Auch hier führte die Verwendung einer geschlossenen Software zu einem System mit mehr Fähigkeiten, aber weniger Freiheit. Drei Reaktionen darauf stellten sich ein, alle drei maßgeblich auf Initiative der FSF, die dafür bei vielen der Community auf Ablehnung stieß: Das Insistieren auf Freiheit sei übertrieben. 1997 startete Miguel des Icaza ein neues Projekt für einen Grafik-Desktop namens GNOME (<@1 fliess kursiv>GNU Network Object Model Environment<@$p>), das ausschließlich freie Software verwendet. »Harmony« ist eine Bibliothek, die »Qt« ersetzt und somit auch KDE zu einer vollständig freien Software macht. Schließlich veränderte Troll Tech auf Drängen der GNU/Linux-Welt 1998 die Lizenzbedingungen für »Qt«, so dass die Bibliothek seither in freien Umgebungen einsetzbar ist (vgl. <@6 Caps>Stallman<@$p>, 1999, S. 66 f). Die Popularität und die inhärenten Vorteile des freien Entwicklungsmodells haben aus unterschiedlichen Gründen eine Reihe von Firmen veranlasst, den Quellcode ihrer proprietären Software offenzulegen. Ob ein Softwareprojekt erfolgreich von einem geschlossenen in einen offenen Modus umgeschaltet werden kann, ist jedoch fraglich. Der spektakulärste Versuch ist Netscapes Mozilla, das von vielen als gescheitert angesehen wird. Zunächst fand der offengelegte Quelltext eine enthusiastische Aufnahme. Bereits Stunden nach der Freigabe setzte ein Strom von Code ein, der jedoch bald versiegte. Die Hauptarbeit, die da<\h>ran geleistet wurde, kam von Netscape-Angestellten. Einer der Protagonisten, Jamie Zawinski, schrieb in seiner öffentlichen Kündigung: »Warum auch immer, das Projekt wurde von Außenstehenden nicht angenommen. Es blieb ein Netscape-Projekt ... die Wahrheit ist, dass dank der Tatsache, dass die Gruppe der Mozilla-Kontributoren aus etwa 100 Vollzeit arbeitenden Entwicklern von Netscape und etwa 30 freien Entwicklern von außen bestand, die nur einen Teil ihrer Zeit investierten, das Projekt immer noch vollständig Netscape gehörte – denn nur diejenigen, die den Code schreiben, kontrollieren das Projekt wirklich.«<@3 hoch fliess>43<@$p>Im selben Text nennt Zawinski eine Reihe möglicher Gründe für das Scheitern. Was Netscape freigab, war nicht der Code der aktuellen Version des Navigators, u.a. fehlten die Module für E-Mail, Java- und Kryptografie. Code, den Netscape von anderen Firmen lizenziert hatte, wurde vor der Freigabe entfernt. Bei dem, was übrig blieb, handelte es sich um eine riesige Menge Code, aber nichts, was man tatsächlich hätte kompilieren und benutzen können. Die Menge war so groß, dass es sehr lange gedauert hätte, sich einzuarbeiten, um sinnvolle Beiträge leisten zu können. Zu vermuten ist, dass auch die innere Struktur einer Software, die in einem geschlossenen Modus erarbeitet wird (so genannter Spaghetti-<\h>Code), eine andere ist, als die, die in einem freien Projekt entsteht. Selbst wenn der Quellcode vollständig einsehbar ist, können die grundlegenden Design<\h>entscheidungen hinter den Details der Codierung unsichtbar bleiben.<@3 hoch fliess>44<@$p> In seinem Fazit warnt Zawinski davor, aus dem Scheitern von Mozilla zu schließen, dass das Open Source-Modell insgesamt nicht funktioniere. »Wenn man daraus eine Lehre ziehen kann, so ist es die, dass man nicht einfach ein sterbendes Projekt nehmen, es mit dem magischen Feenstaub des ›Open Source‹ besprenkeln und erwarten kann, dass dann alles märchenhaft funktioniert« (<@6 Caps>Zawinski<@$p>, 3/1999).Eine letzte Frage stellt sich in Bezug auf das freie Entwicklungsmodell: Kann es etwas grundsätzlich Neues hervorbringen? GNU/Linux, XFree86 oder GIMP sind von exisitierender proprietärer Software ausgegangen. Das bedeutet natürlich nicht, dass es einfach wäre, diese Programme unter freien Bedingungen zu rekonstruieren, aber die grund<\h>sätzlichen Designentscheidungen und die Zielvorgaben lagen vor, als die Projekte begannen. Es scheint, als könnte ein freies Projekt nur ein bekanntes Ziel verfolgen, aber sich keine neuen setzen. Ein Beleg dafür könnte der Betriebssystemkern des GNU-Projekts, der »Hurd«, sein, der dem seinerzeit aufregendsten, innovativsten Ansatz folgte, der Mikrokernarchitektur. Dass bis heute keine einsatzfähige Version des Hurd zustande gekommen ist, ist zweifellos auf dieselben Gründe zurückzuführen, aus denen sich Mikrokerne auch in der akademischen und kommerziellen Forschung nicht haben durchsetzen können. Torvalds dagegen wählte bei seinem Ansatz für Linux das altbewährte Makrokernmodell. Raymond fragt: »Nehmen wir einmal an, Linus Torvalds hätte versucht, während der Entwicklung [von Linux] fundamentale Innovationen im Design von Betriebssystemen vorzunehmen; ist es überhaupt vorstellbar, dass der so entstandene Kernel so stabil und erfolgreich wäre, wie der, den wir jetzt haben?« Er behauptet kathegorisch: »Im Basar-Stil kann man testen, debuggen und verbessern, aber es wäre sehr schwierig, ein völlig neuartiges Projekt im Basar-Modus zu starten« (<@6 Caps>Raymond<@$p>, 1998, Kap. 9). Bei einem Betriebssystem, also einer Infrastruktur für Anwendungssoftware, ist es wichtiger, dass es stabil und zuverlässig ist, als dass es radikal neue Strukturen ausbildet, was auch zur Folge hätte, dass alle darüber laufende Anwendungssoftware umgeschrieben werden müsste. Wenn auch noch zu beweisen wäre, dass auch freie Projekte zu revolutionären konzeptionellen Sprüngen in der Lage sind, so steht doch außer Zweifel, dass sie sich für eine evolutive Weiterentwicklung in kleinen iterativen Schritten hervorragend eignen. Tatsächlich hat sich in einigen Fällen der Fokus der Innovation von den proprietären Vorlagen zu den freien Projekten verlagert:@1 fliesskursiv Zitat:»War das PC-X[-Window] anfangs ein Anhängsel der großen, seriösen Workstation-X-Entwicklung, hat sich das Verhältnis inzwischen umgekehrt. Auch Jim Gettys, einer der Urväter von X, sieht heute XFree86 als die Gruppe, die die Entwicklung von X an sich weiter trägt. X.org soll diese Rolle wieder übernehmen, sofern die Gruppe endlich mal ins Rollen kommt, aber im Moment ist es die XFree86-Gruppe.«<@3 hoch fliess>45<@$p>@1 fliess mit: @2  ZÜ 1:Die Software@1 fliess mit:@1 fliess ohne:Die Zahl und die funktionale Bandbreite der freien Programme ist un<\h>überschaubar. Ein großer Teil hat »infrastrukturellen« Charakter. Besonders die Bereiche Internet, Betriebssysteme, Emulatoren und Entwicklerwerkzeuge ziehen die Aufmerksamkeit freier Entwickler auf sich. Schon grafische Desktop-Umgebungen (KDE, Gnome), die man ebenfalls als informatische Infrastruktur ansehen kann, sind vergleichsweise jung. Viele Applikationen stammen aus Universitäten und hier vor allem aus dem naturwissenschaftlich-technischen Bereich. Insgesamt lässt sich sagen, dass für alle Einsatzbereiche des Computers, einschließlich Büroanwendungen, Spiele, bis zu Videoschnittsystemen oder 3D-Modellierung freie Software exisitiert. Wer sich eine der GNU/Linux-Distributionen beschafft, erhält damit bereits zahlreiche stabile Softwarepakete. @1 fliess mit:Freie Software ist – ebenso wie proprietäre Software – nirgends katalogisiert. Ein großes Portal für GNU/Linux-Software, das »Dave Central« von Dave Franklin, verweist auf mehr als 4<\!q>000 Programme.<@3 hoch fliess>1<@$p> Die größten Kategorien sind Netzwerkprogramme, Programmierung, Systemwerkzeuge und Büroanwendungen. Handelt es sich dabei überwiegend um stabile, direkt anwendbare Programme, so bietet »SourceForge«<@3 hoch fliess>2<@$p> einen Einblick in die aktuellen Entwicklungsaktivitäten. SourceForge ist ein Angebot der VA Linux Systems Inc.<@3 hoch fliess>3<@$p>, das freien Entwicklern Hard- und Softwareressourcen wie CVS-Repositorien, Mailinglisten, Bug-Tracking, Dateiarchive, Foren und eine webbasierte Administration zur Verfügung stellt. Dort waren im Juni 2001 fast 22 000 Projekte gelistet, zwei Drittel davon auf GNU/Linux, ein knappes Drittel auf X Window, ein Fünftel mit stabilen Produktionsversionen. Ein Drittel wird in der Programmiersprache C erstellt, ein weiteres knappes Drittel in C++, die übrigen in diversen Sprachen (Perl, Java, PHP, Python, Assembler). Bis auf wenige Ausnahmen stehen die Projekte unter einer von der OSI gutgeheißenen Lizenz. Die Hälfte der Software richtet sich an Entwickler, die andere Hälfte an Endnutzer. Bei den Anwendungsbereichen bilden Internet, Kommunikation, System- und Entwicklungssoftware, Multimedia und Spiele die größten Gruppen. Ein weiterer wichtiger Startpunkt für die Suche nach bestimmten GNU/Linux-Programmen und für tägliche Updates über neue Projekte und neue Versionen ist »Freshmeat«.<@3 hoch fliess>4<@$p> Im Folgenden werden Kurzbeschreibungen und Verweise auf einige ausgesuchte freie Softwareprojekte geben, bevor auf ausgesuchte Projekte näher eingegangen wird.@1 fliess ohne:<@5 Klötzchen (kleines p)>p  <\i><@1 fliesshalbfett>GNU-Software<@3 hoch fliess>5<@$p> umfasst mehr als 200 im Rahmen des GNU-Projekts erstellte Programme<@3 hoch fliess>6<@$p> von den »Binutils« über ein CAD-Programm für Schaltkreise bis zu Spielen, darunter Standards wie CVS, Emacs, Ghostscript und Ghostview für die Erzeugung und Darstellung von PostScript- und PDF-Dateien, der Dateimanager »Midnight Commander«, das Notensatzprogramm »lilypond«, der Webseiten-Downloader »wget« und GNOME.<@5 Klötzchen (kleines p)>p  <\i><@1 fliesshalbfett>GNOME<@$p> (<@1 fliess kursiv>GNU Network Object Model Environment<@$p>)<@3 hoch fliess>7<@$p>, eine grafische Desktop-Umgebung. Das Projekt wurde Anfang 1998 gegründet. Es verwendet das aus dem GIMP-Projekt (s.u.) hervorgegangene <@4 Pfeil (Umschalt/Alt #)>’<@$p> Toolkit »Gtk« und umfasst viele Werkzeuge für die tägliche Arbeit, wie Datei-Manager und Office-Applikationen, insgesamt über 230 Programme.<@5 Klötzchen (kleines p)>p  <\i><@1 fliesshalbfett>BIND<@$p> (<@1 fliess kursiv>Berkeley Internet Name Daemon<@$p>)<@3 hoch fliess>8<@$p> ist der de facto Standard-DNS-Server für das Internet. Das Domain-Name-System des Internet wurde anhand von BIND entwickelt. BIND wird ebenso wie DHCP (<@1 fliess kursiv>Dynamic Host Configuration Protocol<@$p>) und INN (<@1 fliess kursiv>InterNetNews-Package<@$p>, ursprünglich geschrieben von Rich Salz) heute vom <@1 fliess kursiv>Internet Software Consortium<@3 hoch fliess>9<@$p> betreut. <@5 Klötzchen (kleines p)>p  <\i><@1 fliesshalbfett>Sendmail<@3 hoch fliess>10<@$p> wickelt den Transport von 90 Prozent des weltweiten <\h><\n>E-Mailverkehrs ab. Es stammt aus der Berkeley Universität. Sein Autor, Eric Allman, gründete 1997 aus dem Projekt eine Firma,<@3 hoch fliess>11<@$p> parallel dazu ist die quelloffene, freie Version weiterhin verfügbar.<@5 Klötzchen (kleines p)>p  <\i><@1 fliesshalbfett>Squid<@3 hoch fliess>12  <@$p> ist ein Proxy Server, der häufig bei großen ISPs zum Einsatz kommt.<@5 Klötzchen (kleines p)>p  <\i><@1 fliesshalbfett>SAMBA<@3 hoch fliess>13<@$p> ist ein Datei- und Druck-Server für Unix. Seit der Version 2.0 werden auch MS-Windows-Clients unterstützt. Seither ist es das wichtigste Instrument zur Integration von GNU/Linux- und Windows-Umgebungen.<@5 Klötzchen (kleines p)>p  <\i><@1 fliesshalbfett>PERL<@$p> (<@1 fliess kursiv>Practical Evaluation and Reporting Language<@$p>)<@3 hoch fliess>14<@$p> ist die Standard-Skriptsprache für Apache-Webserver (s.u.). PERL ist auf Unix vor allem aufgrund seiner leistungsstarken Zeichenkettenverarbeitung sehr beliebt. Das umfangreichste Archiv mit freien PERL-Skripten ist »CPAN«.<@3 hoch fliess>15<@$p><@5 Klötzchen (kleines p)>p  <\i><@1 fliesshalbfett>Ghostscript<@3 hoch fliess>16<@$p> ist ein PostScript-Interpreter, geschrieben von L. Peter Deutsch, unter GPL entwickelt, dann auf die Aladdin-Lizenz umgeschwenkt.<@3 hoch fliess>17<@$p><@5 Klötzchen (kleines p)>p  <\i><@1 fliesshalbfett>Majordomo<@$p> war der vorherrschende Mailinglisten-Server im Internet. Er wurde 1992 von Brent Chapman in PERL geschrieben. Heute tritt das zeitgemäßere <@1 fliesshalbfett>GNU Mailman<@3 hoch fliess>18<@$p> an seine Stelle.<@5 Klötzchen (kleines p)>p  <\i><@1 fliesshalbfett>WINE<@$p> (<@1 fliess kursiv>Wine Is Not an Emulator<@$p>)<@3 hoch fliess>19<@$p> ist eine Windows-Emulationsbibliothek für Unix, die es erlaubt, Windows-Applikationen in einem Unix-Fenster zu starten.<@5 Klötzchen (kleines p)>p  <@1 fliesshalbfett><\i>Zope<@$p> (<@1 fliess kursiv>Z Object Publishing Environment<@$p>)<@3 hoch fliess>20<@$p> ist eine Webapplikationsplattform für die Generierung von dynamischen Webseiten. Basiert auf der Skriptsprache »Python« und auf Produkten der Firma Digital Creations, die auf Empfehlung eines <@1 fliess kursiv>Venture Capital<@$p>-Investors 1998 unter eine freie Lizenz <@3 hoch fliess>21<@$p> gestellt wurden.<@5 Klötzchen (kleines p)>p  <\i><@1 fliesshalbfett>OpenBIOS<@3 hoch fliess>22<@$p> ist ein Projekt, eine standard-konforme Firmware zuschaffen, die proprietäre PC-BIOSe ersetzen soll. Die erste Entwicklerversion wurde im November 1998 freigegeben. @1 fliess mit:@2  ZÜ 2:BSD @1 fliess mit:@1 fliess ohne:<t-1>1977 hatte Bill Joy an der Berkeley University die erste <x@1 fliess kursiv><t-1>Berkeley Software Distribution<@$p><t-1> (BSD) mit Unix-Programmen und im Jahr darauf die vollständige Unix-Distribution 2.11BSD zusammengestellt. Die Version 4.2BSD (1983), die ein schnelles Dateisystem und TCP/IP enthielt, wurde auch unter kommerziellen Anbietern von Unix-Rechnern populär und trug viel zur weltweiten Verbreitung von Unix und Internetzwerken bei. Während sich die Anbieter kommerzieller Unix-Versionen gegenseitig die Lizenzdaumenschrauben anlegten, gab die <x@1 fliess normal><t-1>Berkeley-Gruppe 1989 zunächst die Netzwerkelemente aus BSD als Networking Release 1 u<@$p><t-1>nter der eigens dafür entwickelten freien BSD-Lizenz heraus. In einer im Vergleich zum GNU/Linux-Projekt wenig beachteten offenen, internetbasierten Entwicklung wurden daraufhin alle geschützten Unix-Module durch freien Code ersetzt, der 1991 als <x@1 fliess normal><t-1>Networking Release 2 er<@$p><t-1>schien. Diese Version portierte Bill Jolitz auf den PC und veröffentlichte sie Anfang 1992 als 386/BSD unter der BSD-Lizenz. Um diese Version sammelten sich enthusiastische Nutzer und Entwickler, die sie als NetBSD-Gruppe weiter pflegten. Damit migrierte das Projekt vollends aus der Akademie ins Internet. Die Forschungsgruppe an der Berkeley Universität legte 1995 letzte Änderungen als 4.4BSD-Lite, Release 2 vor und lös<\h>te sich dann auf.<x@3 hoch fliess><t-1>23@1 fliess mit:Gleich zu Anfang der Weiterentwicklung des 386/BSD kam es, aus weitgehend persönlichen Gründen, zu einer Spaltung. Die <@1 fliesshalbfett>NetBSD<@$p>-Gruppe<@3 hoch fliess>24<@$p> zielte darauf, eine möglichst große Verbreitung auf möglichst viel Plattformen zu erreichen, mit dem Ergebnis, dass es heute fast keinen 32-Bit-Prozessor mit einer Memory Management Unit gibt, auf dem NetBSD nicht läuft. Das zweite daraus hervorgegangene Projekt <@1 fliesshalbfett>Free<\h>BSD<@3 hoch fliess>25<@$p> zielte darauf, ein leistungsfähiges, stabiles Unix-Server-Betriebssystem für Intel-CPUs zu schaffen. FreeBSD ist auch auf den Alpha portiert, aber der Schwerpunkt liegt auf Intel. FreeBSD hat heute die größte Installationsbasis der aus dem Net Release 2 abgeleiteten Sys<\h>teme. 1997 kam es innerhalb der NetBSD-Gruppe zu einer weiteren Spaltung. Da<\h>raus ging ein Projekt namens <@1 fliesshalbfett>OpenBSD<@3 hoch fliess>26<@$p> hervor, das das Betriebssystem auf Sicherheit hin optimiert. Die unterstützten Hardwareplattformen sind ähnlich wie bei NetBSD. Die drei Projekte bestehen freundschaftlich nebeneinander und übernehmen Innovationen voneinander. Diese Zusammenarbeit gibt es auch mit der Welt des parallel heranreifenden GNU/Linux und anderer Betriebssysteme,<@3 hoch fliess>27<@$p> mit XFree86, Apache, KDE und weiteren Projekten. Während eine GNU/Linux-Distribution um den Kernel herum Softwarepakete aus den unterschiedlichsten Quellen teilweise in Binärform zusammenträgt, gibt es bei BSD ein zentrales Source Code-Management für den Kernel <@1 fliess kursiv>und<@$p> alle Werkzeuge. Installiert wird aus dem Quellcode. Installierer wie d<@1 fliess normal>er »Red Hat Package-Manager« (rpm) oder »dselect«, die fertige Binary-Packages installieren, wie das in der PC-Welt üblich ist, sind in der Unix-Welt eine relativ junge Erfindung. Zur Automatisierung der Source Code-Installatation dient bei FreeBSD die »Ports Collection«. Sie umfasst heute einen Source-Baum mit »Make Files« für mehrere Tausend Applikationen. Diesen Baum spielt sich der Nutzer auf seine lokale Festplatte und wählt die Make-Dateien der Softwarepakete aus, die er installieren möchte. Make is<@$p>t ein Projektverwaltungswerkzeug, mit dem man automatisiert Software übersetzen kann. Ruft man die entsprechenden Make-Dateien auf, so werden die Pakete für den Apache, KDE usw. direkt vom Original-FTP-Server geholt, eventuell für FreeBSD angepasst, Variablen werden gesetzt, der Quellcode kompiliert, installiert und in einer Packagedatenbank registriert, so dass sich alles wieder sauber deinstallieren lässt. Bei der Erstinstallation erzeugt der Befe<@1 fliess normal>hl »make world« auf diese Weise das gesamte BSD-System. Updates erfordern nur, dass ein Eintrag im entsprechenden Make File geän<@$p>dert wird, und sofort oder auch in regelmäßigen Abständen werden die aktuellen Sourcen z.B. des Apache über das Internet auf den eigenen Rechner übertragen. Ein Sys<\h>temadministrator kann das Update auf einem Rechner durchführen und dann von dort aus auf alle BSD-Rechner in seinem Netz exportieren, was der Verwaltung einer größeren Menge von Rechnern sehr förderlich ist. Sämtliche freie Softwareprojekte, die man aus der GNU/Linux-Welt kennt, laufen ebenfalls auf den drei BSD-Varianten. Neben mehreren Tausend freien Applikationen lassen sich auch proprietäre nur binär verfügbare Applikationen, wie z.B. WordPerfect oder E-Commerce-Software, unter BSD betreiben. Unter den Benutzern von FreeBSD finden sich so illustre Namen wie Yahoo und Hotmail. Microsoft versucht seit dem Ankauf von Hotmail erfolglos, diesen kostenlosen E-Mail-Service auf NT-Server umzustellen. Der größte FTP-Server der Welt, mit 5 000 bis 6 000 Nutzern <t-1>gleichzeitig und einem Terra-Bit an täglichem Datendurchsatz ist ein Free<\h>BS<t$>D-Rechner.<@3 hoch fliess>28<@$p> @2  ZÜ 2:Debian GNU<\!q>/<\!q>Linux@1 fliess mit:@1 fliess ohne:Debian<@3 hoch fliess>29<@$p> startete 1993, als Debra und Ian Murdock (aus deren Vornamen sich »Debian« zusammensetzt) im Usenet zur Mitarbeit an einer neuen GNU/Linux-Distribution aufriefen. Die Distributionen der Zeit litten noch an Kinderkrankheiten. Abhängigkeiten und Konflikte wurden nicht berücksichtigt. Zu Binärpaketen fand sich kein Quellcode. Fehler wurden in neue Versionen übertragen, obwohl bereits Korrekturen vorlagen. Die Lizenzen von Komponenten ließen sich nicht ersehen. Hier wollten die Murdocks und ein Dutzend Mitstreiter Abhilfe schaffen. @1 fliess mit:<t-2>Bei Debian wie bei den anderen Distributionen steht nicht die Entwicklung von Software selbst, sondern ihre Integration in ein stabiles Gesamtsystem im Vordergrund. Zentrales Instrument, um aus einer Anhäufung von Einzelelementen ein System zu komponieren, ist die Paketverwaltung, die bei Debian »<x@1 fliess normal><t-2>dpkg«<x@1 fliess kursiv><t-2> <@$p><t-2>heißt. Sie protokolliert bei der Installation, welche Dateien zu einem Paket gehören, so dass sie sauber deinstalliert oder von neuen Versionen überschrieben werden können. Sie verhindert, dass konfligierende Programme installiert werden und dass Programme, die von anderen abhängen, nur mit diesen zusammen installiert werden (ein <x@1 fliess kursiv><t-2>cron<@$p><t-2>-Paket, mit dem sich Kommandos automatisch zu festgelegten Zeiten ausführen lassen, z.B. benötigt einen Mailserver, da es bei Fehlern E-Mails verschickt). Sie verwaltet Konfigurationsdateien, integriert Hilfedateien ins System und macht dem System <x@4 Pfeil (Umschalt/Alt #)><t-2>’<@$p><t-2> <x@1 fliess kursiv><t-2>Shared Libraries<@$p><t-2> bekannt. Mit dem Programm »alien« lassen sich auch Pakete anderer Distributionen, wie Red Hat oder Slackware, auf Debian installieren und umgekehrt. <t$>Die Arbeit begann mit finanzieller Unterstützung der FSF. In den ers<\h>ten drei Jahren bauten die Murdocks das Projekt auf. Seither wird der Projektleiter jährlich aus den Reihen der Mitarbeiter gewählt. Es waren dies Bruce Perens, Ian Jackson, seit 1999 ist es Wichert Akkerman. Der Projektleiter übergibt die Verantwortung für die Betreuung einzelner <\n>Pakete, für Dokumentation, Lizenz- und <@1 fliess kursiv>Policy<@$p>-Fragen,<@3 hoch fliess>30 <@$p>Webseiten, Mailingslisten usw. an andere Freiwillige. Das Debian-Team besteht aus weltweit etwa 500 Mitarbeitern im Alter von 13 bis 70 Jahren. Ein Paketbetreuer pflegt Pakete, die er selbst benutzt und gut kennt, und an deren Qualität er daher ein persönliches Interesse hat. Er verfolgt die Entwicklung in den jeweiligen Softwareprojekten, wählt neue Versionen und neue Software aus, die in die Debian-Distribution aufgenommen werden, stellt sie zusammen, überprüft ihre Lizenzen daraufhin, ob die Programme in Debian aufgenommen werden können und spielt den Vermittler zwischen den Debian-Nutzern und den eigentlichen Softwareentwicklungsprojekten. Anders als bei anderen GNU/Linux-Distributionen steht hinter Debian keine Firma, die Werbung und Vetrieb übernimmt. Debian ist eine Projektgruppe unter dem Schirm der SPI. Debian GNU/Linux ist die »freieste«, dem GNU-Projekt am nächsten stehende GNU/Linux-Distribution. Ähnlich wie das GNU-Projekt und im Gegensatz zu den anderen GNU/Linux-Distributionen ist Debian sehr darauf bedacht, ausschließlich Software unter freien Lizenzen aufzunehmen. Zum Beispiel schloss Debian aufgrund der problematischen Lizenz der Qt-Bibliothek bis zu deren Revision die grafische Desktop-Umgebung KDE von seiner Distribution aus (vgl. <\h><@6 Caps>Carter<@$p>, 6/2000). Dafür ist ein Kriterienkatalog (die <@1 fliess kursiv>Debian Free Software Guidelines<@$p>) erarbeitet worden, an dem Lizenzen überprüft werden (unbeschränkte Weitergabe, Verfügbarkeit des Quellcodes, Modifikationsfreiheit, keine Diskriminierung von Personen und Gruppen, keine Diskriminierung von Einsatzbereichen usw.). Statt einer Lizenz regelt ein »Gesellschaftsvertrag« (der <@1 fliess kursiv>Debian Social Contract<\!q><@3 hoch fliess>31<@$p>) das Verhältnis unter allen an Debian Beteiligten. Darin heißt es, dass Debian 100-prozentig freie Software bleiben wird, dass neue Komponenten als freie Software der Gemeinschaft zur Verfügung gestellt werden, dass Probleme nicht verborgen werden und dass den Anwendern und der Gemeinschaft freier Software oberste Priorität zukommt. Da zugestanden wird, dass einige Debian-Anwender nicht umhin können, Programme einsetzen zu müssen, die nicht den Kriterien für freie Software genügen, wurde für solche Programme ein eigener Bereich namens »<@1 fliess kursiv>non-free<@1 fliess normal>«<@1 fliess kursiv> <@$p>auf dem FTP-Archiv eingerichtet. Sie sind nicht Bestandteil des Debian-Systems (<@1 fliess kursiv>main<@$p>), werden aber dennoch für eine Zusammenarbeit mit ihm aufbereitet und in der <@1 fliess kursiv>Bug<@$p>-Datenbank und den Mailinglisten mitbehandelt. Das Debian-Team bemüht sich darum, dass die Lizenzen von solchen Programmen angepasst werden und erzielte dabei auch schon einige Erfolge. Ein weiterer Bereich namens »<@1 fliess kursiv>non-US<@1 fliess normal>«<@$p> enthält Kryptografiesoftware, die in den USA und anderen Ländern besonderen Auflagen unterliegt. Zu den weiteren Projekten gehören Debian GNU/Hurd, das die gleichen Pakete, aber an Stelle von Linux als Kernel den GNU-Hurd verwendet, und das Debian-Beowolf-Projekt, bei dem viele Rechner zu einem Cluster zusammengeschaltet werden. Aus Debian sind russische, französische, italienische und japanische Distributionen hervorgegangen, und auch Corel hat seine GNU/Linux-Distribution auf Basis von Debian erstellt. Debian GNU/Linux ist die Distribution mit der größten Anzahl von Paketen (in der Version 2.1 »Slink«<@3 hoch fliess>32<@$p> waren es 1 500 Pakete, in der Version 2.2 »Potato« vom Frühjahr 2000 schon doppelt so viele), der größten Anzahl unterstützter Architekturen (Intel, Alpha, 68000, Power-PC, Sparc, Arm, Amiga, Atari, RS/6000, UltraSparc), der größten Anzahl Mitarbeiter und der längsten Testphase vor einem <@1 fliess kursiv>Release<@$p>.<@3 hoch fliess>33<@$p> @2  ZÜ 2:XFree86 @1 fliess mit:@1 fliess ohne:<t-2>Das X Window-System wurde 1984 von Jim Gettys und anderen am MIT entwickelt und ist heute in der GNU/Linux- und Unix-Welt die Standardgrundlage für grafische Benutzeroberflächen. Noch am MIT entstand unter Industriebeteiligung zu seiner Implementierung und Weiterentwicklung das X-Konsortium, das 1986 die erste kommerzielle Version vorlegte. 1993 endete das universitäre Engagement, und die Technologie ging an das jetzt inkorporierte X Consortium Inc. mit weltweit mehr als 60 Mitgliedsunternehmen über. Vier Jahre später übertrug dieses Gremium die Verantwortung für die Software an das Open Group-Konsortium,<x@3 hoch fliess><t-2>34<@$p><t-2> das ein breiteres Spektrum von Technologien standardisiert, testet und zertifiziert.@1 fliess mit:Als 1992 das MIT-X-Konsortium die Version X11R5 veröffentlichte, war zum ersten Mal auch ein X-Server für PCs dabei: X386. Da er nicht sehr schnell und stabil war, begann eine kleine Gruppe sogleich, ihn weiterzuentwickeln. Am Anfang waren es vier Entwickler, die einige kleine Patches austauschten, und ein paar Tester. Das erste <@1 fliess kursiv>Release<@$p>, das daraus folgte, hieß X386 1.2.E (»E«, wie erweitert). Da GNU/Linux einen wachsenden Bedarf nach einer Implementation des X Window-Systems für PC-basierte, unix-artige Systeme schuf, entstand daraus ein eigenständiges Projekt, das sich mit einem Wortspiel auf<\f>X<@1 fliess kursiv>Three<@$p>86 »XFree86«<@3 hoch fliess>35<@$p> nannte. GNU/Linux und XFree86 haben sich gegenseitig vorangetrieben. GNU/Linux ist weitergekommen, weil es eine grafische Oberfläche gab, und XFree86 ist weitergekommen, weil es mit GNU/Linux eine frei verfügbare Plattform dafür gab. Die XFree86-Gruppe hat seither etwa zwei bis vier Releases im Jahr herausgebracht. XFree86, das heute etwa zweieinhalb Millionen Code-Zeilen umfasst, läuft auf einer breiten Palette von Prozessoren (Intel, Alpha, Sparc, Power-PC, dem Itzy, einem Handheld von Compaq, usw.) und Betriebssystemen (GNU/Linux, OS/2, Minix, Echtzeitbetriebssysteme wie QNX usw.). Mit etwa 600 Entwicklern weltweit ist XFree eines der größten freien Projekte. Die Zahl der XFree86-Nutzer wird konservativ auf 12 bis 14 Millionen geschätzt. <t-1.9>XFree86 verwendet eine Lizenz,<x@3 hoch fliess><t-1.9>36 <@$p><t-1.9>die auf die ursprüngliche MIT-X Window-Lizenz zurückgeht, die wiederum von der BSD-Lizenz abgeleitet, aber noch schwächer als diese ist. Sie erlaubt die Verwendung, Verbreitung, Modifikation und den Verkauf unter der einzigen Bedingung, dass der Copyright-Vermerk erhalten bleibt. Sie verzichtet insbesondere auf die Auflage, dieselben Freiheiten auch für abgeleitete Software zu gewährleisten und den freien Code nicht in proprietären zu integrieren. Diese Lizenz ermöglichte einerseits, dass viele proprietäre Unix-Systeme heute X-Server enthalten, die auf XFree86 basieren. Andererseits führte sie zu einer traurigen Episoden in der Geschichte der freien Software. Im April 1998 kündigte die Open Group eine geänderte Lizenzpolitik für X Window an. Die neue Version X11R6.4 wurde quellgeschlossen und nur für zahlende Kunden verfügbar.<x@3 hoch fliess><t-1.9>37<@$p><t-1.9> Sogleich hielt die Open Group ausgerechnet einige Sicherheits-<x@1 fliess kursiv><t-1.9>Bugfixes<@$p><t-1.9> zurück. XFree86 hatte seinen Code immer mit den Kollegen aus der Industrie geteilt, nun wollten diese sie von ihren Entwicklungen ausschließen – und das in einer Gruppe, die sich »Open« nennt. Eine weitere Zusammenarbeit wurde unmöglich. Im September 1998 sah sich die Open Group schließlich gezwungen, die Entscheidung zurückzunehmen und X11R6.4 wieder unter einer quelloffenen Lizenz freizugeben. Hintergrund war neben dem Protest vor allem, dass sich der Fokus der Entwicklung zu XFree86 verlagert hatte, und innerhalb des Konsortiums kaum noch jemand daran arbeitete. War das PC-X anfangs ein Anhängsel der »großen, seriösen« Workstation-X-Entwicklung, hatte sich das Verhältnis inzwischen umgekehrt. Auch Jim Gettys, einer der Urväter von X, sieht heute XFree86 als die Gruppe, die die Entwicklung von X an sich weiter trägt. Im Mai 1999 gründete sich aus der Open Group die Organisation X.Org mit den üblichen Industriemitgliedern,<x@3 hoch fliess><t-1.9>38<@$p><t-1.9> das nun die offizielle Verwaltung der X Window-Technologie mit ihren Standards und Referenzimplementationen übernahm. XFree86 wurde zum Ehrenmitglied. <t$>Das freie Projekt war von Beginn an mit einem industriellen Erbe <\n>belastet. Das Konsortiums-X Window setzt auf das proprietäre Toolkit Motif auf, und auch die darüber liegende Desktopumgebung CDE (Common Desktop Environment) ist nicht quelloffen. Eine GNU/Linux-Distribution, die ihren Anwenderinnen eine Fensteroberfläche anbieten wollte, musste für Motif und CDE Lizenzen erwerben und deren Bedingungen natürlich auch an die GNU/Linux-Nutzer weitergeben. Die aber wollten sich das Maus<\h>klicken nicht vergraulen lassen und entwickelten 1997 LessTif als Ersatz für Motif, und als quelloffene Alternative zu CDE entstand KDE (s.u.). In dem freien Projekt GIMP (s.u.) entstand als weitere Alternative zu Motif die Toolkit-Library Gtk, auf der der Desktop Gnome beruht. Auch zu einer anderen Seite hin gestaltet sich die Zusammenarbeit mit der Welt des »geistigen Eigentums« problematisch. Ein Fenstersys<\h>tem muss naturgemäß eng mit der Grafikhardware zusammenarbeiten. Die Hersteller von Grafikkarten hüten den Quellcode ihrer Treiber und vor allem ihre Hardwaredokumentation. Anderen Unternehmen, die interoperierende Produkte herstellen, geben sie sie nur unter einer Vertraulichkeitsvereinbarung heraus. Solche Verträge mit anderen Firmen zu schließen, ist üblich – mit 600 auf der ganzen Welt verteilten Individuen aber unmöglich. Damit das XFree-Projekt seinen Entwicklern Zugang zu diesen Informationen geben kann, müssen diese förmliche, wenn auch nicht stimmberechtigte Mitglieder der XFree86 Project Inc. werden und sich zur Nichtweitergabe verpflichten. Die Freiheit wird dadurch gewährleistet, dass jeder Mitglied werden kann. Dennoch ergeben sich daraus zwei Klassen von Quellcode: der <@1 fliess kursiv>Release-Code<@$p>, der für jeden frei verfügbar ist, und die <@1 fliess kursiv>Internal Development Sources<@1 fliess normal>,<@$p> die nur den offiziellen Mitgliedern zugänglich sind.<@1 fliess kursiv>Core-Team<@$p>-Mitglied Dirk Hohndel gesteht die Bedenken über die fehlende Absicherung der Freiheiten in der XFree86-Lizenz zu, die dazu führe, dass jemand ein proprietäres Produkt daraus mache und die freie Softwarewelt ausschließen könne. Tatsächlich sei XFree86 weiterhin der einzige X-Server für GNU/Linux. Die wenigen Anbieter proprietärer Produkte hätten im Markt nicht Fuß fassen können. @1 fliesskursiv Zitat:»Das zeigt, dass man in einem Open Source-Projekt rein mit der Qualität des Produkts, das man abliefert, dafür sorgen kann, dass für jemanden, der versucht, hier Closed Source zu gehen und daraus ein proprietäres Produkt zu machen, überhaupt kein Platz ist. Denn jeder der heute hingeht und dieses Projekt zumachen und daraus seine eigene Sache machen will, muss damit rechnen, dass wir all das, was er anbietet, auch anbieten. Wir bieten es mit Sourcen an und frei verfügbar.«<@3 hoch fliess>39<@$p>@1 fliess mit:@2  ZÜ 2:KDE @1 fliess mit:@1 fliess ohne:Die integrierten grafischen Arbeitsoberflächen von Desktop-Betriebssys<\h>temen wie MS-Windows oder Mac-OS haben wesentlich zur Popularisierung des PCs beigetragen. Unter Unix sind verschiedene Entwicklungen in diese Richtung unternommen worden. Eine davon, das <@1 fliess kursiv>Common Desktop Environment<@$p> (CDE), hielt zusammen mit XFree86 und Motif Einzug in verschiedene GNU/Linux-Distributionen. Da Motif und CDE jedoch proprietär sind, sperren sie sich einer freien Weiterentwicklung. Als z.B. der Distributor Red Hat 1998 Sicherheitsprobleme in CDE entdeckte, entfernte er den Desktop aus seiner Distribution, da solche Bugs aufgrund des fehlenden Quellcodes nicht einfach behoben werden können.<@3 hoch fliess>40<@$p>@1 fliess mit:Hier setzt das <@1 fliess kursiv>K Desktop Environment<@$p> (KDE)<@3 hoch fliess>41 <@$p>an. Das Projekt wurde 1996 von Matthias Ettrich ins Leben gerufen und in der Version 1.0 im Juli 1998 freigegeben. KDE bietet ein konsistentes <@1 fliess kursiv>Look-and-Feel<@$p> des Desktops und der darauf laufenden Applikationen, in dem sich jede Windows- oder Mac-Nutzerin sofort zuhause fühlen wird. Neben Mausbedienung, Fenstern und <@1 fliess kursiv>Drag-and-Drop<@$p> bietet KDE, wie in Unix üblich, mehrere virtuelle Desktops. Es läuft auf einer Vielzahl von Unix-Systemen, darunter Solaris, Irix, HP-UX, AIX, Linux und den BSD-Varianten. Die Bildschirmtexte sind in über 30 Sprachen internationalisiert, darunter Mazedonisch, Isländisch und Bretonisch. Inzwischen wird auch Unicode unterstützt. Alle KDE-fremden Applikationen für Unix laufen problemlos auf einem KDE-Desktop, und umgekehrt laufen die meisten KDE-Applikationen auch auf anderen Desktops. Neben den eigenen Bibliotheken verwendet KDE die C++-Klassenbibliothek Qt der norwegischen Firma Troll Tech AS. Diese Entscheidung traf in der freien Szene auf Kritik, da Qt ein proprietäres Produkt war. Wie meistens, wenn eine nützliche, aber proprietäre Software eine freie Entwicklung unmöglich macht, finden sich Leute, die die betreffende Funktionalität von Grund auf neu schreiben, in diesem Fall das Projekt GNOME (1998). Der »Desktop-Krieg« zwischen KDE und GNOME<\!q><@3 hoch fliess>42<@$p> führte u.a. dazu, dass Troll Tech seine Lizenzpolitik änderte. Ab Version 2.0 liegt die quelloffene Version von Qt unter einer eigenen Lizenz, der <@1 fliess kursiv>Qt Public <\n>License<@$p> (QPL), vor, die es für freie Entwicklung auf Unix-Systemen zur freien Verfügung stellt und für proprietäre Entwicklungen den Erwerb einer kommerziellen Lizenz vorschreibt. Die QPL hat die meisten GNU/Linux-Distributoren dazu veranlasst, KDE in ihre Pakete aufzunehmen. Auch die in Bezug auf Lizenzpolitik sensibelste Distribution Debian nahm QT ab Version 2 auf (vgl. Carter, 6/2000). Die KDE-eigenen Bibliotheken und die KDE-Applikationen werden unter der Lib<\h>rary GPL (s.u.) veröffentlicht. Das KDE-Manifest bekennt sich ausdrücklich zur kommerziellen Verwendung.<@3 hoch fliess>43<@$p>KDE ist mit über zwei Millionen Codezeilen eines der umfangreichs<\h>ten freien Projekte. Mehr als 200 Entwickler aus zahlreichen Ländern, darunter Deutschland, Norwegen, USA, Kanada, Argentinien, Namibia und Australien, arbeiten aktiv daran mit. Das CVS lief anfangs an der Medizinischen Universität zu Lübeck und zog im Februar 2000 zu SourceForge in den USA um. An den verschiedenen Mailinglisten ist die soziale Organisation des Projekts gut zu erkennen. Dazu gehören: eine Liste für Anwender, auf denen diese sich gegenseitig selbst helfen, die aber auch von den Entwicklern mitgelesen wird, um gegebenenfalls Hilfestellung leisten zu können; eine Liste für Entwickler, für die eine Schreibberechtigung auf Anfrage erteilt wird und die für Fragen rund um Entwurf und Entwicklung gedacht ist; eine Liste für die Autoren und Übersetzer der Dokumentation; eine Liste für Lizenzfragen und schließlich eine geschlossene Mailingliste, auf der das Kern-Team die allgemeine Richtung von KDE sowie Fragen des Marketings und der Öffentlichkeitsarbeit diskutiert. Die Aufnahme in das Kern-Team erfolgt aufgrund von fortwährenden Verdiensten um die Weiterentwicklung von KDE. Laut Kalle Dalheimer, einem der Hauptentwickler, liegt einer der wichtigsten Gründe des Erfolg von KDE, eine ständig wachsende Zahl engagierter Entwickler anzuziehen, in den Bibliotheken, die es möglich machen, attraktive, mit vielen Features versehene Applikationen zu schreiben, ohne umfangreiche Projektkenntnisse haben zu müssen. Viele Funktionen lassen sich mit nur wenigen Codezeilen integrieren. Damit macht KDE den Einstieg in die Programmierung von Anwendungen mit grafischer Benutzeroberfläche deutlich einfacher. KDE-Entwickler können die erforderlichen Kenntnisse zu einem großen Teil während der Arbeit an ihrem Teilprojekt erwerben, was natürlich viel motivierender ist, als sich erst lange mit den Strukturen vertraut machen zu müssen, bevor man in die eigentliche Entwicklungsarbeit einsteigen kann.<@3 hoch fliess>44<@$p>@2  ZÜ 2:Apache @1 fliess mit:@1 fliess ohne:<t-2>1989 erfand Tim Berners-Lee am CERN das WWW. Ursprünglich als Werkzeug für die Vernetzung der Informationsflut am europäischen Hoch<\h>energiephysikzentrum gedacht, mit öffentlichen Mitteln entwickelt und daher selbstverständlich freie Software, verbreiteten sich das <x@1 fliess kursiv><t-2>HyperText Transfer Protocol<@$p><t-2> (HTTP) und die Seitenbeschreibungssprache <x@1 fliess kursiv><t-2>HyperText Markup Language<@$p><t-2> (HTML) rasch im Internet. Das W3 brachte in das bis dahin rein text- und kommandozeilenbasierte Internet erstmals eine ähnliche Benutzerfreundlichkeit, wie sie Fenster- und Maus-Oberflächen auf dem PC einführten. Berners-Lee schrieb 1990 auch den ersten Webserver und einen Client, der, wie heute nur noch Netscape, sowohl einen Betrachter wie einen Editor enthielt. Nicht nur das Lesen, sondern auch das Schreiben des neuen Formats sollte allen offen stehen. Wenig später entwickelte ein Team unter Leitung von Rob McCool am NCSA der Universität Illinois einen weiteren Webserver. Da ebenfalls mit Steuergeldern finanziert, war auch diese Software quelloffen und frei für Modifikationen. In den ersten Jahren des maßgeblich durch das Webformat vorangetriebenen Internetbooms war der NCSA-Webserver der meistverbreitete. @1 fliess mit:Als McCool das NCSA verließ, kam die Entwicklung des Servers ins Stocken. Viele Leute fingen an, eigenständig Fehler darin zu beheben und neue Funktionalitäten einzubauen, doch es fehlte eine Sammelstelle, die die neuen Elemente zusammenführte. Zu diesem Zweck gründete sich 1995 die Apache-Group.<@3 hoch fliess>45 <@$p>Der Name »Apache« ist ein Wortspiel auf »<@1 fliess kursiv>a<\!q>patchy server<@$p>«, der durch zahlreiche Patches veränderte Quellcode des NCSA-Servers. Die erste Version des Apache-Webservers wurde noch 1995 herausgegeben. Ein knappes Jahr später hatte der Apache den NCSA-Webserver von der Spitze verdrängt und war zum meistgenutzen Webserver im Internet geworden. Im Juli 1999 hatte unter den Webservern im Internet der Apache einen Anteil von knapp 62 Prozent, Microsoft einen von 22 und Netscape von etwas über sieben Prozent. Im August 2001 lag Apache bei 58 Prozent, Microsoft hatte mit 26 Prozent weiter ausgebaut und iPlanet/Netscapes Anteil ist auf 4 Prozent zurückgegangen.<@3 hoch fliess>46<@$p>Der Apache läuft derzeit auf allen gängigen Unix-Derivaten, auf Windows, Siemens BS2000 und Amiga. Er ist so stabil, dass er auch für <\h>sicherheitskritische Anwendungen geeignet ist. Dank eines modularen Aufbaus wird sein Funktionsumfang ständig erweitert. Das Kernsystem dient dazu, normale HTML-Seiten auszuliefern. Zusätzlich gibt es über 100 verschiedene Module für den Apache, angefangen von CGI (das <@1 fliess kursiv>Common Gateway Interface<@$p> für Skripte) und Protokollierungsfunktionen, die anzeigen, wie häufig eine Seite aufgerufen wurde, bis zu URL-Manipulationsroutinen und serverseitige Funktionen (<@1 fliess kursiv>Server-side-Includes<@$p>) wie Java-Servlets. Drei eigenständige Projekte arbeiten unter dem Dach von Apache. Das PHP-Projekt<@3 hoch fliess>47<@$p> stellt das PHP-Modul für die Datenbankanbindung ans Web zur Verfügung. Das mod_perl-Projekt<@3 hoch fliess>48<@$p> integriert einen Interpreter für Perl in den Apache. Ein ähnliches Modul gibt es auch für Python. Das Jakarta-Projekt<@3 hoch fliess>49<@$p> ist durch eine Kooperation mit Sun hinzugekommen und stellt eine freie <@1 fliess kursiv>Servlet Engine <@$p>für den Apache zur Verfügung. Mittlerweile gibt es einen großen Markt von Drittanbietern von Modulen für den Apache, einige davon quelloffen, andere proprietär.Die Apache-Lizenz<@3 hoch fliess>50<@1 fliess normal> <@$p>ähnelt der von BSD. Der Quellcode kann ohne <\n>Lizenzgebühren auch für proprietäre Zwecke eingesetzt werden. Es gibt nur zwei Einschränkungen: Wenn eine Firma den Apache modifiziert und weitervertreiben will, muss sie angeben, dass ihr Produkt auf dem Apache basiert, und dieses Produkt darf dann nicht Apache heißen. Die weiteren Einschränkungen der GPL, dass auch bei abgeleiteter Software der Quellcode mitvertrieben werden muss, gelten bei der Apache-Lizenz nicht. Die Apache-Group (das <@1 fliess kursiv>Core Team<@$p>) besteht derzeit aus 22 aktiven Mitgliedern, überwiegend aus den USA, aber auch aus Kanada, England, Neuseeland, Italien und drei von ihnen aus Deutschland. Die <@1 fliess kursiv>Apache Software Foundation<@3 hoch fliess>51<@$p> wurde am 1. Juni 1999 gegründet, um das Projekt <\n>finanziell und rechtlich zu unterstützen und Verträge mit Firmen zu ermöglichen. Es handelt sich um ein nicht profitorientiertes Unternehmen mit Sitz in Delaware, USA. Der Vorstand setzt sich ausnahmslos aus Leuten der Apache-Group zusammen. In der Apache-Group waren auch schon vor Gründung der Foundation mehrere Vertreter von Firmen beteiligt. So unterhält IBM, das den Apache in seinem Produkt »WebSphere« einsetzt die Entwicklung der Serversoftware mit einem eigenen Apache-Entwicklerteam. Auch Siemens liefert den Apache mit seinen BS2000 Systemen als Standard-Webserver aus und stellt einen Mitarbeiter dafür ab, dafür zu sorgen, dass Portierungsarbeiten vorgenommen werden, und darauf zu achten, dass zukünftige Versionen des Apache auf BS2000 lauffähig sind. Auch <\n>Apple hat den Apache auf Rhapsody und dann auf Mac OS X portiert und ist mit einem Mitarbeiter in der <@1 fliess kursiv>Apache Software Foundation<@$p> vertreten.@2  ZÜ 2:GIMP @1 fliess mit:@1 fliess ohne:Das Projekt GIMP (<@1 fliess kursiv>GNU Image Manipulation Program<@$p>)<@3 hoch fliess>52<@$p> ist mit dem Ziel gestartet, ein Bildbearbeitungsprogramm ähnlich Adobes »Photoshop« für GNU/Linux zu erstellen. Anfang 1995 begannen die beiden Informatikstudenten Peter Mattis und Spencer Kimball, die Basis des GIMP zu schreiben. Ein Jahr später stellten die beiden die Version 0.54 der Öffentlichkeit vor. Bereits in dieser frühen Version fand das GIMP so große Verbreitung, dass Mitte 1996 immer mehr Leute mitentwickeln und ihre eigenen Features einbauen wollten. Auf der GIMP-Mailingliste wurde der Rauschanteil immer höher, so dass die aktiven Entwickler sich in eine zweite Liste zurückzogen, die zwar immer noch öffentlich war, ihnen aber die Möglichkeit gab, nicht direkt sachdienliche Äußerungen auf die allgemeine Liste zu verweisen. Auch Teilnehmer anderer Projekte treffen sich gern in Chatkanälen, aber GIMP ist eines der wenigen, das den IRC als ein eigenständiges Kommunikationsmittel neben den Mailinglisten benutzt.@1 fliess mit:Als Window-Manager, der die Knöpfe, Fenster und anderen grafischen Bedienelemente zeichnet, stand den beiden nur das proprietäre <@1 fliess normal>Motif<@1 fliess kursiv> <@$p>zur Verfügung. Deshalb machten die beiden sich daran, ein eigenes Toolkit namens Gtk zu entwickeln, das Anfang 1997 im GIMP 0.99 erstmals zum Einsatz kam. Als Grafik-Toolkit ist Gtk für alle Software unter X Window nützlich, weshalb die motif-ersetzenden Routinen vom GIMP abgetrennt wurden und <@1 fliess normal>Gtk<@$p> als eigenständiges Projekt weiterläuft. So verwendet auch das Projekt Gnome das Gtk-Toolkit. Wie mehrfach erwähnt scheuen es Entwickler, ihre Arbeit zu dokumentieren. Und gerade das bereits sehr komplexe GIMP bedarf eines Handbuches, um all seine Möglichkeiten auszuschöpfen. Zwei Benutzer, Karin und Olof S. Kylander, widmeten sich dieser Aufgabe und legten Mitte 1998 das <@1 fliess kursiv>GIMP Users Manual <@$p>vor, das inzwischen auf 600 Seiten angewachsen ist. Es erscheint auch in gedruckter Form, ist aber weiterhin im Internet frei unter der <@1 fliess kursiv>Open Content License<@$p> verfügbar.<@3 hoch fliess>53<@$p> Das Pro<t-2>gramm selbst steht unter der GPL. Die Schnittstellen stehen unter der <\h>LGPL,<t$> so dass die Filter und andere Erweiterungen, die Firmen für GIMP oder Photoshop schreiben, in Binärform angeschlossen werden können, ohne dass der Quellcode dafür offengelegt werden muss.<*h"mehr">Anders als die meisten Projekte verfügt GIMP über keine formelle Organisationsform, sei es eine Stiftung, einen Verein oder auch nur ein <\h><@1 fliess kursiv>Core-Team<@$p>. Wer auf den Mailinglisten hilfreiche Dinge äußert, gehört dazu. Ein formalisiertes Konfliktlösungsverfahren wie beim Apache gibt es nicht. Änderungen werden auch dann programmiert, wenn jemand nicht damit einverstanden ist – und setzen sich dann durch oder nicht.<@3 hoch fliess>54<@$p> Diese Nichtinstitutionalisierung hat natürlich ihre Vorteile, doch für die Kontinuität des Projekts stellt sie auch eine Gefahr dar. Das mussten die anderen Teilnehmer feststellen, als Mattis und Kimball ihren Universitätsabschluss machten und von der Bildfläche verschwanden, ohne das Projekt an Nachfolger übergeben zu haben. Aus der Konfusion schälten sich erst nach und nach Leute heraus, die fähig und willens waren, neue Versionen zu entwickeln. Auch das Fehlen von Ansprechpartnern für die einzelnen Elemente des GIMP führte zu Konflikten. Ende 1998 begann <\n>Daniel Eggert das GIMP zu internationalisieren, d.h. er übersetzte alle ursprünglich englischen Menüs ins Deutsche und stellte vor allem den Quellcode so um, dass man weitere Sprachen sehr einfach einbinden kann. Diese mühsame und verdienstvolle Arbeit tat er auf der Mailinglis<\h>te kund, wo die Nachricht im Rauschen unterging. Enttäuscht über die Nichtreaktion beschloss er, sein eigenes GIMP-Projekt aufzumachen und Mitstreiter aufzufordern, mit ihm zusammenzuarbeiten. Die Spaltung bedrohte das Projekt als Ganzes, doch nach kurzer Zeit konnte die Verstimmung beigelegt und Eggerts Änderungen in das Hauptprojekt integriert werden. <\d><*h"Standard"><\c>@2  ZÜ 1:Lizenzmodelle@1 fliess mit:@1 fliess ohne:Wie aus den vorangegangenen Projektbeschreibungen zu ersehen ist, sind für freie Software die Lizenzen, also die vertraglichen Rechte und Pflichten, unter denen die Urheberrechtsinhaber ihre Werke veröffent<\h>lichen, von besonderer Bedeutung. Daher geht dieser Abschnitt näher auf die Dimensionen, die diese Lizenzen regeln sowie auf einige wichtige Beispiele ein. @1 fliess mit:Software ist in Deutschland durch § 2 Abs. 1 Satz 1 UrhG rechtlich geschützt. Dies betrifft insbesondere das Erstveröffentlichungsrecht (§ 12 UrhG). Die Autorin kann ihr Werk unter frei gewählten Bedingungen veröffentlichen, die in einer Lizenz, also einem Privatvertrag zwischen der Urheberin und dem Verwerter oder dem Endnutzer festgelegt werden: »Unter einer ›Lizenz‹ versteht man die Einräumung von Nutzungsrechten an Schutzrechten.«<@3 hoch fliess>1<@$p> Die Lizenzmodelle der freien Software umgehen das Urheberrecht nicht etwa oder verzichten auf seine Inanspruchnahme, sondern setzen darauf auf, um den offenen, kooperativen Prozess der Erstellung und Weiterentwicklung abzusichern. Bei der Installation einer Software, gleich ob frei oder unfrei, erscheint in der Regel auf einem der ersten Bildschirme ein langer Lizenztext. Im Falle von proprietärer Software steht darin üblicherweise, dass der Nutzer nicht etwa das Programm, sondern nur ein eingeschränktes Nutzungsrecht daran erworben hat, dass er nicht mehr als eine einzige Sicherungskopie erstellen darf, dass er das Programm nur auf einem einzigen Rechner installieren darf, dass er nicht versuchen wird, es einem <@1 fliess kursiv>Reverse Engineering<@$p> zu unterziehen, dass der Hersteller keinerlei Haftung und Gewährleistung für sein Produkt übernimmt und dergleichen mehr. Diese in juristischen Fachbegriffen abgefassten Texte liest normalerweise niemand, tatsächlich schließt man jedoch durch das Anklicken der Option »Akzeptieren« einen Vertrag.<@3 hoch fliess>2<@$p> Die gleiche Prozedur findet sich auch bei freier Software, nur dass die Vertragsbedingungen hier anders lauten. In den 60er-Jahren war, wie bereits ausgeführt, alle Software in einem bestimmten Sinne frei. Ökonomisch war sie noch nicht zu einer <\h>Ware geworden, daher galt sie auch juristisch nicht als Werk, das den Schutz des Urheber-, respektive Copyright-Rechts oder des Patentrechts genießen kann. Als 1969 der Marktführer IBM unter der Drohung einer kartellrechtlichen Zerschlagung begann, seine Bündelung von Hard- und Software aufzugeben, war der Startschuss für die Entwicklung einer eigenständigen Softwareindustrie gefallen. Um die seit zwei Jahrzehnten ausstehende Revision des U.S. Copyright-Gesetzes vorzubereiten und Richtlinien zu seiner Anwendung auszuarbeiten, berief der amerikanische Kongress 1974 die CONTU ein. Wie der Name schon sagt, ging es vor allem darum, das Gesetz den Herausforderungen durch neue Technologien, vor allem Computer, anzupassen.<@3 hoch fliess>3<@1 fliess normal> <@$p>Die CONTU empfahl, Computerprogramme zukünftig als »literarische« Werke unter den Schutz des Copyright zu stellen. Die grund-      legende Copyright-Reform von 1976 folgte dieser Empfehlung, stellte <\h>jedoch eine spezifischere Softwareregelung aus, bis die CONTU ihren Abschlussbericht vorlegen würde, was 1978 geschah. In der folgenden Revision von 1980 übernahm die US-Legislative die CONTU-Vorlage wörtlich (mit einer folgenreichen Ausnahme<@3 hoch fliess>4<@$p>). Dem Gesetz wurde die Definition von »Computerprogramm« (§ 101 USCA) und besondere Schrankenbestimmungen für diese Kategorie von Werken (§ 117 USCA)<@3 hoch fliess>5<@$p> hinzugefügt.Damit hatten ab 1976 Autoren und Firmen die Möglichkeit, ein <\n>Copyright auf ihre Software anzumelden. Wie bei allen anderen Werken üblich, brachten sie dazu einen Copyright-Vermerk (©, Jahr der Erstveröffentlichung, Name des Copyright-Besitzers) darauf an, mussten zwei Kopien bei der <@1 fliess kursiv>Library of Congress <@$p>hinterlegen und konnten ihr Copyright beim <@1 fliess kursiv>Copyright Office<@$p> gebührenpflichtig registrieren lassen. Eine Regis<\h>trierung war zwar keine Bedingung für den Copyright-Schutz, galt jedoch als Nachweis im Streitfall und war Voraussetzung für Schadensersatzansprüche bei Copyright-Verletzungen.<@3 hoch fliess>6<@$p> Im selben Jahr veröffentlichte Bill Gates den bereits erwähnten »<@1 fliess kursiv>Open Letter to Fellow Hobbyists<@1 fliess normal>«,<@$p> in dem er – noch ohne Verweis auf das Copyright – ein ökonomisch-moralisches Recht auf die Verwertung der eigenen Software einklagte: »Wie sich die Mehrheit der Hobbyisten bewusst sein muss, stehlen die meisten von euch eure Software. Für Hardware muss man bezahlen, aber Software ist etwas zum Weiterverschenken. Wen kümmert es schon, ob die Leute, die daran gearbeitet haben, bezahlt werden? Ist das fair? ... Ihr verhindert, dass gute Software entwickelt wird. Wer kann es sich schon erlauben, professionelle Arbeit umsonst zu leis<\h>ten?«<@3 hoch fliess>7<@$p> Mit dem Copyright und ab 1981 auch dem Patenschutz für Software wurden die Auseinandersetzungen in die Gerichtssäle getragen. In einem notorischen Fall wurde Microsoft zum Gegenstand eines langjährigen Urheberrechtsstreits. Apple hatte 1983 die am Xerox PARC entwickelte grafische Benutzeroberfläche mit Fenstern, Menüs und Maussteuerung als Erster auf den Massenmarkt gebracht. Das Windows, mit dem Microsoft 1985 nachzog, glich auffällig dem Apple-Desktop. App<\h>le verklagte Microsoft, das <@1 fliess kursiv>Look & Feel <@$p>des Macintosh plagiiert zu haben. Das Verfahren endete erst 1995 mit einer Niederlage von Apple, das sofort erneut gegen das gerade erschienene Windows 95 klagte. Dieser Streit endete schließlich mit einem Vergleich. Seit Mitte der 70er-Jahre werden Verbreitungsstücke von Computerprogrammen, anders als im Fall von Büchern, in der Regel nicht mehr verkauft, sondern lizenziert. Der Inhaber des Copyright oder in Kontinentaleuropa der urheberrechtlichen Verwertungsrechte kann im Kaufvertrag und den allgemeinen Geschäftsbedingungen die Konditionen und den Nutzungsumfang festlegen, unter denen er die Software seinen Kunden zur Verfügung stellt. In Deutschland wurde ein wirkungsvoller Urheberrechtsschutz für Computerprogramme mehr als 15 Jahre nach den USA eingeführt. Erst die Computerrechtsnovelle von 1993, die eine Richtlinie des Europäischen Rates umsetzte, unterstellte sie dem Schutz für Sprachwerke (§ 2 Abs. 1 UrhG) und führte die »Besonderen Bestimmungen für Computerprogramme« (§§<\!q>69a bis 69g) in das Urheberrechtsgesetz ein. Zum Schutz von Datenbankherstellern folgten 1998 die Paragraphen<\!q>87a bis 87e<\!q>UrhG (vgl. <@6 Caps>Siepmann<@$p>, 1999, Abs. 56). Auf die aktuellen Entwicklungen im Bereich der proprietären<\n>Lizenzen wird am Ende dieses Abschnitts eingegangen. Hier sollen die Lizenzen der freien Software behandelt werden. Dazu ist es zunächst erforderlich, Free Software bzw. Open Source-Software von den benachbarten Konzepten Freeware, Shareware und <@1 fliess kursiv>Public Domain<@$p>-Software abzugrenzen, die häufig zu Verwirrung führen.<@3 hoch fliess>8<@$p> Wie die englischen Begriffe andeuten, stammen alle genannten Konzepte aus dem US-amerikanischen Kultur- und Rechtsraum. Auf die Übertragbarkeit von Free Software bzw. Open Source auf die deutschen Rechtsverhältnisse wird unten eingegangen.<@1 fliess kursiv><*h"mehr">Shareware<@$p> ist copyright-geschützte Software, die von ihrem Autor (in der Regel ohne Quellcode und ohne Veränderungserlaubnis) kostenlos, aber mit der verbindlichen Bitte veröffentlicht wird, ihm bei regelmäßiger Nutzung des Programms einen bestimmten oder beliebig höheren Geldbetrag zukommen zu lassen. Da sie dem Nutzer erlaubt, das Programm weiterzugeben und auszuprobieren, bevor er es in täglichen Gebrauch nimmt, überschneidet sich die Kategorie mit der <@1 fliess kursiv>Demoware,<@$p> auch <@1 fliess kursiv>Crippleware<@$p> genannt, die einen gegenüber der Vollversion eingeschränkten Leistungsumfang bietet. <@1 fliess kursiv>Freeware <@$p>ist eine copyright-geschützte Software, die von ihrem Autor – in der Regel ohne Quellcode und ohne Veränderungserlaubnis – kostenlos, frei weitergebbar und oft ohne Lizenzbedingungen veröffentlicht wird.<@3 hoch fliess>9<@$p><@1 fliess kursiv><*h"Standard">Public Domain<@$p>-Software schließlich ist nicht copyright-geschützt,<@3 hoch fliess>10 <@$p>entweder weil sie gesetzlich nicht schützbar ist<@3 hoch fliess>11 <@$p>oder weil der Autor auf sein Copyright verzichtet,<@3 hoch fliess>12<@$p> dieses verfallen oder aus formalen Gründen verwirkt worden ist. Das Werk (mit oder ohne Quellcode) wird dadurch gemeinfrei. Jede Nutzung bis hin zur Beanspruchung eines Copyrights durch einen Dritten ist zulässig. <@1 fliess kursiv>Free Software <@$p>und das jüngere Konzept der <@1 fliess kursiv>Open Source-Software<@$p> unterscheiden sich dadurch von den genannten drei Kategorien, dass sie das Copyright/Urheberrecht der Autoren in Anspruch nehmen und zugleich in ihren Lizenzen spezifische Nutzungsfreiheiten festschreiben. Die Details variieren, doch die drei zentralen Punkte, die von den verschiedenen Lizenzen geregelt werden, betreffen die Beifügung des Quellcodes zum Binärcode der Software, das Recht, Kopien anzufertigen und weiterzugeben sowie das Recht, die ursprüngliche Software zu modifizieren und die abgeleitete Software zu verbreiten.@2  ZÜ 2:BSD-Lizenz @1 fliess ohne:Eine der ersten Quellcode-Lizenzen war diejenige, unter der AT&T sein Unix an Universitäten verkaufte. Sie ging auf eine Lizenz zurück, unter der die Bell-Labs Telefonherstellern Software zur Verfügung gestellt hatten. Diese Firmen durften AT&Ts Software benutzen und auch modifizieren, nicht aber weiterverbreiten. Das gleiche Lizenzmodell findet man im frühen Unix.<@3 hoch fliess>13<@$p> Als die Berkeley Universität begann, Unix-Versionen zu verbreiten, die eigenen Code zusammen mit dem von AT&T enthielten, erarbeiteten die Anwälte von AT&T und der Universität 1979 zu diesem Zweck eine Lizenz.<@3 hoch fliess>14 <@$p>Die BSD-Lizenz beginnt mit einem <\n>Copyright-Vermerk und erlaubt die Verwendung und Weiterverbreitung der Software in Quell- und Binärform, mit oder ohne Veränderungen, solange der Copyright-Vermerk und der Lizenztext mitverbreitet werden, in allen Werbematerialien der Satz »Dieses Produkt beinhaltet Software, die von der Universität von Kalifornien in Berkeley und ihren Kontributoren entwickelt wurde.«  genannt und der Name der Universität und seiner Kontributoren nur mit schriftlicher Genehmigung verwendet wird, um für abgeleitete Software zu werben. Schließlich folgt ein auch in proprietärer Software üblicher Passus, in dem alle Garantie- und Haftungsansprüche, die sich aus der Verwendung der Software ergeben könnten, zurückgewiesen werden.<@3 hoch fliess>15<@$p>@1 fliess mit:Diese einfache freie Softwarelizenz hat die Lizenzen einer Reihe anderer Projekte, wie das MIT-X Window System, XFree86 und Apache inspiriert und wird heute noch im Wesentlichen unverändert von den freien BSD-Versionen verwendet. Sie enthält jedoch eine Unklarheit und ein praktisches Problem. Sie schreibt nicht explizit vor, dass abgeleitete Software ebenfalls im Quellcode verfügbar sein muss. Sie muss zwar unter dieselbe Lizenz gestellt werden, insofern vererbt sich auch die Freiheit, die abgeleitete Software im Quellcode weiterverbreiten zu dürfen, doch was geschieht, wenn eine Firma die Quellen für ihre Erweiterungen gar nicht erst veröffentlicht, lässt der Text offen. Das praktische Problem ergibt sich aus der Werbeklausel. Da eine ganze Reihe anderer Projekte diese Vorschrift übernahmen und die Berkeley Universität durch ihren Namen ersetzten, muss jede Werbung für eine Kompilation solcher Programme eine entsprechend große Zahl dieser Hinweise enthalten. In einer NetBSD-Version von 1997 zählte Ri<\h>chard Stallman 75 Programme, deren Lizenzen jeweils die Angabe eines solchen Satzes vorschrieben. Stallman überzeugte den Rektor der Berkeley Universität von dem Problem, so dass die BSD-Lizenz ab 1999 diese Klausel nicht mehr enthielt.<@3 hoch fliess>16<@$p>Neben einer »generischen« BSD-artigen Lizenz<@3 hoch fliess>17<@$p> haben die BSD-Projekte eine Reihe Varianten hervorgebracht. OpenBSD verwendet die Original-Berkeley-Lizenz und teilweise eine Modifikation, bei der die Werbevorschrift entfernt wurde.<@3 hoch fliess>18<@$p> FreeBSD umfasst eine ganze Reihe Ports mit zusätzlichen Einschränkungen, da sie proprietäre Software enthalten, Exportkontrollen unterliegen (Kryptografie) oder ihre Autoren eine kommerzielle Verbreitung untersagen.<@3 hoch fliess>19<@$p> Auch andere Projekte verwenden von der BSD abgeleitete Lizenzen, wie die MIT-X-, Open Group X-<@3 hoch fliess>20 <@$p>und die XFree-Lizenz, die des Apache,<@3 hoch fliess>21<@$p> die von Zope,<@3 hoch fliess>22<@$p> von Python,<@3 hoch fliess>23<@$p> PNG/zlib,<@3 hoch fliess>24<@$p> Tcl/Tk<@3 hoch fliess>25<@1 fliess normal> <@$p>und die von Amoeba.<@3 hoch fliess>26<@$p>In der BSD-Lizenz kamen zwei Faktoren zusammen. Zum einen durfte AT&T als Privatunternehmen, dem der Staat ein Monopol in der Telegrafie und Telefonie zugewiesen hatte, sich nicht in anderen Wirtschaftsbereichen wie dem Softwaremarkt engagieren. Es hätte seine Software also gar nicht zu denselben Bedingungen verwerten können, wie Microsoft und andere Unternehmen. Zum anderen herrscht in den USA die Auffassung, dass die Ergebnisse öffentlich finanzierter Forschung und Entwicklung der Allgemeinheit gehören. Auch die Berkeley Universität wäre daher nicht in der Lage gewesen, ihre Unix-Entwicklungen kommerziell zu vertreiben. Die BSD-Lizenz bekräftigte insofern nur Rechte, die die Allgemeinheit nach vorherrschender Auffassung ohnehin hat, fixierte das ab 1976 möglich gewordene Copyright an Berkeley-Software und verhinderte, dass ein Dritter diese Software geringfügig veränderte und unter restriktiven Konditionen weiterverbreitete. Der Berkeley-Code und alle darauf aufbauende Software sollte immer frei weitergebbar und modifizierbar bleiben.@2  ZÜ 2:GNU General Public License@1 fliess mit:@1 fliesskursiv Zitat:»Um ein Programm unter das Copyleft zu stellen, stellen wir es zuerst unter das Copyright; dann fügen wir als Rechtsmittel Vertriebsbedingungen hinzu, die jedermann das Recht geben, den Code des Programms  oder jedes davon abgeleiteten Programms zu nutzen, zu ändern oder weiter zu verteilen, aber nur, wenn die Vertriebsbedingungen unverändert bleiben. So werden der Code und die gewährten Freiheiten rechtlich untrennbar.«<@3 hoch fliess>27<@$p>@1 fliess mit:@1 fliess ohne:Im neu entstehenden Softwaremarkt der späten 70er herrschte im Gegensatz dazu eine nachgerade paranoide Haltung. Jeder Käufer erschien den Firmen als potenzieller »Pirat«. Das ihnen gerade zugestandene Copyright schien ihnen nicht ausreichend, so dass sie selbst die ausführbaren Binärversionen ihrer Software nur herausgaben, wenn ihre Kunden eine Vertraulichkeitsvereinbarung (NDA) unterzeichneten.<@3 hoch fliess>28<@$p> Dies war allenfalls praktikabel, solange sie es mit einer überschaubaren Zahl von Industriekunden zu tun hatten. Mit dem PC etablierten sich die so genannten Massenmarktlizenzen, auf die am Ende dieses Kapitels eingegangen wird. Den Quellcode hüteten sie ohnehin als Geschäftsgeheimnis. Veränderungen durch die Nutzer sollten verhindert werden. Eine kooperierende Community wurde verboten. Die Regel, die die Eigentümer proprietärer Software etablierten, lautete: »Wenn Sie Software mit Ihrem Nachbarn austauschen, sind Sie ein Pirat. Wenn Sie Änderungen haben wollen, bitten Sie uns darum, sie zu machen« (<@6 Caps>Stallman<@$p>, 1999, S. 54).@1 fliess mit:Die Hacker der ersten und zweiten Generation reagierten auf diese Entwicklung, indem sie ihre Software in die <@1 fliess kursiv>Public Domain <@$p>stellten oder indem sie zwar ein Copyright dafür anmeldeten, sie aber als Freeware deklarierten, meist ohne sich weitere Gedanken über die Details einer Lizenz zu machen. Wenn sie nicht gar ins Lager der proprietären Software wechselten. Eine, wenn man so will, fundamentalistische Reaktion war die von Richard Stallman. 1984 startete er das GNU-Projekt. Er schrieb auch viel einflussreiche Software, doch seine wohl folgenreichste Erfindung ist das »Copyleft«.<@3 hoch fliess>29 <@$p>Als Betriebssystementwickler dachte er nicht nur über die technischen Grundlagen von Software nach, sondern näherte sich auch ihrem rechtlichen Status auf tief greifendere Weise als die Verfasser anderer freier Lizenzen. In Zusammenarbeit mit juristischen Beratern der FSF, wie dem Columbia-Professor für Recht und Rechtsge<t-1>schichte Eben Moglen (vgl. z.B. <x@6 Caps><t-1>Moglen<@$p><t-1>, 1999), entstand daraus die <x@1 fliess kursiv><t-1>GNU <t$>General Public License <@$p>(GPL).<@3 hoch fliess>30<@1 fliess normal><@$p><t-1>Die Präambel der GPL beginnt mit der Feststellung: <t$>»Die meisten Softwarelizenzen sind dafür gemacht, dir die Freiheit wegzunehmen, sie mit anderen zu teilen und sie zu verändern. Im Gegensatz dazu hat die <@1 fliess kursiv>GNU General Public License<@$p> die Absicht, deine Freiheit abzusichern, freie Software zu teilen und sie zu verändern – um sicherzustellen, dass die Software für alle ihre Nutzer frei ist.«<t-1> Um sicherzustellen, dass die Software auch in Zukunft frei bleibt, unterliegen die Freiheiten Bedingungen, »die es jedem verbieten, dem Nutzer diese Freiheiten zu verweigern oder ihn auffordern, auf sie zu verzichten«. Das Hinzufügen weiterer Restriktionen wird untersagt. Die Freiheiten und ihre Bedingungen umfassen im Einzelnen:<t$>@1 fliess ohne:<@5 Klötzchen (kleines p)>p  <\i><@$p>Die Freiheit, das Programm für jeden Zweck auszuführen (Ziff. 0),<@3 hoch fliess>31<@1 fliess normal><@5 Klötzchen (kleines p)>p  <\i><@$p>die Freiheit, den Quellcode des Programms wörtlich zu kopieren und zu verbreiten, sofern der Copyright-Vermerk und die Lizenz mit kopiert und verbreitet wird. Die Erhebung einer Gebühr für die physikalische Übertragung einer Kopie und für andere Dienstleistungen, wie eine Gewährleistung, wird ausdrücklich erlaubt (Ziff. 1),<@5 Klötzchen (kleines p)>p  <\i><@$p>die Freiheit, das Programm zu verändern und diese veränderte Version zu kopieren und zu verbreiten, sofern das abgeleitete Werk Angaben über die Änderung enthält und gebührenfrei und unter denselben Lizenzbedingungen<@3 hoch fliess>32<@$p> veröffentlicht wird, wie das ursprüngliche Programm. Von der Bedingung ausgenommen sind Teile des veränderten Programms, die unabhängige Werke darstellen und separat verbreitet werden (Ziff. 2),<@5 Klötzchen (kleines p)>p  <\i><@$p>die Freiheit, das Programm oder abgeleitete Versionen in Objektcode- oder ausführbarer Form zu kopieren und zu verbreiten, sofern der <\h>dazugehörige maschinenlesbare Quellcode oder ein schriftliches, mindes<\h>tens drei Jahre gültiges Angebot, diesen Quellcode auf Anfrage bereitzustellen, beigefügt ist (Ziff. 3).@1 fliess mit:@1 fliess ohne:Die weiteren Sektionen der GPL betreffen den Verfall der Lizenzrechte durch Verstöße (Ziff. 4), das Verbot, den Empfängern der Software irgendwelche weitergehenden Restriktionen aufzuerlegen (Ziff. 6), Konflikte mit anderen (z.B. Patent-) Ansprüchen, die dazu führen können, dass das Programm nicht weiterverbreitet werden darf (Ziff. 7), mögliche landesspezifische Restriktionen, die dazu führen können, dass diese Länder von der Verbreitung des Programms ausgeschlossen werden (Ziff. 8), mögliche Änderungen der GPL durch die FSF (Ziff. 9) und schließlich den üblichen Gewährleistungs- und Haftungsauschluss, in dem Maße es das anwendbare Recht zulässt (Ziff. 11 und 12).<@3 hoch fliess>33<@$p>@1 fliess mit:Ziffer 5 erläutert, wie dieser Lizenzvertrag zwischen Copyright-Inhaber und Nutzer zu Stande kommt. Darin heißt es, durch die Veränderung oder Verbreitung des Programms oder abgeleiteter Werke zeige der Nutzer seine Einwilligung in die Lizenzbedingungen an. Sektion 10 eröffnet den Copyright-Inhabern die Möglichkeit, zu entscheiden, ob sie erlauben wollen, dass Teile ihres Programms in andere freie Programme integriert werden, die nicht unter der GPL stehen. Grundsätzlich ist eine enge Kopplung von GPL-Software nur mit anderer Software erlaubt, die ebenfalls unter der GPL oder einer kompatiblen Lizenz steht.<@3 hoch fliess>34<@$p> Diese Sektion erlaubt Ausnahmen nach Einzelfallentscheidung durch die Copyright-Inhaber. Die Kopplung von GPL-Software mit Softwarebibliotheken, die nicht im Quelltext veröffentlicht werden, regelt eine eigenständige Lizenz, die LGPL (s.u.).Stallmans Ziel mit dieser juristischen Konstruktion war es nicht, Software einfach zu verschenken (wie es bei Freeware oder <@1 fliess kursiv>Public Domain-<@$p>Software geschieht; er betont, dass GNU nicht in der <@1 fliess kursiv>Public Domain <@$p>sei, was es erlauben würde, alles damit zu machen, auch die weitere Verbreitung zu beschränken), sondern systematisch einen Bestand an nützlicher Software aufzubauen, der für alle Zeiten – genauer: für die Laufzeit der Schutzfrist des am längsten lebenden Urhebers – frei bleiben wird. Das klingt nach einem ideologischen, utopistischen Programm. Tatsächlich sind die Beweggründe dahinter ganz pragmatisch: Techniker wollen Dinge erledigen und Probleme lösen, ohne daran durch legalistische Mauern gehindert zu werden. Sie hassen Redundanz. Wenn jemand anderes ein Problem gelöst hat, wollen sie nicht die gleiche Arbeit noch einmal machen müssen. Sie wollen ihr Wissen mit anderen teilen und von anderen lernen, und nicht durch NDAs, ausschließende Lizenzen oder Patente daran gehindert werden. In juristischen Begriffen gesprochen gewährt die GPL auf der Basis des Ausschließlichkeitsrechts der Urheberin ein <\n>bedingtes, einfaches Nutzungsrecht an jedermann. Die Bedingungen werden in der GPL formuliert. Verstößt ein Lizenznehmer gegen sie, verfallen die Nutzungsrechte automatisch und die Verbotsrechte von Copyright-/Urheberrecht treten wieder in Wirkung. <*h"mehr">Die wichtigste Bedingung besteht darin, dass die von einer unter der GPL stehenden Software abgeleiteten Programme ebenfalls unter der GPL stehen müssen. Ziel dieser von ihren Gegnern häufig als »infektiös«, richtiger als »impfend« bezeichneten Klausel ist es, eine Privatisierung von kollektiv erzeugtem Wissen zu verhindern und den Gesamtbestand an freier Software beständig zu erweitern. Sie hat verschiedene Aspekte. Verbesserte Versionen von GPL-Software dürfen nicht anders denn als <@1 fliess kursiv>Free Software <@$p>verbreitet werden. Die MIT-X-Window-Lizenz z.B., der dieser Vererbungsmechanismus fehlt, führte dazu, dass Firmen die Software auf neue Hardware portiert und proprietär geschlossen, d.h. dem offenen kooperativen Entwicklungsprozess entzogen haben. Wenn Programmierer GPL-Software um eigene oder um Werke Dritter, die unter einer unfreien Lizenz stehen, erweitern, kann das Gesamtwerk nur die Freiheiten der restriktivsten Lizenz gewähren. Da die freie Softwarebewegung nicht bereit ist, mögliche technische Vorteile durch den Verzicht auf Freiheiten zu erkaufen, untersagt sie dies. Programmierern, die bei Firmen oder Universitäten angestellt sind und ihre Verbesserungen an die Community zurückgeben möchten, sagt ihr Arbeitgeber nicht selten, dass er ihre Werke zu einem proprietären Produkt machen will. Erfährt er jedoch, dass die Lizenz dies nicht zulässt, wird er die verbesserte Software gewöhnlich unter der GPL freigeben, statt sie wegzuwerfen (vgl. <@6 Caps>Stallman<@$p>, 1999, S. 60).<*h"Standard"><t1>Da der GPL häufig eine antikommerzielle Ausrichtung unterstellt wird, ist die meistgehörte Erläuterung, dass Freiheit im Sinne von Redefreiheit, nicht von Freibier gemeint ist. In der Präambel heißt es: »Wenn wir von freier Software sprechen, meinen wir Freiheit, nicht Preis ... die Freiheit, Kopien freier Software zu verbreiten (und für diese Dienstleis<\h>tung einen Preis zu berechnen, wenn man möchte).« In Ziffer 1, Seite 2 wird die Gebührenerhebung für Dienstleistungen im Zusammenhang mit freier Software ausdrücklich erlaubt: »Sie dürfen für den eigentlichen Vorgang der Übertragung einer Kopie eine Gebühr verlangen und, wenn Sie es wünschen, auch gegen ein Entgelt eine Garantie für das Programm anbieten.« Während es von abgeleiteten Werken explizit heißt, dass ihr Autor sie »gebührenfrei« an jedermann lizenzieren muss (Ziff. 2 b), enthält die GPL keine derartige Aussage über die Verbreitung unveränderter Kopien. Dass über die genannten Dienstleistungen hinaus für die Lizenzierung der Nutzungsrechte allein keine Gebühr erhoben werden darf, wird zwar nirgends ausgeführt, kann jedoch wohl angenommen werden.<t$>Nicht nur die <@1 fliess kursiv>Free Software Foundation<@$p> selbst, sondern viele andere Programmierer sehen in der GPL den besten Mechanismus, um die freie kooperative Weiterentwicklung ihrer Werke zu sichern. So entsteht seit 1984 ein freies Paralleluniversum zur proprietären Software, das es heute erlaubt, alles mit einem Computer zu tun, was man möchte, ohne sich den Zumutungen der Unfreiheit aussetzen zu müssen. In der Gesamtschau ist es keine Übertreibung, die GPL als den größten Hack der Wissensordnung zu bezeichnen, seit britische Verlegergilden das Urheberrecht erfanden. @2  ZÜ 3:GPL und deutsches Recht@1 fliess ohne:Die GPL ist, wie die meisten der hier behandelten Lizenzen, im US-amerikanischen Rechtsraum formuliert worden. Dort ist sie bislang ebenfalls noch nicht gerichtlich überprüft worden, hat aber doch in zahlreichen Fällen zu außergerichtlichen Einigungen mit Softwareunternehmen geführt (vgl. <@6 Caps>Powell<@$p>, 6/2000). Zwar betont Stallman, dass sie den in den Beitrittsländern verbindlichen Auflagen des Berner Urheberrechtsüber<\h>einkommens genüge, dennoch ist zu überprüfen, ob sie auch unter den deutschen Rechtsbedingungen einen gültigen Lizenzvertrag darstellt. Hier wird auf die GPL näher eingegangen, da sie vergleichsweise komplexe Regelungen enthält und in der deutschen Rechtsliteratur einige Würdigung erfahren hat. Für die anderen Lizenzen mit sinngemäßen Regelungen gilt gleichermaßen das hier über die GPL Gesagte.@1 fliess mit:Eine Autorin hat auch nach dem deutschen Urherbergesetz (UrhG) ein Ausschließlichkeitsrecht an ihrem Werk, demzufolge sie über seine Veröffentlichung und Verwendung entscheiden kann. Die Kernpunkte der GPL lassen sich ohne weiteres auf das deutsche Urheberrecht abbilden.<@3 hoch fliess>35<@$p> Die Autorin kann verfügen, dass ihre Software von anderen auf einzelne oder alle Arten genutzt werden darf (§ 31, Abs. 1, 2 UrhG, Einräumung von Nutzungsrechten). Dies kann das Vervielfältigungs- (§<\!q>16 UrhG), das Verbreitungsrecht (§ 17 UrhG) und das Recht auf Bearbeitungen und Umgestaltungen (§ 23 UrhG) umfassen.Die Frage, ob durch den impliziten Mechanismus der Ziff. 5 GPL (»Indem Sie das Programm modifizieren oder verbreiten [...],  bekunden Sie Ihre Einwilligung in diese Lizenz.«) überhaupt ein Vertrag zwischen Urheber und Softwarenutzer zustande kommt, wird von Metzger/Jaeger bejaht. Es handle sich um einen Lizenzvertrag, »den der Nutzer durch die Verwertungshandlungen konkludent annimmt, wobei es eines Zugangs dieser Annahmeerklärung an den Urheber gemäß § 151 S.1 BGB nicht bedarf« (<@6 Caps>Metzger/Jaeger<@$p>, 1999, V.1). Siepmann dagegen hält diese automatische Einverständniserklärung für eine Fiktion: »Die Schutzrechte des Urhebers hängen nicht davon ab, ob die Lizenz anerkannt wird« (<@6 Caps>Siepmann<@$p>, 1999, Abs. 97). Allerdings behauptet die GPL auch gar nicht, dass die Schutzrechte erst durch das Einverständnis entstehen, denn auf diese wird durch den Copyright-Vermerk am Anfang unmissverständlich hingewiesen. Die Frage scheint vielmehr, ob die Gewährung der Nutzungsrechte und vor allem die daran geknüpften Bedingungen auf diese Weise vertraglich bindend werden. Auf diesen Punkt wird am Ende des Kapitels bei den Massenmarktlizenzen eingegangen. Auch im folgenden Abschnitt, demzufolge der Empfänger bei jeder Weitergabe des unveränderten und jedes abgeleiteten Programms automatisch eine Lizenz vom Urheber erhält, sieht Siepmann eine Fiktion, die im deutschen Recht keinen Sinn habe (vgl. ebd., Abs. 98).Die Knüpfung der Nutzungsrechte an bestimmte Bedingungen (den Quellcode zugänglich zu machen, abgeleitete Werke wiederum unter die GPL zu stellen, Änderungen an Dateien kenntlich zu machen), deren Nichteinhaltung den Vertrag automatisch auflöst (Ziff. 4, GPL), halten Metzger/Jaeger als ein bedingtes Nutzungsrecht im Sinne des Paragraphen 158 BGB ebenfalls für rechtsgültig. Diese auflösende Bedingung gelte auch für zukünftige abgeleitete Werke, wie die Möglichkeit einer Vorausverfügung des § 40 UrhG zeige. Siepmann hält diesen Abschnitt für rechtlich größtenteils überflüssig (ebd., Abs. 96).Die größten Unterschiede zwischen Copyright und Urheberrecht <\n>betreffen die Urheberpersönlichkeitsrechte. Während das Recht des Urhebers auf Namensnennung (§ 13 UrhG) durch die Verpflichtung zur Beibehaltung der Copyright-Vermerke in Ziff. 1 und zur Angabe von Veränderungen in Ziff. 2 a GPL gewahrt wird, scheint der Integritätsschutz des § 14 UrhG problematischer. Danach kann der Urheber Entstellungen und andere Beeinträchtigungen seines Werkes verbieten. Zwar muss ein Bearbeiter nach Ziff. 2 a GPL seine Änderungen an den entsprechenden Dateien kenntlich machen, dennoch ist der Fall vorstellbar, dass die Verbreitung einer entstellten Version eines Programms zu einer Rufschädigung des Autors der ursprünglichen Version führt. Dies wiegt umso schwerer, als die Ehre, die sich Programmierer für die Qualität ihres Codes in der Community erwerben können, eine nicht unmaßgebliche Motivation darstellt. In diesem Fall könnte nach deutschem Recht der ursprüngliche Autor die Verbreitung der abgeleiteten Software verbieten. Metzger/Jaeger kommen zu dem Schluss, »dass es trotz der weitreichenden Freistellung der Open Source-Software durch die GPL für den Bearbeiter bei dem Risiko bleibt, sich in Ausnahmefällen einem Verbot des Urhebers auf der Grundlage des § 14 UrhG gegenüber zu sehen.«Mögliche Konflikte ergeben sich aus Ziff. 9 GPL, in der die FSF mögliche überarbeitete oder neue Versionen der GPL ankündigt und die <\n>Option eröffnet, ein Programm unter eine bestimmte und »jede spätere Version« der GPL zu stellen. Der Lizenznehmer könnte dann wählen, ob er den Bedingungen der spezifizierten oder der jeweils aktuellen Version folgt. Wird keine Versionsnummer der Lizenz angegeben, kann er eine aus allen je veröffentlichten Versionen wählen. Von der Unsicherheit <\n>abgesehen, in welchen Vertrag man eigentlich genau eingewilligt hat, lassen sich Konflikte mit § 31 Abs. 4 UrhG vorstellen: »Die Einräumung von Nutzungsrechten für noch nicht bekannte Nutzungsarten sowie Verpflichtungen hierzu sind unwirksam.« Theoretisch sind neue Nutzungsarten denkbar, die in früheren Versionen der GPL nicht expliziert sind und daher Verbotsrechte von Urhebern wirksam werden lassen. Die in Ziff. 9 GPL zumindest implizierte (wenn auch durch die Wahlfreiheit des Lizenznehmers aufgeweichte) Vorausverfügung ist also nach deutschem Recht nicht zulässig. Das Problem kann vermieden werden, indem Autoren die Versionsnummer der GPL, unter der sie ihr Werk veröffentlichen wollen, angeben.Problematischer stellen sich der generelle Gewährleistungs- (Ziff. 11, GPL) sowie der Haftungsausschluss (Ziff. 12) dar. Bei einem deutschen Lizenznehmer wird das deutsche Recht wirksam, das unabhängig von der vertraglichen eine gesetzliche Haftung<@3 hoch fliess>36 <@$p>vorschreibt, die nicht durch Erklärungen geändert werden kann. Die schuldrechtlichen Fragen sind im Vertragsrecht (BGB) und im Allgemeinen Geschäftsbedingungsgesetz (AGBG) geregelt. Hier kommen Siepmann und Metzger/Jaeger übereinstimmend zu dem Schluss, dass beide Ausschlüsse unwirksam sind. Sie verstoßen gegen die absoluten Klauselverbote des § 11 AGBG. Salvatorische Klauseln wie »Schadensersatzansprüche sind ausgeschlossen, soweit dies gesetzlich zulässig ist« sind nach § 2 Abs. 1 Nr. 2 AGBG unwirksam (vgl. ebd., Abs. 55). Wenn die Nutzungslizenz unentgeltlich eingeräumt wird – was eine Schenkung gem. § 516 BGB ist (vgl. ebd.,  Abs. 44–46) –, »ist die Haftung gem. § 521 BGB auf Vorsatz und grobe Fahrlässigkeit beschränkt, die Gewährleistungspflicht für Sach- und Rechtsmängel gem. §§ 523, 524 BGB auf arglistig verschwiegene Fehler« (<@6 Caps>Metzger/Jaeger<@$p>, 1999, VI. 3). Wird die Software kommerziell in einer Distribution oder vorinstalliert auf einem Rechner vertrieben, sehen Metzger/Jaeger und Siepmann die Anwendbarkeit des Kaufgewährleis<\h>tungsrechts gegeben.<@3 hoch fliess>37<@$p> Ob auch das Produkthaftungsgesetz, das sich ausdrücklich nur auf körperliche Gegenstände bezieht, zur Anwendung <t-1>kommt, ist umstritten. Fielen auch Computerprogramme darunter, würde hiernach eine Haftung sogar unabhängig vom Verschulden bestehen.<x@3 hoch fliess><t-1>38<@$p>Die Gültigkeit der GPL ist weder in Deutschland noch in den USA oder in irgendeinem anderen Land bisher gerichtlich überprüft worden. Konflikte sind in allen Fällen außergerichtlich beigelegt worden (s.u.). Dennoch ist nach der Interpretation der Rechtslage davon auszugehen, dass sich Entwickler, die ihre eigene Software unter die GPL stellen oder sich an GPL-Projekten beteiligen, ebenso wie Endnutzer freier Software ihrer Freiheiten sicher sein können. Vorsicht ist jedoch bei möglichen Haftungsansprüchen geboten. Hier kommt den Autoren und besonders den Distributoren eine Sorgfaltspflicht zu. Sie sollten sich über die Herkunft der verbreiteten Programme, ihre Qualität und mögliche Fehler kundig machen und bei Bekanntwerden von Mängeln ihre Kunden unterrichten. Bei Vernachlässigung ist davon auszugehen, dass sie für überschriebene Dateien, gelöschte Festplatten oder Schäden durch »trojanische Pferde« haftbar gemacht werden können.@2  ZÜ 3:Library / Lesser GPL@1 fliess ohne:Die GNU <@1 fliess kursiv>Library General Public License<@3 hoch fliess>39 <@$p>der FSF datiert vom Juni 1991. Die Präambel erläutert zunächst die GPL und betont, dass die Leserin ihre Software, Bibliotheken eingeschlossen, ausschließlich für andere freie Programme nutzbar machen kann, wenn sie sie unter diese Lizenz stellt. Dann führt sie die LGPL als Spezialfall einer Lizenz für »ganz bestimmte Bibliotheken« ein und betont, dass sie sich erheblich von der gewöhnlichen GPL unterscheide. @1 fliess mit:Die Grundintention der LGPL entspricht der der GPL. Sie schreibt <\n>alle Freiheiten der GPL fest. Die Bibliothek muss frei kopier-, verbreit- und modifizierbar sein, der Quellcode von Kopien und Bearbeitungen verfügbar sein, und sie spricht die Urheber ebenso von Haftungs- und Gewährleistungsansprüchen frei. In die LGPL scheint eine größere Sensibilität für den Integritätsschutz der kontinentaleuropäischen Urheberpersönlichkeitsrechte eingegangen zu sein. So heißt es in der Präambel: »Wird die Bibliothek von einem Dritten modifiziert und verbreitet, möchten wir, dass die Empfänger wissen, dass es sich nicht um die Originalversion handelt, sodass Probleme, die von anderen hinzugefügt wurden, nicht auf die Reputation der ursprünglichen Autoren zurückfällt.«Der Hauptunterschied zur GPL besteht darin, dass Programme, die die freie Bibliothek unter dieser Lizenz einlinken und damit ein ausführbares Ganzes bilden, nicht selbst diesen Freiheiten unterstehen müssen. Als Grund für eine separate schwächere Lizenz gibt die Präambel an, dass Bibliotheken die Unterscheidung zwischen Veränderung einer Software und ihrem einfachen Gebrauch verschwimmen lassen. Wird die Bibliothek (beim <@4 Pfeil (Umschalt/Alt #)>’<@$p> Booten oder zur Laufzeit) in ein anderes Programm eingelinkt, entsteht ein <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Executable<@$p> das Teile der Bibliothek enthält und somit ein abgeleitetes Werk darstellt. »Linkt man ein Programm mit einer Bibliothek, ohne sie zu verändern, entspricht dies in gewisser Weise einer einfachen Benutzung der Bibliothek, so wie man ein Hilfs- oder Anwendungsprogramm benutzt. In einem buchstäblichen und rechtlichen Sinne ist das gelinkte <I>Executable<$> jedoch ein Verbundwerk, eine Ableitung von der ursprünglichen Bibliothek, und die gewöhnliche <I>General Public Li<\h>cense<$> behandelt es als solche.«<@3 hoch fliess>40<@$p> Mit anderen Worten, stünde die Bibliothek unter der GPL, müsste auch das abgeleitete Ganze unter die GPL gestellt werden. Um nu<@1 fliess normal>n Programmierern in der proprietären Welt Anreize zu geben, freie Werkzeuge zu benutzen, lockert die LPGL diese Auflagen. Den Ausschlag gab die mit dem GNU C Compiler »gcc« verbundene C-Laufzeitbibliothek »libc«, die ursprünglich unter der GPL stand. Daher musste jedes Programm, das mit dem gcc ko<@$p>mpiliert wurde, wiederum unter der GPL stehen, was viele Entwickler von seiner Verwendung ausschloß. An dieser Stelle, heißt es weiter in der Präambel, erfülle die GPL ihr Ziel nicht, das Teilen von Software zu fördern: »Wir sind zu dem Schluss gekommen, dass schwächere Auflagen die gemeinsame Nutzung besser fördern könnten. [...] Dies <@1 fliess kursiv>Library General Public License<@$p> dient dazu, es Entwicklern unfreier Progamme zu erlauben, freie Bibliotheken zu verwenden, und gleichzeitig deine Freiheit als Nutzer solcher Programme zu bewahren, die darin enthaltenen freien Bibliotheken zu verändern. [...] Es ist unsere Hoffnung, dass das zu einer schnelleren Entwicklung von freien Bibliotheken führen wird.«<@3 hoch fliess>41<@$p>Zusammengesetzte Werke, die eine LGPL-Bibliothek verwenden, dürfen unter Bedingungen eigener Wahl verbreitet werden, sofern diese Bedingungen zulassen, dass der Kunde sie verändern und die Software <@1 fliess kursiv>reverse engineeren<@$p> darf, um diese Veränderungen zu <@1 fliess kursiv>debuggen<@$p>. Auch wenn die anderen Bestandteile des zusammengesetzten Werkes quellgeschlossen sind, muss für die LGPL-Bibliothek der Quellcode zugänglich sein, damit ein Anwender ihn ändern und neu einlinken kann, um ein verändertes <@1 fliess kursiv>Executable <@$p>zu erzeugen.<@3 hoch fliess>42<\f><@$p>Stärker noch als in der GPL muss der Verbreiter solcher abgeleiteter Werke hier sogar »verifizieren«, dass der Anwender den Quellcode der Bibliothek erhalten hat (Ziff. 6 d).Eine modifizierte Version der Bibliothek muss selbst wieder eine <\n>Bibliothek sein. In ihrer Ausführung darf sie nicht von Daten aus einem möglicherweise proprietären Anwendungsprogramm abhängig sein (Ziff. 2 a; d). Damit soll verhindert werden, dass LGPL-Bibliotheken derart verändert werden, dass sie nicht mehr von freien Programmen genutzt werden können, da sie nur mit Bestandteilen der proprietären Software zusammen funktionieren. Die meisten GNU Bibliotheken vor Februar 1999 und ein Großteil der weiteren Bibliotheken, die bei einem GNU/Linux-System eingesetzt werden, stehen unter der LGPL. Stallman betont an verschiedenen Stellen, dass es sich um eine strategische Entscheidung handelt, ob ein Entwickler seine Bibliothek unter die GPL oder unter die Library-GPL stellt: »Es gibt keinen ethischen Grund, proprietäre Anwendungen auf Grundlage des GNU-Systems zuzulassen, strategisch jedoch scheint es, dass ihr Verbot eher dazu beitragen würde, Leute von der Benutzung des GNU-Systems abzuhalten, als sie dazu zu bringen, freie Anwendungen zu schreiben« (<@6 Caps>Stallman<@$p>, 1999, S, 63). Im Februar 1999 wurde die <@1 fliess kursiv>Library GPL <@$p>durch die <@1 fliess kursiv>Lesser General Public License<@3 hoch fliess>43<@$p> ersetzt (die verwirrenderweise ebenfalls mit LGPL abgekürzt wird). Der Namenswechel erfolgte, da »Library GPL« die irreführende Vorstellung vermittelt habe, dass sie ausschließlich auf Bibliotheken anwendbar sei (vgl. <@6 Caps>Stallman<@$p>, 1999a). Die neue LGPL enthält eine Reihe Änderungen in den vorangestellten Erläuterungen, die nicht Teil der Lizenz sind. So wird als möglicher Grund, statt der GPL die LGPL zu verwenden, genannt, dass der weitestmögliche Einsatz der Bibliothek dazu dienen kann, sie als de-facto-Standard zu etablieren. Umgekehrt wäre bei einer freien Bibliothek, die das tut, was auch eine verbreitete unfreie leis<\h>tet, nichts damit gewonnen, sie auf freie Software zu beschränken. Schließlich könne die Erlaubnis, eine freie Bibliothek, wie die GNU C-Bibliothek, in unfreier Software zu verwenden, dazu führen, dass sie mehr Menschen befähige, einen größeren Korpus freier Software wie das GNU Betriebssystem oder GNU/Linux zu benutzen. Der Lizenztext selbst ist weitgehend wortgleich mit der Library GPL. Der wesentliche Unterschied besteht darin, dass unter der Kategorie »Werke, die die Bibliothek benutzen« jetzt auch dynamisch zur Laufzeit eingebundene <@1 fliess kursiv>Shared Libraries <@$p>erfasst werden (die neu eingefügte Ziff. 6.b). Auch <@1 fliess kursiv>Dynamic Link Libraries <@$p>(eine von Microsoft zwar nicht erfundene, aber mit seinen DLLs verbreitete Technologie) müssen somit den Tatbestand der Veränderbarkeit und Ersetzbarkeit erfüllen.<@3 hoch fliess>44<@$p>Seit Einführung der neuen Lizenz ruft Stallman dazu auf, mehr <\n>Bibliotheken unter der GPL zu veröffentlichen (Stallman, 1999a) und in anderen Fällen die Lesser GPL zu verwenden. Die strategischen Motive erläutert er am deutlichsten in der Begründung für den Namenswechsel von »Library GPL« zu »Lesser GPL«. Dort schreibt er: »Die Entwickler freier Software müssen Vorteile für einander schaffen. Die gewöhnliche GPL für eine Bibliothek zu verwenden, verschafft den Entwicklern freier Software einen Vorteil gegenüber den proprietären Entwicklern: eine Bibliothek, die sie nutzen können, während proprietäre Entwickler sie nicht nutzen können« (ebd.). Grundsätzlich empfiehlt er daher die Verwendung der GPL, doch sei sie nicht für jede Bibliothek vorteilhaft. Vor allem wenn der proprietären Software die Features der freien Bibliothek aus unfreien Bibliotheken einfach zur Verfügung stünden, wie das bei der GNU C-Bibliothek der Fall ist, brächte die GPL keinen Vorteil für die freie Software, »so ist es für diese Bibliothek besser, die Library GPL zu verwenden.«In anderen Fällen bietet die freie Bibliothek bedeutende einzigartige Funktionalitäten, wie die GNU <@1 fliess kursiv>Readline.<@$p> Hier empfiehlt Stallman die GPL: »Die Readline-Bibliothek implementiert das Editieren der Eingaben und ihre History für interaktive Programme. Das ist eine Funktion, die im Allgemeinen anderswo nicht verfügbar ist. Sie unter die GPL zu stellen und ihre Verwendung auf freie Programme zu beschränken, gibt unserer Community einen echten Vorsprung. Mindestens ein Anwendungsprogramm ist heute freie Software, weil das notwendig war, um die Readline verwenden zu dürfen.« In dem Fall fördert die GPL die weitere Entwicklung freier Software. Universitätsprojekte und jetzt, da auch Unternehmen freie Software entwickeln, auch kommerzielle Projekte können auf diese Weise beeinflusst werden, ihre Software ebenfalls unter die GPL zu stellen.@1 fliesskursiv Zitat:»... wir können viel mehr erreichen, wenn wir zusammenhalten. Wir, die Entwickler freier Software, sollten uns gegenseitig unterstützen. Indem wir Bibliotheken freigeben, deren Verwendung auf freie Software beschränkt ist, können wir dazu beitragen, dass unsere freien Softwarepakete die proprietären Alternativen überbieten. Die ganze freie Softwarebewegung wird mehr Popularität gewinnen, weil freie Software als Ganze besser gegenüber der Konkurrenz abschneidet« <@1 fliess normal>(ebd.). <@$p>@1 fliess ohne:@2  ZÜ 2:<t0f"FFScala"><t$f$>Weitere offene Lizenzen@1 fliess mit:@1 fliess ohne:Der ersten Generation von Lizenzen (BSD, GPL und MIT-X) folgten ab <\n>etwa 1997 eine Fülle weiterer Lizenzmodelle. Viele Projekte verfassten eigene Lizenztexte oder variierten bestehende, meist mit einer individuellen Note, aber oft nicht mit der juristischen Sorgfalt, die in die GPL geflossen ist. In jüngster Zeit kommen die Lizenzen von Unternehmen wie Apple, Sun und IBM hinzu, die freien Lizenzen mehr oder weniger ähneln.<@3 hoch fliess>45<@$p> Um sich als freie Software oder Open Source-Software zu qualifizieren, muss die Lizenz die Freiheit gewähren, das Programm gebührenfrei und ohne Einschränkungen auszuführen, es zu kopieren und weiterzuverbreiten, der Zugang zum Quellcode und das Recht, Veränderungen vorzunehmen, müssen gewährleistet sein. Die Hauptunterschiede betreffen die Art, wie die Lizenzen das Verhältnis von freiem zu proprietärem Code und den Status von abgeleiteten Werken regeln.@1 fliess mit:In den Lizenzen manifestieren sich die politische Philosophie der Projekte und ihr Verhältnis zur Community, zur Wirtschaft und zur Gesellschaft allgemein. Wird Software zu Paketen integriert und in Distributionen aggregiert, so treten auch ihre Lizenzen in Wechselwirkung miteinander. Je mehr Lizenzmodelle in Gebrauch sind, desto schwieriger wird die Wahl einer Lizenz und desto unübersichtlicher ihre Wechselwirkungen. Daher gibt es verschiedene Bestrebungen, Lizenzen nach bestimmten Kriterien zu bewerten und zu klassifizieren. Die FSF unterscheidet sie in ihrer Liste danach, ob es sich um freie Software und um Copyleft handelt und ob eine Lizenz kompatibel zur GPL ist, d.h. ob die Software unter ihr mit GPL-Software gelinkt werden darf.<@3 hoch fliess>46<@$p> Auch OpenBSD vergleicht in seinem <@1 fliess kursiv>Copyright Policy<@$p>-Dokument eine Reihe anderer Lizenzen und bewertet sie danach, ob die entsprechende Software in die OpenBSD-Distribution aufgenommen werden kann.<@3 hoch fliess>47<@$p>Peter Deutsch, Autor von »Ghostscript«, einer weit verbreiteten Implementation der Seitenbeschreibungssprache »PostScript«, erfasst in seiner Klassifikation die <@1 fliess kursiv>Free Redistribution Licenses<@$p> (FRLs) (vgl. <@6 Caps>Deutsch<@$p>, 1996). Im Vordergrund steht für ihn die Möglichkeit zur freien Weitergabe, nicht die Modifikation, weshalb auch Shareware auftaucht. Letztlich geht es ihm darum, seine eigene <@1 fliess kursiv>Aladdin Ghostcript Free Public License <@$p>(AGFPL) vis à vis den anderen Lizenzmodellen zu positionieren. Deutsch unterscheidet vier Gruppen von FRLs nach ihren Bedingungen für die Weiterverbreitung, den Umständen, unter denen Zahlungen erforderlich sind und den Bedingungen für den Zugang zum Quellcode. Modifikationsfreiheit ist für ihn kein eigenes Kriterium. Die vier Kategorien sind: (1) Unbeschränkte Lizenzen, z.B. von X Window, Tcl/Tk,<@3 hoch fliess>48<@$p> IJG JPEG,<@3 hoch fliess>49 <@$p>PNG/zlib<@3 hoch fliess>50<@$p> oder zip/unzip.<@3 hoch fliess>51<@$p> Autorinnen, die eine solche Lizenz verwenden, möchten nur sicherstellen, dass sie als Autoren genannt bleiben, darüber hinaus stellen sie keine Bedingungen. Der Quellcode ist meist verfügbar, doch die Lizenz schreibt seine Mitverbreitung nicht vor. (2) Shareware, die üblicherweise nicht unter freier Software aufgeführt wird, da der Quellcode in der Regel nicht verfügbar ist und bei regelmäßigem Gebrauch eine Lizenzgebühr verlangt wird, Deutschs Kriterium ist aber eben die freie Weitergebbarkeit. (3) Die GNU-Lizenzen GPL und LGPL. (4) <@1 fliess kursiv>Not-for-profit<@$p> FRLs. Deutschs wichtigstes Beispiel hierfür ist die <@1 fliess kursiv>Aladdin (Ghostcript) Free Public License<@$p> (AFPL).<@3 hoch fliess>52 <@$p>Sie ist von der GPL abgeleitet,<@3 hoch fliess>53 <@$p>setzt aber drei abweichende Akzente. Sie besagt, dass der Quellcode bei jeder Distribution enthalten sein muss, während die GPL in bestimmten Fällen nur einen Hinweis darauf vorschreibt. Sie untersagt ausdrücklich eine Gebührenerhebung für die Software. »Ziel der AGFPL ist es, eine bestimmte Klasse von  GPL-›Trittbrettfahrern‹ auszuschließen: Firmen, die GPL’te Software auf eine Weise mit kommerziellen Anwendungen bündeln, die die Integrität von Ersterer wahrt, sie aber von Letzterer aus nahtlos aufrufbar macht, was im Effekt (funktional) die GPL’te Software zu einem Teil der Anwendung macht, und zugleich den Buchstaben der GPL gehorcht« (<@6 Caps>Deutsch<@$p>, 1996). Und drittens löst sie das Problem, dass ihr Autor sehr wohl Geld mit seiner Software verdienen will, indem sie eine Doppellizenzierung erlaubt. Die Weiterverbreitung von Ghostscript und die Weiterverwendung des Codes in kommerziellen Produkten ist erlaubt, sie erfordert jedoch eine von der AFPL verschiedene, gebührenpflichtige Lizenz. Die kommerzielle Version der Software namens »Artifex Ghostscript« ist identisch mit dem freien Aladdin Ghostscript, aber Artifex bietet außerdem Support, Fehlerbehebung im Kundenauftrag und Beigaben zu den Hard- oder Softwareprodukten anderer Anbieter.<@3 hoch fliess>54 <@$p>Mit dieser Doppellizenzierung vertritt die AFPL die Haltung, »... dass diejenigen, die bereit sind zu teilen, von den Vorteilen des Teilens profitieren sollten, während diejenigen, die selbst nach kommerziellen Regeln spielen, sich an diese Regel halten müssten, um die Vorteile der Software zu erlangen, die für andere frei weitergebbar ist«  (<@6 Caps>Deutsch<@$p>, 1996).Die FSF bezeichnet diese Kategorie der <@1 fliess kursiv>Not-for-profit FRLs<@$p> als »halb-freie« Software. Zwar gewähre sie Individuen alle Freiheiten, daher sei sie viel besser als proprietäre Software. Aufgrund der Restriktionen könne sie aber nicht in eine freie Betriebssystemumgebung (z.B. das GNU-Sys<\h>tem) integriert werden. Aufgrund der Wechselwirkung der Lizenzen in einem Gesamtsystem würde ein einziges halb-freies Programm das gesamte Sys<\h>tem halb-frei machen: »Wir meinen, dass freie Software für jeden da sein sollte – auch für Unternehmen, nicht nur für Schulen oder Hobby-Nutzer. Wir möchten die Geschäftswelt einladen, das gesamte GNU-System zu nutzen und deshalb sollten wir keine halb-freien Programme aufnehmen.«<@3 hoch fliess>55<@$p>Die <@1 fliess kursiv>Open Source Definition<@$p> (OSD)<@3 hoch fliess>56 <@$p>ist eine weitere Richtlinie zur Bewertung von Lizenzen. Sie stammt von Bruce Perens, dem ehemaligen Maintainer von Debian GNU/Linux. Debian sah sich angesichts der Nachbarlizenzen herausgefordert, genauer zu definieren, was die Freiheit sei, die das Projekt meint. Diese Positionen formulierte Perens nach einer E-Maildiskussion mit den anderen Debian Entwicklern 1997 im <@1 fliess kursiv>Debian Social Contract,<@$p> einem Bekenntnis, dass Debian zu 100 Prozent freie Software bleiben, dass das Projekt alle Neuerungen an die Community zurückgeben und keine Fehler verstecken wird, sowie in den <@1 fliess kursiv>Debian Free Software Guidelines <@$p>(DFSG).<@3 hoch fliess>57<@$p> Aus dem Geist dieser beiden Texte heraus entstand die OSD. Raymond spielte bei der Universalisierung eine Rolle, da er Perens in seine Bemühungen »das Konzept der freie Software an Leute zu verkaufen, die Krawatten tragen« (<@6 Caps>Perens<@$p>, 1999, S. 173), eingespannt hatte. Raymond hielt die DFSG für das richtige Dokument, um Open Source zu definieren. Perens entfernte alle debian-spezifischen Referenzen, tauschte »Free Software« gegen »Open Source-Software« aus und änderte den Namen der Lizenz. Schließlich registrierte er für SPI, der Schirmorganisation von Debian, ein <@1 fliess kursiv>Certification Mark <@$p>(CT) auf den Begriff »Open Source«. Ein CT ist eine Form von Trademark, eine Art Gütesiegel, das den Produkten von Dritten verliehen werden kann.<@3 hoch fliess>58<@$p> Nachdem Raymond und Perens mit dem dezidierten Ziel, die Open <\h>Source-Kampagne und ihr CT zu verwalten, die <@1 fliess kursiv>Open Source Initiative<@$p> (OSI) gegründet hatten, wurde das Eigentum an dem CT von SPI auf die OSI übertragen. Gut zwei Dutzend Lizenzen hat die OSI gutgeheißen und zertifiziert, sodass sie offiziell den geschützten Titel »Open Source ™« tragen dürfen.<@3 hoch fliess>59<@$p>Die OSD ist, wie gesagt, keine Lizenz, sondern ein Standard, an dem Lizenzen sich messen lassen.<@3 hoch fliess>60<@$p> Neben den eingangs genannten Freiheiten und den beiden problematischen Punkten, die im Anschluss behandelt werden, enthält die OSD einige Besonderheiten. Während die meis<\h>ten Lizenzen die Nutzungen ihrer Software ohne Einschränkung an jedermann freistellen, gibt es einige, die explizite Ausnahmen vorsehen. In der Erläuterung zur OSD ver. 1.0 führt Perens das Beispiel einer Lizenz des Aufsichtsgremiums der Universität von Kalifornien in Berkeley an, die die Verwendung eines Elektronikdesign-Programms durch die südafrikanische Polizei untersagt (vgl. <@6 Caps>Perens<@$p>, 1999, S. 179). Obgleich das Anliegen zu Zeiten der Apartheid löblich gewesen sei, sei ihr Sinn heute weggefallen, die Lizenzvorschrift für diese und alle abgeleitete Software bestehe jedoch weiter. Ebenso sei es verständlich, dass Autoren den Einsatz ihrer Software in der Wirtschaft, der Genforschung oder einer Abtreibungsklinik untersagen wollten, doch auch diese Anliegen gehörten nicht in eine Lizenz. Deshalb schreibt die OSD für Open Source-Lizenzen vor, dass sie nicht gegen Personen oder Gruppen (Ziff. 5) und gegen Einsatzgebiete (Ziff. 6) diskriminieren dürfen. Bei der Weitergabe an Dritte soll die Lizenz wirksam sein, ohne dass Eigentümer (der Copyright-Halter) und Lizenznehmer einen Vertrag unterzeichnen (Ziff. 7). Die Gültigkeit von unterschriftslosen Lizenzverträgen wird derzeit auch für den proprietären Bereich diskutiert (s.u.), insofern ist die Erläuterung zur Ziff. 7 der OSD ver 1.0 (ebd., S. 179) eher ein frommer Wunsch. In der Erläuterung zur ver. 1.7<@3 hoch fliess>61<@1 fliess normal> <@$p>heißt es, dass damit eine Schließung durch zusätzliche Anforderungen wie ein NDA ausgeschlossen werden soll. Die OSD Ziff. 8 besagt, dass die gewährten Rechte nicht davon abhängig gemacht werden dürfen, dass das Programm Teil einer bestimmten Distribution ist. Es muss frei bleiben, auch wenn es von dieser Distribution getrennt wird. <t-1>Zu den von der OSI zertifizierten Lizenzen<x@3 hoch fliess><t-1>62<@$p><t-1> gehören die GPL und LGPL, die BSD-Lizenz,<x@3 hoch fliess><t-1>63<x@1 fliess normal><t-1> <@$p><t-1>die MIT- oder X-Konsortium-Lizenz, die Artistic <\h>License (für Perl entwickelt),<x@3 hoch fliess><t-1>64<@$p><t-1> die Mozilla Public License (<t-3>MPL),<x@3 hoch fliess><t-3>65<@$p><t-3>die Qt Public License (QPL),<x@3 hoch fliess><t-3>66<@$p><t-3>die IBM Public License,<x@3 hoch fliess><t-3>67<@$p><t-3> die MITRE<t-1> Collaborative Virtual Workspace License (CVW License),[<x@3 hoch fliess><t-1>68<@$p><t-1> die Ricoh Sou<\h>rce Code Pub<\h>lic License,<x@3 hoch fliess><t-1>69<@$p><t-1> die Python-Lizenz<x@3 hoch fliess><t-1>70<@$p><t-1> und die zlib/libpng-<\h>Lizenz.<t$>Bevor auf die beiden kritischen Punkte eingegangen wird, sei noch die Möglichkeit der Mehrfachlizenzierung erwähnt. Wie beim bereits genannten Ghostscript gibt es eine Reihe Projekte, die dieselbe Software gleichzeitig unter einer freien und einer kommerziellen Lizenz anbieten. Eine weiteres Beispiel ist »Sendmail«,<@3 hoch fliess>71 <@$p>das Eric Allman an der Berkeley Universität entwickelte. 1997 gründete er eine Firma, die parallel zur freien Version eine kommerzielle Lizenz mit Supportleistungen anbietet.<@3 hoch fliess>72<@$p>Die MPL ist die einzige Lizenz, die diese Möglichkeit ausdrücklich erwähnt. Ziff. 13 erlaubt es dem ursprünglichen Entwickler, nämlich Net<\h>scape, nicht aber den Kontributoren, ihren Code unter die MPL und zugleich eine alternative Lizenz zu stellen, unter denen Nutzer ihre Wahl treffen können. Darin ist die Handschrift von Perens zu erkennen, der denjenigen, die ihre Software frei belassen und sie zugleich verkaufen möchten, eine beliebige kommerzielle Lizenz plus der GPL als freie Lizenz empfiehlt (vgl. <@6 Caps>Perens<@$p>, 1999, S. 185). Eine eigenartige Konstruktion ist die CVW-Lizenz des MITRE. Sie ist nur eine Art Rahmenlizenz, in der die Warenzeichen von MITRE von der Werbung für abgeleitete Werke ausgeschlossen werden. Darüber hinaus stellt sie dem Empfänger der Software frei, ob er sie unter der GPL oder der MPL nutzen möchte, die beide in der CVW-Lizenz enthalten sind. @2  ZÜ 3:Verhältnis von freiem und proprietärem Code@1 fliess ohne:Darf freier Code in unfreien integriert werden? Die GPL untersagt dies, bietet aber mit der LGPL eine Ausnahmemöglichkeit für Bibliotheken und andere Programme. Die BSD- und davon abgeleitete Lizenzen, also diejenigen, die Deutsch »unbeschränkte Lizenzen« nennt, erlauben, alles mit ihrer Software zu machen – auch, sie in unfreie Software zu integrieren und Änderungen zu privatisieren –, solange der Hinweis auf den Copyrighthalter erhalten bleibt und nicht mit dessen Namen geworben wird. Beide Lizenzfamilien, die BSD und die GPL, stammen aus Universitäten und spiegeln die Auffassung wieder, dass öffentlich finanzierte Werke ohne Einschränkung allen gehören. Der Grund, dass viele Projekte BSD-artige Lizenzen verwenden, liegt darin, dass sie eine Zusammenarbeit mit Unternehmen erlauben, die ihr geistiges Eigentum nicht freizügig teilen möchten. Hohndel vom XFree86-Projekt sagt dazu:@1 fliesskursiv Zitat:»Die Geschichte von vielen Projekten, nicht nur von X, auch von Dingen wie Sendmail oder BIND, zeigt, dass diese eher abschreckenden Bestimmungen der GPL gar nicht nötig sind. Für uns ist der große Grund, warum wir mit einer GPL überhaupt nichts anfangen können, wiede<\h>rum die Zusammenarbeit mit den Firmen. Denn viele kommerzielle Unix-Systeme enthalten [...] heute X-Server, die im Wesentlichen auf XFree86 basieren. Und viele von den Firmen, mit denen wir zusammenarbeiten und von denen wir Source Code bekommen – es gibt ja etliche Grafikkartenhersteller, die heute selber die Treiber für ihre Karten schreiben –, würden niemals zulassen, dass eine so virenartige Lizenz wie die GPL, die sich in den Code reinfrisst und nie wieder daraus weggeht, in ihren Code reinkommt. Von daher folgen wir dieser sehr, sehr freien Lizenz, die uns bis jetzt sehr gut gedient hat, und werden das auch weiterhin tun.«<@3 hoch fliess>73<@$p>@1 fliess ohne:Die »sehr, sehr freie« BSD-Lizenz erlaubt es z.B., dass Microsoft so frei war, große Mengen FreeBSD-Code in Windows zu verwenden. Das Monopol ist dadurch in keiner Weise geschmälert worden. Kein Nutzer hat dadurch mehr Freiheit gewonnen. Würde FreeBSD unter der GPL stehen, hätte Microsoft es nicht verwenden dürfen oder Windows wäre heute freie Software.@1 fliess mit:Die OSD besagt, dass eine Lizenz andere Software nicht »kontaminieren« darf (Ziff. 9), d.h. sie darf nicht vorschreiben (wie im Falle einer Version von Ghostscript), dass alle andere Software, die auf demselben Medium verbreitet wird, ebenfalls freie Software sein muss. In den Erläuterungen zur OSD ver 1.7 heißt es, dass die GPL diese Auflage erfülle, da sich ihr »Kontaminierungseffekt« nur auf Software bezieht, die mit der GPL-Software zu einem funktionalen Ganzen gelinkt wird, nicht aber auf solche, die nur mit ihre zusammen verbreitet wird.<@3 hoch fliess>74<@$p>Die <@1 fliess kursiv>Perl Artistic License <@$p>erlaubt es ausdrücklich (Ziff. 5), den Perl-Interpreter mit proprietärem Code zu linken und betrachtet das Ergebnis nicht als abgeleitetes Werk, sondern als Aggregation. Auch die Aggregation in einer proprietären Distribution ist erlaubt, solange das Perl-Paket »eingebettet« wird, dies sei »der Fall, wenn kein offensichtlicher Versuch unternommen wurde, die Schnittstelle dieses Paketes für den Endnutzer der kommerziellen Distribution offenzulegen« (Ziff. 8). Im Umkehr<\h>schluss kann man annehmen, dass andernfalls die Integration in proprietäre Software verboten ist. @2  ZÜ 3:Status abgeleiteter Werke@1 fliess ohne:Dürfen Veränderungen privatisiert werden? Die GPL schließt dies entschieden aus. Der Oberbegriff »Free Software«, der z.B. <@1 fliess kursiv>Public Domain<@$p>-Software beinhaltet, bedeutet, dass sie verwendet, kopiert, mit Quellcode weiterverbreitet und verändert werden darf, schließt aber nicht aus, dass Kopien und Modifikationen ohne diese Freiheiten, etwa ausschließlich als <@1 fliess kursiv>Executable<@$p>, verbreitet werden. Das engere, wenn auch nicht auf die GPL beschränkte »Copyleft« bedeutet darüber hinaus, dass modifizierte Versionen freier Software gleichfalls unter einer Copyleft-Lizenz stehen müssen und keine weiteren Nutzungseinschränkungen hinzugefügt werden dürfen – einmal frei, immer frei. @1 fliess mit:<t-2>In der OSD ist diese Lizenzvererbung nur eine Kannvorschrift. Ziffer 3 <t$>besagt, dass eine Lizenz zulassen muss, dass abgeleitete Werke unter dieselbe Lizenz gestellt werden; in seiner Erläuterung schreibt Perens, dass sie es aber nicht vorschreiben muss. Ziffer 4 OSD enthält zudem ein Schlupfloch für die Veränderbarkeit. Unter der Überschrift »Integrität des Quellcodes des Autors« heißt es dort, dass die Verbreitung modifizierter Versionen des Quellcodes eingeschränkt werden kann, aber nur, wenn es zulässig ist, <@1 fliess kursiv>Patch<@$p>-Dateien zusammen mit dem unveränderten Quellcode zu verbreiten, die erst bei der Kompilierung dieses »Patch<\h>works« die modifizierte Version erzeugen. Diese Integration von <@1 fliess kursiv>Patches<@$p> automatisieren Werkzeuge, weshalb der zusätzliche Aufwand vertretbar sei. Damit soll eine Trennung zulässig werden, die die Integrität des »Originals« wahrt und trotzdem Modifikation möglich macht. Die Verbreitung von Objektcode-Versionen darf nicht eingeschränkt werden, die Lizenz kann aber vorschreiben, dass abgleitete Werke einen anderen Namen tragen müssen. Als Grund für diesen Passus nennt Perens, dass er die <@1 fliess kursiv>Qt Public License <@$p>(QPL) von Troll Tech in die Open Source-Definition aufnehmen wollte. Diese erlaubt Modifikationen ausschließlich in der Form von Patches. Die QPL ist zwar eine freie Lizenz, aber inkompatibel zur GPL. Daher darf <@1 fliess kursiv>Qt<@$p> nicht mit GPL’ter Software gelinkt werden, doch  hierfür erteilt die FSF eine Sondergenehmigung.<@3 hoch fliess>75<@$p>Die unbeschränkten Lizenzen (wie BSD, Apache oder X) erlauben, dass Veränderungen privatisiert werden. So ist es möglich, den Quell<\h>code einer Software unter X-Lizenz zu verändern und Binaries davon verkaufen, ohne deren Quelltext offen zu legen und die modifizierte Version wieder unter die X-Lizenz zu stellen. Tatsächlich gibt es eine Reihe Workstations und PC-Grafikkarten, für die ausschließlich unfreie Versionen von X Window verfügbar sind. Die <@1 fliess kursiv>Perl Artistic License<@$p> (AL) verlangt, dass Änderungen kenntlich gemacht werden und bietet dann vier Optionen für das weitere Vorgehen: a) Die Änderungen müssen in die <@1 fliess kursiv>Public Domain<@$p> gestellt oder auf andere Weise frei verfügbar gemacht werden; b) die Änderungen werden ausschließlich innerhalb eines Unternehmens oder einer Organisation genutzt; c) die Nicht-Standard-<@1 fliess kursiv>Executables <@$p>erhalten einen neuen Namen und dürfen ausschließlich zusammen mit der Standardversionen verbreitet werden; d) andere Verbreitungsvereinbarungen mit dem Copyrighthalter werden getroffen (Ziff. 3). Sie bietet somit einigen Spielraum für Privatisierungen.<@3 hoch fliess>76<@$p>Die CVW-Lizenz des MITRE ist auch hier ungewöhnlich. Ziffer 5 besagt, dass derjenige, der geänderten Quellcode an MITRE übermittelt, einwilligt, sein Copyright daran an MITRE zu übertragen, das die Änderungen auf seiner Website zugänglich macht.<@3 hoch fliess>77<@$p>@2  ZÜ 3:Beispiele für Lizenzkonflikte@1 fliess ohne:Lizenzfragen betreffen vor allem Entwickler und Vertreiber von Software. Anwender, die die Software nicht selbst modifizieren möchten, haben mit jeder der genannten Lizenzen die Gewähr, sie (bei den meisten auch für kommerzielle Zwecke) einsetzen und an Freunde weitergeben zu dürfen. Ein Distributor hat vor allem auf die verschiedenen Lizenzbedingungen für die Aggregierung und den Vertrieb zu achten. Auch eine Entwicklerin muss sehr genau entscheiden, welche Lizenz sie für ihre Programme benutzt und welche Programme sie in ihren eigenen Werken verwendet, da sie mit dem Code auch dessen Lizenz importiert. Die einzelnen Lizenzen in einem zusammengesetzten Werk treten in Wechselwirkung. Kombinationen aus proprietärem und freiem Code sind, wenn die beteiligten freien Lizenzen es zulassen, als Ganze proprietär. Untersagt eine freie Lizenz wie die GPL die Verbindung mit unfreiem Code, muss dieser entweder freigegeben oder die Verbindung darf nicht durchgeführt werden. In der Praxis sind die Wechselwirkungen jedoch erheblich komplexer und werden durch den Zuwachs an neuen Lizenzmodellen immer undurchschaubarer.<@3 hoch fliess>78<@$p> @1 fliess mit:Ein postives Beispiel dafür, dass es der Welt der freien Software immer wieder gelingt, die Rechteinhaber proprietärer Angebote zu überzeugen, ihre Software unter eine freie Lizenz zu stellen, ist die bereits mehrfach angesprochene Qt-Bibliothek der Firma Troll Tech, auf die sich der freie Desktop KDE stützt. Troll Tech hat auf Drängen der FSF Qt ab Version 2.0 unter eine eigene Lizenz, die <@1 fliess kursiv>Qt Public License <@$p>(QPL)<@3 hoch fliess>79<@$p> gestellt, die seine proprietäre Verwendung ausdrücklich ausschließt. Die QPL lässt Modifikationen nur separat vom Original in Form von Patches zu und gewährt Troll Tech ihre Verwendung in anderen Produkten (Ziff.<\!q>3). Programme, die mit der Qt-Bibliothek linken, müssen selbst quelloffen sein (Ziff. 6). Die QPL ist eine <@1 fliess kursiv>Not-for-profit<@$p>-Lizenz. Wer Qt kommerziell einsetzen möchte, muss die <@1 fliess kursiv>Professional Edition<@$p> unter einer herkömmlichen kommerziellen Lizenz erwerben. Troll Tech hat die Copyright-Rechte an der »Qt Free Edition« und das Recht, die QPL zu ändern, an die im April 1998 errichtete <@1 fliess kursiv>KDE<@$p> <@1 fliess kursiv>Free Qt Foundation<@3 hoch fliess>80<@$p> abgetreten – ein außergewöhnlicher Schritt eines Unternehmens auf die freie Community zu.@1 fliesskursiv Zitat:»Sollte Troll Tech jemals aus irgendeinem Grund einschließlich, aber nicht beschränkt auf einen Aufkauf von Troll Tech, eine Fusion oder einen Konkurs, die Qt Free Edition aufgeben, so wird die letzte Version der Qt Free Edition unter der BSD-Lizenz freigegeben. Ferner, sollte Troll Tech nach Einschätzung einer Mehrheit in der KDE Free Qt Stiftung die Weiterentwicklung von Qt eingestellt haben und nicht mindes<\h>tens alle zwölf Monate eine neue Version herausgeben, so hat die Stiftung das Recht, die Qt Free Edition unter der BSD-Lizenz freizugeben.«<@3 hoch fliess>81<@1 fliess normal>@1 fliess mit:@1 fliess ohne:Die GPL ist vergleichsweise eindeutig. Stallman gibt zwei Beispiele für die postiven Auswirkungen ihres Schließungsverbots:@1 fliesskursiv Zitat:»Nehmen wir GNU C++. Warum haben wir einen freien C++-Compiler? Nur, weil die GPL sagte, dass er frei sein muss. GNU C++ ist von einem Industriekonsortium namens MCC <@1 fliess normal>[The Microelectronics and Computer Technology Corporation, mit Mitgliedsfirmen wie 3M, Eastman Kodak, Hewlett-Packard, Lockheed und Motorola] <@$p>entwickelt worden, das vom GNU C-Compiler ausgegangen ist. MCC machen ihre Werke normalerweise so proprietär wie es nur geht, aber das C++-Frontend haben sie zu freier Software gemacht, weil die GNU GPL vorschrieb, dass das der einzige Weg war, auf dem sie es hätten verbreiten können. Das C++-Frontend enthält viele neue Dateien, aber da sie dazu gedacht sind, mit dem GCC gelinkt zu werden, fand die GPL sehr wohl Anwendung auf sie. Der Vorteil für unsere Community ist offenkundig. Oder nehmen wir GNU Objective C. NeXT wollte dieses Frontend ursprünglich proprietär machen. Sie schlugen vor, es als ›.o-Dateien‹ zu veröffentlichen und die Anwender diese mit dem Rest des GCC linken zu lassen. Sie dachten, damit könnten sie um die Auflagen der GPL herumkommen. Aber unser Anwalt sagte, dass sie sich so den Auflagen nicht entziehen könnten, dass das nicht erlaubt ist. Und so machten sie das Objective C-Frontend zu freier Software. Diese Beispiele haben sich vor Jahren zugetragen, doch die GNU GPL fährt fort, uns freie Software zu bringen« <@1 fliess normal>(<@6 Caps>Stallman<@1 fliess normal>, 1998)<@$p>. @1 fliess mit:@1 fliess ohne:<*h"mehr">NeXT dachte, es könne die »Objective C«-Erweiterungen zu einem separaten Modul erklären und ohne Quellcode ausliefern. Nach einer Klage der FSF, der Copyright-Inhaberin des GCC, lenkte NeXT ein – seither erfreut sich die freie Softwarewelt der spendierten Software. Auch die Firma Be hat GPL-Quellen unrechtmäßig verwendet, was bald entdeckt wurde und eben<t-2>falls zum Einlenken führte. Gleiches geschah im Falle einer Firma, die die bereits erwähnte <@1 fliess normal>Readline<@$p><t-2>, eine Bibliothek, die das Editieren der Kommandozeile ermöglicht und unter der GPL steht, in einem unfreien Programm verwendete. Auch hier stellte der Entwickler sein ei<\h>genes Programm auf Nachfragen ebenfalls unter die GPL. Meist sind jedoch Lizenzverstöße nicht so einfach festzustellen – naturgemäß, wenn die betreffende Firma den Quellcode ihrer abgeleiteten Software nicht freigibt.<t$> @1 fliess mit:Doch nicht nur Firmen, sondern auch einige freie Projekte, die mit Firmen zusammenarbeiten, haben ihre Probleme mit der GPL. Patrick M. Hausen von BSD sagt dazu:@1 fliesskursiv Zitat:»Wenn Sie ein Stück Software, das unter der GPL ist, in Ihre eigene Software einbauen, dann wird die gesamte Arbeit, die Sie gemacht haben, automatisch eine aus der GPL-Software abgeleitete Arbeit, und damit gilt die GPL für alles, was Sie getan haben. ... Und so ist es eben schwierig, GPL’te Open Source-Software in kommerziellen Produkten mit anderen Teilen, sei es mit Treibern, die unter einem Nondisclosure Agreement stehen, sei es mit Libraries, mit einer Oracel-Datenbank-Library oder mit Motif oder was immer zu kombinieren, um ein Gesamtes zu entwickeln. Mir bleibt letzten Endes nichts anderes übrig, als den GPL'ten Teil komplett neu zu schreiben.«<@3 hoch fliess>82<@$p>@1 fliess mit:@1 fliess ohne:Auch Kalle Dalheimer von KDE sieht durch die Einschränkung der Verbreitung von Binaries in der GPL die Endnutzerfreundlichkeit von Software behindert: @1 fliesskursiv Zitat:»Ich kritisiere nicht die GPL, weil sie mit der (derzeitigen) Qt-Lizenz inkompatibel ist, sondern weil sie so gestaltet ist, dass sie die Weiterentwicklung von Linux <@1 fliess normal>behindert.<@$p> In den Anfangstagen war sie sicherlich gut und nützlich, aber jetzt hat sich die GPL überlebt – zumindest für alles, was im weitesten Sinne eine ›Komponente‹ ist.«<@3 hoch fliess>83<@$p>@1 fliess mit:@1 fliess ohne:Die »einschüchternde« Wirkung der GPL taucht auch in der Begründung für Netscapes »permissivere« Mozilla-Lizenz (s.u.) auf:@1 fliesskursiv Zitat:»Netscape ist daran interessiert, die Verwendung und Weiterentwicklung des Communicator-Quellcodes durch andere kommerzielle Entwickler anzuregen. Netscape war besorgt, dass diese anderen Firmen zögern würden, sich an der Entwicklung zu beteiligen, wenn der Code von einer so strikten Lizenz wie der GPL kontrolliert würde, die vorschreibt, dass alle verwandte Software ebenfalls als freie Quellen veröffentlicht werden muss. Im besten Falle müsste jeder zweite kommerzielle Entwickler sehr genau die rechtlichen Folgen untersuchen, die ein Free Source-Entwicklungsunterfangen mit sich bringen kann. Diese Einstiegshürde allein könnte ausreichen, sie davon abzuhalten, überhaupt damit zu beginnen. Daher wurde entschieden, die Hürde zu beseitigen, indem eine etwas weniger restriktive Lizenz verwendet wird.«84@1 fliess ohne:Mike Loukides schlägt einen schärferen Ton an. Die »Chamäleon«-<\h>Lizenz, von der er spricht, ist die <@1 fliess kursiv>Sun Community Source License<@$p> (s.u.), die die lizenzierte Software frei mache, wenn sie im Kontext freier Software erscheint, und kommerziell, wenn sie mit proprietärer Software kombiniert wird.@1 fliesskursiv Zitat:<t-1>»Bei allem Respekt für RMS <x@1 fliess normal><t-1>[Richard M. Stallman]<@$p><t-1> denke ich, dass es auf der Hand liegt, dass die GPL die Akzeptanz von Open Source-Software deutlich verzögert hat. Die GPL war ganz klar ein wichtiges State<\h>ment, aber die Ideologie war dem, was die meisten Leute praktizieren wollten, weit voraus. Wir alle kennen Leute, die keinen Nutzen aus GNU-Software ziehen konnten, weil sie nicht genau verstanden, was die Lizenz bedeutet, was sie aufgeben würden, wofür sie mutmaßlich in ein paar Jahren verklagt werden könnten und so weiter. Wir alle wissen von Firmen, die keine GNU-Software auf ihren Maschinen zulassen, weil sie das Risiko nicht eingehen wollen, dass jemand sich ein paar Zeilen Quellcode ausleihen und damit das geistige Eigentum der Firma kompromittieren könnte. Ob man mit dieser Einschätzung übereinstimmt, ist nicht die Frage. Tatsache ist, dass eine Lizenz nicht sehr geholfen hat, die Leuten Angst macht, die Software zu benutzen. [...] Als Fazit bleibt, dass die GPL im Wesentlichen einen Zwang ausübt und so auch beabsichtigt ist. Lässt man die Moral beiseite, hat das der Sache einfach geschadet. Der richtige Weg, freie Software zu popularisieren, war es nicht, Leuten zu drohen, die sich entschieden haben, sie zu benutzen. [...] Hier bietet die Chamäleon-Lizenz ein wichtiges neues Modell an. Sie ist viel entwicklerfreundlicher und kann Entwickler zum Open Source-Modell verführen. Sie ist Zuckerbrot, nicht Peitsche. [...] Obgleich sie auch den kommerziellen Weg einschlagen können, hat ihnen Sun einen impliziten Anreiz gegeben, den Open Source-Weg zu gehen – oder ihn zumindest in Erwägung zu ziehen. [...] Es ist eine Gelegenheit, die pragmatischen Entwicklern die Tore zur Community öffnet: Leuten, die gerne herausbekommen möchten, wie sie mit ihrer Arbeit Geld verdienen können, und die von Lizenz-Fanatismus abgestoßen sind.«<x@3 hoch fliess><t-1>85<@$p><t-1>@1 fliess mit:@1 fliess ohne:Es liegt auf der Hand, dass Stallman seiner Zeit voraus war. Die wachsende Popularität der GPL belegt das. Sie zeigt, wie »realitätstauglich« die GPL tatsächlich ist. Ideologie ist vielmehr, dass es ein bisschen Freiheit geben könnte – sozusagen Freiheit in der unverbindlichen Probepackung. »Wir alle wissen«, dass mit einem »impliziten Anreiz«, »Freiheit in Erwägung zu ziehen«, nichts gewonnen ist. Wie wir am Beispiel des FreeBSD-Codes in Microsoft-Windows gesehen hatten, führt diese Art von unabgesicherter Freiheit dazu, dass ein Unternehmen sich an den Produkten der Gemeinschaft bereichern kann, ohne etwas an diese Gemeinschaft zurückzugeben. Microsoft hat das Zuckerbrot gern gegessen, ohne sich im Mindesten auf den Weg der freien Software gelockt zu fühlen. Stallman antwortet auf diese »Angst vor Freiheit«:@1 fliess mit:@1 fliesskursiv Zitat:»Das Hauptargument für den Begriff ›Open Source-Software‹ ist, dass ›Freie Software‹ einige Leute nervös macht. Es stimmt, über Freiheit zu sprechen, über ethische Fragen, über Verantwortlichkeiten und nicht nur über Bequemlichkeit heißt, Leute aufzufordern, über Dinge nachzudenken, die sie lieber ignorieren würden. Das kann Unbehagen hervorrufen und einige Leute dazu bringen, die Idee deshalb zu verwerfen. Daraus folgt aber nicht, dass die Gesellschaft besser dran wäre, wenn wir aufhören würden, über diese Dinge zu sprechen. [...] Vor Jahren bemerkten freie Softwareentwickler diese unbehagliche Reaktion und einige begannen einen Weg auszuprobieren, der sie vermeiden sollte. Sie dachten, indem sie über Ethik und Freiheit schwiegen und nur über die unmittelbaren praktischen Vorteile gewisser freier Software sprachen, könnten sie die Software an bestimmte Nutzer effektiver ›verkaufen‹, besonders in der Wirtschaft. Der Begriff ›Open Source‹ wird als Fortsetzung dieser Vermeidungsstrategie angeboten, einer Strategie, ›für die Wirtschaft akzeptabler‹ zu sein. Sie hat sich nach ihren eigenen Maßstäben als effektiv erwiesen. Heute wechseln viele aus rein praktischen Gründen zu freier Software. Das ist gut für den Anfang, aber das ist nicht alles, was wir machen müssen! [...] Früher oder später werden diese Nutzer eingeladen, um irgendeines praktischen Vorteils willen wieder zu proprietärer Software zurückzuwechseln. Zahlreiche Firmen bemühen sich, solche Verführungen anzubieten, und warum sollten Nutzer sie ablehnen? Nur, wenn sie die Freiheit, die freie Software ihnen bietet, um ihrer selbst willen schätzen gelernt haben. Es ist an uns, diese Idee zu verbreiten – und um das zu tun, müssen wir über Freiheit sprechen. Zu einem gewissen Maß kann eine ›Stillschweigestrategie‹ gegenüber der Wirtschaft für die Community nützlich sein, aber wir brauchen auch reichlich Auseinandersetzung über Freiheit« <\h><@1 fliess normal>(<@6 Caps>Stallman<@1 fliess normal>, 1999b)<@$p>.@1 fliess mit:@2  ZÜ 2:Open Source-Lizenzen aus Unternehmen@1 fliess mit:@1 fliess ohne:Auf aktuelle Entwicklungen bei den rein proprietären Lizenzen ist im ersten Teil des Buches bereits eingegangen worden. Hier soll es um die unternehmensgestützten Open Source-Lizenzen gehen, die seit Net<\h>scapes Schritt in die Bewegung hinein entstanden sind. @1 fliess mit:Die <@1 fliess kursiv>Mozilla Public License<@$p> hat von der OSI das Gütesiegel der Open Source-Kompatibilät erhalten. Auf den zweiten Blick ist sie jedoch, ähnlich wie die <@1 fliess kursiv>Not-for-profit Freely-redistributable<@$p> Lizenzen, wenn auch aus anderen Gründen, als »halbfrei« einzuordnen. Die Open Source-Lizenz entstand, als Netscape sich Anfang 1998 entschloss – beraten von Raymond und Perens –, den Quellcode der »Communicator 5.0 Standard Edition« (bereinigt um allen Code, der geistiges Eigentum Dritter ist, sowie die Kryptografiemodule, die US-amerikanischen Ausfuhrbeschränkungen unterliegen) freizugeben. Die Codebasis wurde unter dem Namen »Mozilla« in ein freies Softwareprojekt überführt. Der Lizenztext<@3 hoch fliess>86<@$p> trägt deutlicher als alle bislang behandelten Lizenzen die Handschrift <\n>von Copyright-Anwälten. Erstmals spricht eine freie Lizenz auch mög<\h>liche Patentansprüche an.<@3 hoch fliess>87 <@$p>Als nach ausgiebigen Diskussionen und <\n>öffentlicher Kommentierung ein Entwurf der Lizenz veröffentlicht wurde, richtete sich die Kritik vor allem gegen die Sonderrechte, die sich Netscape darin vorbehielt. Daraufhin entschloss sich das Unternehmen, zwei Lizenzen herauszugeben, die <@1 fliess kursiv>Netscape Public License 1.0 <@$p>(NPL) für den im März 1998 freigegebenen Communicator-Code und alle davon abgeleiteten Werke sowie die <@1 fliess kursiv>Mozilla Public License 1.0<@$p> (MPL), die Autoren für <\n>eigenständige Werke benutzen können, für die sie Netscape keinen privilegierten Zugang geben möchten. MPL und NPL bestehen aus einem identischen Hauptteil, dem bei der NPL die »Zusätze« beigefügt sind, die den Sonderstatus von Netscape regeln.<@3 hoch fliess>88<@$p> Die MPL gewährt gebührenfrei alle Freiheiten und verlangt, dass alle Modifikationen in Quellcodeform zugänglich gemacht und unter dieselbe Lizenz gestellt werden (Ziff. 3.2.). Ziffer 3.6. erlaubt allerdings, die veränderte oder unveränderte Software ausschließlich in Objektcodeversionen unter einer beliebigen anderen Lizenz zu verbreiten, sofern ein Hinweis auf den freien Quellcode beigefügt wird. Auch die Verbindung von MPL-Code mit Code unter anderer Lizenz zu einem <@1 fliess kursiv>Larger Work <@$p>ist zugestanden (Ziff. 3.7).<@3 hoch fliess>89<@$p> Ein sol<@1 fliess normal>ches »größeres Werk« wir<@$p>d nicht als abgeleitetes Werk interpretiert, kann also ebenfalls unter eine restriktivere Lizenz gestellt werden, solange der ursprüngliche und der davon abgeleitete Quellcode weiterhin von NPL oder MPL regiert werden. Damit wird eine Aufspaltung in die freie Code-Welt der Entwickler und in die der reinen Anwender erzeugt, denen alle möglichen Restriktionen auferlegt werden können. Zwar werden aus der freien Quellcodebasis immer auch freie <@1 fliess kursiv>Binaries<@$p> verfügbar sein, aber es ist leicht denkbar, dass ein Unternehmen wie Microsoft den freien Code um eigene attraktive Funktionalitäten erweitert und das Gesamtpaket ausschließlich in proprietärer Form verbreitet. Fühlen sich genug Nutzer von diesen Zusatzfunktionen angezogen und sind bereit, die Unfreiheit in Kauf zu nehmen, verlieren die freien Entwickler ihre Anwender und das freie Projekt wird scheitern.Die NPL/MPL nimmt die ungewöhnliche Trennung zwischen dem »ursprünglichen Entwickler« (für den NPL-Code ist das Netscape, für ein nicht abgeleitetes eigenständiges Werk jeder Autor, der es unter die MPL stellt – Ziff. 2.1) und den »Kontributoren« (Ziff. 2.2) vor. In den »Zusätzen« der NPL wird eine weitere Unterscheidung zwischen den Versionen des Communicator unter dem Markennamen »Netscape« <@1 fliess normal>(<@$p>»<@1 fliess kursiv>branded version<@$p>«) und den freien Versionen unter dem Projektnamen »Mozilla« vorgenommen.<@3 hoch fliess>90<@$p> Kontrovers sind die Abschnitte, die es Netscape erlauben, den NPL-Code einschließlich der Modifikationen durch Dritte in seinem <@1 fliess kursiv>branded code <@$p>(Ziff. V.3.) und im Laufe von zwei Jahren nach der ursprünglichen Freigabe von Mozilla auch in anderen Produkten (Ziff. V.2.) zu verwenden, ohne dass es an seine eigene Lizenz gebunden ist. Net<\h>scape behält sich außerdem vor, Code unter der NPL unter anderen Bedingungen als der NPL an Dritte zu lizenzieren. Die Zusatzbestimmungen heben somit effektiv die Freiheitsvorschriften im Haupttext der NPL für die Firma Netscape wieder auf (Ziff. V.1.). Zur Begründung hieß es, Netscape verwende einen Teil des Codes für den Client »Communicator« auch in seinen Server-Produkten und wolle sicherstellen, dass es Veränderungen am Server-Code vornehmen – <\n>also auch Modifikationen von freien Entwicklern in den proprietären <\h>Code aufnehmen – kann, ohne diesen gleichfalls unter die NPL stellen zu müssen. Außerdem hat Netscape Verträge mit Dritten über die Bereitstellung von Quellcode. Auch sie sollen von den Modifikationen freier Entwickler profitieren können, ohne ihre eigenen Erweiterungen unter der NPL zugänglich machen zu müssen.<@3 hoch fliess>91<@$p> Die Gefahr, die davon ausgeht, spielt Netscape jedoch herunter. Es relizenziere den Code nur in dem Zustand an einem bestimmten Datum. Wenn ein solcher Lizenznehmer nicht mit der Community zusammenarbeite, indem er seine eigenen Entwicklungen in die freie Codebasis zurückgibt, werde der von ihm lizenzierte Code mit der Zeit immer stärker von der freien »Standardversion« abweichen.<@3 hoch fliess>92<@$p> Dies, so wird suggeriert, sei eine hinreichende Motivation für Hersteller, ihre Software freizugeben.Das Argument unterschlägt, dass schon der Hautpteil der NPL (= MPL) mit Ziffer 3.7<@1 fliess normal>,<@$p> »<@1 fliess kursiv>Larger Works<@$p>«, die Möglichkeit zulässt, Mozilla oder Teile davon mit proprietären Modulen zu koppeln. Nach Ziffer 3.6, »<@1 fliess kursiv>Distribution of Executable Versions<@$p>«<@1 fliess normal>,<@$p> würde dann ein Hinweis auf die Verfügbarkeit des Quellcodes der freien Bestandteile ausreichen. Dann kann sich jeder aus dem freien Pool bedienen, Änderungen vornehmen und das Ergebnis zu Bedingungen der eigenen Wahl verwerten. Freie Entwickler, die eine solche Privatisierung ausschließen wollen, können ihre eigenständigen Beiträge zum Mozilla-Code (nicht aber Modifikationen von NPL-Code)<@3 hoch fliess>93<@$p> unter die MPL (= NPL ohne Zusätze) oder eine kompatible Lizenz, wie die BSD, die LGPL oder fast jede andere Lizenz stellen oder sie gar nicht veröffentlichen. Ausgeschlossen ist allein die GPL, von der Netscape behauptet, dass sie inkompatibel mit allen Lizenzen außer sich selbst sei.<@3 hoch fliess>94<@$p>Auch die FSF ist der Auffassung, dass Module unter der GPL und Module unter der MPL nicht gelinkt werden dürfen.<@3 hoch fliess>95<@$p>Vergleichsweise harmlos, obgleich es ebenfalls viel Kritik auf sich gezogen hat, ist Netscapes Vorrecht, die Lizenz zu ändern. Denselben Mechanismus sieht auch die GPL vor. Keine Dynamik einzubauen wäre angesichts des raschen Wandels in Technologie und Recht auch unseriös. Doch selbst wenn Netscape oder gar die FSF plötzlich entscheiden sollten, MPL/NPL rsp. GPL in eine vollständig proprietarisierende Lizenz zu verwandeln, hätte das keine rückwirkenden Folgen für alle vorangegangenen freien Versionen. Eine radikale Lizenzänderung würde allerdings unweigerlich eine Spaltung in der weiteren Entwicklung der Codebasis auslösen.<@3 hoch fliess>96<@$p>Es ist hier nicht der Ort, um über die Hintergründe von Netscapes <\n>Businessentscheidung zu spekulieren. Die veröffentlichten Motive sind, »unsere beiden Ziele miteinander auszubalancieren: einerseits die Free-Source Entwickler-Community einzubeziehen und andererseits weiter unsere wirtschaftlichen Ziele zu verfolgen... Die NPL und die MozPL bemühen sich beide um einen Mittelweg zwischen der Förderung quellfreier Entwicklung durch kommerzielle Unternehmen und dem Schutz von Free-Source Entwicklern.«<@3 hoch fliess>97<@$p>Netscape wollte also nicht die Tore zu seiner gesamte Produktlinie öffnen,<@3 hoch fliess>98<@$p> sondern nur zu Mozilla, d.h. dem bereinigten Code der <@1 fliess normal>Communicator 5.0 Standard Edition.<@$p> Netscape schenkte der freien Software<\h>community einen Webbrowser und wollte – quasi als Gegenleistung – die »freie« (im Doppelsinn von »freiwilliger« und »unbezahlter«) Zuarbeit proprietär verwerten. »Netscape glaubt daran, dass es im Interesse aller ist, die auf der Codebasis des Communicator entwickeln, ihre Änderungen an die Entwicklergemeinschaft zurückzugeben. Auf diese Weise werden sie die Früchte eines verteilten Entwicklungsmodells ernten.«<@3 hoch fliess>99 <@$p>Viele in der Diskussion damals unterstellten Netscape, dass es solche Aussagen ehrlich meinte, doch spätestens als AOL Netscape (für vier Milliarden Dollar) kaufte, kamen lautstarke Zweifel auf. Entscheidend sind nicht solche Aussagen, sondern der lizenzrechtliche Status des gemeinsam bearbeiteten und genutzten Codes. Zwar wies Jamie Zawinski, im ersten Jahr <@1 fliess kursiv>Core-Team<@1 fliess normal>-<@$p>Mitglied bei mozilla.org, darauf hin, dass die Freiheiten, die Mozilla durch die NPL gewährt wurden, nicht nachträglich entfernt werden können (<@6 Caps>Zawinski<@$p>, 1998) – einmal aus der Copyright-<\h>Flasche, kann der Code-Geist nicht zurückgerufen werden. Doch die weitere Pflege und Entwicklung des Codes hängt mit der NPL/MPL zu einem gewissen Maß vom Wohlwollen ihres industriellen Hauptnutzers ab.Netscapes Lizenzmodell hat eine Reihe Nachfolger gefunden. In der Regel verwenden sie die MPL, also ohne den Sonderstatus, den die NPL dem ausgebenden Unternehmen verleiht.<@3 hoch fliess>100<@$p> Open Source hielt sogar schon Einzug in die Hochfinanz. Den Originaltext der MPL ver. 1.0, ein<\h>schließlich Netscapes Vorrecht, die Lizenz zu ändern, verwendet Price-Waterhouse für sein FpML™ (<@1 fliess kursiv>Financial products Markup Language<@$p>),<@3 hoch fliess>101<@$p> ein XML-basiertes Protokoll für <@1 fliess kursiv>Business-to-Business<@$p> E-Commerce im Bereich von Finanzderivativen.Auch Sun Microsystems hat neben anderen Linzenzmodellen MPL-<\n>Varianten in Verwendung. Bei der <@1 fliess kursiv>Sun Public License ver 1.0<@3 hoch fliess>102<@$p> für »NetBeans«, eine in Java geschriebene integrierte Entwicklerumgebung, handelt es sich um den Text der MLP 1.1, in dem zusätzlich zur Freiheit des Codes auch die der Dokumentation hinzugefügt und natürlich die Markennamen ausgetauscht wurden. Eine andere Strategie verwendet Sun für sein NFS (<@1 fliess kursiv>Network File System<@$p>). Die <@1 fliess kursiv>Sun Industry Standards <\h>Source License 1.0 <@$p>(SISSL)<@3 hoch fliess>103 <@$p>besteht zu großen Teilen aus der MPL 1.1. Die Ziffern der MPL, die sich auf die Modifikationen durch »Kontributoren« im Gegensatz zum »ursprünglichen Entwickler« beziehen, entfallen. Sun bindet mit der SISSL vor allem den Originalcode, also seinen eigenen. Die »Auslieferung« einer »Kontributorenversion« unterstellt sie einer besonderen Auflage. 120 Tage vorher muss sie die Anforderungen eines Standardisierungsgremiums erfüllen (Ziff. 3.0), was mit Hilfe von standardisierten Kompatibilitätswerkzeugen getestet wird. Nur für den Fall, <t-1>dass die geänderte Version vom Standard abweicht, verpflichtet die SISSL <t$>den Autor, die Dokumentation und eine Referenzimplementation seiner Software unter denselben Bedingungen wie die der SISSL öffentlich zu machen. Wer die Standards einhält, kann also Modifikationen auch proprietär halten. Suns erklärtes Ziel ist es, die NFS-Infrastruktur zu stärken, deren Spezifikationen es als öffentliche Standards an die IETF übertragen hat.<@3 hoch fliess>104<@$p><*h"mehr">Für Java und Jini (<@1 fliess kursiv>Java Intelligent Network Infrastructure<@$p>) führte Sun im Januar 1999 eine dritte Lizenz ein, die <@1 fliess kursiv>Sun Community Source License<@$p> (SCSL),<@3 hoch fliess>105<@$p> die eigentlich drei Lizenzen in einer ist. In kumulativer Abstufung enthält sie 1.) die <@1 fliess kursiv>Research Use License<@$p>, die für Evaluation, Forschung, Entwicklung und Prototyping potenzieller Produkte die größten Freiheiten gewährt, 2.) die <@1 fliess kursiv>Internal Deployment Use License <@$p>für eine sehr begrenzte interne Distribution, um Produkte vor ihrer Markteinführung zu testen, und schließlich 3.) die <@1 fliess kursiv>Commercial Use License<@$p>, die jeder unterzeichnen muss, der Original und Modifikationen verbreiten möchte. Nach (1) darf der Quellcode einschließlich Modifikationen nur an andere Lizenznehmer verbreitet werden. Nach (2) muss jeder Code, wie bei der SISSL, den Test mit dem <@1 fliess kursiv>Java Compatibility Kit (<@$p>JCK) bestehen. Wie beim NFS handelt es sich bei Java und Jini um infrastrukturelle Technologien, mit einem Kern, den Sun unter enger Kontrolle hält, und einer Vielzahl möglicher Dienste drumherum, die von möglichst vielen ausgebaut werden müssen, damit der Wert des Systems für alle Beteiligten wächst. Um Javas Cross-Plattform-Versprechen »einmal geschrieben, überall lauffähig« halten zu können, muss Sun dafür sorgen, dass in allen Implementationen ein für alle verbindlicher Standard eingehalten wird. Das Anliegen ist umso verständlicher, als Microsoft einen schwer gewichtigen Versuch unternommen hat, Java auf kompatibilitätsbrechende Weise zu verändern. Die beiden Unternehmen standen sich deshalb gerade vor Gericht gegenüber, als die SCSL verfasst wurde. Zur Wahrung der Einhaltung der Standards dienen zwei Mechanismen: Arbeitsausschüsse der beteiligten Organisationen und der JCK. Das <@1 fliess kursiv>Java Compatibility Kit<@$p> ist ein komplexes Werkzeug, dessen Benutzung an eine eigene Lizenz und vor allem – da er so komplex ist – an einen kostspieligen Supportvertrag mit Sun oder zertifizierten Dritten gebunden ist (<@6 Caps>Paolini<@$p>, 1998).<@3 hoch fliess> <@$p>Das sind nun keine Anforderungen mit denen die freie Softwarewelt üblicherweise zu tun hat. Mike Loukides gesteht Sun, bei aller Kritik an der SCSL, zu, dass es mit dieser Konstruktion auch auf die Angriffe von Microsoft antworte: »Das ist ein Problem, mit dem sich die Open Source-Community bislang noch nicht konfrontiert sah, und es ist nicht klar, wie ein vollständig offener Prozess reagieren würde. Was würde zum Beispiel geschehen, wenn Microsoft beschlösse, dass es in ihrem Interesse liegt, in Perl Inkompatibilitäten einzuführen und ihre Entwickler um diese privatisierte Version zu scharen. Ich glaube, dass das passieren wird – und zwar eher früher als später«(<@6 Caps>Loukides,<@$p> 12/1999).<*h"Standard">Sun vermeidet sorgfältig, die SCSL als eine »Open Source«-Lizenz zu bezeichnen. Weder sie noch die SISSL ist von der OSI als OSD-gemäß zertifiziert worden. Und auch die FSF identifiziert beide als inkompatibel mit der GPL. Wie Netscape möchte Sun das Beste aus der Welt der proprietären und der Open Source-Lizenzen vereinigen. Als Vorteile von Letzteren sehen Gabriel und Joy eine verstärkte Innovation, die Vergrößerung der effektiven Arbeiterschaft, verbesserte Qualität und eine schnellere Kommerzialisierung: »Eine teilnehmende Organisation kann die Früchte von Experten ernten, die nicht bei ihr angestellt sind« (<@6 Caps>Gabriel/Joy<@$p>, 1999). Auch der kommerzielle Vorteil ist deutlich. Sun wird die erste Adresse für Java und Jini bleiben: »Selbst wenn die Quellen offen liegen, bleibt der Mehrwert immer noch bei denen, die besonders qualifiziert mit ihnen umgehen können.« Und auch von den Lizenznehmern fließen Einnahmen zurück, nicht mehr, wie bislang, vor Beginn der Entwicklung, sondern in dem Moment, wo auch der Lizenznehmer mit seinen Produkten Geld verdient (vgl. <@6 Caps>Paolini<@$p>, 1998).Die Community, die die SCSL vorsieht und schafft, besteht aus einer »entwickelnden Organisation« und »hinzukommenden Organisationen«. Freie Entwickler tauchen nicht auf, sind allenfalls als Lehrende und Studierende hoch willkommen. Wo die freien Lizenzen ein Netzwerk von Individuen im Auge haben, zielt die SCSL auf ein Netzwerk von Firmen: »So stützt sich z.B. die Lizenzierung der Jini-Technologie auf eine <\h>Gemeinschaft von Firmen, die Jini-Geräte bauen und verkaufen möchten. Sun Microsystems ist die ›entwickelnde Organisation‹, die die ursprüngliche Jini-Infrastruktur erfunden, entwickelt und aufgebaut hat« <\h>(<@6 Caps>Gabriel<\!q>/<\!q>Joy<@$p>, 1999).<t-1>Auch andere Softwareunternehmen entfernen sich stärker von Net<\h>scapes Lizenzmodell. So hat IBM seinen Jikes-Compiler unter die <x@1 fliess kursiv><t-1>IBM Public License 1.0<x@3 hoch fliess><t-1>106<@$p><t-1> gestellt, die der MPL ähnelt und von der OSI als OSD-kompatibel zertifiziert worden ist. Kontroverser wurde die <x@1 fliess kursiv><t-1>Apple Public Source License <@$p><t-1>(APSL)<x@3 hoch fliess><t-1>107<@$p><t-1> aufgenommen, die u.a. »Darwin« (den Kern von Mac OS X) und den <x@1 fliess kursiv><t-1>Darwin Streaming Server <@$p><t-1>(einschließlich QuickTime-Streaming) für registrierte Entwickler zugänglich macht. Mit ihrem Sonderstatus für das ausgebende Unternehmen ähnelt sie der NPL. Kontributoren müssen Apple eine unwiderrufliche Lizenz erteilen, die Copyright- und Patentrechte an ihren Modifikationen zu nutzen (Ziff. 3), gleichzeitig behält sich Apple vor, die APSL im Falle von Patentstreitigkeiten pauschal zu widerrufen (Ziff. 9.1.c). Modifikationen der Kontributoren müssen im Quellcode freigegeben und mit einem Onlineformular bei Apple angemeldet werden (Ziff 2.2.c), gleichzeitig behält sich Apple alle Rechte am Originalcode und an seinen eigenen Modifikationen vor und entbindet sich selbst bei deren Nutzung von seiner eigenen Lizenz (Ziff 11). Im März 1999 verurteilten Perens (der inzwischen die OSI verlassen und auf die Seite der FSF gewechselt war) und andere in einem offenen Brie<k20>f<x@3 hoch fliess><t-1>108<@$p><t-1> die Entscheidung der OSI, der APSL das OSD-Gütesiegel zu verleihen. Unter anderem wiesen sie darauf hin, dass erhebliche Teile des von Apple unter der APSL – und eigenem Copyright – veröffentlichten Code unwesentliche Modifikationen von Code aus der Berkeley Universität und der Carnegie-Mellon Universität seien. Diese sind vom Steuerzahler finanziert, stehen unter freien Lizenzen, und sollten deshalb von der APSL verschont bleiben. Raymond verteidigte die Entscheidung zu<\h>nächst, doch schließlich wurde die Zertifizierung der APSL zurückgenommen.<x@3 hoch fliess><t-1>109<@$p><t-1><t$>Die Lizenzkonstruktionen von Netscape, Sun, IBM und Apple weichen deutlich vom herkömmlichen proprietären Modell ab. Hier sei an den Unterschied zwischen »kommerzieller« und »proprietärer« Nutzung erinnert. Kommerzielle Software wird von Firmen entwickelt, die damit Geld verdienen wollen. Sie ist in der Regel proprietär, es gibt jedoch auch kommerzielle Software unter der GPL (z.B. GNU Ada) und ebenso nicht kommerzielle proprietäre Software. Streng genommen haben auch GPL-Programme einen <@1 fliess kursiv>proprietarius<@$p>, einen Eigentümer, nämlich den oder die jeweiligen Copyright-Halter. Und auch die FSF hat keine Einwände dagegen, mit freier Software Geld zu verdienen – ganz im Gegenteil. Der Unterschied liegt in der Gewichtung. Priorität der FSF sind die Freiheiten. Priorität der genannten Unternehmen ist ein, wenn auch ungewöhnliches, so doch unzweifelhaft auf Profitmaximierung zielendes Business<\h>modell. Beide gleichen sich darin, dass sie eine Gemeinschaft von Entwicklern erzeugen, die Quellcode miteinander teilen und sich zu gegenseitiger Kooperation verpflichten. Bei den Kommerziellen wird man durch einen schlichten Mausklick auf den »Akzeptieren«-Knopf der Lizenz sowie gelegentlich durch eine Registrierung Mitglied in dieser In-Group. Unter den Kooperierenden gibt es hier eine Partei mit besonderen Vorrechten. Sun beispielsweise wird als Eigentümerin von Plattformtechnologie von deren Gedeihen in jedem Fall profitieren, selbst wenn es keine direkten Lizenzzahlungen bekommen sollte. Für solche geschlossenen Gemeinschaften ist der Begriff »<@1 fliess kursiv>Gated Communities<@$p>« geprägt worden. Tim O’Reilly erläutert ihn an einem Beispiel. Sein Unternehmen verwendet ein Softwarepaket für Verlage namens »CISpub«, ein proprietäres Produkt mit höchstens einigen Hundert Nutzern, die aber alle Zugriff auf den Quellcode haben und ihn aktiv weiterentwickeln. Der freie Austausch ist auf die Nutzerbasis beschränkt, um Nutzer zu werden, muss man CISpub käuflich erwerben. O’Reilly hält es auch für unwahrscheinlich, dass sich irgendjemand anderes dafür interessieren könnte: »Dies ist ein spezialisiertes Paket, in einer spezialisierten Sprache für einen spezialisierten Industriebereich« (<@6 Caps>O’Reilly<@$p>, 5/2000). Den wichtigsten Vorteil sieht O’Reilly darin, dass umzäunte Gemeinschaften einen Weg darstellen, auf dem Open Source-Ethik und <\h><\n>-Methodologie in die Welt des spezialisierten Business vordringen könne, zu denjenigen, die noch nicht bereit sind, vollständig den Open Source-Weg zu beschreiten. Im Übrigen schließe das Modell an die frühen Tage von Unix und IBM-Software an, in denen ebenfalls Lizenznehmer reichlich Code entwickelten und mit der Nutzerbasis teilten: @1 fliesskursiv Zitat:»Kurzum, wenn ›Gated Source Community‹ bedeutet, dass ich meine Änderungen nur mit anderen Lizenznehmern des Basispakets teilen kann, aber nicht mit Außenstehenden, bin ich immer noch dafür. [...] Damit das funktioniert, muss man Lizenzbedingungen verwenden, die es erlauben, die Modifikationen an andere Lizenznehmer zu verbreiten und öffentliche Repositorien für den beigesteuerten Code, Dikussionsforen, in denen die Anwender einander finden können, und andere Mechanismen zu unterhalten, die den Open Source-Entwicklern vertraut sind.<@3 hoch fliess>110<@$p> Es hilft auch, wenn man eine modulare Architektur hat, die es Leuten einfacher macht, Dinge hinzuzufügen, ohne mehr als nötig ändern zu müssen. [...] Der Begriff ›Gated Source Community‹ mag einige negative Konnotationen beinhalten, da er Exklusivität suggeriert, doch im Grunde bedeutet er nicht mehr, als dass Anwender einer bestimmten Software in die Lage versetzt werden, ihre eigene private Gemeinschaft für den Zugang und die gemeinsame Nutzung des Quell<\h>codes zu bilden. Das ist ein sehr ansprechendes Szenario für die Anbieter wie die Kunden gleichermaßen« <@1 fliess normal>(ebd.).<@$p>@1 fliess mit:@1 fliess ohne:O’Reilly nimmt dem negativen Beiklang weiterhin die Spitze, indem er auf die GPL-Community verweist:@1 fliesskursiv Zitat:»Übrigens könnte man auch von der GPL (nicht jedoch von den BSD-artigen Lizenzen) sagen, dass sie eine Art umzäunte Gemeinschaft schaffe, da (theoretisch) nur diejenigen, die der Lizenz zustimmen, den Quellcode weiterverbreiten dürfen. Aufgrund der Lizenzbedingungen handelt es sich um eine missionarische umzäunte Gemeinschaft, die ständig bemüht ist, neue Mitglieder zu gewinnen. Dennoch bleibt sie für diejenigen verschlossen, die die Lizenzvereinbarung nicht befolgen möchten«<@1 fliess kursiv> <@1 fliess normal>(ebd.)<@1 fliess kursiv>.@1 fliess mit:@1 fliess ohne:In der Tat stecken beide Modelle Territorien von Privat- und Kollektiveigentum ab. Gabriel/Joy sehen den Vorteil des Open Source-Modells (das, anders als »freie Software« keine »politischen Überzeugungen« darüber verkörpere, was Eigentum sein kann und was nicht) u.a. darin, dass »... es einen Selbstorganisationseffekt gibt, durch den die Grenzen zwischen proprietären Interessen und Gemeinschaftsinteressen laufend neu festgelegt werden« (<@6 Caps>Gabriel/Joy,<@$p> 1/1999). Diese Selbstorganisation nimmt in den Lizenzen Form an, was es umso plausibler erscheinen lässt, dass Debian seine Lizenz einen »Gesellschaftsvertrag« nennt.@1 fliess mit:Stallman schreibt (in seiner Kritik an der APSL): »Der Geist der freien Software ist es, dass wir eine Gemeinschaft bilden, um auf der Software-Allmende zu kooperieren.«<@3 hoch fliess>111<@$p> Diese Idee eines <@1 fliess kursiv>Commons<@$p>, zu Deutsch »Allmende« oder genauer »Wissens-Allmende«, ist beim GNU-Projekt am konsequentesten durchdacht, das darauf zielt, einen vollständigen freien Softwarekorpus aufzubauen, der erlaubt, alles mit dem Computer zu machen. Wissen braucht, ebenso wie natürliche Ressourcen, Pflege und Weiterentwicklung, also Investitionen. Die werden im Falle einer Allmende von den Allmendgenossen getragen, hier der Community freier Entwickler. Zur Erhaltung und Pflege der gemeinsamen Investitionen werden bestimmte Nutzungen ausgeschlossen. Deshalb steht GNU-Software ausdrücklich nicht in der <@1 fliess kursiv>Public Domain,<@$p> sondern schöpft den <\n>Eigentumsanspruch des Copyright voll aus. Auch das GNU-Projekt ist <\n>also ein geschlossener Wissensraum, eine <@1 fliess kursiv>Gated Community<@$p>. Ihre Grenzen werden von der GPL abgesteckt. Die Vereinbarungen, die die Allmendgenossen unter sich treffen, nehmen bei Wissensgütern die Form von Lizenzen an. Wer sie akzeptiert und respektiert, gehört dazu. Wer gegen sie verstößt, wird automatisch von der Nutzung ausgeschlossen (die Verfallsklausel Ziff. 4 GPL). Die GPL ist der Maßstab, dem eine Lizenz genügen muss, damit eine Software in das GNU-Projekt aufgenommen werden kann, also die Grenze ins Innere der Allmende passieren darf. @3 Fussnoten:<t0z9.5f"FFScala"><\c><t20z12f"Univers-CondensedBold">Gesellschaftliche Potenziale freier Software@2  ZÜ 1:<t0z9.5f"FFScala">@1 fliess ohne:Wenn freie Software den Nutzern Vorteile bietet, übersetzt sich dies natürlich auch in volkswirtschaftliche Effekte. Zwei aus öffentlicher Sicht bedeutsame Bereiche sollen im Folgenden hervorgehoben werden: ihre Bedeutung in der Bildung sowie für ärmere Gruppen und Länder. Doch auch der öffentliche Sektor selbst beginnt freie Software zu nutzen. Nimmt man den Trend zu einer für den Bürger transparenteren Verwaltung mit weitgehendem Akteneinsichtsrecht hinzu, wie er sich in der aktuellen Diskussion um die Informationsfreiheitsgesetze in Deutschland und Europa ausdrückt, so könnte man fast von einem morphogenetischen Feld des quelloffenen Codes sprechen. @1 fliess mit:Ebenso wie Firmen geben auch Behörden neben praktischen Gründen für den Einsatz freier Software häufig an, dass sie die Abhängigkeit von einzelnen Herstellern vermeiden wollen: »Skandinavien, Deutschland und Frankreich sind die Länder, in denen Linux am weitesten verbreitet ist. Einige sagen, der Grund dafür sei, dass Unternehmen und <\h>Regierungen vermeiden wollen, zu abhängig von amerikanischen Produkten  – sprich Microsoft – zu werden.«<@3 hoch fliess>1<@$p>Welche Folgen eine solche Abhängigkeit haben kann, zeigte sich 1998 in Island. Um seine Schrift in der digitalen Welt zu bewahren und lebendig zu halten, bat das Land Microsoft, eine Unterstützung für Isländisch in Windows zu implementieren. Es war sogar bereit, für diese <\n>Arbeit zu bezahlen, doch Microsoft sah den Markt als zu klein an und winkte ab. Ohne Zugang zum Quellcode und ohne das Recht, ihn zu mo<\h>difizieren, ist das Land vollkommen abhängig von Microsofts Gnade.<@3 hoch fliess>2<@$p> Daraufhin wurde ein Projekt gestartet, die Sprachunterstützung in GNU/Linux zu implementieren, was dank seiner Freiheiten problemlos möglich war. Die öffentliche Verwaltung und viele Bürger migrierten auf das freie Betriebssystem. Die Modifikationsfreiheit führt dazu, dass freie Software in einem viel größeren Umfang lokalisiert wird als proprietäre.<@3 hoch fliess>3<@$p>Der Staat als Marktregulierer hat die Aufgabe, Monopolauswüchse zu verhindern sowie kulturelle und Marktvielfalt zu gewährleisten. Immer mehr seiner Institutionen setzen diese Aufgabe auch in ihrer eigenen Praxis um. In Finnland gibt es bereits Behörden, die komplett mit GNU/Linux arbeiten. In Frankreich soll Quelloffenheit zum Ausschreibungskriterium für Softwarebeschaffung werden.<@3 hoch fliess>4<@$p> Ähnliche Initiativen gibt es in Dänemark, Brasilien und Polen. In Deutschland setzen sich u.a. das Bundeswirtschaftsministerium, das Bundesamt für Sicherheit in der Informationstechnologie (BSI) sowie die Koordinierungs- und Beratungsstelle der Bundesregierung für Informationstechnik (KBSt) für den Einsatz freier Software ein. Der KBSt-Brief vom Februar 2000 spricht ein klares Votum dafür aus und gibt praktische Handreichungen (<@6 Caps>KBSt-Brief<@$p>, 2/2000). Auf der Konferenz »Effizienter Staat 2000« im April des Jahres wurde von einer Dienststelle mit 260 Mitarbeitern berichtet, die durch die Umstellung auf <t-1>GNU/<t$>Linux gegenüber einer NT-Lösung fast 50<\!q>000 Mark eingespart hat (vgl. <@6 Caps>Gehring<@$p>, 6/2000). Um auch privaten Awendern in den verschiedensten Branchen freie Software näher zu bringen, legte das Bundesministerium für Wirtschaft und Technologie im März 2001 die Broschüre »Alternative Betriebssysteme. Open-Source-Software. Ein Leitfaden für kleine und mittlere Unternehmen« vor. Auch der EU-Kommissar für die Informationsgesellschaft, Erkki Liikanen, kritisiert die Nichtverfügbarkeit des Sourcecodes kommerzieller Produkte, stellte eine Bevorzugung offener Software bei Ausschreibungen in Aussicht und empfahl die Förderung von Open Source-Projekten.<@3 hoch fliess>5<@$p> @2  ZÜ 2:Freie Software in der Bildung@1 fliess mit:@1 fliess ohne:Computerbildung mit proprietärer Software heißt unweigerlich, dass die Schülerinnen und Studierenden wenig mehr lernen, als Menü-Punkte zu bedienen. Mit einem mächtigen modularen Betriebssystem wie Unix dagegen lernen sie ebenso unweigerlich viel über die Funktionsweisen eines Computers. Durch die Offenheit der Quellen können sie diese studieren. Mehr noch erlaubt es die Lizenz, mit ihnen zu experimentieren, Änderungen vorzunehmen und dann zu schauen, ob die neu kompilierte Software das tut, was man erwartet hat. Vor allem autodidaktische Neigungen werden von quelloffener Software angeregt und gefördert.@1 fliess mit:Die Unabhängigkeit von einzelnen Herstellern und die Vermittlung von Computerkenntnissen als Kulturtechnik und nicht nur als Kompetenz, die Software einzelner Hersteller bedienen zu können, sieht auch Peter Bingel als seinen Bildungsauftrag. Der Lehrer an der Gemeinschaftshauptschule Pennenfeld in Bonn Bad-Godesberg engagiert sich besonders für die Vernetzung der einzelnen Initiativen an Schulen untereinander und mit der Gemeinschaft der freien Software allgemein. »Freie Software schafft, so denke ich, einen informationellen Freiraum. Entscheidend ist, dass sie für jeden verfügbar und zu besitzen ist und mit <t-1><\n>ihrer Hilfe ein von kommerziellen Interessen unabhängiger Raum entsteht. Und ich denke, dieser freie Raum ist besonders für den Bildungsbereich wichtig. [...] Allein durch freie Software ist man in der Lage, sich informationstechnologisch von der Vorherrschaft einiger weniger <x@1 fliess kursiv><t-1>Global Players<@$p><t-1> zu lösen.«<x@3 hoch fliess><t-1>6<@$p><t-1> Bingel warnt insbesondere vor der Kommerzialisierung der Wissensbasis durch das Engagement von Firmen wie Microsoft, <\n>Apple, IBM und Sun im Bereich von Lernsoftware und bei der Errichtung kommerzieller Bildungsangebote. Er schätzte Mitte 1999, dass deutlich über zehn Prozent aller deutschen Schulen GNU/Linux einsetzen. Heute dürfte die Zahl höher liegen. Häufig geht das auf die Eigeninitiative einzelner Lehrer zurück, die sich die notwendigen Kenntnisse in ihrer Freizeit aneignen. Unterstützung, eine Übersicht über die für Schulen <t$>geeignete Software und Hilfe finden sie bei dem von Bingel mitgegründeten Verein <@1 fliess kursiv>Freie Software und Bildung<@$p> (FSuB).<@3 hoch fliess>7<@$p> Dort findet sich unter dem Stichwort »Software« eine Vielzahl von Programmen, die weite Teile nicht nur des Informatikunterrichts abdecken. Auch spricht er den Vorteil für Schüler an, eine grafische Oberfläche in ihrer Heimatsprache zu benutzen, wenn deren Muttersprache nicht Deutsch ist. Nur freie Software er<t-1>laubt es außerdem, den Schülerinnen Kopien der Programme mit nach Hause <t$>zu geben.Die Zahl der für den Unterricht einsatzfähigen Programme wächst beständig. Durch die Aktivitäten von SEUL<@3 hoch fliess>8<@$p> in den USA, von OFSET<@3 hoch fliess>9<@$p> in Frankreich, von den Entwicklern des KDE-Projekts<@3 hoch fliess>10<@$p>, durch die Aktivitäten des FSuB und PingoS<@3 hoch fliess>11<@$p> sowie durch die der <@1 fliess kursiv>Open Web School<@3 hoch fliess>12<@$p> (die letzten drei in Deutschland beheimatet) werden zunehmend nicht nur Programme für den Unterricht entwickelt, sondern auch Nischen wie Zeugnisdruck, Bundesjugendspiele oder Schulbuchausleihe abgedeckt. Nicht zu vergessen sind die vielfältigen Aktivitäten von Red Escolar in Mexico,<@3 hoch fliess>13<@$p> Brasilien, Singapur und neuerdings auch in Polen, die alle dafür sorgen, dass ein freies System wie Linux immer mehr Schülern vom Bildschirm entgegenblickt. Im Jahr 2000 entschloss sich das Land Schleswig-Holstein zu einem ungewohnten Schritt. Am dortigen Landes-Bildungsserver wurde zum Einsatz auf dem Schülerdesktop kmLinux<@3 hoch fliess>14<@$p> entwickelt und an alle weiterführenden Schulen des Landes verschickt. Dabei handelt es sich um eine aus der Schachtel zu installierende, fertig vorkonfigurierte und mit zahlreicher Schulsoftware ausgestattete Distribution, die mittlerweile in der Version 2 vorliegt. Die Erstausgabe von 5<\!q>000 Exemplaren war innerhalb wenig<t-1>er Monate vergriffen. Während das Land Schleswig-Holstein die eigenen Schulen kostenlos damit ausstattet, werden Schulen im übrigen Bundesgebiet auf Wunsch über den FSuB beliefert. Nicht unerwähnt bleiben soll au<t$>ch die Vielzahl der javabasierten Applikationen, die sys<\h>tem<\h>unabhängig auch auf Linux-Rechnern ihren Dienst tun. Als Beispiel sollen hier nur die Java-Applets von Walter Fendt<@3 hoch fliess>15<@$p> aufgeführt werden. Weitere Applets, vor allem für den Physikbereich, finden sich im Internet zuhauf.In der Regel wird GNU/Linux in der Schule aber bisher als Server eingesetzt. Bundesweit dürfte die Zahl der unter GNU/Linux laufenden Schulserver die der Windows-Server übersteigen. So gibt es bereits seit 1998 auf die Belange der Schulen zugeschnittene, kostenlose Linux-Server, die als Blackbox-Systeme auch von Computerneulingen in der Lehrerschaft eingerichtet und bedient werden können. Mittlerweile ist es so, dass z.B. eine Stadt wie Bonn alle Schulen mit Rechnern ausstattet  – wobei die Server prinzipiell unter Linux laufen, es sei denn, die Schule wünscht explizit etwa anderes, was aber äußerst selten der Fall ist. Andere Städte, u. a. im Ruhrgebiet,  haben nachgezogen. Dabei sind die GNU/Linux-Server-Lösungen sogar so erfolgreich, dass sie als innovative Lösungen erste Preise einheimsen. So geschehen im Februar 2001, als die Bertelsmann-Stiftung den Server der Zeitschrift <@1 fliess kursiv>c’t<@$p> und des offenen Deutschen Schulnetzes (ODS) als beste Schul-Serverlösung aufs Siegerpodest hob. Er wird hauptsächlich von Reiner Klaproth in Dresden<@3 hoch fliess>16<@$p> entwickelt, über die Zeitschrift <@1 fliess kursiv>c’t<@$p> vertrieben und ist viele Tausend Mal im Einsatz. Diesen ersten Preis teilte sich der c’t-ODS-Server mit einer bayerischen Schülergruppe, die für ihre Schule eine auf GNU/Linux basierte Netzwerklösung entwickelte. Der dritte Preis (einen zweiten gab es nicht) ging an den GEE-Server,<@3 hoch fliess>17<@$p> eine auf einer Standard-Linux-Distribution von A.<\!q>Leonhardt und W. Mader an der Gesamtschule Eiserfeld entwickelte Serverlösung, die auch schon etliche Hundert Male im Einsatz ist. Selbst proprietäre – und leider recht teure – Schul-Server-Lösungen unter Linux gibt es, z.B. Easy-Admin. Der Markt ist also da, und der Einsatz von freien Lösungen wie GNU/Linux im Bildungsbereich wächst.<t-1>In Europa hat man hier und da mittlerweile auch in politisch höheren Kreisen die Bedeutung einer eigenständigen Softwareindustrie und eines freien Zugangs zu Informationen erkannt. Dies geht allerdings nicht ohne das Fundament einer entsprechenden Bildung. Bundesbildungsminis<\h><t$>terin Edelgard Bulmahn möchte die Entwicklung von Software im Bildungsbereich fördern und hat dazu ein Förderprogramm ins Leben gerufen: »Neben dem zu erwartenden Verbreitungs- und langfristigen Nutzungsgrad der Software sind die technische und didaktische Qualität sowie der Innovationsgehalt der Entwicklungen für eine Förderung und deren Höhe ausschlaggebend. Dabei spielen auch die in Verbindung mit <t1>dem Inhaltsangebot parallel zu entwickelnden neuen Konzepte für zugehörige Dienstleistungen (spezifische Info-Angebote für Lehrerinnen und Lehrer, Nutzer-Hotline, Updates, etc.) und kooperative Ansätze über <t$>eine offene Plattform gerade im Schulbereich <t-1>eine entscheidende Rolle.«<x@3 hoch fliess><t-1>18<@$p>Auffallend ist in dem obigen Zitat der Begriff »offene Plattform«. Das BMBF drückt sich vorsichtig aus. Offene Plattformen, auf denen Programme entwickelt werden und laufen, gibt es jedoch nicht sehr viele. Dass auch an höhe<t2>rer Stelle erkannt worden ist, dass zur eigenen Entwicklung unbedingt der freie Zugriff auf Wissen gehört und die eigene Entwicklung nur durch diese freie Verfügbarkeit gesichert ist, lässt hoffen. In der sich abzeichnenden neuen Wissensordnung könnte freie Software ebenso wie andere öffentliche Wissensgüter tatsächlich eine hohe Priorität erlangen. Die Schaffung einer »Gesellschaft, in der wir leben wollen« (<x@6 Caps><t2>Richard Stallman<@$p><t2>), ist ganz wesentlich auch ein<t$>e bildungspolitische Aufgabe.@2  ZÜ 2:Freie Software in den Nicht-G8-Ländern@1 fliess mit:@1 fliess ohne:Eine weitere, besonders betroffene Nutzergruppe sind die weniger Betuchten dieser Welt. Der offenkundigste Vorteil für sie liegt darin, dass Softwarekosten gespart werden können. Aber auch bei der Hardware ist es nicht erforderlich, den Hase-und-Igel-Wettlauf mitzumachen, bei dem die neueste Software nach der neuesten Hardware verlangt und umgekehrt. GNU/Linux und andere freie Software wird in Versionen entwickelt, die ältere Hardware einsetzbar macht. Ein Intel-386er, eine Rechnergeneration, die kommerzielle Nutzer längst ausgemustert haben, genügt den meisten heutigen Anforderungen. Das erlaubt es Organisationen wie der <@1 fliess kursiv>Computer Bank<@$p>, Hardwarespenden von Firmen, Behörden und Privatleuten entgegenzunehmen, sie mit freier Software auszustatten und an diejenigen umzuverteilen, die sich selbst solche Rechner nicht leisten können. <@1 fliess kursiv>Computer Bank<@$p> ist ein australisches Projekt, das Niedrigverdienende, Community-Gruppen und benachteiligte Schulen mit alten Rechner ausstattet.<@3 hoch fliess>19<@$p>@1 fliess mit:Ende 1998 gab die mexikanische Regierung Pläne für ein <@1 fliess kursiv>Scholar Net Program<@$p> bekannt, das in den folgenden fünf Jahren GNU/Linux an 140<\!q>000 Schulen im ganzen Land mit fast 20 Millionen Schülern einführen wird. Die mexikanischen Schüler sollen Web- und E-Mail-Zugang erhalten, mit Textverarbeitung und Tabellenkalkulation vertraut gemacht werden und im Computerraum ihre Recherchen und Hausaufgaben machen können – umso wichtiger in einem Land, in dem nur eine verschwindend kleine Minderheit einen Computer zu Hause hat. Parallel dazu bietet ein Projekt für eine digitale Bibliothek den Schülern die Möglichkeit, auf Lehrbücher im Netz zuzugreifen. Die Wahl von GNU/Linux begründete der Projektleiter Arturo Espinosa Aldama damit, dass die Kosten für kommerzielle Software nicht tragbar gewesen wären. Eine Microsoft-Lösung hätte das Land 124 Millionen Dollar gekostet. Nicht allein das Budget sprach für die Einführung von GNU/Linux, sondern auch die bessere Verlässlichkeit, Anpassungsfähigkeit und Effizienz im Vergleich zu proprietärer Betriebssys<\h>tem-Software.<@3 hoch fliess>20<@$p> <@1 fliess kursiv>Scholar Net<@$p> schließt Fernunterricht über Fernsehen (RED EDUSAT) und E-Mail ein. Schulen und Schüler erhalten E-Mail-Adressen und persönliche Webseiten. Koordiniert wird das Projekt von der <@1 fliess kursiv>Universidad Nacional Autónoma de México<@$p> und dem <@1 fliess kursiv>Instituto Latinoamericano de la Comunicación Educativa<@$p>. Das Geld stammt von der Regierung und aus Spenden. In der ersten Phase hat das Projekt eine GNU/Linux-Distribution (mit Servern wie Sendmail, httpd, diald und Squid, und für die Workstations GNOME-Applikationen wie gwp, gnumeric und Netscape) namens »Red Escolar«<@3 hoch fliess>21<@$p> zusammengestellt und sie bei den Bildungsbehörden und Schulen in den einzelnen Bundesländern bekanntgemacht. Möchte eine Schule von dem Angebot Gebrauch machen, erhält sie in der nächsten Phase Unterstützung bei der Installation auf der vorhandenen Hardware sowie Training. Espinosa ist über die Rückendeckung durch die Behörden erfreut: »Wir hatten mit einigem Widerstand von Seiten der Entscheidungsträger gerechnet, aber aufgrund der Aufmerksamkeit, die GNU/Linux in der Computerpresse allgemein genießt, wird GNU/Linux jetzt von der Universität als gangbare Alternative angesehen, und wir werden akzeptiert, weil wir etwas vorweisen können.«<@3 hoch fliess>22<@$p> Im Bildungsministerium denkt man bereits über die nächste Phase nach, in der die Entwicklung von Bildungssoftware im Vordergrund stehen soll. Während in Mexico und Brasilien die Regierung diesen Kurs fördert, gehen die ersten Schritte in die freie Software meist auf <@1 fliess normal>Graswurzel<\h>initiativen<@$p> zurück, wie das folgende Beispiel aus Malaysien zeigt. Die Vereinigung der von der Erbkrankheit Thalassaemie Betroffenen war mit der <\n>Bitte an die malaysische <@1 fliess kursiv>Open Source Group<@$p> herangetreten, ihnen bei der Errichtung einer E-Community zu helfen. Geld aus einem staatlichen Förderprogramm war in Aussicht gestellt, doch Papier ist geduldig. Statt dessen fand sich ein 155er Pentium mit 500 MB Festplatte – nach allen Standards auch das schon wenig mehr als Computerschrott. Darauf installierte die Gruppe das schlanke FreeBSD, dazu Apache, Majordomo, Sendmail, Bind und Perl. Damit hatte die <@1 fliess normal>Community<@$p> eine Web-Site, Mailinglisten, ein webbasiertes Diskussionsforum und ein Formular, um sich für Veranstaltungen zu registrieren.<@3 hoch fliess>23<@$p> Sobald das Geld eintrifft, werden sie die Hardware und das Informationsangebot erweitern. Die <@1 fliess kursiv>Open Source Group<@$p> wird der Gemeinschaft dabei helfen, das System selbst zu unterhalten und dann zu einer anderen hilfsbedürftigen Non-Profit-Gemeinschaft weiterziehen. Von einem Beispiel aus der höheren Bildung in der Türkei berichtet Emre Demiralp (vgl. <@6 Caps>Demiralp<@$p>, 1998). Im Fachgebiet Elektrotechnik der technischen Universität Istanbul wurde 1998 eine Version des türkischen GNU/Linux (Turkuvaz) entwickelt. Bis 1991 standen den Studenten elf PCs zur Verfügung. Es gab dauernd Probleme mit Viren und Sys<\h>temzusammenbrüchen. Ab 1992 stand ein Sun-Rechner zu Verfügung. Über das Mailbox-System BITNET wurde die Leitung des Fachbereichs auf ein neues Betriebssystem namens GNU/Linux aufmerksam. Die Software war kostenlos und es gab genug Informationen im Internet. Die Angehörigen der Universität stellten jedoch fest, dass die Administration der Rechner keine leichte Aufgabe war. Sie ist zeitintensiv, und es war fast unmöglich, mit wenigen Administratoren Hunderte von Studenten zu betreuen. Da es zu teuer war, Fachkräfte für die Systempflege einzustellen, wurden Studenten als Administratoren gewonnen. Sie sollten die Systeme warten und konnten zugleich mehr lernen. Heute sind schätzungsweise 100 Studenten an der Pflege des Systems beteiligt. Sie opfern ihre Freizeit, um ihr Wissen zu erweitern, wodurch sie bei ihrem Abschluss eine zusätzliche Auszeichnung erhalten können. Außerdem werden Informationen über GNU/Linux und verwandte Themen gesammelt und den Studenten zur Verfügung gestellt. Alle zwei Monate wird auch ein türkisches Online-Magazin publiziert. Mittlerweile bestehen die Rechnerpools aus 70 Pentium 166ern, von denen 50 unter Linux laufen. Die Wartung wird durch die Studierenden vorgenommen. Das System selbst wird von etwa 500 Studenten rund um die Uhr genutzt. Die längs<\h>te Zeit ohne einen Crash waren 90 Tage. Afrika hat die geringste Durchdringung mit Computern und Internet. Während 1995 nur acht afrikanische Staaten über einen Internetzugang verfügten, waren es Ende 1997 immerhin schon 42, und seit 1999 sind alle Länder Afrikas, außer Somalia, am Internet.<@3 hoch fliess>24<@$p> Die Bandbreite wird in den nächsten Jahren unter anderem mit Hilfe neuer Satelliten und eines Unterwasser-Glasfaserkabels, das den gesamten Kontinent umspannt, erheblich erweitert. Nazir Peroz, Sprecher der Fachgruppe »Informatik und Dritte Welt« der Gesellschaft für Informatik und Dozent an der TU Berlin, berichtete, dass GNU/Linux an Universitäten in Simbabwe, Äthiopien und Mosambik verwendet wird, vor allem da es sich um ein robustes Betriebssystem handelt. Peroz’ Ansprechpartner in sind zwar an freier Software interessiert, scheitern jedoch oft an grundlegenden Problemen der Informatikausbildung und der Ausstattung der Universitäten. Neben den technischen Bedingungen sind Wissen und Bildung <\h>Voraussetzungen für eine Teilhabe an der globalen Informationsgesellschaft. Die Informatikausbildung an afrikanischen Hochschulen sei zu wenig praxisorientiert. Hier sieht Peroz Herausforderungen für den Informationstechnologie-Transfer. In der Arbeitsgruppe <@1 fliess kursiv>Computer Information Transfer <@$p>(AG-CIT) unterstützt er afrikanische Dozenten und Studierende über das Internet bei Informatikfragen. So arbeitet die AG-CIT z. B. mit dem Fachbereich Informatik der Universität Simbabwe in Harare zusammen.<@3 hoch fliess>25<@$p><*h"mehr">Einen gewaltigen Zulauf könnte freie Software in China erhalten. Mit einer Zuwachsrate von zehn Millionen verkauften PCs pro Jahr nimmt die Computerisierung dort rasch zu. Da eine Kopie von Microsoft Windows zu einem Preis verkauft wird, der einem halben Jahreslohn eines Arbeiters entspricht, handelt es sich bei der verwendeten Software überwiegend um nicht autorisierte Kopien. In diesem Segment herrschten bislang Microsoft-Betriebssysteme vor, obgleich sie der Handhabung chinesischer Schriftzeichen nicht besonders entgegenkommen. Am Softwareinstitut der Chinesischen Akademie der Wissenschaften wurde im Juni 2000 die erste chinesischsprachige 64-Bit-Version von GNU/Linux vorgestellt. Während bei Windows vier bis sechs Tasteneingaben nötig sind, um ein Zeichen aufzurufen, benötigt das neue <@1 fliess kursiv>Chinese 2000 <@$p>der Wissenschaftsakademie nur durchschnittlich 2,5 Tasteneingaben. Federal Software, der größte Softwarevertreiber des Landes, verkaufte 1999 sein chinesisches GNU/Linux in beinahe 200<\!q>000 Exemplaren, etwa 200 Mal häufiger als das chinesische Windows. Federal bietet denjenigen »Bluepoint-Linux« zum Preis von 10 Yuan (2 US-Dollar) an, die eine Kopie illegaler Software aushändigen. Verschiedene wichtige Regierungsbehörden beschlossen, das einheimische »Red Flag Linux« einzusetzen. Auch Red Flag wurde an der Wissenschaftsakademie in Zusammenarbeit mit Compaq entwickelt (vgl. <@6 Caps>Dwyer<@$p>, 2000). <*h"Standard">Ein Gutteil der primären Ressourcen der Welt liegt in den ärmeren Ländern des »Südens«. Das Eigentum an Wissen und Kapital, und damit die Ausbeutung dieser Ressourcen dagegen konzentriert sich im »Norden«. Das traditionelle Wissen des Südens, z. B. über Pflanzenheilkunde, wird ohne Kompensation enteignet, um die Informationsverarbeitungsmaschinerie des Nordens zu speisen. Wo in ärmeren Ländern Computer verfügbar sind, ist häufig auch »kostenfreie« proprietäre Software zu haben, doch die kann man nur unter der Gefahr der Einschüchterung und Verfolgung durch Unternehmen und Polizei verwenden. Vor allem die WIPO und die WTO stehen für eine aggressive Agenda der Durchsetzung von geistigen Eigentumsrechten in aller Welt. Freie Software bietet auch in dieser Hinsicht eine bessere Alternative. SatelLife<@3 hoch fliess>26<@$p> ist eine internationale <@1 fliess kursiv>Non-Profit<@$p>-Organisation, die die Kommunikation im Gesundheitswesen in den Entwicklungsländern verbessern hilft. Sie hat dazu ein umfassendes Netz aus Funk, Telefon und dem niedrig fliegenden Satelliten HealthSat-2 errichtet. HealthNet ist ein Mailboxnetz, das das FidoNet-Protokoll verwendet. Die Software für seine Satelliten-Gateways und die Bodenstationen hat SatelLife auf der Basis von GNU/Linux entwickelt. @1 fliesskursiv Zitat:»Zuerst einmal mussten die Mitarbeiter von SatelLife Technologien finden und meistern, die billig genug für Anwender in den ärmsten Ländern der Welt sind und gleichzeitig lebenswichtige medizinische Daten schnell und zuverlässig übermitteln können. Die Organisation hatte nicht die finanziellen Mittel einer IT-Abteilung eines Großunternehmens für den Hard- und Softwarekauf, sodass sie freie Open Source-Software benutzte, um Anwendern Diskussionsgruppen bereit zu stellen. Und weil das Internet an Bedeutung stets zunimmt, musste SatelLife sicherstellen, dass Anwender ohne Internet-Browser trotzdem Informationen über das Internet bekommen können. Man griff, wann immer möglich, auf gebrauchte Hardware zurück und vertraute auf den Rat von Forschungsinstituten und Diskussionsgruppen, statt auf teure Beratungsfirmen.«<@3 hoch fliess>27<@$p>@1 fliess mit:@:<*p(0,0,0,12.5,0,0,g,"Deutsch")>Nicht nur NGOs, sondern auch die »große« Entwicklungspolitik hat sich der <f"FFScala-Italic">Digital Divide<+f"Univers-Condensed">28<$f$> angenommen. Das <f"FFScala-Italic">Third World Network of Scientific Organizations<f$>, aber auch Organisationen wie die UNESCO, die Weltbank (InfoDev) und USAID betreiben Initiativen, um die IT-Infrastruktur, Ausbildung und Forschung in Entwicklungsländern zu verbessern und die Kooperationen zu fördern. Das <f"FFScala-Italic">United Nations Development Programme<f$> (UNDP) betreibt seit 1992 das <f"FFScala-Italic">Sustainable Development Networking Program<f$> (SDNP), eine Initiative zur Förderung der lokalen wie der Internet-Netzwerkinfrastruktur in Entwicklungsländern. Hier soll den Menschen geholfen werden, Wissen für eine nachhaltige Entwicklung miteinander <t-2>auszutauschen. <@$p>»Informations- und Kommunikationstechnologien sind heute grundlegend für die Lösung aller Entwicklungsfragen in den sich entwickelnden Ländern und relevant für alle Haupttätigkeitsfelder des UNDP. Sie sind ein Kerninstrument für die Erzielung einer nachhaltigen menschlichen Entwicklung und eines, das es Entwicklungsländern ermöglicht, in das 21. Jahrhundert zu springen.«<@><+f"Univers-Condensed">29<$f$> Corel<+f"Univers-Condensed">30<$f$> und Red Hat statten das Programm mit ihren GNU/Linux-Distributionen aus. Auch die UNESCO verbreitet kostenlose GNU/Linux-CD-ROMs an wissenschaftliche, Bildungs- und Community-Projekte in Lateinamerika:@1 fliess ohne:@1 fliesskursiv Zitat:»Wir glauben, dass Linux eine sehr wichtige Rolle beim Modernisierungsprozess in Lateinamerika und in der Karibik spielen und Netzwerke errichten helfen kann, die es einer großen Zahl von Universitäten, Colleges, Schulen und Bildungszentren ermöglichen, am Internet teilzunehmen und dank dieses fabelhaften Werkzeugs ihr wissenschaftliches und kulturelles Niveau zu verbessern. Kurz, Linux ist das Mittel, um die technologische Kluft zwischen den Ländern abzubauen. Linux bietet Zugang zur ›Informatik der fortgeschrittensten Länder‹ in einer Form, die den eingeschränkten wirtschaftlichen Fähigkeiten in unserer Region entgegenkommt. Linux ist eine neue Art der Informatik, bei der ›die technische Qualität und die Solidarität der Menschen‹ das Wichtigste ist.«<@3 hoch fliess>31<@$p>@1 fliess mit:@1 fliess ohne:Auf der Tagung <@1 fliess kursiv>»Colloque Inforoute et Technologies de l’Information«<@$p> im Oktober 1997 in Hanoi ebenso wie auf der von der UNESCO in Zusammenarbeit mit dem <@1 fliess kursiv>International Council for Science<@$p> organisierten »World Conference on Science« im Juni 1999 in Budapest spielte die Bedeutung von freier Software für die Teilhabe am technologischen Fortschritt eine wichtige Rolle. Freie Software macht keine Hungernden satt, aber sie kann helfen, den Wissens- und Kommunikationsraum von Internet und Computer zu eröffnen.@1 fliess mit:@3 Fussnoten:<t0z9.5f"FFScala"><\c>@2  ZÜ 1:<*t(70.866,0,"1  ")>Wirtschaftliche Potenziale freier Software@1 fliess mit:@1 fliesskursiv Zitat:»Die Linux-Community, eine temporäre, selbstverwaltete Zusammenkunft unterschiedlichster Individuen die einer gemeinsamen Aufgabe nachgehen, ist ein Modell für eine neue Art von wirtschaftlicher Organisation, die die Grundlage für eine neue Art der Ökonomie bilden könnte.«<t-5> <f"FFScala">(<x@6 Caps><t-5>Harvard Business Review<@$p><t-5f"FFScala">, September 1998)<t$>@1 fliess mit:@1 fliess ohne:Die Bewegung der freien Software mag auf den ersten Blick als eine Abweichung vom allgemeinen Marktgeschehen erscheinen. Auf den zweiten wird man sie für einen »Rückfall« in den ursprünglichen Zustand der Softwarebranche vor Beginn ihrer Kommodifizierung halten können. Noch genaueres Hinsehen fördert dann jedoch eine Reihe von Korres<\h>pondenzen und Strukturähnlichkeiten zu Trends der »offiziellen« Ökonomie zutage. @1 fliess mit:Seit den 60er-Jahren wird hier ein Strukturwandel von der Industrie- und Warengesellschaft hin zur Informations- und Dienstleistungsgesellschaft attestiert. Information wird anstelle von Materie und Energie zur zentralen industriellen Ressource und Ware. Als einen Wendepunkt nannte der <@1 fliess kursiv>Club of Rome<@$p> in seiner Studie »Grenzen des Wachstums« das Jahr 1970. Die Indikatoren für stofflichen Reichtum, die bis dahin mit dem Bruttoinlandsprodukt korrelierten, entwickeln sich seither umgekehrt proportional: je reicher ein Land, desto geringer der stoffliche <\h>Anteil an diesem Reichtum. In dem Maße, in dem das Internet zur Infrastruktur der Wirtschaft wird, werden die Grenzen zwischen Unternehmen und die zwischen Nationalökonomien fließend. »Virtuelle Unternehmen« bestehen aus nicht mehr als einem Planungskern, der für die Dauer eines Projekts Kooperationen mit anderen eingeht und Leistungen nach Bedarf von außen zukauft. Jeremy Rifkin sagt in seinem neuen Buch das Verschwinden des Eigentums voraus, dessen Besitz durch den Zugang (»Access«) zu Dingen und Wissen ersetzt werde:@1 fliesskursiv Zitat:»Unternehmen sind in diesem Übergang vom Besitz zum Zugang schon ein Stück vorangekommen. In einem gnadenlosen Wettbewerb verkaufen sie ihren Grundbesitz, verschlanken ihr Inventar, leasen ihre Ausstattung und lagern ihre Aktivitäten aus; sie wollen sich von jeglichem immobilen Besitz befreien. Dinge, und zwar möglichst viele, zu besitzen, wird in der an Schnelligkeit und Flexibilität orientierten Wirtschaft des neuen Jahrhunderts als überholt und lästig betrachtet. In der heutigen Geschäftswelt wird fast alles geliehen, was ein Unternehmen zu seinem Betrieb braucht« <@1 fliess normal>(<@6 Caps>Rifkin<@1 fliess normal>, 2000)<@$p>. @1 fliess mit:@1 fliess ohne:Den gleichen Trend sieht er bei den Verbrauchern:@1 fliesskursiv Zitat:»<t-2>Zwar werden niedrigpreisige haltbare Dinge auch weiterhin gekauft und verkauft werden, teurere Objekte jedoch, Geräte, Autos oder Häuser, werden zunehmend von Anbietern gehalten werden, die den Konsumenten über zeitlich befristete Leasing- oder Mietverträge, Mitgliedschaften und andere Dienstangebote Zugang und Nutzung gewähren« <x@1 fliess normal><t-2>(ebd.)<@$p><t-2>.<t$>@1 fliess mit: @1 fliess ohne:Und was wird aus der materiellen Produktion, der Landwirtschaft und den nicht informationellen Dienstleistungen? Die technokratische Antwort ist vorhersehbar: Hier werde die menschliche Arbeitskraft zunehmend von intelligenten Maschinen ersetzt. 2050, so prophezeit Rifkin, würden nicht mehr als fünf Prozent der erwachsenen Bevölkerung benötigt, um die herkömmlichen Betriebe in Gang zu halten. Die übrigen 95 Prozent – falls es eine Vollbeschäftigung geben sollte – würden dann, so suggeriert er, in der Informationsökonomie arbeiten, vor allem in der Kulturindustrie, die die letzte Stufe des Kapitalismus darstelle. Wie die Arbeitsbedingungen im Herzen der Hightech-Industrie, im Silicon <\n>Valley, heute aussehen, umreißt Netzaktivist Florian Schneider so:@1 fliesskursiv Zitat:»›Nettokratie‹ ist eine der jüngsten Wortschöpfungen, die den Blick auf die soziale Zusammensetzung der Informationsgesellschaft lenken soll. Den von Arthur Kroker schon 1994 zur ›virtuellen Klasse‹ erhobenen Entrepreneurs steht ein Heer von Netzsklaven gegenüber, die sich bei den Start-Ups in den Silicon-Somethings verdingen [...] Die Arbeitskräfte, sagt Andrew Ross, der als Direktor des American Studies Program an der New York University<@3 hoch fliess>1 <@$p>die Situation im Silicon Valley untersucht hat, seien zur Hälfte Werkvertragsarbeiter, die vor allem darauf angewiesen seien, dass ihre Aktienanteile steigen. Das Durchschnittseinkommen liege mit 50<\!q>000 US-Dollar ungefähr bei der Hälfte dessen, was in den alten Medien verdient werde. Bemerkenswert ist, dass ausgerechnet Künstler mit ihrem flexiblen und selbstlosen Arbeits<\h>ethos das Rollenmodell für die ›freiwillige Niedriglohn-Armee‹ abgeben. Der ›Glamour der Boheme‹ kommt nach Ross einer Einladung zur Unterbezahlung gleich. Dass die ›New Economy‹ aber nicht nur hochqualifizierte Jobs hervorbringt, sondern vor allem Unmengen von vergleichsweise banalen Tätigkeiten wie Telefonieren, Pizza-Bringen oder Saubermachen schafft, wird in den gegenwärtigen Debatten gewöhnlich unterschlagen.«<@3 hoch fliess>2<@$p>@1 fliess mit:@1 fliess ohne:Auf der Konsumentenseite sieht der führende deutsche Mikroökonom Norbert Szyperski durch Online-Auktionen den Markt sich in einen <\n>»Basa<t-2>r« im eigentlichen Sinne verwandeln: Die Güter haben keine feste Ausschilderung, alle Preise werden ausgehandelt. Mit dem neuen Ort des Marktes entstehen neue Regeln, neue Verhältnisse zwischen Anbieter, Käufer und Ware sowie neue Fragen: »Wie kann man ein geschäftliches Vertrauensverhältnis in dieser Medienwelt etablieren, wie man das üblicherweise mit seinen Stammlieferanten oder Stammkunden haben <t-3>konnte? ... Wie macht man internationale Kleingeschäfte? Welche Rechte <t-2>gelten da?«<x@3 hoch fliess><t-2>3<@$p>@1 fliess mit:Verschiebt die auf Informationstechnologie gestützte Ökonomie den Schwerpunkt von materiellen zu immateriellen Gütern, so verschiebt sich gleichzeitig der vom Verkauf von Werkstücken hin zur Lizenzierung von Nutzungen: »Heute schon gibt das reiche obere Fünftel der Weltbevölkerung für den Zugang zu kulturellen Erlebnissen genauso viel aus wie für Fertigerzeugnisse und Dienstleistungen« (<@6 Caps>Rifkin<@$p>, 2000). Mit diesem Wandel geht auch der vom Produkt zum Prozess einher. Bislang wurde eine Software als Werkzeug angesehen, das – zumindest bis zum nächsten <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Upgrade<@$p> – statisch bleibt. Nach dem neuen Verständnis steht auch im kommerziellen Teil der Branche die Implementierung, die laufende Anpassung und die Mitarbeiterschulung im Vordergrund. Software könnte statt als Produkt als eine Dienstleistung zur Generierung, Distribution, Manipulation und Archivierung von Informationsflüssen verstanden werden. Perry Barlow schrieb 1994 in seinem, zum Klassiker gewordenen Aufsatz <@1 fliess kursiv>»The Economy of Ideas«<@$p>, Information sei eine Aktivität, eine Lebensform und eine Beziehung (<@6 Caps>Barlow<@$p>, 1994). Diese Aussage ist der freien Softwarebewegung wie auf den Leib geschneidert. Wird also die GNU/Linux-Commmunity, wie die Zeitschrift <@1 fliess kursiv>Harvard Business Review<@$p> schrieb, zum Modell einer neuen Art von Ökonomie? Ist sie die Avantgarde eines generellen Stukturwandels oder nur ihr Nutznießer? Oder ist sie das Trüffelschwein, das im Möglichkeitsraum des Internet stöbert und dabei Schätze hervorholt, von denen sich die kapitalis<\h>tische Maschinerie nimmt, was sie zu ihrer Verjüngung braucht? Oder ist im Grunde das, was die freie Softwarewelt macht, dasselbe, was seit den 80er-Jahren auch in den Unternehmen geschieht? Eine allgemeine Tendenz zu Dezentralisierung, Abbau von Hierarchien und offenen Systemgrenzen ist auf jeden Fall nicht zu übersehen. Globalisierung und Flexibilisierung der Wirtschaftsstrukturen sind vielfach verzeichnete Trends. Die Projekte der freien Software übertreffen <\n>jede management- und organisationstheoretische Vision an Dezentralisation, lockerer Kooperation und zwangloser Koordination. Genauso wie die Frage nach dem »Standort« eines multinational operierenden Unternehmens wenig Sinn macht, kann man auch von den Softwareprojekten nicht sagen, sie seien amerikanisch, deutsch oder sonstwie nationalstaatlich zuzuordnen. Ihr Standort ist das Internet. Auch das Konzept vertikaler Kooperation ist der Wirtschaft nicht fremd. Der »Wertschöpfungspartner auf der Nachfrageseite«, wie es im <t-1>Ökonomenjargon heißt, also die Anwenderin, wird zur »Koproduzentin«:<t$>@1 fliesskursiv Zitat:»Die freie Mitwirkung ist etwas, was praktisch wie eine Epidemie durch die gesamte dienstleistende und wissensintensive Industrie hindurchgeht. Nicht umsonst versucht man Kooperation und <@1 fliess normal>Competition<@$p>, also Wettbewerb, auch begrifflich in Cooptition zu fassen, weil wir nicht mehr so scharf sagen können: Wer ist eigentlich mein Gegner oder mit wem mache ich gemeinsame Entwicklung und mit wem treffe ich mich nur beim Vertreter, möglicherweise beim Kunden, als Konkurrent? Koproduktion ist der Begriff für die Dienstleistung schlechthin. Wenn Sie heute z.<\!q>B. das Gesundheitswesen neu diskutieren, sprechen Sie nicht mehr über die Frage: Was sollte der Arzt mit dem Patienten tun, wenn der in die Klinik oder in die Sprechstunde kommt? Sondern wir gehen davon aus, dass derjenige, der noch leben will, selber Koproduzent seiner Gesundheit sein muss.«<@3 hoch fliess>4<@$p>@1 fliess ohne:Softwareanwender, die ein Programm benutzen, testen, Fehler zurückmelden und Verbesserungsvorschläge machen, sind Koproduzenten, ob von Microsoft-Windows oder von GNU/Linux. Wie oben gezeigt,<@3 hoch fliess>5<@$p> ist nicht einmal die Kooperationsgemeinschaft oder, wie hier vorgeschlagen, die Allmendgenossenschaft ein Privileg der freien Software. Auch herkömmliche Unternehmen errichten und pflegen ihre <@1 fliess kursiv>Gated<@$p> <@1 fliess kursiv>Communities<@$p>. Bliebe also die Frage nach dem Preis.@1 fliesskursiv Zitat:»Traditionell bestimmt sich der Preis einer Ware aus den Faktoren Angebot und Nachfrage. Da die Freie Software per Definition beliebig kopiert und verteilt werden darf, und weil die Reproduktion im Zeitalter von CD-ROM und Internet mit keinen nennenswerten Kosten verbunden ist, wird das Angebot beliebig groß. Der Preis für die Software muss demnach selbst bei überwältigender Nachfrage beliebig klein werden. In der Realität befindet er sich tatsächlich auf dem Niveau des Selbstkos<\h><t-1>tenpreises für die Reproduktion. Einen Internetanschluss vorausgesetzt, ist Freie Software ohne zusätzliche Kosten erhältlich« <x@1 fliess normal><t-1>(<x@6 Caps><t-1>Hetze<x@1 fliess normal><t-1>, 1999). <t$>@1 fliess mit:@1 fliess ohne:Die nahezu kostenlose Distributionsmöglichkeit gilt auch für proprietäre Software, doch die ist eben in der Regel nicht kostenlos. Es ist gerade der <@1 fliess kursiv>Free Beer<@$p>-Aspekt der freien Software, der gestandenen Ökonomen Rätsel aufgibt. Meist ordnen sie das Phänomen zunächst dem Marketing zu. Freie Software entspräche demnach einem Werbegeschenk, das Kunden für andere Produkte oder Dienstleistungen gewinnen soll, etwa einem Mobiltelefon, das in der Erwartung kostenlos abgegeben wird, dass die Grund- und Verbindungsgebühren über den Vertragszeitraum hinweg nicht nur die Kosten des Gerätes, sondern einen Profit darüber hinaus einspielen.<@3 hoch fliess>6<@1 fliess normal> <@$p>Auch Rifkin sieht dies als einen generellen Trend: »Im klassischen Industriezeitalter wollten Unternehmen vorrangig ihre Produkte verkaufen; kostenlose Servicegarantien setzten Kaufanreize. Heute ist dies geradezu umgekehrt. Immer häufiger geben Unternehmen ihre Produkte buchstäblich umsonst ab: Sie hoffen statt dessen auf langfristige Servicebeziehungen zu ihren Kunden« (<@6 Caps>Rifkin<@$p>, 2000).@1 fliess mit:Solche Mechanismen findet man an zahlreichen Stellen in der emergierenden Internetökonomie. Kostenlos zugängliche Information in Form von redaktionell erstellten Magazinen, aufbereiteten Portalen und thematischen Diskussionen generiert eine Aufmerksamkeit, die in Form von Bannern, Listen potenzieller Kunden und anderen Mechanismen an die Werbeindustrie verkauft werden kann. Information wird zur Markt<\h>einführung und zum Betatesten eine Zeit lang kostenlos angeboten, um dann Gebühren dafür zu erheben. Proprietäre Software wird als Demoversion mit eingeschränkter Funktionalität oder Laufzeit verschenkt, um zum Kauf der Vollversion zu locken. Client-Software (Web-Browser, View<\h>er und Player für Text-, Audio- und Video-Formate) wird kostenlos abgegeben, um Informationsanbieter zu bewegen, die Editier- und Serversoftware für diese Formate zu erwerben. Klassische Werbegeschenke, wie kleine Werkzeuge oder Spiele, sollen Kunden auf Webseiten locken und helfen, einen Firmen- oder Markennamen zu etablieren. Dabei handelt es sich natürlich nicht um freie Software. Weder ist hier der Quellcode verfügbar, noch darf er modifiziert werden. Doch mancher Nutzerin mag es gleich sein, ob ihre Programme von Microsoft oder von der FSF kommen, solange sie nur kostenlos sind. Auch freie Software wird in diesem Marketingsinne als Ergänzung zur Aufwertung von proprietären Produkten verwendet. So wird beispielsweise der ApacheWebserver von IBM zusammen mit dessen Serversystemen vertrieben. Für SCO-Unix gibt es die <@1 fliess kursiv>Skunkware-CD<@$p>, auf der freie Software für dieses Betriebssystem verteilt wird. Computerfachbücher und Zeitschriften enthalten als Zugabe CDs mit freier Software. <*h"mehr">Ein anderes »Geschenkmodell« benutzen Unternehmen, die den Quellcode bestehender oder neuer Software freigeben.<@3 hoch fliess>7<@$p> Gründe dafür können sein, dass sie ein neues Produkt in einem saturierten Marktsegment einführen und dazu Aufmerksamkeit generieren, dass sie Entwicklerressourcen aus der freien Szene einbinden wollen oder, dass ihr Busi<\h>ness-Plan sich nicht auf den Verkauf von Software, sondern von Dienstleistungen stützt. Es handelt sich dabei also um traditionelle Firmen, deren Aktivitäten in den Bereich der freien Software hineinragen, während umgekehrt die freie Softwareszene sich geldökonomische Tätigkeitsfelder erschließt. <*h"Standard">Werbegeschenke haben, im Gegensatz zu reiner Werbung, einen Gebrauchswert. Sie werden kostenlos abgegeben und auf anderen Wegen, letztlich vom Käufer der beworbenen Ware, bezahlt. Die freie Software wäre in Analogie das Geschenk, das die Dienstleistungen (Distribution, Support usw.) bewirbt, deren Bedingung sie zugleich ist. Die Analogie findet darin ihre Grenzen, dass Werbegeschenke in einer engen geldökonomischen Schleife von »Geschenk« und erhofftem Ertrag desjeniegen stehen, der das Geschenk vorfinanziert. Freie Software lebt jedoch weiterhin wesentlich davon, dass die Menschen Arbeit in sie stecken, die die Reproduktion ihrer Arbeitskraft anderweitig sichern können. Bei allen Ähnlichkeiten bleibt der zentrale Unterschied: Freie Software wird nicht aus pekuniären Gründen erstellt. Perry Barlow schrieb 1994 unter der Zwischenüberschrift »Information ist ihre eigene Belohnung«: @1 fliesskursiv Zitat:»Und dann gibt es da das unerklärliche Vergnügen an Information selbst, die Freuden des Lernens, Wissens und Lehrens; das seltsame gute Gefühl, wenn Information in einen herein und aus einem heraus kommt. Mit Ideen zu spielen ist ein Zeitvertreib, für den Menschen bereit sind, eine Menge Geld auszugeben, wenn man sich den Markt für Bücher und Seminare anschaut. Wir würden für solche Vergnügungen wahrscheinlich noch mehr Geld ausgeben, wenn wir nicht so viele Gelegenheiten hätten, für Ideen mit anderen Ideen zu bezahlen. Das erklärt viel von der kollektiven ›freiwilligen‹ Arbeit, die die Archive, <\h>Newsgroups und Datenbanken im Internet füllt. Seine Bewohner arbeiten also keineswegs ›umsonst‹, wie weithin angenommen wird. Sie werden vielmehr in einer anderen Währung als Geld bezahlt. Es ist eine Ökonomie, die fast vollständig aus Information besteht. Das könnte die vorherrschende Form des Handels zwischen Menschen werden, und wenn wir fortfahren, die Wirtschaft strikt auf einer monetären Grundlage zu modellieren, könnten wir gründlich in die Irre geführt werden«<@1 fliess normal> (<@6 Caps>Barlow<@1 fliess normal>, 1994).@1 fliess mit:@1 fliess ohne:Dass Menschen Dienstleistungen lieber mit Gegenleistungen als mit Geld bezahlen, motiviert auch die Tauschringe, die in vielen deutschen Städten und andernorts sprießen. Sie sind dem Phänomen der freien Software noch am ehesten verwandt in der gemeinsamen Überzeugung, dass es neben dem Geld noch andere würdige Werte gibt: »Nichts zerstört die Idee von Gemeinschaft schneller als sie mit einem Preisschild zu versehen« (<@6 Caps>Lewis<@$p> 7/2000). Auch bei »Oekonux«, einer Gruppe, die über die Ökonomie von GNU/Linux diskutiert, herrscht die einhellige Auffassung, dass sich Geld und freie Entfaltung gegenseitig ausschließen:@1 fliesskursiv Zitat:»Der Aspekt des ›Nichtkommerziellen‹ [...] oder der Entwicklung außerhalb von Verwertungszusammenhängen wie ich das nennen würde, ist IMHO [in my humble opinion; <@1 fliess normal>meiner bescheidenen Meinung nach]<@$p> der entscheidende Aspekt. [...] Wenn ›<@1 fliess normal>free beer<@$p>‹ für Verwertungsfreiheit steht, dann bin ich auch für free beer, so hat’s Stallman allerdings nicht gemeint. Warum verwertungsfrei? Nur außerhalb solcher Verwertungszusammenhänge (sprich: außerhalb von Lohnarbeit) ist wirkliche Entfaltung des kreativen Menschen möglich. Das hat auch Eric Raymond erkannt, irgendwo auf seiner Seite zitiert er Untersuchungen, aus denen hervorgeht, das Menschen viel unproduktiver sind, wenn sie ›für Geld arbeiten‹ als wenn sie ›<@1 fliess normal>for fun<@$p>‹ sich entfalten. Verwertung und Entfaltung ist ein unaufhebbarer Widerspruch!«<@3 hoch fliess>8<@$p>@1 fliess ohne:Auch wenn es ein Widerspruch ist: Während sich viele mit freier Software entfalten, verdienen einige damit ihr Geld. Eine frühe Analyse der Nutzungsschritte frei weiterverbreitbarer Software und der jeweiligen Möglichkeit von Einkünften findet sich bei Peter Deutsch (1996). Die wichtigsten Chancen für Softwarearbeiter sieht er in der Unterstützung von Anwendern bei Installation und Bedienung sowie bei Anpassung und Erweiterung. Da der Quellcode offen ist, bieten sich dazu auch für Dritte Gelegenheiten, die bei proprietärer Software auf den Hersteller und lizenzierte Partner beschränkt sind. Deutsch weist auch auf die unter Gewinngesichtspunkten widersprüchliche Position der Autoren hin. Es sei in ihrem Interesse, rasch unfertige Software mit mangelhafter Dokumentation zu verbreiten, da dadurch ihre Chancen für Dienstleistungen stiegen. Ist die Software von guter, stabiler Qualität und gut dokumentiert, sei der Bedarf nach Support minimal, besonders, da sich freie Software meist an Entwickler richte. Reine Anwendersoftware gebe es nur in sehr geringem Umfang. Tatsächlich sei aus diesen Gründen die Support<\h>industrie derzeit (1996) verschwindend klein. Ihm sei nur eine Firma bekannt, die einen jährlichen Umsatz von mehr als einer Million Dollar hat, nämlich Cygnus. Deshalb erwartete er, dass mehr Autoren versuchen werden, in Form von Shareware oder einer Doppellizenzierung mit freien <@1 fliess kursiv>Not-for-profit-<@$p> und parallelen kommerziellen Versionen eine Entschädigung zu erhalten. Aus der Sicht eines kommerziellen Distrubutors sei freie Software kein besonders profitables Geschäft. Selbst ihre Leistung, Programme ausfindig zu machen, könne durch immer ausgefeiltere Suchmaschinen überflüssig werden. Sein Fazit: »Die Erträge, die Autoren ausschließlich mit Lizenzen für ›Frei Redistibuierbare Software‹ [FRS] erzielen können, und die Art der Erwartungen der Endnutzer machen es unwahrscheinlich, dass funktional reiche endnutzerorientierte FRS anders denn als Shareware geschrieben wird. Das Modell der FRS kann jedoch für stärker auf Entwickler ausgerichtete Applikationen gut funktionieren. Es wird interessant sein, zu beobachten, wie FRS und Nicht-FRS im neu entstehenden Bereich der wieder verwendbaren Komponenten gegeneinander abschneiden« (ebd.). Die Bewegung ist inzwischen auf dem besten Wege, Deutschs erste Einschätzung zu widerlegen. Seine Frage nach den Komponentenarchitekturen bleibt jedoch weiter interessant. @1 fliess mit:Eine ähnliche Lagebeschreibung lieferte im selben Jahr Jim Kingdon (12/1996), der vorher für Cygnus und die FSF gearbeitet hatte und zu der Zeit bei Cyc<\h>lic Software beschäftigt war. Cygnus (heute Red Hat) mit damals 50 Beschäftigten und das kleinere Startup Cyclic (heute OpenAvenue<@3 hoch fliess>9<@$p>) waren die frühen Vorzeigebeispiele für erfolgreiche Geschäftsmodelle mit freier Software. Kingdon nennt außerdem das X Window-Firmenkonsortium, das damals etwa 25 Personen beschäftigte, und die FSF, für die acht Angestellte arbeiteten. Als Supportdienstleister erwähnt er Yggdrasil und als Distributor Red Hat. »Fünf erfolgreiche Wege, Geld zu verdienen, sind: Auftragsprogrammierung von Portierungen und neuen Features, Supportverträge, Schulungen, Consulting/Anpassung und Telefonsupport.« Er beschreibt sie ausführlich und gibt Beispiele aus der Zeit. Seine Folgerung: »Leute in freien Softwarefirmen können durchaus ihren Lebensunterhalt verdienen. Und freie Softwarefirmen müssen keineswegs die Gemeinschaften der Anwender und Kontributoren hemmen, die in vielen Projekten der freien Software eine so gewichtige Rolle gespielt haben.« Was die Endnutzersoftware betrifft, ist Kingdon optimis<\h>tischer als Deutsch: »Software für Computer-Hacker ist erst der Anfang« (<@6 Caps>Kingdon<@$p>, 12/1996).Seither hat die freie Software eine gewaltige Dynamik entfaltet. Heute gibt es die unterschiedlichsten Hybridformen in einer wachsenden Grauzone zwischen Open Source und E-Commerce. Auch <@1 fliess kursiv>Venture Capital<@$p> und das schnelle Geld der Börsengänge sind im Spiel. Zu den spektakulärsten Ereignissen von 1998 gehörte der oben erwähnte Deal zwischen IBM und der Apache-Gruppe, bei dem die IBM-Anwälte entsetzt fragten, wie sie denn einen Vertrag mit einer Website schließen sollten. Die Phase der Unschuld ist lange vorüber. Spätestens nach dem überaus erfolgreichen Börsengang von Red Hat im August 1999 strömte das Geld in alles, was einen Pinguin (das Maskottchen von GNU/Linux) trug. Im Dezember 1999 brach VA Linux Systems alle Rekorde, als die 30-Dollar-Aktie am ersten Tag für ganze 300 Dollar gehandelt wurde. Die Höchstmarke unter allen IPOs stand bei 606 Prozent am ersten Tag, VA trieb sie auf 697,50 Prozent. Im Januar 2000 zog TurboLinux Investitionen in Höhe von 50 Millionen Dollar von Dell, Compaq, Toshiba, NEC u.a. an, nachdem zuvor bereits Intel zugeschlagen hatte. Intel hat ebenfalls in Red Hat und SuSE investiert. Caldera Systems bekam 30 Millionen Dollar und meldete sein IPO an. LinuxCare bekam 32 Millionen Dollar von Sun. Covalent, Support-Anbieter für Apache, hat eine Finanzspritze von fünf Millionen Dollar erhalten und der kommerzielle Supporter Linuxcare eine von 30 Millionen Dollar. Zu den Investoren gehören Dell und Oracle.Börsenwerte und Risikokapitalinvestitionen sind zu einem erheb<\h>lichen Teil Psychologie. Ein anderer Indikator für das wirtschaftliche Potenzial der freien Software sind seine Marktanteile. Da freie Software ohne Registrierung installiert und kopiert werden kann, gibt es keine genauen Zahlen über die Installationen oder die Nutzer. Näherungszahlen liefern automatische Verfahren, um die benutzte Software von im <\n>Internet sichtbaren Host-Rechnern abzufragen. Nach einer Erhebung vom Juni 2001 liefen 29 Prozent aller erfassten Rechner mit öffentlichen Webseiten, etwa 8,4 Millionen, auf GNU/Linux.<@3 hoch fliess>10<@$p> Bei der Serversoftware liegt freie Software mit dem Apache bei 62 Prozent stabil auf Platz eins.<@3 hoch fliess>11<@$p> Für GNU/Linux gibt es außerdem eine freiwillige Selbstregistrierung, den <@1 fliess kursiv>Linux Counter<@$p>,<@3 hoch fliess>12<@$p> der im Juli 2001 auf über 182<\!q>000 stand. Harald T. Alvestrand schätzt aufgrund dieser Angabe und weiterer Indizien die weltweite Zahl der GNU/Linux-Nutzer auf 15 Millionen.<@3 hoch fliess>13<@$p> Zu den meis<\h>ten anderen Projekten liegen keinerlei Anhaltspunkte vor. Schließlich kann man sich mit der Frage, wieviel Geld sich mit freier Software verdienen lässt, an die Wirtschaftsberatungsbranche wenden. Eine Studie des Investment-Dienstleisters <@1 fliess kursiv>WR Hambrecht + Co <@$p>im Mai<\!q>2000 kam zu dem Ergebnis: »Unsere Schätzung: im Jahr 2003 mehr als zwölf Milliarden Dollar. Die Erträge aus dem Markt für Linux-Produkte und -Dienstleistungen werden voraussichtlich mit einer jährlichen Wachstumsrate von 90 Prozent in die Höhe schießen, von zwei Milliarden Dollar im Jahr 2000 auf mehr als zwölf Milliarden Dollar in 2003. [...] Obgleich die Ertragsmöglichkeiten, die Firmen darstellen, die sich auf Linux-Produkte und -Dienstleistungen konzentrieren, groß erscheinen mögen, sind wir der Ansicht, dass diese Schätzungen erst der Anfang sind« (Patel, 5/2000). Vor allem mit der weiteren explosiven Ausbreitung des Internet und dem Anbruch einer mobilen und Post-Desktop-Ära sieht die Studie ein mögliches lawinenartiges Anwachsen des freien Softwaremarktes voraus.Diese Erwartung mag ein Jahr später als überoptimistisch erscheinen, doch wird deutlich, dass freie Software ein Marktvolumen in einer Größen<\h>ordnung hat, die sich durchaus mit den mehr als sechs Milliarden Dollar Einnahmen messen kann, die Microsoft im Jahr 2000 erzielte. Doch hatte Stefan Meretz nicht geschrieben, dass Verwertung und Entfaltung in einem unaufhebbaren Widerspruch zu einander stünden? Und Lewis, dass nichts eine Community schneller abtöte, als sie mit einem Preis zu versehen? Dass mit einem Mal hunderte Millionen Dollar in einen sozialen, technischen, kreativen Prozess fließen, in dem Geld bislang nur eine sehr eingeschränkte Rolle spielte, wird ihn ohne Frage verändern. Die bislang zu beobachtenden Folgen waren ambivalent. Auf der positiven Seite sind Einrichtungen wie <@1 fliess kursiv>Sourceforge<@3 hoch fliess>14<@$p> von VA Linux und das <@1 fliess kursiv>Red Hat Community Center<@3 hoch fliess>15<@$p> zu nennen, mit denen Unternehmen aus der freien Software wertvolle Ressourcen für die Community zur Verfügung stellen. Problematischer scheint sich das Verhältnis zu traditionellen Softwareunternehmen zu gestalten, wie die Spannungen zwischen Corel und KDE zeigen.<@3 hoch fliess>16<@$p> Der Markt steht auf der sozialen Bewegung. Ob er auch mit ihr fällt, ob die traditionelle Softwareindustrie sich an den Prozessinnovationen und der Kultur der Hacker erneuert und welchen Weg die Bewegung einschlagen wird, wie sie sich erneuern und zugleich ihre Werte an die nächsten Generationen vermitteln kann, muss sich noch zeigen. Im Folgenden werden die wirtschaftlichen Potenziale freier Software aus Sicht der verschiedenen beteiligten Akteure betrachtet, der Anwender, der Dienstleister (Distributoren und Systemhäuser), der Autoren und der Handbuchbranche.<@3 hoch fliess>17<@$p> @2  ZÜ 2:Anwender von freier Software@1 fliess mit:@1 fliess ohne:<t-1>Hauptgewinner sind die Anwender, die den Gebrauchswert der Program<\h>me erhalten. Ob private Anwender oder Systemverwalter, sie können sich aus dem Pool freier Software selbständig bedienen und von den Vorteilen wie Flexibilität, Stabilität, Investitions- und Betriebssicherheit profitieren. Kooperative Hilfe finden sie im Internet sowie bei den Distributoren und anderen Dienstleistern. Freie Software bietet die Chance, sich von proprietären Anbietern zu emanzipieren und sich selbst zu qualifizieren. Sie fordert zum Ausprobieren und zum Lernen heraus. Dabei vermittelt sich durch freie Software neben dem Wissen über die Anwendung auch das Wissen über die zu Grunde liegende Technologie. Die Gängelung durch herkömmliche Industriepraktiken macht nicht nur eigene Modifikationen unmöglich, sondern kann schon die reguläre Anwendung eines Produktes schwierig machen, wie eine Erfahrung der Tageszeitung <x@1 fliess kursiv><t-1>taz<@$p><t-1> zeigt: »Wir hatten das Problem, dass wir unser NT-Netzwerk vernünftig back<\h>upen wollten und feststellten, dass wir überhaupt keine vernünftige Dokumentation darüber bekommen, wie ich das jetzt eigentlich machen kann. Man muss ein NDA<x@1 fliess kursiv><t-1> <@$p><t-1>unterschreiben und bekommt dann unter Umständen die Hinweise – das weiß man vorher gar nicht genau. Und wenn man dann daraus etwas entwickelt, kann ich das noch nicht einmal weitergeben.«<x@3 hoch fliess><t-1>18<x@1 fliess normal><t-1> <@$p><t-1>Dagegen erlaubt freie Software eine individuelle Anpassung und Weiterentwicklung durch eigene Mitarbeiter oder externe Dienstleister. Die Integration in bestehende Systeme und die Herstellung von Interoperabilität mit anderen freien oder proprietären Lösungen ist möglich.@1 fliess mit:Die <@1 fliess kursiv>taz<@$p> setzt offene Software seit mehr als zwölf Jahren ein, als erstes im Bereich der Mail-, News-Domain-Name- und Webserver sowie den ge<t-1>samten GNU-Tools. Die EDV läuft seit etwa 1995 unter GNU/Linux. Die System<\h>ad<t$>ministratoren und Softwareentwickler Ralf Klever und Norbert Thies begannen mit einem kleinen System von Slackware, das sie nach und nach bis zum heutigen Stand ausbauten. Den Ausschlag für GNU/Linux gab die Tatsache, dass die Firewall-Software im Quelltext verfügbar ist. Damals begann man, im Internet vorsichtiger zu werden und zwischen das eigene Firmennetz und das Internet eine Schutzmauer zu errichten. Anders als bei einem anonymen, proprietären Produkt, konnten die<@1 fliess kursiv> taz<@$p>-Entwickler hier den Quelltext überprüfen und genau feststellen, wie die Filter funktionieren.Heute laufen in der <@1 fliess kursiv>taz<@$p> alle Systeme von den Entwickler- und Administrationsarbeitsplätzen über das Archiv bis zu den Arbeitsplätzen in der Redaktion unter GNU/Linux. Etwa 140 GNU/Linux-Worksta<\h>tions sind im Einsatz. Im Textarchiv laufen jeden Tag 2<\!q>000 Agenturmeldungen ein, die direkt indiziert und in die Redaktionen verteilt werden. Das Textarchiv der <@1 fliess kursiv>taz<@$p> selbst beinhaltet die Volltexte von mehr als zwölf Jahren, da die Zeitung nahezu seit ihrer Gründung die Texte elektronisch archiviert hat. Der Fotodatenbank, die mit WAIS <@1 fliess kursiv>(Wide Area Information Servers)<@$p> indiziert wird, werden pro Tag 500 Bilder hinzugefügt. Klever betont, dass das System zuverlässig und rund um die Uhr laufe. Es müsse nur alle halbe Jahre um einige Gigabyte an Festplatten erweitert werden. Auch das Redaktionssystem ist eine Eigenentwicklung. Es ist als Intranetlösung realisiert worden und läuft als Module von Apache und als Dämonen, die im Hintergrund die Texte verwalten. Klever beziffert die Softwarekosten für einen Arbeitsplatz mit etwa 70<\!q>Mark, die sich aus den beiden letzten proprietären Produkten ergeben, die hier eingesetzt werden, die Office-Software und das Layout-System. Über freien Ersatz wird bereits nachgedacht. Die Softwarekosten pro Server belaufen sich auf exakt null Mark. Alle Programme stammen aus dem Internet und werden im Haus angepasst und erweitert. Die Stabilität und Zuverlässigkeit von GNU/Linux im Serverbereich ist nach Einschätzung der beiden <@1 fliess kursiv>taz<@$p>-Entwickler hoch. Firewall und Webserver laufen drei bis vier Monate ohne Neustart durch. Auch die Desktops seien leicht zu adminis<\h>trieren, Updates werden einfach auf die einzelnen Workstations gespielt. Neben der sehr guten Verfügbarkeit und Testmöglichkeit hebt Klever die Qualität und den guten Support für freie Software hervor. Durch ihre weite Verbreitung im Internet werden Fehler schnell bekannt und behoben. In den Newsgroups oder direkt von den Entwicklern bekomme man sehr schnell kompetente Antworten auf seine Fragen. »Das sind alles Aspekte, die uns proprietäre Betriebssysteme und Software in dieser Form nie hätten geben können.«<@3 hoch fliess>19<@$p>Zu denselben Ergebnissen, wenn auch aus einem ganz anderen Wirtschaftszweig, kommt Bernd Driesen, Leiter der Datenverarbeitung bei Babcock.<@3 hoch fliess>20<@$p> Das Unternehmen Babcock-BSH befasst sich mit der Planung und Konstruktion von Anlagen, Apparaten und Komponenten der Verfahrenstechnik vor allem in der chemischen, der Nahrungsmittel- und der Pharmaindustrie. Driesen sieht GNU/Linux als ein ideales System, um die verschiedenen Rechnerwelten des Unternehmens miteinander zu verbinden. RS/6000-Rechner mit dem IBM-Unix AIX dienen als CAD-Arbeitsplätze. Die übrigen 120 Arbeitsplätze sind Windows-PCs an einem Novell-Netzwerk. Früher mussten die CAD-Zeichnungen umständlich und fehlerträchtig per FTP zwischen den beiden Welten ausgetauscht werden. Mit Samba und heute dem Netware-Emulator »Mars« unter Linux erscheinen den PC-Nutzern die Unix-Dateisysteme einfach als ein weiteres Netzlaufwerk. Für die Installation der Windows-PCs verwendet Driesen eine einzige Linux Boot-Diskette. Darauf befindet sich ein komplettes GNU/Linux mit allen wichtigen Funktionalitäten. Dadurch, dass GNU/Linux beim Booten automatisch die richtige Netzwerkkarte findet, hat der Rechner nach dem Starten einen direkten Zugriff auf den Supportserver. Von dort wird dann eine vorbereitete Windows-Installation eingespielt. Innerhalb von etwa einer halben Stunde lässt sich so ein komplettes laufendes Win<\h>dows-System, mit allen Standardprogrammen, den wichtigsten Netzwerkdruckern, Internet-Browser usw. installieren. Ferner ist GNU/Linux bei Babcock als Router vom lokalen Netz ins Internet und als Firewall im Einsatz. Als Proxy-Server dient Squid und als Webserver im Intranet der Apache. Nachteile von freier Software hat Driesen in all diesen Einsatzgebieten noch keine festgestellt. Abstürze habe es bisher keine gegeben. Die meisten Dienste liefen monatelang absolut störungsfrei durch.<t-1>Freie Software ist kostengünstiger. Einer Studie des Wirtschaftsberatungsunternehmens Gartner zufolge betragen bei Internetprojekten die Kosten für Software<\h>lizenzen im Schnitt nur etwa zehn Prozent der Inves<\h>titionen.<x@3 hoch fliess><t-1>21<@$p><t-1> Mit anderen Worten, auch wenn die Software gebührenfrei <\h>beschafft werden kann, sind die gesamten Betriebskos<\h>ten (<x@1 fliess kursiv><t-1>Total Cost of Ownership<@$p><t-1>) natürlich nicht gleich null. Cygnus, die erste Firma, die kom<t-2>merziellen Support für GNU-Software anbot, warb entsprechend mit dem <t-1>Slogan »Wir machen freie Software erschwinglich«. Doch auch Installation und Wartung kosten weniger als bei vergleichbaren kommerziellen Produkten. Wie in jede neue Software müssen sich die Anwender zu<\h>nächst einarbeiten. Die Einstiegshürde ist hier zum Teil größer als bei proprietärer Software, doch führt die Offenheit zu einem höheren Maß an Verständnis dafür, wie die Software arbeitet. Alle folgenden Anpassungen und Entwicklungen sind daher mit weniger Aufwand verbunden. Bei einem Wechsel der Hardware oder der Plattform können die erworbenen Kenntnisse weiterverwendet werden. Das bedeutet eine Unabhängigkeit von den Herstellern und damit eine hohe Investitionssicherheit. Ein weiteres Beispiel aus der Erfahrung der <x@1 fliess kursiv><t-1>taz<@$p><t-1>:<t$>@1 fliesskursiv Zitat:»Ich muss nicht, wenn ich eine Plattform oder das Betriebssystem-<\h>Release wechsle, eine neue Lizenz vom Softwarehersteller kaufen oder habe Beschränkungen, wie sie z. B. bei Sun-Maschinen sehr üblich sind, dass sie abhängig sind von einem Motherboard, und die Software nur auf dem Board läuft. Wenn der Rechner kaputt geht, habe ich ein Problem und muss erst einmal zwei, drei Wochen hin und her faxen, bis ich eine neue Lizenz bekomme. Oder ein anderes Beispiel: Ein Hersteller von ISDN-Routing-Software teilte uns mit, als wir von [dem Betriebssystem] Sun-OS auf Solaris umgestiegen sind, dass wir unsere gesamten ISDN-Karten neu kaufen müssten, weil die alten Karten nicht mehr von ihrer Software gepflegt werden. Dabei ging es um Inves<\h>titionskosten in Höhe von 100<\!q>000 Mark. Da hätte ich, wenn ich die Sourcen gehabt hätte, gerne jemanden darauf angesetzt, der das für mich analysiert und dann die entsprechende Anpassung gemacht hätte.«<@3 hoch fliess>22<@$p>@1 fliess mit:@1 fliess ohne:Durch ihre Unabhängigkeit von Produktzyklen, Marktkonzentration und Insolvenzen bietet die freie Software eine unübertreffliche Sicherheit der getätigten Investitionen in Menschen, Hard- und Software. Die kontinuierliche Entwicklung kann durch immer neue Generationen von Programmierern aufrecht erhalten werden. @1 fliess mit:Die Angst vor der Abhängigkeit von einem einzigen Lieferanten einer Software ist für viele Anwender in Unternehmen und Behörden ein wichtiges Argument für freie Software. Diese schließt Monopolbildung aus und fördert eine lokale Infrastruktur von kleinen und mittelgroßen Dienstleistern. Weltweite Unternehmen wie IBM gehen erst seit vergleichsweise kurzer Zeit in diesen Support-Markt für GNU/Linux & Co. Wäh<\h>rend sich die Hardware-, Software- und Internetindustrie in den USA konzentriert, kann das offene Kooperationsmodell auf Basis des Internet unvergleichlich breitere kreative und produktive Ressoucen erschließen. »Die große Beliebtheit von Open Source-Software (OSS) in Deutschland und die hohe Zahl der OSS-Entwickler und -Entwicklungen in Europa sind ein deutliches Zeichen dafür, dass die innovative Kraft überall vorhanden ist. Die Zukunft der Informationstechnologie liegt nicht in einem kalifornischen Tal sondern in den Weiten des Internet« (<@6 Caps>Hetze<@$p>, 1999, S. 9).<*h"mehr">Ferner sind Anwender zu nennen, die auf Grundlage freier Software neue Märkte gründen, z. B. das <@1 fliess kursiv>Internet Service Provider<@$p>-Geschäft. Wie so oft beim Internet hat es seine Wurzeln im nicht kommerziellen Engagement. Das Individual Network (IN)<@3 hoch fliess>23<@1 fliess normal> <@$p>beispielsweise ging aus der Berliner Mailbox-Szene hervor, die 1980 das Internet kennen- und über die beiden Universitäten der Stadt nutzen lernte. Das Internet außerhalb der kostspieligen Standleitungsinfrastruktur beruhte damals auf UUCP über Wählleitungen und bot im Wesentlichen die Dienste Mail und News. 1990 schlossen die Berliner Sites eine Vertrag mit der Uni Dortmund über einen UUCP-Zugang, reservierten die Domain »in-berlin.de« und gründeten das IN-Berlin.<@3 hoch fliess>24<@1 fliess normal> <@$p>Ein Jahr darauf entstand zusammen mit drei ähn<\h>lichen Gruppen das bundesweite Indiviual Network e.V. Der Verein erwarb als Solidargemeinschaft IP-Kontigente von EUnet, DFN und anderen Netzbetreibern, um sie seinen Mitgliedern für die private, nicht kommerzielle Nutzung zur Verfügung zu stellen. In einer Zeit, als ein Internetzugang außerhalb der Universitäten kaum verfügbar und wenn, sehr teuer war, stellte der IN für viele nicht nur die einzige Möglichkeit dar, das Internet zu nutzen, sondern auch einen Ort, um Gleichgesinnte zu treffen und mehr über die Technologie zu lernen. In der Frühzeit des IN wurden NeXT- und Sun-Rechner oder K9Q-Router eingesetzt. Im Laufe der Zeit wurden beim IN-Berlin alle Rechner auf GNU/Linux umgestellt und einer auf BS<t-1>D. »Wir können uns über mangelnde Qualität der Software wirklich nicht beklagen. Wir haben unter unseren aktiven Leuten auch einige Entwickler, die auch an Linux-Projekten mitarbeiten, bsw. jemand, der hauptberuflich bei AVM arbeitet und auch ISDN-Treiber für Linux entwickelt. Von daher können wir, wenn es Probleme gibt, direkt eingreifen.«<x@3 hoch fliess><t-1>25<@$p><*h"Standard">Nicht wenige kommerzielle ISPs, wie z.B. das Berliner SNAFU, wurden von ehemaligen Mitgliedern des IN gegründet. Zu ihrer Hochzeit um 1996 hatte diese Graswurzelbewegung der Netzkultur IP-Zugänge in 84 Städten Deutschlands. Es gab knapp 60 Regional-Domains mit knapp 7<\!q>800 angeschlossenen Rechnern und schätzungsweise 70<\!q><\!q>000 Teilnehmern. Damit dürfte das IN der größte reine Internetprovider in Deutschland gewesen sein.<@3 hoch fliess>26<@$p>Eine ähnliche Geschichte ist die von Rick Adams, ehemals Betreiber des größten Usenet-Knotens der Welt und Autor von BNews, der am weitest verbreiteten Usenet-News-Software, der schließlich den kommerziellen ISP UUNET gründete:@1 fliesskursiv Zitat:»Und er hat wirklich erfunden, was uns heute als selbstverständlich erscheint: das kommerzielle Internet Service Provider-Geschäft. Wenn Leute über freie Software und Geld nachdenken, glauben sie oft, sie würden direkt Bill Gates zuarbeiten, weil er das Paradigma perfektioniert hat, Software in eine Schachtel zu stecken, sie auszuliefern, die Kundenbasis einzuschließen und durch die Upgrades an sich zu binden. Aber Rick hat dieses Modell einfach links liegen gelassen. Und er war der Erste, der sagte ›Ich werde auf der Grundlage von freier Software ein ernsthaftes Geschäft aufbauen‹. Das bestand im Wesentlichen darin, eine Dienstleistung anzubieten, die die Leute benötigten, die die Software verwenden, die miteinander sprechen, die sie verbreiten, die on<\h>line zusammenarbeiten.«<x@3 hoch fliess><t-1>27<@$p>@1 fliess mit:@1 fliess ohne:Eine Netzgeneration später waren es Firmen wie Amazon und Yahoo, die eine neue Klasse von »Infoware«-Dienstleistung<@3 hoch fliess>28<@1 fliess normal> <@$p>auf Grundlage freier Software entwickelten.@1 fliess mit:@2  ZÜ 2:Erstellung freier Software@1 fliess mit:@1 fliess ohne:Auf die Motivlagen freier Entwickler wurde oben bereits eingegangen. Es hatte sich gezeigt, dass hier kreative, kulturelle, soziale und Lern<\h>aspekte an die Stelle einer pekuniären Entlohnung treten. Auch hierin steht die Bewegung in einem größeren Trend zur Neubewertung der <\n>s<t4>ozialen, kulturellen, kreativen Arbeit außerhalb der Lohnarbeit, w<t$>ie <\n>Szyperski sie benennt:@1 fliesskursiv Zitat:»Wir leben, oberflächlich betrachtet, in der so genannten Freizeitgesellschaft. Wenn Sie sich mal ein Jahr vorstellen, dann haben wir 8<\!q>760 Stunden zur Verfügung. Die Erwerbstätigkeit in Deutschland macht nur noch 1<\!q>900 Stunden aus – im Gegensatz zu Japan, wo es etwa noch 2<\!q>300 sind, aber irgendwo um diese Größe pendelt sich das in der Welt ein. Wenn Sie nun davon ausgehen, dass wir zehn Stunden am Tag für Schlaf, Ruhe, Speisen, Pflege brauchen, dann bleiben insgesamt 3<\!q>210 Stunden im Jahr übrig. Was ist das für eine Zeit? Wollen wir die jetzt <t-1>wieder in die Erwerbszeit integrieren? Wollen wir sie nur in die Schlaf-, <t$>Pflege- und Entertainment-Zeit integrieren? Oder – und das ist ein Vorschlag, der sehr intensiv z.<\!q>B. von Amitai Etzioni im Zusammenhang mit der Reaktivierung des Kommunalverständnisses und der kommunalen Aktivitäten diskutiert wird –, wollen wir die Zeit investieren, um für unsere Gemeinschaften konstruktiv Beiträge zu leisten, die nicht in irgendeiner Form über Geld abgerechnet werden? Das sind Sozial<\h>leistungen, das sind Sportleistungen, das ist aber auch die Selbstverwaltung unserer Organe, das ist das Lehren und Lernen außerhalb der geordneten Schulen, das ist aber auch das Erziehen der Kinder, das ist das Machen der Familie und ich weiß nicht, was alles. Das heißt, wenn wir so einen großen Teil der verfügbaren Zeit ›Freizeit‹ nennen, dann miss<\h>deuten wir schon ihre Funktion. Darum gibt es Bemühungen, das als ›Sozialzeit‹, ›Gemeinschaftszeit‹ oder wie auch immer – mir ist da <\n>jedes Wort lieb – zu bezeichnen.«<@3 hoch fliess>29<@$p>@1 fliess mit:@1 fliess ohne:Neben der unbezahlten freiwilligen Arbeit gibt es auch Firmen, die sich auf die Entwicklung freier Software spezialisieren. Die mehrfach genannte Firma Cygnus Solutions war eine der ersten. 1989 wurde sie in Kalifornien gegründet und portierte vor allem den GNU C Compiler und andere GNU-Entwicklerwerkzeuge auf neue Plattformen. Diese Ports verkaufte sie zusammen mit Support. Wiederum wird die Dienstleistung bezahlt, das Ergebnis ist, wie es die GPL vorschreibt, frei. Mit zu besten Zeiten mehr als 100 Softwareingenieuren war Cygnus die erste Adresse für die Weiterentwicklung des gcc, des GNU Debuggers und verwandter Werkzeuge und damit auch die erste Adresse für Firmen, die diese einsetzten und Beratung benötigten. GNUPro, ein Werkzeugkasten zum Konfigurieren und Testen, wurde von Cygnus entwickelt und liegt heute auf etwa 200 Plattformen vor. @1 fliess mit:Ab 1995 konzentrierte sich die Fir<\h>ma vor allem auf eingebettete Echtzeitbetriebssysteme, ein stark fragmentierter neuer Markt mit Hunderten von Produkten, von denen keines einen nennenswerten Marktanteil hat. Mit eCos (<@1 fliess kursiv>embedded Cygnus operating system<@$p>)<@3 hoch fliess>30<@$p> schuf es ein konfigurierbares und portierbares System, das viele Anwendungsgebiete dieser kleinen und schnellen Betriebssysteme abdeckt. Beim 1998 vorgelegten eCos verließ Cygnus seine Nähe zum GNU-Projekt und stellte es, statt unter die GPL, unter eine von der <@1 fliess kursiv>Netscape Public License<@$p> abgeleiteten Lizenz.<@3 hoch fliess>31<@$p> Die Fima hatte Vertretungen in Nordamerika, Japan und England und wurde seit 1997 in der <@1 fliess kursiv>»Software Magazines List«<@$p> der führenden 500 Softwareunternehmen aufgeführt (vgl. <@6 Caps>Tiemann<@$p>, 1999). Im November 1999 wurde Cygnus für 674 Millionen Dollar von Red Hat aufgekauft.<@3 hoch fliess>32<@$p>Zahlreiche weitere Firmen sind dem Vorbild gefolgt und setzen entweder vollständig oder in einem Teil ihres Angebots auf freie Software. Sie entwickeln Programme aus bestehender freier Software oder integrieren Bestandteile freier Software in ihre Programme, die aus lizenzrechtlichen Gründen ihrerseits frei sein müssen. Die Einbindung von Bibliotheken in proprietäre Software ist nach dem FSF-Modell nur dann zulässig, wenn sie unter der Lesser GPL stehen. Freie Compiler wie der gcc, und andere Tools wie CVS oder Free<\h>CASE werden ebenfalls in der freien Softwareentwicklung eingesetzt. In dem Fall kommt es nicht zu einer Lizenzvererbung, d.h. die entstehenden Produkte dürfen proprietär sein. Entsprechend ist der Arbeitsmarkt in diesem Bereich gewachsen. Dem kommt entgegen, dass freie Software häufig in Forschung und Lehre eingesetzt wird, akademische Informatiker also meist fundierte Erfahrungen in die Arbeitswelt mitbringen. Und um Entwickler, die sich in der freien Szene verdient gemacht haben, reißen sich die Unternehmen ohnehin. Firmen der freien Software, die in der ersten Hälfte der 90er-Jahre, oft von Studenten, gegründet wurden, bauten auf Idealismus und eine gute Idee. Auch die Manager dieser Firmen waren Anfänger. Sie starteten klein, und mit der Zahl der Anwender sind auch die Erfahrungen und Komptenzen gewachsen. Heute, so Rudolf Strobl, sind Kenntnisse über Businesspläne, betriebswirtschaftliche Auswertungen und Marketing wesentliche Voraussetzungen für den Erfolg: »Erfolgsfaktoren waren früher technisches Know-how und viel Enthusiasmus. Heute würde ich sagen, sind es gute Managementfähigkeiten, und Ideen und Kapital werden immer wichtiger. [...] Früher finanzierte sich das Firmenwachstum aus dem Umsatz. Die Geschwindigkeit wäre heute zu gering, um ein <@1 fliess kursiv>major player<@$p> zu werden. Kapital ist auf alle Fälle ein Thema.«<@3 hoch fliess>33<@$p> Genau hier setzte Strobls 1998 gegründete New Technologies Management GmbH (NTM) an. Strobl, der seit Anfang der 90er-Jahre mit diversen Firmen wie ARTICON, dem Linux-Magazin, Cybernet und Linuxland im GNU/Linux-Bereich aktiv ist, möchte mit NTM junge Firmen dabei unterstützen, ihren Markt zu finden. Heute gibt es in der freien Software bereits arrivierte Firmen. Außerdem migrieren Firmen aus dem Windows-Bereich, wo die Konkurrenz immer schon da war, zu GNU/Linux. Ein Start bei null ist somit schwieriger als Mitte der 90er-Jahre. Besonders die Finanzierung einer Neugründung ist eine Herausforderung. Eine Eigenfinanzierung ist nur in selte<t-2>nen Fällen möglich. Banken reagieren oft mit Unverständnis auf die neuen <t$>Geschäftsmechanismen in diesem Sektor oder doch mit Zurückhaltung, da sie die Entwicklung auf dem GNU/Linux-Markt nur schwer einschätzen können. Deutsche Banken agieren nach Einschätzung <\h>Strobls hier noch deutl<t-1>ich konservativer als amerikanische. Öffentliche Fördermittel erscheinen <t$>zunächst als ein attraktiver Weg, zumal der politische Wille, freie Software zu unterstützen, in den vergangen Jahren deutlich gewachsen ist. So besteht z. B. die Möglichkeiten einer Förderung durch die EU im Rahmen des <@1 fliess kursiv>Information Society Technologies<@$p>-Programmes (IST) mit einem Gesamtvolumen von 6,3 Milliarden Euro.<@3 hoch fliess>34<@1 fliess normal> <@$p>Hier sieht Strobl das Problem in der langen Antragsdauer und in der Auflage, dass das Projekt noch nicht angefangen haben darf, wenn man den Antrag stellt. In einem so dynamischen Markt wie dem von GNU/Linux, bedeutet eine Wartezeit von sechs bis acht Monaten, bis über den Antrag entschieden wird, dass man mit seiner Idee meistens bereits zu spät kommt. Zudem werden auch Fördermittel über eine Bank ausgezahlt, die sicherstellen muss, dass das Geld wie<t-2>der zurückfließt. Die Kapital suchende Startup-Firma steht also wieder vor denselben Schwierigkeiten, wie bei einer direkten Bankenfinanzierung.<t$>Am Erfolg versprechendsten am Ende der 90er sei, so Strobl, eine Finanzierung über <@1 fliess kursiv>Venture Capital <@1 fliess normal>gewesen<@$p>. Wagniskapitalinvestoren (VC) sind spezialisiert auf die <t-1>Probleme bei Firmengründungen, z.B. dass die erwarteten Umsätze nicht <t$>eintreffen und Kapital nachgeschossen werden muss. Desweiteren sind VC typischerweise auf <@1 fliess normal>Hightech<@$p>-Unternehmen wie Internetdienstleister spezialisiert und mit den Bedingungen des neuen Marktes vertraut. Schließlich bringen sie ihre Firmen an die Börse, weil sie ihr Geld zurückgewinnen wollen. Unter den VC gibt es solche, die die Geduld für diesen neuen Markt aufbringen, aber auch solche, die möglichst rasch die Mehrheit einer Firma bekommen wollen. Letztere drängen häufig die Gründer aus den Managementpositionen, wenn die Umsätze nicht schnell genug eintreffen oder die Richtung nicht zu stimmen scheint. Oft geht durch solche erzwungenen Umbesetzungen an der Spitze die Kreativität der Firma, das geistige und kreative Potenzial verloren. Hier engagierte sich die New Technologies Management GmbH, um Linux-Startups auf die Beine zu helfen. Schließlich gibt es noch Unternehmen wie Sun, Netscape und IBM, die ihre konventionell entwickelte Software quelloffen machen und mit Arbeitskraft aus ihrem Haus weiterentwickeln. Netscape erläutert, welchen Gewinn es sich davon verspricht: @1 fliesskursiv Zitat:»Netscape (und der Rest der Welt) profitiert auf eine Reihe von Wegen davon, seinen Quellcode zu verschenken. Zunächst und am Wichtigs<\h>ten von der erwarteten Explosion kreativen Inputs in die Quellcodebasis des Communicators. Netscape ist zuversichtlich, dass dies dem Communicator erlauben wird, seine überragende Position unter der Kommunikationssoftware zu erhalten. Zweitens bewirkt die Befreiung des Quellcodes, dass er an Orte und in Produkte gehen kann, in die Nets<\h>cape nie die Mittel und die Zeit gehabt hätte, ihn selbst zu bringen. Damit wird Internetzugänglichkeit auf das ganze Spektrum von Hardwareplattformen und Softwareprodukten ausgeweitet.«<@3 hoch fliess>35<@$p>@1 fliess ohne:Die WR Hambrecht-Studie stellt fest: »Der Markt für Linux-Anwendungen ist noch im Entstehen begriffen. Wir glauben jedoch, dass die Möglichkeiten, die sich daraus ergeben, dass proprietäre Softwareunternehmen den Quellcode für einige oder alle ihrer Produkte öffnen, ein bedeutendes, einschneidendes Ereignis für die Softwarewelt darstellen könnten. Das Ergebnis könnte Software sein, die zuverlässiger und den Anforderungen und Bedürfnissen der Endnutzer zuträglicher ist« <\h>(<@6 Caps>Patel<@$p>, 2000).@1 fliess mit:@2  ZÜ 2:Dienstleister rund um freie Software@1 fliess mit:@1 fliess ohne:<t-1>Viele der freien Projekte haben keinerlei Bedenken dagegen, dass ihre Software kommerziell verwertet wird. Auch hier setzte die <x@1 fliess kursiv><t-1>Free Software <\h>Foundation <@$p><t-1>Zeichen. Die GPL ist (im Gegensatz zu den <x@1 fliess kursiv><t-1>Not-for-profit-<\h>Lizenzen<@$p><t-1>) dazu gedacht, den Einsatz freier Software in der Wirtschaft zu ermöglichen. Ausdrücklich erlaubt sie, wenn auch nicht für die Software selbst, so doch für Dienstleistungen wie Distribution eine Gebühr zu erheben. Auch die FSF selbst hat von Beginn an einen Teil ihrer Arbeit aus dem Verkauf von Distributionen auf Datenbändern und dann CDs finanziert. Dennoch besteht ein Spannungsfeld zu denjenigen, die den Geld<\h>erwerb durch Dritte ablehnen, das z. B. auf der Linux-EXPO in Durham 1998 zur Sprache kam. Doch »die einzigen, die sich wirklich aufgeregt haben, waren Reporter, die meinten, da muss doch ein Konflikt sein«.<x@3 hoch fliess><t-1>36<@$p><t-1> Im Gegenteil freuen sich viele Entwickler, wenn Firmen ihre Software verkaufen und die Verbreitung damit erhöhen. Dankbar sind sie dafür, dass Distributoren das Packaging, die Produktion von Handbüchern und den Support übernehmen und so die Entwickler entlasten, die sich dadurch auf das konzentrieren können, was ihnen Spaß macht: das Entwickeln. @1 fliess mit:<t-2>Mit der wachsenden Popularität freier Software steigt auch die Nachfrage nach Dienstleistungen wie Distribution, Beratung, Installation, Support, Wartung, Schulung und Gewährleistung. In den Unternehmen, in denen bislang die Techniker hausintern und unter der Hand freie Systeme eingeführt hatten, wächst seit 1999 die Akzeptanz auch im Management. In der <x@1 fliess normal><t-2>WR Hambrecht-Studie<@$p><t-2> ist zu lesen: <t$>»Laut einer Erhebung der <@><f"FFScala-Italic">›Information Week‹<@$p> unter 300 IT-Managern wird Linux nicht mehr nur für das Betreiben von Webservern und E-Mail eingesetzt, sondern zunehmend auch in Bereichen wie Systemverwaltung, dünne Server und selbst Ressourcenplanung in Unternehmen. Diese Daten sind bedeutsam, zeigen sie doch, dass Linux wahrhaft an Legitimation gewinnt, da es in kritischen Unternehmensbereichen eingesetzt wird«<t-2> (<x@6 Caps><t-2>Patel<@$p><t-2>, 5/2000).<t$>Damit entsteht eine Nachfrage auch nach externen Dienstleistungen, z.<\!q>B. der Entwicklung maßgeschneiderter Systemlösungen für Kunden und der Auftragsentwicklung neuer freier Software. Linux-biz.de listet über 160 Firmen in ganz Deutschland, die sich auf Dienstleistungen für GNU/Linux spezialisiert haben.<@3 hoch fliess>37<@$p> Auch die Tatsache, dass Unternehmen wie IBM und HP ein flächendeckendes Service-Angebot für GNU/Linux aufbauen, lässt darauf schließen, dass hier erhebliche Marktchancen gesehen werden. Tatsächlich machen Dienstleistungskosten den Löwenanteil im IT-Bereich aus. Eine <@1 fliess normal>Gartner-Group<@$p>-Studie über Softwareinvestitionen von 1998 ergab, dass ungefähr 80 Prozent aller Internet- Investitionen in den Service (<@1 fliess kursiv>Professional Services, Consulting, Tech-Support <@$p>usw.) fließen und nur 20 Prozent zu gleichen Teilen in das Anlagevermögen, also Hard- und Software.<@3 hoch fliess>38<@$p> Patrick Hausen vom BSD-Projekt erläutert den Zusammenhang von unbezahlter Entwicklung in den Projekten und bezahlter in der Dienstleistungsbranche:@1 fliesskursiv Zitat:<t-2>»Ich habe vielleicht zehn Entwickler, die diese Apache-Software für Gottes<\h>lohn schreiben. Ich habe 100<\!q>000 Anwender. Dann brauche ich für diese 100<\!q>000 Anwender 1<\!q>000 Consultants, von denen jeder 100 Anwender betreuen kann. D.h., ich habe 1<\!q>000 Consultants, die davon leben können, und ich habe 100<\!q>000 Anwender, die durch den Einsatz dieses kos<\h>tenlosen Webservers mehr Geld für vernünftiges Consulting ausgeben können und letztendlich ja auch wieder etwas tun mit dieser Software, was hoffentlich in der einen oder anderen Form Umsatz generiert.«<x@3 hoch fliess><t-2>39<@$p>@1 fliess ohne:Über den Dienstleistungsmarkt äußert sich die WR Hambrecht-Studie am optimistischsten: »Die vielleicht bedeutendsten Gelegenheiten im Linux-Markt stellen Linux-Dienstleistungen, Support und Schulung dar. Wir sagen Erträge von fast 500 Millionen Dollar im Jahr 2000 voraus, die mit einer jährlichen Zuwachsrate von 100 Prozent auf beinah vier Milliarden Dollar in 2003 ansteigen werden. Auch wenn traditionelle Unternehmen wie IBM und Hewlett Packard begonnen haben, Linux-Beratung und Support anzubieten, ist diese Marktgelegenheit noch weit gehend unerschlossen« (<@6 Caps>Patel<@$p>, 2000).@2  ZÜ 3:Systemhäuser, Hard- und Softwarehersteller@1 fliess ohne:In den Anfangsjahren von GNU/Linux waren die verschiedenen Dienstleistungssegmente noch nicht ausdifferenziert. 1992 – der erste stabile Linux-Kernel war gerade erschienen – setzten sich die Enthusiasten Sebastian Hetze, Dirk Hohndel, Olaf Kirch und Martin Müller zusammen, um das »Linux Anwenderhandbuch« (LHB) zu schreiben. Einen Verlag fanden sie dafür nicht, da noch niemand von GNU/Linux gehört hatte. So gründeten sie ihren eigenen, die LunetIX Müller & Hetze GbR,<@3 hoch fliess>40<@$p> und ein Jahr später stand das LHB in den Bestsellerlisten der Computerliteratur vor Titeln zu Microsoft-Produkten wie Word und Excel. Aus dem Verlag wurde schnell ein Linux-Systemhaus. Noch im selben Jahr stellte es seine erste GNU/Linux-Distribution, die LunetIX Software-Distribution (LSD) zusammen und vertrieb sie über die Lehmanns-Buchhandlungen und kurz darauf auch über eMedia des Heise Verlags.@1 fliess mit:<*h"mehr">LunetIX bot auch von Anfang an Support für GNU/Linux an, doch gab es in den ersten Jahren dafür fast keinen Bedarf. Zwar gab es auch in dieser Zeit bereits Unternehmen, die Linux einsetzten, doch waren es in der Regel firmeninterne Systemadministratoren und Entwickler, die Projekte mit freier Software realisierten. Diese Art von <@1 fliess kursiv>bottom-up <@$p>Verbreitung bedeutete, dass keine Etats bereitstanden, um Dienstleistungen von außen einzukaufen. Die Praktiker in den Unternehmen mussten sich selbst behelfen und konnten das auch dank der Unterstützung, die sie im Internet fanden. Inzwischen hat die Popularität von GNU/Linux auch das Management erreicht, das jetzt entsprechende Etats für externe Dienstleister vorsieht. Im Frühjahr 2000 wurde LuneIX zu einem Teil der Linux Information Systems AG.<@3 hoch fliess>41<@$p> Während Hetze im reinen Distributionsgeschäft nur begrenzte Entwicklungsmöglichkeiten sieht, hält er die individuelle Anpassung an Kundenerfordernisse, die Neuentwicklung im Kundenauftrag, Wartung, Schulung und andere Dienstleistungen für die Erfolg versprechendsten Geschäftsmodelle für freie Software. Sie seien gegen<\h>über proprietärer Software für den Kunden günstiger, »weil wir die Wiede<t-1>rverwendung von freier Software mit einkalkulieren können. Zudem ist es möglich, unter bestimmten Umständen zusätzliche Ressourcen durch kooperative Entwicklung, die von mehreren Unternehmen getragen wird, und auch von freien Entwicklern – wenn man in der Lage ist, daraus ein richtiges Open Source-Projekt nach dem Basar-Stil zu entwickeln –, einzubinden und weiterhin für den Kunden Kosten zu sparen«.<x@3 hoch fliess><t-1>42<@$p><*h"Standard">Hetzes Vision für die Zukunft der freien Software in Deutschland ist, dass sich eine Vielzahl kleiner Firmen und selbständiger Entwickler he<\h>rausbildet, die untereinander kooperieren können, aber auch mit dem <\n>Basar der freien Szene, um auch große Projekte im industriellen Maßstab durchzuführen. »Das ist mein Entwicklungsmodell, meine Vorstellung für eine Integration von wirtschaftlichem Arbeiten mit Open Software und der Entwicklung von Open Source-Software, die an erster Stelle eben nicht mit kommerziellen Interessen verbunden ist.«<@3 hoch fliess>43<@$p>Für die zweite Gründergeneration in Deutschland steht die Linux-Dienstleistungsfirma Innominate,<@3 hoch fliess>44<@$p> die im Mai 1997 von den beiden Informatikstudenten Raphael Leiteritz und Sascha Ottolski gestartet wurde. Ein Jahr darauf wurde sie Berliner Landessieger im Startup-Wettbewerb von Stern, Sparkasse und McKinsey. Innominate war auch die erste deutsche Linux-Firma, die <@1 fliess kursiv>Venture Capital <@$p>bekam, nämlich von der BMP AG. Sie versteht sich als <@1 fliess kursiv>Allround<@$p>-Systemhaus für GNU/Linux und BSD und bietet die gesamte Palette von Dienstleistungen an, von Schulung über Support bis zur Entwicklung von kundenspezifischen Lösungen. Zu der bei Innominate entwickelten Software gehört der Lingo-Kommunikations-Server auf GNU/Linux, der kleinen und mittleren Unternehmen, die kaum IT-Know-how im Haus haben, eine kostengünstige, stabile Lösung bietet. Zu freien Projekten hat die Firma Bug-Reports, Feature-Vorschläge und einige kleine Patches beigesteuert. Eine Vermittlerrolle zwischen Anbietern und Anwendern spielt sie durch die Website www.linuxbiz.de, die die Firma zusammen mit dem Linux-Verband (LIVE) betreibt. Hier können sich kommerzielle Linux-Anbieter kostenlos eintragen lassen, und der Kunde kann sehen, wo er in seiner Nähe GNU/Linux-Support findet.Zunehmend sind auch mittlere und große Firmen an Innominate herangetreten, die GNU/Linux-Lösungen einsetzen möchten. Ihre etwa 75 Kunden zeigen das Interesse über eine Vielfalt der Branchen hinweg. Darunter befanden sich die Karl Blatz Gruppe, Produzent der Kinderfernsehsendungen »Bibi Bloxberg« und »Benjamin Blümchen«, das Deutsche Institut für Normung (DIN), das Deutsche Institut für Bautechnik, der Internet-Serviceprovider X-Online und die Bundesdruckerei.@1 fliesskursiv Zitat:»Für diese Firmen ist der Kostenfaktor bei der Anschaffung der Software überhaupt kein Thema mehr. Sie interessiert die <@1 fliess normal>total cost of ownership<@$p>, d.h.: ›Was kostet mich die Software im Laufe des Einsatzes?‹ Und da ist der Kaufpreis nur noch ein verschwindend geringer Bestandteil. Diese Firmen sehen, dass Linux sehr wartungsarm und sehr administrationsarm ist und damit Kosten eingespart werden können. Diese Firmen sind an der Offenheit des Systems interessiert, an der Reaktionsschnelligkeit, wenn es Probleme gibt, wenn Fehler auftauchen und natürlich an der Sicherheit, die Linux einfach auch mit bietet.«<@3 hoch fliess>45<@1 fliess normal>@1 fliess mit:@1 fliess ohne:Anbieter von proprietärer Soft- und Hardware setzen freie Software als Basistechnologie ein. Für Cobalt Micro war GNU/Linux der Schlüssel, mit dem es den Servermarkt für eine neue Hardwareplattform öffnen konnte. Silicon Graphics ersetzt seine Unix-Variante Irix durch GNU/Linux und hofft auf diese Weise, Synergieeffekte nutzen zu können. Zu den Firmen, die GNU/Linux-Hardware vertreiben, gehören VA Linux, Cobalt, Atipa, aber auch traditionelle Unternehmen wie IBM, Compaq und Dell. Die <@1 fliess normal>WR Hambrecht-Studie<@$p> kommt zu der Einschätzung: »Linux-Hardwareunternehmen vertreiben die entscheidenden Systeme, auf denen die wachsende Zahl der weltweiten Webseiten, E-Mails und internen Informationsnetzen läuft. Wir erwarten, dass die Erträge aus Linux-Hardware von beinah 1,2 Milliarden Dollar im Jahr 2000 mit einer jährlichen Wachstumsrate von 83 Prozent auf über 7,7 Milliarden in 2003 anwachsen werden. [...] Der Linux-Softwaremarkt befindet sich in seinem frühen Stadium von Wachstum und Reife. Die Einnahmen aus dem Verkauf von Linux-Client- und Server-Betriebssystemen werden voraussichtlich von 160 Millionen Dollar im Jahr 2000 mit einer jährlichen Wachstumsrate von 67 Prozent auf mehr als 700 Millionen in 2003 ansteigen.«<@1 fliess normal> (<@6 Caps>Patele<@1 fliess normal>, 5/2000)@1 fliess mit:Ein Marktsegment, das heute noch in den Kinderschuhen steckt, auf das sich aber große Erwartungen richten, ist das für eingebettete Systeme. Die Studie sieht hier ein Potenzial, das den gesamten derzeitigen <\n>Linux-Markt als winzig erscheinen lässt. Auf ähnliche Weise ist freie Software für Anbieter von standardisierten Branchenlösungen interessant. Die Betriebssystemplattform und die Softwareumgebung sind für die Kunden unwesentliche, aber eventuell kostenrelevante Nebenleistungen des eigentlichen Geschäfts. Hier kommen ebenfalls GNU/Linux und die freien BSD-Derivate zum Einsatz. Aus demselben Grund bieten sie sich auch als Basis für kundenspezifische Softwareentwicklung an. Softwarehäuser finden dazu eine Fülle wiederverwendbarer Bausteine und können unter Umständen sogar aus dem kooperativen freien Entwicklungsprozess Nutzen für ihr jeweiliges Projekt ziehen. @2  ZÜ 3:Distributoren@1 fliess ohne:Alle freie Software lässt sich nahezu kostenlos aus dem Internet herunterladen. Warum sollte also jemand Geld dafür ausgeben? Bezahlen lassen sich die Distributoren nicht für die Software, sondern für die Zeit, die sie investieren, um sie aufzubereiten. Distributionen sind Sammlungen freier Software auf CD-ROM (früher auf Disketten). Sie ersparen dem Anwender also zunächst die Download-Zeit, die bei den fast 20 CDs, die eine GNU/Linux-Distribution heute umfasst, erheblich sein kann. Dann wählen sie die neuesten stabilen Versionen der Programme aus, achten darauf, dass alle erforderlichen Komponenten vorhanden sind und die Programme untereinander zusammenarbeiten, und versehen das Ganze mit einem Installationsprogramm, das dem Anwender die Konfigurierung erleichtert. Dazu gibt es ein Handbuch und Telefonberatung. Neben den GNU/Linux-Distributoren wie Red Hat und SuSE bieten auch Firmen wie PrimeTime Freeware oder Walnut Creek Sammlungen freier Software an. @1 fliess mit:Die SuSE GmbH ist 1992, im selben Jahr wie LunetIX, von vier Studenten in Nürnberg mit dem Ziel gegründet worden, Auftragsprogrammierung anzubieten. Ihre erste GNU/Linux-Distribution kam im Frühjahr 1993 auf den Markt. Damit ist SuSE einer der ältesten Anbieter kommerzieller GNU/Linux-Distributionen weltweit. Das US-amerikanische Red Hat hat ungefähr ein Jahr später angefangen. Im Juli 1999 standen unter der SuSE Holding AG neben dem Stammhaus in Nürnberg die SuSE München GmbH, die SuSE Rhein-Main AG (Vorstand: Dirk Hohndel), die SuSE Inc. in Oakland, Kalifornien sowie der Verlag SuSE Press, eine Werbeagentur und die SuSE-Labs. Letztere bieten <@1 fliess kursiv>Third Level Support <@$p>für Großkunden, d.h., technische Unterstützung und <@1 fliess kursiv>Bug-Fixes <@$p>auf <\h>Source Code-Ebene mit garantierten Reaktionszeiten rund um die Uhr. Neben Distributionen und Dienstleistungen für Privat- und Firmenkunden vertreibt SuSE als VAR (<@1 fliess kursiv>Value Added Reseller<@$p>) auch GNU/Linux-Komplettsysteme auf Rechnern von Herstellern wie IBM, Compaq, Siemens oder SGI. Die Firma, die seit Beginn ihres Bestehens von Jahr zu Jahr um etwa 100 Prozent gewachsen ist, erzielte im Geschäftsjahr 1998/99 zirka 14 Millionen US-Dollar und war damit der größte Umsatzmacher weltweit im GNU/Linux-Umfeld. SuSE beschäftigte zirka 150 Mitarbeiter weltweit. Davon arbeiteten gut 60<\!q>Prozent in der Technik, im <@1 fliess normal>Support<@$p> und im Servicebereich.SuSEs Kernkompetenz liegt in der Weiterentwicklung von Linux. »Open Source« bedeutet für SuSE, dass alle Software, die von der Firma entwickelt wird, wo immer es möglich sei, an die Entwicklergemeinde zurückfließe. Tatsächlich ist SuSE jedoch im Vergleich zu Konkurrenten wie Mandrake und Red Hat die geschlossenste Distribution. Zentrale Komponenten wie das Konfigurationswerkzeug YaST und die Initialisierungsskripte der Firma waren proprietär. Die YaST-Lizenz von 2000 gewährt zwar die Freiheit seiner Modifikation und kostenlosen Verbreitung, nicht jedoch die der Verbreitung gegen Entgelt. <*h"mehr">Harald Milz, Vorstand der SuSE München GmbH, sieht die Zukunft der Computerindustrie darin, dass die Hardware immer billiger werde, bis sie zur <@1 fliess kursiv>Commodity<@$p>, zur Massenware geworden ist und es möglich sei, die Hardware bei einem Kundenprojekt oder beim Kauf eines Rechnersystems fast kostenlos mitzuliefern. Die Software werde nach seiner Auffassung den gleichen Weg gehen. Zumindest die Betriebssystem-Software und ein großer Teil der <@1 fliess kursiv>Middleware<@$p> und der Systemadministrationswerkzeuge werde durch den Preisdruck und durch die Stückzahlen immer billiger. Open Source sei somit die einzig mögliche Strategie auf dem IT-Markt. <@3 hoch fliess>46<@$p><*h"Standard">Distribution als Massenmarkt wird auf absehbare Zeit ihre Bedeutung behalten. Tim O’Reilly führt den Gründer und CEO von Red Hat an: »Bob Young pflegt zu sagen: ›Wir sind keine Softwarefirma. Wir sind eine Verkaufs-, Marketing- und Vertriebsfirma, genauso wie Dell.‹ Ob Red Hat tatsächlich als Gewinner aus dem sich abzeichnenden Linux-Distributionskrieg und dem wahrscheinlich nächsten großen Wall Street-Beuterausch hervorgehen wird oder nicht, ist nebensächlich. Tatsache ist, dass es hier eine Geschäftsmodellnische gibt.«<@3 hoch fliess>47<@$p>Auch Hersteller von proprietärer Software wissen den nahezu kos<\h>tenlosen Distributionseffekt für sich zu nutzen. So gibt Corel seine Textverarbeitung »Word Perfect« für GNU/Linux und Star Division (heute Sun) sein Office-Paket »StarOffice« für persönlichen Gebrauch kostenlos ab.<@3 hoch fliess>48<@$p> Beide sind auf den großen Linux-Distributionen enthalten. Gleichzeitig lässt sich Corel den Einsatz seines Textverarbeitungssystems in Unternehmen und Behörden bezahlen. Das hat zwar nichts mit <@1 fliess kursiv>free software<@$p> und mehr mit <@1 fliess kursiv>free beer<@$p> zu tun, zeigt aber, dass ein anderes Businessmodell dazu führen kann, dass der <@1 fliess kursiv>de facto<@$p>-Status – freies Kopieren im Privatbereich – legalisiert wird. Selbst Microsoft steht in dem Ruf, sich bewusst zu sein, dass eine gewisse Durchlässigkeit der Copyright-Regeln mehr nützt als schadet.@2  ZÜ 3:Projekt-Hosting und Portale@1 fliess ohne:<t-1>Ein weiteres neues Businessmodell ist direkt aus dem Kooperationsmodell der freien Software hervorgegangen. Unternehmen wie Andover.Net und Collab.Net<\!q><x@3 hoch fliess><t-1>49<x@1 fliess normal><t-1> <@$p><t-1>betreiben seit 1999 Sites, auf denen Käufer und Anbieter von Open Source-Projekten einander finden können. Unternehmen schreiben hier gleichsam Programmieraufträge samt Honorierung aus, freie Entwickler und Firmen können sich darum bewerben. Andover.Nets <\h>»<x@1 fliess normal><t-1>QuestionExchange«<x@3 hoch fliess><t-1>50<x@1 fliess normal><t-1> <@$p><t-1>verwendet dafür ein Auktionssystem. Collab.Nets <t$>»<@1 fliess normal>sourceXchange«<@3 hoch fliess>51<@$p> und »<@1 fliess normal>EarthWeb.com«<@$p> bieten freien Projekten und »umzäunten Communities« Serverplatz und Marktplätze für Support. Profite machen diese Firmen mit Werbeeinnahmen und mit Transaktionsgebühren. Das bereits genannte <@1 fliess normal>SourceForge<@3 hoch fliess>52<@$p> mit seinem kostenlosen <@1 fliess kursiv>Hosting<@$p> von freien Projekten wird zwar im Wesentlichen vom <@1 fliess kursiv>Value Added Reseller<@$p> VA Linux<\!q><@3 hoch fliess>53<@$p> finanziert, greift jedoch auch auf Werbebanner zurück. Die Nachrichten- und Meinungsportale wie Freshmeat.net, Slash<\h>dot.org, Linux Weekly News<@3 hoch fliess>54<@$p> oder das Linux-Magazin<@3 hoch fliess>55<@$p> finanzieren sich gleichfalls über Anzeigen. Die Einschätzung der <@1 fliess normal>WR Hambrecht-Studie<@$p> ist auch hier rosig:<t-1> <t$>@1 fliesskursiv Zitat:»Erfolgreiche Linux-Internetsites werden Webzugriffe in wachsende Gelegenheiten für Werbung, E-Commerce und Business-to-Business-Entwicklungs- und Supportdienstleistungen übersetzen. [...] Die Einnahmen aus Online-Linux-Werbung werden voraussichtlich von 33 Millionen Dollar im Jahr 2000 auf beinah 500 Millionen in 2003 ansteigen, was einer jährlichen Zuwachsrate von 132 Prozent entspricht. Diese Schätzung umfasst nur Einnahmen aus Onlinewerbung, und wir rechnen damit, dass weitere bedeutende Einnahmequellen entstehen werden, wie Online-Open-Source-Märkte. Die Erträge aus Linux-Werbung mögen nicht sehr groß erscheinen, doch ist zu bedenken, dass die Gewinnspanne bei Onlinewerbung sehr hoch ist (typischerweise bei 70 Prozent der Bruttospanne)«<@1 fliess normal> (<@6 Caps>Patel<@1 fliess normal>, 2000).<@$p><@1 fliess normal>@2  ZÜ 3:Dokumentation@1 fliess ohne:»Coder schreiben keine Dokumentation«, dieser Grundregel sind wir bereits mehrfach begegnet. Wenn Softwareautoren schon ihre eigene Arbeit für sich und ihre Kollegen nicht beschreiben, nehmen sie sich umso weniger die Zeit, den Anwendern zu erklären, wie sie mit ihren Programmen arbeiten können. Kaum eine Software und keine so komplexe wie ein Betriebssystem ist so selbsterklärend, dass man auf ein Handbuch verzichten könnte. Auch diejenigen, die sich die Software aus dem Internet beschaffen oder lizenzierte CDs von Freunden ausleihen oder kopieren, benötigen Fachliteratur. Wir hatten bereits gesehen,<@3 hoch fliess>56<@$p> dass schon kurz nach Erscheinen des ersten stabilen Linux-Kernels die erste Dokumentation zu GNU/Linux erstellt wurde. Das »Linux Anwenderhandbuch« (LHB) von Hetze, Hohndel, Kirch und Müller ist heute, inzwischen in der siebten Auflage, immer noch das deutschsprachige Standardwerk. Dass das LHB zu einem Bestseller unter der Computerliteratur werden konnte, ist umso bemerkenswerter, als es parallel zur gedruckten Version im Netz frei verfügbar<@3 hoch fliess>57<@$p> und unter den Bedingungen der GPL frei kopier- und weitergebbar ist. So sind viele der LHB-Texte in die Online-Dokumentation (die <@1 fliess kursiv>man pages<@$p>) deutschsprachiger GNU/Linux-Distributionen, wie der von SuSE, eingegangen.@1 fliess mit:Die wichtigste Bezugsquelle für Bücher zu freier Software in Deutschland ist <@1 fliess normal>Lehmanns<@$p>, eine Kette von Fachbuchhandlungen mit den Schwerpunkten Informatik, Medizin, Naturwissenschaften, Technik und Wirtschaft mit Filialen in 20 Städten. Bernd Sommerfeld, Buchhändler bei Lehmanns Berlin, übernahm 1985 die junge EDV-Abteilung und baute sie zur »Unix-Buchhandlung in Deutschland« auf. Ab 1990 waren hier auch schon die Bücher der <@1 fliess kursiv>Free Software Foundation<@$p> zu GNU Emacs, Lisp und dem gcc erhältlich, ebenso Software selbst wie Minix und Coherent (das erste kommerzielle PC-Unix). 1992 hörte Sommerfeld zum ersten Mal durch Studenten der FU-Berlin und durch Diskussionen auf der Usenet-Newsgroup comp.os.minix von GNU/Linux. @1 fliesskursiv Zitat:»Linux gab es zur dieser Zeit ausschließlich im Internet per ftp. Ich aber wollte es auf Disketten an alle die verkaufen, die den Zugang zum Netz noch nicht hatten. Sebastian Hetze, damals Mathematikstudent in Berlin, war gerne bereit, eine Distribution ›LunetIX LSD‹ auf sieben Disketten mitsamt photokopiertem Handbuch zu erstellen. Ich erinnere mich gut, wie zur Cebit 1993 der Messestand von Lehmanns wegen des ersten Linux-Anwenderhandbuchs und der ersten Distribution auf Disketten regelrecht gestürmt wurde. Auch Verleger und Softwarefirmen witterten interessante Geschäfte« <@1 fliess normal>(<@6 Caps>Sommerfeld<@1 fliess normal>, 1999, Kap. 7)<@$p>.<@1 fliess normal>@1 fliess ohne:Lehmanns begann bereits 1992 mit E-Commerce, und das auf GNU/Linux: Als erste Buchhandlung weltweit vertrieb es Bücher über das Internet. Der von LunetIX entwickelte <@1 fliess kursiv>Lehmanns Online Bookshop<@$p> (LOB)<@3 hoch fliess>58<@$p> bietet eine komfortable Suche im Kataloge der lieferbaren Bücher, CDs und Videos. Zu vielen gibt es durchsuchbare Abstracts. Die Bestellung erfolgt über einen Warenkorb, wird per E-Mail bestätigt und frei Haus geliefert. In einem Vergleichstest verschiedener Online-Buchläden durch die <@1 fliess kursiv>c’t <@$p>1999 ließ LOB in der Gesamtwertung und vor allem bei den Suchfunktionen, der Übersichtlichkeit, Handhabung und Lieferzeit fast alle Konkurrenten hinter sich (vgl. <@6 Caps>Kuri<@$p>, 19/1999).@1 fliess mit:Ab etwa 1994, erinnert sich Sommerfeld, begannen aufgeschlossene Computerbuchverlage die ersten GNU/Linux-Bücher zu publizieren. Neue Distributoren wie Su<K>se<$>, Caldera, Yggdrasil, DLD und LST brachten GNU/Linux-CDs heraus. Linux-Zeitschriften wie das amerikanische <@1 fliess kursiv>Linux Journal <@$p>und das deutsche Linux-Magazin kamen auf den Markt. Sommerfeld gibt selbst verschiedene GNU/Linux-Distributionen heraus. Auch durch die Ini<\h>tiierung des ersten internationalen Kongresses »Linux & Internet« 1994 in Heidelberg, bei dem sich 340 Entwickler und Freunde freier Software aus Finnland, England, den USA und Frankreich zum ersten Mal live trafen, machte er sich um GNU/Linux verdient. Marktführer unter den Verlagen von Handbüchern für freie Software ist <@1 fliess normal>O’Reilly.<@$p> O’Reilly & Associates<@3 hoch fliess>59<@$p> ist 1978 als Consulting-Firma für technische Schriften entstanden, die z.B. Unix-Handbücher verfasste, die Workstation-Anbieter zusammen mit ihrer Hard- und Software ausliefern. Mitte der 80er begann die Firma, die Rechte an ihren Auftragswerken zu behalten, um sie auch an andere Softwarehersteller zu lizenzieren. Spätestens auf der MIT X-Konferenz 1988 wurde deutlich, dass es einen großen Bedarf nach Handbüchern gab, die unabhängig von der Software zu erwerben sind. Aus den Auftragsdokumentationen wurden Bücher und aus O’Reilly ein Verlag. Heute hat O’Reilly 120 Titel im Programm mit den Schwerpunkten Unix, X, Internet und andere offene Systeme. Vor allem unter Entwicklern hoch angesehen sind Referenzwerke zu freier Software.<@3 hoch fliess>60<@$p> An der großen Nähe zur technischen Community merkt man, dass O’Reilly aus der Computerwelt und nicht aus dem traditionellen Verlagsgeschäft kommt. Alle Lektoren sind ehemalige Programmierer. Die Auflagen sind bewusst klein, um in häufigen Nachdrucken die Leserreaktionen und die raschen Änderungen der Software aufnehmen zu können. O’Reilly unterstützt freie Software auch durch Veranstaltungen, wie dem <@1 fliess kursiv>Open Source Summit<@$p> im April 1998 und jährliche Konferenzen zu Perl und <@1 fliess kursiv>Peer-to-Peer-Computing<@$p>. Perl ist eines der Steckenpferde von Verlags<\h>chef Tim O’Reilly. Er hostet perl.com und gibt das <@1 fliess kursiv>Perl Resource Kit<@$p> he<\h>raus. Er ist dabei immer darum bemüht, freie Software auch im kommerziellen Softwaremarkt zu stärken.<@3 hoch fliess>61<@$p>O’Reilly hat sich früh auch mit Online-Publishing beschäftigt. Softwarehersteller wollten ihre Dokumentation auch online anbieten. In den 80ern entstanden mehrere Dutzend Formate für die digitale Präsentation von Handbüchern und Hilfetexten. Der O’Reilly-Verlag hätte seine Bücher in zahlreichen unterschiedlichen Formaten anbieten müssen. Um der Abhängigkeit von Formatentwicklern zu entkommen, begann er 1991 ein eigenes Format namens DocBook für Online-Handbücher zu entwickeln. Der Kreis, der diesen freien Standard entwickelte, erweiterte sich zur Davenport Group.<@3 hoch fliess>62<@$p> DocBook hat sich unter den offenen Systemen und auch in der GNU/Linux-Community weithin durchgesetzt. 1993 entstand aus der Arbeit an einem Browser für das DocBook-Format der <@1 fliess kursiv>Global Network Navigator<@$p> (GNN). Sein Inhalt bestand anfangs aus einem Teil des von O’Reilly herausgegebenen »Whole Internet User’s <\h>Guide & Catalog« von Ed Krol. GNN begann als Demo und wuchs zu einem neuen Informationsdienst mit Nachrichten, Artikeln und Rezensionen über Internetdienste heran. 1995 wurde er an AOL verkauft. Mit <@1 fliess kursiv>Web Review<@$p> und dem <@1 fliess kursiv>World Wide Web Journal<@$p> setzt der Verlag seine Online-Publishing-Aktivitäten fort. Im Bereich Online-Bücher werden heute thematische Auswahlen aus dem Verlagsprogramm zu Handbibliotheken (Java, Schlüsseltechnologien für Webmaster) zusammengestellt. Diese <t-2>sind jedoch ebensowenig wie die gedruckten Bücher in irgendeinem Sinne <t$>frei oder offen. Die Referenzbibliotheken z.B. werden im Web zum Preis von 59,95 Dollar im Jahr angeboten. Weiterverbreiten oder modifizieren darf man sie nicht.Der Verlag beschäftigt über 200 Menschen und hat Niederlassungen in den USA, Japan, Frankreich, Deutschland, Großbritannien, Taiwan und der Volksrepublik China. Tim O’Reilly schätzt, dass er im Laufe der Jahre Bücher über Open Source-Software im Wert von mehr als 100 Millionen Dollar verkauft hat und sein open Source-bezogener Umsatz im Jahr 1998 größer war, als der von SuSE oder Red Hat. Dennoch ist er der Ansicht, dass nicht er, sondern Bill Gates der größte Open Source-Profiteur sei, da dieser offene Standards, Protokolle und Software in seine proprietäre Software integriert und seinen Kunden Upgrades für MS-Windows und Office verkauft, damit sie Internet-Funktionalität erhalten.<@3 hoch fliess>63<@$p> <\c>@2  ZÜ 1:Sicherheit@1 fliess mit:@1 fliess ohne:<*h"mehr">Ein immer wieder genannter Vorteil von freier gegenüber proprietärer Software ist ihre größere Sicherheit. Der Problemkomplex Sicherheit lässt sich in die Bereiche Betriebssicherheit (Stabilität, Verfügbarkeit, Inves<\h>titionssicherheit, Haftung) und Systemsicherheit (Abhörschutz, Kryptografie, Firewalls) gliedern. Die Experten vom Bundesamt für die Sicherheit in der Informationstechnik <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’ <@$p>BSI) über das Bundeswirtschafts- und For<\h>schungministerium bis zum Chaos Computer Club <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’ <@$p>CCC) sind der einhelligen Überzeugung, dass die Quelloffenheit einer Software essenzielle Voraussetzung für ihre Sicherheit unter allen Gesichtspunkten ist. Proprietäre Software, bei der die Anwenderin sich einzig auf das Wort des Anbieters verlassen kann, muss grundsätzlich als suspekt angesehen werden. Überprüfbarkeit durch Experten des eigenen Vertrauens, d.h. Quelloffenheit, ist Voraussetzung für Vertrauensbildung. Sie ist notwendige, wenn auch nicht hinreichende Voraussetzung. Für die Sicherheit ist nichts gewonnen, wenn einem Interessenten der Quellcode zwar offengelegt, ihm aber nicht die Möglichkeit gegeben wird, diesen Quellcode anzupassen und daraus selbst ein lauffähiges System zu kompilieren. Besonders die Vertreter des BSI sagten mit deutlichen Worten, dass sie aus Sicht der IT-Sicherheit lieber heute als morgen einen breiten Einsatz von freier Software in der Verwaltung sehen würden. Gleichwohl müssten die Interessen der Anwender hinsichtlich der Bedienbarkeit und Verfügbarkeit von Anwendungen berücksichtigt werden. (Fachgespräch 7/1999).@1 fliess mit:Im Kontrast zu dieser qualifizierten <@1 fliess normal>Einschätzung<@$p> trifft freie Software weithin auf ein erhebliches Misstrauen in Bezug auf Sicherheitsfragen. Ingo Ruhmann, Spezialist für IT-Sicherheit und Datenschutz und heutiger Leiter des BMBF-Projekts »Schulen ans Netz«, hält vertrauensbildende Maßnahmen und eine Öffentlichkeitsarbeit für erforderlich, die breite Anwenderkreise über die Sicherheitsvorzüge freier Software aufklären.<t-3>Umgekehrt herrscht an vielen Orten weiterhin ein pauschaler Vertrauensvorschuss gegenüber proprietärer Software. <t$>Dieses Vertrauen ist bei Einzelbetrachtungen sicher zu rechtfertigen, jedoch in pauschaler Form unbegründbar, weil die zahlreichen bekannt gewordenen Mängel und sicherheitsbedenklichen Features das blinde Vertrauen in proprietäre Software stark relativieren sollten<t-3>. Zu der langen Kette gehört die mangelhafte Implementierung des <x@1 fliess kursiv><t-3>Point-to-point-Tunneling<@$p><t-3>-Protokolls von Micro<\h>soft,<x@3 hoch fliess><t-3>1 <@$p><t-3>die Fernsteuerung fremder Rechner mit Hilfe von MS Back-Orifice<x@3 hoch fliess><t-3>2<@$p><t-3> oder die Identifizierungsnummern, die Intel in seinem neuesten Prozessor einbrannte, die auch Microsoft nicht nur jeder einzelnen Version von <\h>Windows98 und Anwenderprogrammen mitgibt, sondern die sich selbst in den Dokumenten finden, die mit Word, Excel oder PowerPoint erstellt werden. »<t$>Wenn man das in einem größeren Zusammenhang sieht – es wird ja viel diskutiert  über weltweites Aufnehmen von E-Mails, Telefongesprächen und Telefaxen durch gewisse Sicherheitsbehörden – dann kann man in Software, in die man nicht hineinsehen kann, auch kein Vertrauen mehr haben.<t-3>«<x@3 hoch fliess><t-3>3<@$p><t-3><t$>Ebenso unbegründbar ist das Vertrauen in die stabilen Besitzverhält<t-1>nisse der Unternehmen, auf die man sich in IT-Sicherheitsfragen verlässt:<t$>@1 fliesskursiv Zitat:»1997 hat die Bundesregierung erklärt, dass im Bereich der Bundesbehörden 65<\!q>000 PCs mit Microsoft-Betriebssystemen im Einsatz sind. Heute werden es einige Tausend mehr sein. Keiner dieser PCs lässt sich auf Sicherheitsmängel überprüfen, ob er jetzt im Berlin-Bonn-Verkehr eingesetzt wird oder nicht. Im Übrigen wurde die genutzte Kryptografie-Hardware im Bonn-Berlin-Verkehr von einer Firma geliefert, die, nachdem der Kaufentschluss gefallen war, von der südafrikanischen Firma Persetel Q Holdings gekauft wurde, die während der Zeiten des Boykotts gegen Südafrika groß geworden sind, mit besonders guten Kontakten zum südafrikanischen Militär. Im Herbst 1999 wurde dieser Firmenteil immerhin von der deutschen Firma UtiMaco gekauft. Für Blackbox-Systeme ist daran wichtig: Das Know-how um die Kyptografiemodule war für einige Zeit für Beteiligte offengelegt, deren Vertrauenswürdigkeit niemand geprüft hat. Hier gibt es einige Probleme, die dadurch nicht besser werden, wenn wir uns auf große kommerzielle Anbieter verlassen.«<@3 hoch fliess>4<@$p> @1 fliess mit:@1 fliess ohne:Im <@1 fliess normal>Office<@$p>-Bereich bedarf der Wechsel zu quelloffenen Systemen keiner weiteren Evaluierung mehr, trifft aber auf andere Schwierigkeiten. Zum einen ist Microsoft-Software in der Verwaltung und in der Wirtschaft heute Quasi-Standard. Eine Abteilung, die im Alleingang umsteigt, läuft somit Gefahr, nur in eingeschränktem Umfang Daten mit Kommunikationspartnern austauschen zu können. Zum anderen stehen unter GNU/Linux integrierte, grafisch zu bedienende Oberflächen, die Anwender in diesem Bereich für ihre <@1 fliess normal>Office-<@$p>Applikationen erwarten, erst seit relativ kurzer Zeit zur Verfügung. Die Anforderungen an den Anwender bei Installation und Wartung sind heute unter GNU/Linux noch deutlich höher als unter Mac-OS oder MS-Windows. Angesichts der kurzen Zeit, in der sich die grafischen Oberflächen KDE und Gnome sowie andere Merkmale der Benutzerfreundlichkeit entwickelt haben, ist jedoch damit zu rechnen, dass dieser Unterschied bald verschwinden wird. @1 fliess mit:Ruhmann fordert einen grundlegendenWandel in der Sicherheitskultur: »Weg von dieser Aufregung: ›Der und der Browser hat mal wieder dies und jenes Sicherheitsloch‹ und der Panik, die dann hinterher kommt. Oder diese mittlerweile notorischen Meldungen über neue Viren, bei denen die Hälfte der Mails sich auf Viren bezieht, die gar nicht existieren. Diese Aufregung ist ja ganz schön und nützlich, sie führt aber letztendlich nur zu reiner Panikmache ohne vernünftige Differenzierung. Open Source-Software kann diese vernünftige Differenzierung dadurch leisten, dass sie eine überprüfbare Sicherheit bietet, nur müssen dann auch die Mechanismen dafür her.«<@3 hoch fliess>5<@$p> Frank Rieger vom Chaos Computer Club (CCC) sieht Mängel in der Sicherheitskultur auch auf Produzentenseite. Sicherheit ist häufig keine Priorität im Entwicklungsprozess und kein strukturelles Element eines Produkts, sondern eine Ergänzung, über die erst kurz vor der Markteinführung nachgedacht werde. Oft genug sei nicht klar, wer eigentlich dafür zuständig ist, dass ein Produkt am Ende sicher und vor der Auslieferung auch daraufhin getestet worden ist. Auch eine Zertifizierung bietet hier keine Gewähr, zumal, wenn sie wie im Falle von Windows NT zehn Jahren zuvor für ein komplett anderes Produkt erteilt wurde. Schließlich werden Produkte, die von sich aus relativ sicher sein können, mit unsicheren Voreinstellungen und Installationswerten ausgeliefert. Rieger führt als Beispiel die damals aktuelle SuSE-Distribution an, die nach einer Standardinstallation unnötig viele Ports offen ließ. Auch wenn es möglich ist, ein SuSE-Linux wasserdicht zu machen, zeigt es sich doch bei allen Computersystemen, dass viele Endanwender die Standardeinstellungen nie verändern.<@3 hoch fliess>6<@$p> Für Ruhmann bedeutet eine IT-Sicherheitskultur, »dass das Bewusstsein um IT-Sicherheit steigen muss; dass es kein Kostenfaktor ist, sondern ein erheblicher Nutzenfaktor; und dass der Grad der Panik aus Unwissenheit bei irgendwelchen Problemen dadurch herabgesetzt werden muss, dass die Leute auf der einen Seite einfach kompetenter mit IT umgehen können und auf der anderen Seite ein kompetenteres Beratungsangebot zur Verfügung haben. ... Qualität sollte nicht vom Glauben abhängig sein, sondern von überprüfbaren Kriterien.«<@3 hoch fliess>7<@$p> Und Voraussetzung für eine Überprüfung ist der Zugang zum Quellcode.@2  ZÜ 2:Betriebssicherheit<t0f"FFScala">@1 fliess ohne:Auf der fundamentalsten Ebene ist nach der Betriebssicherheit von Sys<\h>temen, nach ihrer Ausfallhäufigkeit, Stabilität und Verfügbarkeit zu fragen. In diesen Punkten sind freie Softwaresysteme wie GNU/Linux durch eine weitergehende Fehlersicherheit proprietären Systemen deutlich überlegen. Freie Software ist natürlich nicht per se fehlerfreier, doch wenn ein Fehler auftritt, findet sich meist sehr schnell Unterstützung im Internet oder es kann eine Softwareentwicklerin angeheuert werden, die den Fehler beseitigt, während der Benutzer einer proprietären Software darauf angewiesen ist, dass der Hersteller den Fehler für ihn behebt. Zur Inves<\h>titionssicherheit ist im Kapitel »Wirtschaftliche Potenziale« bereits einiges gesagt worden. Verantwortungsbewusste Firmen hinterlegen immerhin den Quelltext ihrer Software, so dass die Kunden weiterhin da<\h>rauf zugreifen können, falls die Firma aufgekauft wird, bankrott geht oder aus anderen Gründen die Software nicht mehr warten kann.@1 fliess mit:Auch die Flexibilität und Anpassbarkeit von quelloffener Software, die dort bereits genannt wurde, ist sicherheitsrelevant:@1 fliesskursiv Zitat:»Wenn ich mir ein kleineres Sicherheitsproblem der letzten Wochen und Monate angucke, wird offenbar, dass Verbesserungen im Sicherheitszusammenhang immer wieder notwendig sind, und zwar im laufenden Betrieb. Die US-Navy hat festgestellt, dass Schiffe in der Adria im Kosovo-Krieg Mails senden und empfangen konnten und dass ihre Soldaten ziemlich blauäugig nicht nur Mails anderer Leute gelesen, sondern auch sensitive Informationen per Mail an ihnen unbekannte Leute weitergegeben haben. Was war die Lösung? Eingehende Mails wurden erlaubt, ausgehende nicht. Das funktioniert mit schlecht anpassbaren Systemen nur auf diese Art und Weise. Man hätte sich natürlich auch andere Wege überlegen können, diese Systeme ein wenig besser der neuen Situation anzupassen. Eine solche Anpassung ist natürlich bei <@1 fliess normal>Blackbox<@$p>-Systemen schlichtweg unmöglich. Bei Open Source-Systemen kann ich entsprechende Veränderungen jederzeit einbringen, wenn ich die Leute kenne oder selbst dazu in der Lage bin.«<@3 hoch fliess>8<@$p>@1 fliess mit:@2  ZÜ 2:Gewährleistung und Haftung@1 fliess mit:@1 fliess ohne:Ein immer wieder zu hörender Einwand gegen freie Software lautet: »Wie kann ich diesen ›langhaarigen Leuten‹ trauen, die mir einen <@1 fliess kursiv>Bug Fix <@$p>schicken?« Natürlich kennt man auch bei proprietärer Software in der Regel die Entwickler nicht persönlich. Der vermeintliche Unterschied besteht darin, dass man ihre Firma im Falle eines Fehlers verklagen könnte. »Hier wird also versucht, Sicherheit durch die Möglichkeit des Rechtsweges herzustellen.«<@3 hoch fliess>9<@$p> Dass diese Strategie bei kommerziellen Produkten auf theoretische und praktische Grenzen stößt, soll hier dargestellt und mit der Lage bei freier Software kontrastiert werden. Da freie Software niemandem gehört und keine großen Unternehmen die Gewährleistung dafür übernehmen, wie kann ein Nutzer sicher sein, dass die Software auch in kritischen Situationen hält, was sie verspricht?@1 fliess mit:Zunächst ist anzumerken, dass auch proprietäre Unternehmen für ihre Produkte keine Gewährleistung übernehmen. Ihre Software wird so angeboten, »wie sie ist« <@1 fliess kursiv>(asis)<@$p>, ohne die darüber hinausgehende Garantie, dass sie für den intendierten Einsatz taugt und mit so weitgehenden Garantie- und Haftungsausschlüssen, wie es die jeweilige nationale Jurisdiktion zulässt.<@3 hoch fliess>10<@$p> Joseph Weizenbaum hat das Missverhältnis zwischen der <\h>Software- und anderen Branchen einmal so beschrieben: Eine Fluggesellschaft, die sich so verantwortungslos verhalten würde wie eine Informatikfirma, wäre innerhalb von zwei Wochen pleite. Das <\h>Produkt<\h>haftungs<\h>recht aus der Welt der materiellen Waren findet auf Software noch keine Anwendung, obgleich es wünschenswert wäre, eine Situation zu schaffen, in der Nutzer erwarten können, dass das, wofür sie bezahlt haben, auch funktioniert.<@3 hoch fliess>11<@$p> Das entspricht jedoch nicht dem Stand der Technik in der Informatik, und auch der tatsächliche Trend geht in die entgegengesetzte Richtung. Das US-amerikanische Produkthaftungsrecht wird derzeit dahingehend korrigiert, dass den Herstellern von Software eine noch stärker verminderte Haftung eingeräumt wird.<@3 hoch fliess>12<@$p> Es ist ferner zweifelhaft, ob die Gerichtsstandsklauseln, in denen Unternehmen eine Jurisdiktion der eigenen Wahl vorschreiben, in der Softwarebranche Anwendung finden können. Das Produkthaftungsrecht ist in verschiedenen Ländern sehr unterschiedlich geregelt, so dass Computerkonzerne, auch wenn sie in den USA weitgehend aus der Haftung entlassen sind, in anderen Ländern mit strikten Auflagen rechnen müssen.Umgekehrt verfügt die Welt der freien Software sehr wohl über Mechanismen der Qualitätskontrolle und der Stabilitätssicherung für offiziell freigegebene <@1 fliess kursiv>Releases<@$p>. Der offene Prozesscharakter und die fehlende Rechtsform vieler Projekte schließt jedoch eine rechtsverbindliche Gewährleistung aus. Entsprechendes gilt auch für Firmen, die freie Software zusammenstellen und vertreiben. SuSE z.B. bietet keine generelle Gewährleistung.<@3 hoch fliess>13<@$p>Anders sieht es bei Firmen aus, die Dienstleistungen auf Basis freier Software anbieten. Da sie die Software, mit der sie arbeiten, kennen und im Sourcecode überprüfen können, können sie auch einschätzen, welche Leistungen sie gewährleisten können und welche nicht. LunetIX z.B. gewährleistet sehr umfänglich, dass das, was mit dem Kunden vereinbart ist und wofür er bezahlt, auch tatsächlich zum Funktionieren gebracht werden kann. »Ich habe die Erfahrung gemacht, dass man viel ehrlicher ist, mit den Sachen die nicht gehen. Eine Freiheit, die wir uns nehmen können, weil wir eben kein Produkt verkaufen, das <@1 fliess kursiv>all singing, all dancing <@$p>ist. Wir können sagen, da hat es seine Stärken, da seine Schwächen. Wenn die Schwächen für euch ein Problem sind, können wir daran arbeiten.«<@3 hoch fliess>14<@$p>Ingo Ruhmann weist darauf hin, dass im Streitfall eine Sachverständigenüberprüfung vor Gericht nur dann überhaupt möglich ist, wenn der Quellcode eingesehen werden kann:@1 fliesskursiv Zitat:»Das Problem an der Sache ist aber: Wieviele Fälle von Prozessen gegen solche Firmen werden überhaupt geführt? Die Kriminalstatistik sagt, dass es jenseits von Kreditkartenbetrug allenfalls ein bis zwei Dutzend Fälle von Computerkriminalität gibt, die vor Gericht landen. Das rührt einfach daher, dass keine Analyse der Systeme möglich ist. Das heißt, wenn ein Fehler auftritt, kann ich mich effektiv nicht mit einiger Sicherheit an die Firma wenden, die für diesen Fehler verantwortlich ist. Wir kennen es, wenn man eine Hotline befragt, dann wird meistens gesagt: ›Das könnte ja auch noch den und den Grund haben. Es könnte der Treiber von irgendeinem Hardwareteil sein.‹ Solange die Sourcen nicht wirklich offen liegen, hat der Anbieter eines prorietären Systems natürlich die Möglichkeit abzustreiten, dass der Fehler bei ihm liegt. Und ich habe keine Möglichkeit, das herauszufinden. <*p(14.173,14.173,14.173,12.5,0,0,g,"Deutsch")>Bei <@1 fliess normal>Blackbox-<@$p>Systemen ist eine Fehlererkennung nur durch einen Funktionsfehler im Betrieb möglich, bei Open Source-Systemen aber eine konkrete Analyse, woran dieser Fehler liegen könnte. Das heißt, <\n>effektiv ist erst in dem Fall, dass ich Open Source-Software einsetze, überhaupt die Möglichkeit gegeben, dass ich diesen Rechtsweg – den viele bei proprietären Systemen für gegeben halten – mit einiger Aussicht auf Erfolg beschreiten kann. Hinzu kommt sogar, dass ich, wenn ich bei bestimmten proprietären Systemen eine Analyse mache und herausfinde, dass der zur Sicherheit benutzte Mechanismus extrem schwach ist, da oftmals sogar rechtlich besonders schlechte Karten habe, weil der Schutz gegen Datenmissbrauch auch einen ausreichenden technischen Hintergrund voraussetzt.«<@3 hoch fliess>15<@$p>@1 fliess mit:@1 fliess ohne:Eine systematische Kontrolle der Funktionstüchtigkeit von <@1 fliess normal>Blackbox-Sy<@$p>s<\h>temen ist ausgeschlossen. Eine gerichtliche Würdigung ist allenfalls möglich, wenn ein Unternehmen den Quellcode seiner proprietären Software (üblicherweise unter Vertraulichkeitsverpflichtung) den Sachverständigen zugänglich macht. @1 fliess mit:Eine Überprüfung vorab ist selbst bei Vorliegen des Quellcodes schwierig, doch nachträglich lassen sich immer wieder bewusst eingebaute Sicherheitslücken nachweisen. Hier sieht Frank Rieger eine Möglichkeit für das Eingreifen des Gesetzgebers. Wenn die Haftbarkeit von Herstellern für Hintertüren drastisch verschärft würde, gäbe es keine andere Möglichkeit mehr, als Sicherheitsprodukte quelloffen zu machen. »Dadurch würde das, was sowieso schon üblich ist bei kleineren Firmen, alles, was Kryptografie und Security ist, Open Source zu machen, auch für große Firmen bindend und verpflichtend werden, und könnte tatsächlich eine ganze Menge bewegen.«<@3 hoch fliess>16<@$p>@2  ZÜ 2:Kryptografie: Security by Obscurity vs. offene Verfahren@1 fliess mit:@1 fliess ohne:Das Schlüsselelement für Sicherheit in digitalen Systemen ist die Kryptografie. Der Schutz von Daten auf einem Rechner (z.B. Patientendaten), die Vertraulichkeit von Kommunikationen (z.B. Vertragsverhandlungen zwischen Unternehmen), die Verhinderung der nachträglichen Veränderung von Dokumenten (z.B. vertragliche Abmachungen), die Sicherung der Identität von Transaktionspartnern durch digitale Signaturen, der <\n>Beleg für das Absenden und den Empfang von Dokumenten, die Verhinderung der Nichtanerkennung von Vereinbarungen und schließlich der gesamte Komplex der digitalen Zahlungsverfahren beruhen sämtlich auf kryptografischen Systemen. Sie waren bis vor kurzem die exklusive Domäne von Militär und Geheimdiensten. Als in den 70er-Jahren Konzerne und Banken begannen, digitale Netze einzusetzen, entstand erstmals ein ziviler Bedarf nach Kryptografie. Seit Anfang der 90er-Jahre verbreitet sich die PC-gestützte private Nutzung von offenen Netzen und damit der allgemeine Bedarf nach kryptografischen Produkten. @1 fliess mit:Zwei Positionen treffen an dieser Frage aufeinander: einerseits das Grundrecht auf Unantastbarkeit der Kommunikation und der Schutz der <\h>Privatsphäre, andererseits das Sicherheitsinteresse des Staates. Die Konfliktlinie verläuft in allen Ländern zwischen den Daten- und Persönlichkeitsschutzverfechtern und den Strafverfolgungs- und Nachrichten<\h>diesten, in der Regel also zwischen dem Wirtschafts- und dem Innenministerium.<@3 hoch fliess>17 <@$p>Schließlich ist Kryptografie nicht nur Voraussetzung für E-Commerce, sondern selbst ein gewinnträchtiger Markt. Wenn Kryptografie Vertrauen in das Internet schaffen soll, stellt sich die Frage, auf welcher Grundlage man Kryptografie-Software vertrauen kann. Auf dem Markt für diese Produkte stehen sich zwei Herangehensweisen gegenüber. Zum einen soll ein Vertrauen in die Sicherheit solcher Systeme erzeugt werden, indem sie in einem vertraulichen Prozess entwickelt werden und ihre Funktionsweise geheimgehalten wird. Dieser so genannte <@1 fliess kursiv>Security by Obscurity<@$p>-Ansatz herrscht unter den proprietären Produkten vor. Dagegen kann die Sicherheit von quelloffenen Systemen, die im Prinzip jeder Interessierte einsehen kann, nur darauf beruhen, dass der Mechanismus auch dann schützt, wenn er bekannt ist. @1 fliesskursiv Zitat:»Es gibt einige Microsoft-Systeme, die ihre Passwörter ganz einfach mit einer xor-Verschlüsselung mit einer relativ einfachen Zahl ablegen. Wenn bekannt wird, wie dieser Mechanismus funktioniert, dann ist er nichts mehr wert. Im Gegensatz dazu, wie allen bekannt, verschlüsselt Unix Passwörter in einer Einwegfunktion und vergleicht sie auch in der verschlüsselten Version. Da es keine Umkehrfunktion dazu gibt, ist es völlig unerheblich, ob diese Einwegfunktion bekannt ist oder nicht. Und jeder kann auch das abgelegte Passwort lesen, weil er damit nichts anfangen kann, denn es liegt ja nicht im Klartext vor.«<@3 hoch fliess>18<@$p>@1 fliess mit:@1 fliess ohne:Rieger nennt als weitere Beispiele GSM, den Standard, der heute in den meisten Mobiltelefonen verwendet wird, und der trotz strenger Geheimhaltung von einer US-amerikanischen Hackergruppe geknackt werden konnte. Auch bei der Telefonkarte der Deutschen Telekom kam ein Hamburger Sicherheits-Consulting-Unternehmen zu dem Ergebnis, dass die von der Telekom gewählte Chipkartentechnologie keine inhärente Sicherheit biete, sondern diese allenfalls in einem Technologievorsprung von etwa drei oder vier Jahren gegenüber möglichen Angreifern beruhe. Diese Vorhersage hat sich bewahrheitet. Mit gefälschten Telefonkarten wird ein gutes Geschäft gemacht. Rieger schätzt, dass es sich um einen der größten Schäden handelt, die dank »Sicherheit durch Obskurität« entstanden sind. Er bezeichnet dieses Modell als ein Wissensmonopol einer Priesterschaft.@1 fliesskursiv Zitat:»Ich designe ein System so, dass es von außen auf den ersten Blick nicht durchschaubar ist und seine inneren Wirkungsweisen nicht verstanden werden können – so glaubt man zumindest. Und wenn man dann der Sache doch nicht so traut, baut man lieber noch so ein paar Gemeinheiten ein, einige Fallen oder Inkonsistenzen, um zu verhindern, dass, falls es einen Security-Bruch gibt, die Sachen zu schnell vorangehen. Dieses geschlossene Modell ist sehr kompatibel mit dem Kulturraum von großen Entitäten wie Siemens z.B. [...] Die haben da eine kleine Abteilung mit einer großen Stahltür davor, und da ist ein Zahlenschloss und ein Iris-Scanner dran und noch eine große Stahltür und dahinter liegen in dem Safe die drei Blätter Papier, auf denen steht, wie’s geht. Das ist etwas, was sie verstehen. Da können sie einen Wachmann davorstellen, der notfalls schießt, und damit ist die Sicherheit dann eigentlich gewährleistet. Dass der Algorithmus, den sie in dem Panzerschrank mit den drei Stahltüren haben, auf jeder beliebigen Chipkarte, die zehn Millionen Mal verteilt wird, nach draußen geht, daran haben sie nicht wirklich gedacht.«<@3 hoch fliess>19<@$p>@1 fliess mit:@1 fliess ohne:Das Priesterschaftsmodell wird ferner durch die zunehmende Mobilität und Informationsdichte infrage gestellt. Angestellte aus Sicherheitsabteilungen wechseln ihre Arbeitgeber häufiger als früher, und keine Vertraulichkeitsvereinbarung<@1 fliess kursiv> <@$p>kann verhindern, dass sie ihr Wissen mitnehmen. @1 fliess mit:Das quelloffene Modell ist wiederum nicht per se sicherer, doch durch das öffentliche Testen und Studieren werden Mängel und Fehler meist relativ schnell gefunden und behoben. Auch Hintertüren sind dadurch nicht ausgeschlossen, doch auch hier ist die Chance, dass jemand sie entdeckt, relativ hoch. <*h"mehr">Rieger weist aber darauf hin, dass freie Software kein Allheilmittel ist. Besonders große Programme, die erst nachträglich quelloffen gemacht werden, sind extreme schwer zu überschauen. Kaum jemand hat den gesamten Quellcode der Kryptografie-Software PGP (<@1 fliess kursiv>Pretty Good Privacy<@$p>) durchgelesen und kann sicher sagen, dass sich darin keine Hintertür befindet. Und wenn sich mehrere Personen die Arbeit teilen, ist die Gefahr, etwas zu übersehen, noch viel größer. Seit Anfang 1998 liegt das Programm PGP 5.0 im Quelltext vor, ein gravierender Fehler, der unter <\n>bestimmten Voraussetzungen die Sicherheit unterminiert, für die das Programm eigentlich sorgen soll, wurde jedoch erst im Mai des darauffolgenden Jahres bekannt.<@3 hoch fliess>20<@$p> Für Rieger ist der einzige Weg zu solchen Kernsicherheitssystemen wie PGP, diese von Anfang an offen zu entwi<\h>ckeln, wie das bei der GNU-PG (s.u.) der Fall ist. Nur eine permanent mitlaufende Überprüfung bietet die Gewähr, dass nicht nachlässig oder gezielt Sicherheitsprobleme eingebaut werden.<@3 hoch fliess>21<@$p><*h"Standard">Die Bundesregierung fasste am 2.<\!q>Juni 1999 einen Beschluss, der die breite Verwendung von starken Kryptografieprodukten durch jedermann ohne jede Beschränkung vorsieht (BMWi, Eckpunkte 1999). Zu den Schritten, die Hubertus Soquat (Referent für IT-Sicherheit im BMWi) in Berlin vortrug,<@3 hoch fliess>22<@$p> gehört die Erhöhung der Verfügbarkeit von Kryptoprodukten – von der Entwicklung über die Produktion und den Vertrieb bis zum Einsatz. Im Zentrum der Aktivitäten zur Sensibilisierung breiter Nutzerschichten steht die Kampagne »Sicherheit im Internet«,<@3 hoch fliess>23<@$p> die sich besonders an individuelle Nutzer sowie an kleine und mittlere Unternehmen richtet. Das BMWi wird die deutsche Kryptografie-Industrie beobachten, um abzuwägen, welche Möglichkeiten bestehen, durch öffentliche Auftraggeber entsprechende Nachfragepotenziale zu realisieren. Zur Förderung des Einsatzes von Verschlüsselung innerhalb der Bundesbehörden und darüber hinaus dient das Pilotprojekt »Sphinx«.<@3 hoch fliess>24<@$p>Auf die Frage, was geschehe, wenn sich herausstellt, dass sich das, was Strafverfolgung und Nachrichtendienste als ihre legitimen Bedürfnisse sehen, und die Eckpunkte des BMWi-Papiers ei<\h>nander ausschlie<\h>ßen, wie z.B. sicherer Nachrichtenverkehr, freie Ver<\h>fügbarkeit starker Kryptografie und ihr massenhafter Einsatz im E-Mail-Verkehr, antwortete Soquat in einem Interview mit der Tageszeitung <@1 fliess kursiv>taz:<@3 hoch fliess>25<@$p> »Dann haben sie [die Sicherheitsdienste] Pech gehabt.« Er verwies darauf, dass Nachrichtendienste und die Strafverfolgungsbehörden nachgeordnete Behörden seien und dass das Bundesverfassungsgericht in einer jüngsten Entscheidung unterstrichen habe, dass es Grenzen für deren Tätigwerden gibt.  <t-2>Erschwert wird die Situation dadurch, dass Kryptografie in vielen Ländern als Waffentechnologie klassifiziert und mit Ein- und/oder Ausfuhrbeschränkungen belegt ist.<x@3 hoch fliess><t-2>26<@$p><t-2> Das führt dazu, dass Hersteller von kryptografischen Algorithmen oder anderer Software, die solche Algorithmen enthält, jeweils eine starke Version für den Inlandsmarkt und eine schwächere für den Export anbieten müssen. Freie Softwareprojekte, die sich nicht nach Ländergrenzen organisieren, stehen hier vor besonderen Problemen.<t$> Das Wassenaar Abkommen<@3 hoch fliess>27<@$p> ist ein Versuch, die verschiedenen nationalen Politiken zu harmonisieren. Für die freie Software ist besonders <\n>interessant, dass es kryptografische Software von Restriktionen ausnimmt, wenn sie gemeinfrei ist. @1 fliesskursiv Zitat:»Nun profitiert als ›Public Domain‹ – in einem speziellen Sinne, der eigentlich heißt: frei weitergebbar – gekennzeichnete Software von Teil 2 der <@1 fliess normal>General Software Note <@$p>des Wassenaar Arrangements. Es gibt relativ glaubwürdige Hinweise, u.a. in Form der neuen australischen Exportrichtlinien, dass wohl darauf gedrungen wird, diese Ausnahme für kryptografische Software aus dem Wassenaar Arrangement zu streichen. Das hieße, dass man diese kryptografische Software innerhalb der EU im kommenden Jahr [2000] zwar noch frei verteilen könnte, aber international nicht mehr. Wenn ich das mit Aspekten, wie der von Herrn Dalheimer angesprochenen großen Internationalität schon in der Softwareentwicklung, dem generisch zu dieser Software gehörenden weltweiten Zurverfügungstellen des Quellcodes, in einen Topf rühre, dann bekomme ich da heraus: einen Angriff auf die legale Erstellbarkeit frei verfügbarer Sicherheitssoftware. Angenommen, diese freie Software wäre nicht mehr per se ausgenommen von den Restriktionen, dann kann ich zwar versuchen, diese Exportrestriktionen zu umgehen oder zu ignorieren (was mit Sicherheit auch viele machen werden), aber bei dem, wovon wir hier reden: Einsatz freier Software in Verwaltungen und Unternehmen, da ist das nicht mehr eine Option. Da muss ich schon davon ausgehen können, dass diese Sachen in einem legalen Rahmen erstellt wurden und erhältlich sind.«<@3 hoch fliess>28<@$p>@1 fliess mit:@1 fliess ohne:Die Kryptografierichtlinien der Bundesregierung haben international Anerkennung gefunden, sind aber auch, z.B. in den USA, auf Kritik gestoßen. Auch Frankreich und Großbritannien haben eine andere Haltung dazu als die Bundesregierung. Diese will sich aber in den laufenden Gesprächen im Rahmen des Wassenaar-Abkommens selbst, seiner Umsetzung innerhalb der EU auf <@1 fliess kursiv>Dual-Use<@$p>-Verordnungsebene<@3 hoch fliess>29<@$p> und bei der Umsetzung dieser europäischen Regelungen auf der nationalen Ebene dafür einsetzen, dass die Grundidee hinter den »Kryptografie-Eckwerten«, also die freie Verfügbarkeit, nicht verwässert wird.<@3 hoch fliess>30<@$p> In einem ersten Schritt ist der Handel mit Kryptografieprodukten zwischen den Mitgliedsstaaten bereits liberalisiert worden.@1 fliess mit:Auch der US-amerikanische Geheimdienst <@1 fliess kursiv>National Security Agency  <@1 fliess normal>(<@4 Pfeil (Umschalt/Alt #)>’ <@$p>NSA) hat seine Haltung gegenüber Verschlüsselungssystemen inzwischen abgeschwächt. Ein Grund könnte sein, dass deutlich geworden ist, dass nicht alle Kryptografiekompetenz in den USA sitzt und selbst dafür Exportverbote wenig Wirkung zeigen. Die neue Strategie der NSA, so vermutet Rieger, sind Hintertüren.@1 fliesskursiv Zitat:»Darin haben sie eine wirklich große Erfahrung, angefangen bei der Krypto-AG, wo über Jahrzehnte hinweg die Schlüssel mit im verschlüsselten Text selbst preisgegeben wurden, bis hin zu Hintertüren, die in irgendwelche Produkte eingebaut werden, z.B. Router, Firewalls, ISDN-Systeme, Betriebssysteme. Da wird, denke ich, in nächster Zukunft deren großes Arbeitsfeld liegen, weil sie genau wissen, dass der Aufwand, die Kryptografie auf dem Transportweg zu brechen, sich so schnell nicht verringern wird, selbst wenn sie noch so viel in die Forschung stecken. Deswegen ist der Weg, den sie gehen werden, Hintertüren. Wir haben auch einen deutlichen Anstieg von Angriffen über Hintertüren gesehen, die von professionellen Industriespionen erfolgen, die solche Hintertüren tatsächlich auch kaufen. Es gibt Leute, die kennen die halt und verkaufen dieses Wissen. Etliches von dem, was an Sicherheitsproblemen da ist, wird mittlerweile nicht publiziert, sondern immer noch unter der Decke gehalten, gerade was ISDN-Systeme betrifft.«<@3 hoch fliess>31<@$p>@1 fliess mit:<*p(0,14.173,0,6,0,0,g,"Deutsch")>@1 fliess ohne:Auch das BSI sieht das Problem heute nicht mehr in den Kryptografiekomponenten selbst, sondern vor allem in den Architekturen der Systeme, die sie verwenden:@1 fliesskursiv Zitat:»Die eigentlichen Probleme in der Sicherheit liegen darin, dass man Software und Systeme mit sehr vielen Unbekannten, d.h. nicht offengelegten Komponenten verwendet. Bei Betrachtung der Kryptografie kann man sich eine starke Kryptografie unter MS NT vorstellen, die über einen Treiber in das System eingebunden wird. Die Dateien werden zwar wunderbar verschlüsselt, aber über einen verdeckten Kanal kommen sie letztendlich doch dorthin, wo sie nicht hin sollen. [...Die Kryptografiekomponente] ist gar nicht der entscheidende Sicherheitsfaktor, sondern die Tatsache, dass ich weiß, dass meine Kryptografiekomponente überhaupt korrekt angesprochen und nicht umgangen wird – all dieses sind die entscheidenden Sicherheitsfragen und diese hängen unmittelbar mit der Transparenz der verwendeten Betriebssystemsoftware zusammen. Insofern laufen das Wassenaar-Aggreement und damit verbundene Aktivitäten im Grunde am eigentlichen Problem vorbei. «<@3 hoch fliess>32<@$p>@1 fliess mit:@1 fliess ohne:Für die Verschlüsselung von Mail mit dem <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Public/Private-Key<@$p>-Verfahren gibt es derzeit zwei offizielle Internetstandards: PGP (<@1 fliess kursiv>Pretty Good Privacy<@$p> von Phil Zimmerman) und S-MIME (<@1 fliess kursiv>Secure Multipurpose Internet Mail Extensions<@$p>). S-MIME setzt für die Generierung der Zertifikate eine zentrale Instanz voraus, die alle Schlüssel beglaubigt.@1 fliess mit:<t-2>Auch Open-PGP ist mittlerweile in Zusammenarbeit von Lutz Donner<\h>h<t$>acke aus Jena und Jon Callas von PGP zur Standardisierung vorbereitet und verabschiedet worden (RFC 2440), so dass auch hier der gleiche Status erreicht ist wie bei S-MIME. Die Zertifizierung von PGP-Schlüsseln beruht auf einem Vertrauensnetz (<@1 fliess kursiv>Web of Trust<@$p>) ohne zentrale oberste Instanz. Es erlaubt ebenfalls, hierarchische Strukturen für die Zertifizierung zu bilden, doch die Grundstruktur ist ein Netzwerk von Leuten, die sich kennen und vertrauen. PGP weist zwei Probleme auf. Erstens steht es nicht unter der GPL. Das heißt, der Source Code ist zwar verfügbar, aber es ist nicht erlaubt, Modifikationen daran vorzunehmen, zusätzliche Features einzubauen, Teile aus PGP zu entfernen, die man für Sicherheitsrisiken hält und Software bereitzustellen, die auf PGP basiert. Zweitens wurde PGP von Network Associates gekauft. Als kommerzielles amerikanisches Produkt unterliegt es damit den Exportbeschränkungen – starkes PGP ab einer bestimmten Schlüssellänge durfte bis vor kurzem nicht ausgeführt werden. Außerdem war Network Associates Mitglied der <@1 fliess kursiv>Key Recovery Alli<\h>ance<@$p>, einem Zusammenschluss von US-Firmen, die eine Infrastruktur für die treuhänderische Schlüsselhinterlegung nach den Vorstellungen der US-Regierung betreiben will. Diese vergibt nur dann Aufträge an eine Firma, wenn sie Mitglied in der <@1 fliess kursiv>Key Recovery Alliance <@$p>ist. Network Associates trat aus dem Konsortium aus und wieder ein. Phil Zimmerman behauptet zwar, es gäbe eine klare Position gegen <@1 fliess kursiv>Key Recovery<@$p>, aber die Firma, der diese Software gehört, hat offensichtlich eine andere Meinung.In der jüngsten Entwicklung von PGP sind Features hinzugekommen, die sicherheitstechnisch bedenklich sind und die Rückwärtskompatibilität einschränken. Einer der Kritikpunkte betrifft das <@1 fliess kursiv>Corporate Message Recovery <@$p>(CMR). Anders als beim firmenexternen <@1 fliess kursiv>Key Recovery<@$p>, das die NSA anstrebt, wird dabei jede Mail an einen Angestellten einer Firma nicht nur mit dessen persönlichem öffentlichen Schlüssel, sondern auch mit dem seines Unternehmens verschlüsselt. Ziel ist, dass die Firma auch dann noch auf die Korrespondenz dieses Mitarbeiters zugreifen kann, wenn er selbst nicht mehr zur Verfügung steht. Auch hier ist vorstellbar, dass zu einem späteren Zeitpunkt eine Überwachung eingerichtet wird, indem man Firmen vorschreibt, eine solche <@1 fliess kursiv>Corporate Message Recovery<@$p> mit einem Schlüssel zu verwenden, der bei staatlichen Stellen hinterlegt ist.<@3 hoch fliess>33<@$p><t-2h99.001>Eine weitere bedenkliche Neuerung ist die <x@1 fliess kursiv><t-2h99.001>Message-ID<@$p><t-2h99.001>, die seit PGP-Version 5.0 unverschlüsselt im Header steht. Offiziell soll sie dazu dienen, eine Mail, die in mehreren Teilen verschlüsselt ist, wieder zusammenzusetzten, doch existiert für diese Aufgabe der technische Standard <x@1 fliess kursiv><t-2h99.001>Multipart-MIME<@$p><t-2h99.001>. Vollkommen unbegründet ist diese <x@1 fliess kursiv><t-2h99.001>Message-ID <@$p><t-2h99.001>in einer PGP-Message, die nur aus einem Teil besteht. Andreas Bogk (CCC) sieht darin einen weiteren Weg, über den eine PGP-Nachricht angegriffen werden kann:<t$h$>@1 fliesskursiv Zitat:»PGP ist ja ein Hybridverfahren, d.h. ich habe einmal ein <@1 fliess normal>Public Key-<@$p>Verfahren, mit dem ein Sitzungsschlüssel verschlüsselt wird. Mit diesem <@1 fliess normal>Session Key<@$p> wird dann in einem klassischen symmetrischen Verfahren der Rest der Mail verschlüsselt. Die beiden Punkte, die ich angreifen kann, sind einmal das <@1 fliess normal>Public Key<@$p>-Verfahren. Wenn ich es schaffe, aus dem <@1 fliess normal>Public Key <@$p>den <@1 fliess normal>Secrete Key <@$p>zu generieren, kann ich die Message lesen. Der zweite Punkt, den ich angreifen kann, ist der <@1 fliess normal>Session Key<@$p>, der für den symmetrischen Schlüssel verwendet wird. Wenn ich den knacke, kann ich die Message auch lesen. Und diese <@1 fliess normal>Message-ID <@$p>bringt nun einen dritten Angriffsweg mit. Der <@1 fliess normal>Session Key <@$p>wird nämlich aus einem Entropie-Pool generiert. Da werden also Zufallsinformationen gesammelt, aus dem der <@1 fliess normal>Session Key<@$p> gewonnen wird. Und direkt nachdem der Session Key generiert wurde, wird diese <@1 fliess normal>Message-ID<@$p> aus demselben Pool generiert. Da ist zwar ein Prozess dazwischen, der sich <@1 fliess normal>Whitening <@1 fliess kursiv>nennt <@$p>und verhindern soll, dass man daraus den Zustand des Pools zurückrechnet, aber erstens ist die Forschung auf dem Gebiet noch ein bisschen dünn und zweitens wird hier grundlos ein weiterer Angriffsweg eingeführt.«<@3 hoch fliess>34<@$p>@1 fliess mit:<*p(0,14.173,0,9,0,0,g,"Deutsch")>@1 fliess ohne:<t-1>Inkompatibilitäten in der PGP-Welt schließlich haben die Umstellung des zu Grunde liegenden <x@1 fliess kursiv><t-1>Public-Private-Key<@$p><t-1>-Algorithmus bewirkt. Das bislang verwendete RSA-Verfahren hat den Nachteil, patentiert zu sein. Das Patent auf dem jetzt in PGP verwendeten Diffie-Hellman-Verfahren ist 1998 ausgelaufen. Dieser Schritt, der die Zugänglichkeit der Technologie erhöht, führte jedoch dazu, dass diejenigen, die PGP 6.0 verwenden, nicht mehr mit denjenigen kommunizieren können, die ältere Versionen benutzen. <t$>@1 fliess mit:Aus diesen Gründen wurde das freie Projekt <@1 fliess kursiv>GNU Privacy Guard<@3 hoch fliess>35 <@$p>ins Leben gerufen. Eine Gruppe um Werner Koch hat dazu den Open PGP-Standard neu implementiert. GnuPG kann sowohl mit alten als auch mit neuen PGP-Versionen kommunizieren. Es enthält keine <@1 fliess kursiv>Corporate <\h>Message Recovery<@$p> oder <@1 fliess kursiv>Message-IDs<@$p>. Und schließlich steht es unter der GPL, so dass sichergestellt ist, dass der Quellcode überprüfbar bleibt <\n>und jeder die Teile, die ihm verdächtig vorkommen, entfernen kann. Als <\n><@1 fliess normal>freie<@1 fliess kursiv> <@$p>Software ist es von den Restriktionen des Wassenaar-Abkommens ausgenommen. »Durch diesen Open Source-Ansatz ist eine dezentrale Evaluierung durch zahlreiche Stellen und Nutzer auch für gehobene Ansprüche realisierbar. ... Mit dem Konzept von GPG könnte ein Werkzeug geschaffen werden, das ohne Einschränkungen für alle Benutzerschichten frei und unentgeltlich verfügbar ist (inklusive Behörden, kommerzielle Nutzer, Privatbenutzer etc.).«<@3 hoch fliess>36<@$p>Das Bundeswirtschaftsministerium ließ seinen Kryptoeckwerten Taten folgen. Als erstes freies Softwareprojekt überhaupt erhielt GnuPG Förderung aus Bundesmitteln in Höhe von mehreren Hunderttausend Mark.<@3 hoch fliess>37<@$p> Ziel des BMWi ist es, dass GnuPG möglichst breit eingesetzt wird, und dazu sind komfortable Benutzerschnittstellen und eine Einbindung in die verschiedenen Betriebssysteme und E-Mail-Programme erforderlich. Im August 2001 lagen Versionen für GNU/Linux, Mac<\h>OS X, Solaris, FreeBSD und weitere Unixe sowie für MS-Windows 95, 98 und NT vor. Die grafische Benutzerschnittstelle <I>GNU Privacy Assistant<$> (GPA) gab es für GNU/Linux und für Windows. Das Task<\h>leistentool <I>Windows Privacy Tray<$> (WinPT) bindet GnuPG in jeden beliebigen Mail-Client ein. Ferner gab es bereits ein PlugIn für MS Outlook, und weitere clientspezifische Plug-Ins waren in Arbeit. Unter GNU/Linux stehen mit KMail <\n>und <f"ZapfDingbats">nn<f$> E-Mail-Clients mit integriertem GnuPG-Support zur Verfügung. GnuPG steht, wie der Namen erwarten lässt, natürlich unter der GPL. Der Schritt des BMWi zu einer »freien eMail-Verschlüsselung für alle« sorgte in den USA für einiges Aufsehen. Als die US-Regierung im Dezember 1999 die Exportbeschränkungen für PGP mit starker Verschlüsselung vollständig fallen ließ, vermutete das Bundesamt für Sicherheit in der Informationstechnik darin eine Reaktion auf die Förderung der freien Alternative durch die Bundesregierung. »Die Erteilung der Exportlizenz durch die US-Regierung kommt überraschend schnell nachdem das deutsche Bundesministerium für Wirtschaft und Technologie (BMWi) das koordinierte Open-Source-Projekt GNU Privacy Guard (GPG) mit 250 000 Mark bezuschusst hat, um die Potenziale von Open Source für den Sicherheitsbereich erschließen zu können. Die erhoffte Signalwirkung für die europäische Wirtschaft könnte auch die US-Regierung zum Einlenken gebracht haben und sei es, um zu verhindern, dass ein von Europa aus geleitetes Projekt Sicherheitsstandards setzen könnte.«<@3 hoch fliess>38<@$p> Weniger zurückhaltende Stimmen mutmaßten, dass PGP Standard bleiben solle, weil die US-amerikanischen Sicherheitsdienste darin eine Hintertür eingebaut hätten.@1 fliesskursiv Zitat:<f"FFScala">@2  ZÜ 2:Militärische Software und Hochsicherheitsbereiche@1 fliess mit:@1 fliess ohne:Die Sicherheitsanforderungen sind beim privaten und wirtschaftlichen Gebrauch andere als in sensitiven Bereichen wie Banken, Atomkraftwerken, Militär und Nachrichtendiensten. Selbst in diesen Einsatzfeldern gewähren Softwareunternehmen ihren deutschen Kunden keinen Einblick in den Quellcode. Für den Einsatz im Hochsicherheitsbereich evaluiert das BSI freie Software. Andere hingegen, wie der Hamburger Informatikprofessor Klaus Brunnstein, halten freie Software für nicht hochsicherheitstauglich. @1 fliess mit:In militärischen Einsatzbereichen geht seit einigen Jahren sowohl in den USA wie in Europa der Trend weg von spezialisierter Software. 1994 gab US-Präsident Clinton im Zuge einer Kostenreduzierung die Anweisung aus, dass Behörden wo immer möglich handelsübliche Software einsetzen sollten. Für spezialisierte Anwendungen in Bereichen wie Militär oder Banken werden auf den Kunden zugeschnittene Informationssysteme eingesetzt. Auch das im Januar 1997 gestartete Programm <@1 fliess kursiv>Information Technology for the 21st Century <@$p>(IT-21) sieht den Übergang zu einem Einsatz von handelsüblichen PCs und Industriestandards in taktischen und unterstützenden Kriegsführungsnetzwerken vor. Ziel von IT-21 ist es, alle US-Streitkräfte und letztlich auch die Alliierten der Amerikaner mit einem Netzwerk zu verbinden, das eine nahtlose Sprach-, Video- und Datenübertragung von geheimer und nicht geheimer, taktischer und nicht taktischer Information über dieselben PCs erlaubt. Konkret heißt dies, dass die gemeinsame Betriebsumgebung der US-Streitkräfte auf Win<\h>dows NT, MS Exchange und MS Office beruhen soll. Die Verwendung aller Nicht-Standard-Netzwerkbetriebssysteme und E-Mail-Produkte wurde zum Dezember 1999 eingestellt (vgl. Clemins, o.J.). Neben den <@1 fliess kursiv>Dual-Use<@$p>-Produkten macht die Entwicklung von Software für den ausschließlich militärischen Einsatz (<@1 fliess kursiv>Military Unique Development<@$p>) einen zunehmend kleineren Anteil aus, der aber als wichtigste Kategorie für die nationale Sicherheit angesehen wird (Waffensysteme, störsichere Kommunikation und Atombombensimulationen). Selbst für diese Kategorie von Software werden Stimmen lauter, die den »Basar« als das beste Entwicklungsmodell ansehen.<t-3>In seinem Aufsatz »Open Source and these United States« zählt C. Justin<t$> Seiferth, Major der US Air Force, die Vorteile auf, die das US-Verteidigungsministerium (DoD) daraus ziehen könnte, wenn es generell eine Open Source-Lizenzierungspolitik betreiben würde. Dadurch könnten Kosten gesenkt sowie die Qualität der Systeme und die Geschwindigkeit, mit der sie entwickelt werden, erhöht werden. Die Zusammenarbeit mit anderen Organisationen, Händlern und Alliierten werde durch quelloffene Software erleichtert. Das Gleiche gelte für die Beziehungen zwischen Behörden unter dem DoD, da sie nicht auf finanziellen Transaktionen beruhen, sondern auf Tausch und Management-Abkommen. Da viele Entwickler die Gelegenheit, interessante Arbeit zu leisten, höher schätzten als finanzielle Entlohnung, sei quelloffene Software auch ein Anreiz, IT-Spezialisten im DoD zu halten. »Die Übernahme von Open Source-Lizenzierung könnte es dem Militär erlauben, von der gegenwärtigen Begeisterung zu profitieren, die die Methoden der offenen Lizenzen auslösen« (<@6 Caps>Seiferth<@$p>, 1999). Die Hauptvorteile einer Übernahme der Open Source-Konzepte für das DoD sieht Seiferth in der Interoperabilität, im langfristigen Zugang zu archivierter Information und in einem Kos<\h>tenfaktor, der bislang verhindert habe, dass das Militär auf dem neuesten Stand der Softwareentwicklung bleibt.<@3 hoch fliess>39<@$p> Offene Dokumentation könne für den jeweiligen Anwendungsbereich angepasst werden. Zur Frage der Sicherheit schreibt er: @1 fliesskursiv Zitat:»Während viele nicht technische Manager glauben, die Freilegung des Quellcodes senke die Sicherheit eines Systems, zeigt die Erfahrung das Gegenteil. Sicherheits-›Löcher‹ sind Unterlassungen oder Schwächen, die in den Code hineingeschrieben wurden. Den Code in ein Binärprogramm zu kompilieren, beseitigt das Sicherheitsloch nicht, noch wird es dadurch vor neugierigen Augen versteckt. [...] Die offene Lizenzierung soll hier Abhilfe schaffen. Die meisten Sicherheitsexperten sind der Ansicht, dass die Freigabe des Quellcodes die Sicherheit eines Softwaresys<\h>tems gerade stärke. [...] Wenn Komponenten von ausländischen oder unvertrauten Zulieferern in ein System integriert werden, machen offene Lizenzen es ferner sehr viel unwahrscheinlicher, dass ein versehentliches oder beabsichtigtes Sicherheitsproblem eingeführt wird. [...] Offene Lizenzen gewährleisten schließlich, dass eine Fehlerkorrektur ungehindert von proprietären Lizenzauflagen eingebaut werden kann« <@6 Caps>(Seiferth, 1<@1 fliess normal>999, S. 43).<@$p>@1 fliess ohne:Es gäbe bereits Open Source-Aktivitäten, auf die eine breitere Übernahme innerhalb des DoD aufsetzen könnte: »Innerhalb des Verteidigungsministeriums sind die <@1 fliess kursiv>National Laboratories<@$p> und die <@1 fliess kursiv>Defense Advanced Projects Agency<@$p> die sichtbarsten Anwender und Produzenten von offen lizenzierten Systemen. Sie haben solche Fortschritte veröffentlicht wie die ursprüngliche Firewall und die Toolkits für Netzwerksicherheit. Oder ein jüngeres Beispiel: Im vergangenen Jahr haben die <@1 fliess kursiv>National Laboratories<@$p> kostengünstige Supercomputer gebaut« (ebd., S. 8). Seiferth bezieht sich dabei auf die FORTEZZA-Algorithmen der NSA, die im Defense Messaging System verwendet werden, und auf das Firewall-Toolkit der DARPA, als zwei extrem erfolgreiche Beispiele dafür, wie eine offene Lizenz die Akzeptanz und die Qualität von Systemen im Sicherheits<\h>bereich dramatisch erhöhen kann (vgl. ebd., S. 34). Ferner führt er die Forschungsförderung für eine der größten Erfolgsgeschichten der freien Software an, das Internetprotokoll TCP/IP. Mit einem jährlichen IT-Haushalt von mehr als 57 Milliarden Dollar stelle die US-Regierung ein bedeutendes Marktgewicht dar, das sie verwenden könne, um auch proprietäre Anbieter zu ermuntern, ihre Produkte unter freie Lizenzen zu stellen. @1 fliess mit:@2  ZÜ 2:Vertrauenswürdige Instanzen@1 fliess mit:@1 fliess ohne:Softwaresysteme haben mittlerweile eine Komplexität erreicht, die nur noch wenige Experten durchschauen können, gleichzeitig sind sie ein Massenmarktprodukt. Es besteht also eine Dichotomie zwischen einer Minderheit von Experten und der Mehrheit von Benutzerinnen, die zwar im Prinzip die Möglichkeit hat, freie Softwaresysteme zu überprüfen – effektiv hat sie sie jedoch nicht. Für eine Mehrheit ist – bei quelloffener ebenso wie bei proprietärer Software – das Funktionsprinzip von IT-<\h>Sicherheit nichts als der Glaube, dass sie funktioniert. Der Mehrheit ist es nur möglich, auf das Urteil von Experten zu vertrauen.<\c>@1 fliesskursiv Zitat:»Wenn ich mir irgend ein System beschaffe, und ich bin mir nicht klar, wie sicher das ist, kann ich mir eine Sicherheitsfirma anheuern, die mir das erklärt und die ich selber dafür bezahle. Das ist die eine Lösung. Die andere Möglichkeit wäre, dass Distributoren dafür zahlen, dass sie sichere Systeme haben und das jemanden haben prüfen lassen. Das ist die kommerzielle Variante. Oder, und das wäre vielleicht das Effektivste, diese vertrauenswürdigen Instanzen beteiligen sich direkt an der Entwicklung.<*t(28.346,0,"1  ")>	Wer könnten diese Instanzen denn sein? Bisher hatten wir für solche Überprüfungen ein Standardmodell: Der TÜV-IT oder das BSI bekommt einen Antrag auf den Tisch, ein bestimmtes System auf einen bestimmten Sicherheitsgrad hin zu prüfen. Die bekommen auch den Quelltext, machen hinterher einen Stempel drauf, nachdem sie Gebühren kassiert haben, und das war’s. Wenn ich Open Source-Software habe, kann ich mir diese Sicherheitsakteure für entsprechende Zwecke, für die ich die Software hinterher einsetzen will, aussuchen. Das können entweder staatliche Akteure sein, d.h. Datenschutzbeauftragte, wenn es um Datenschutzfragen geht, das BSI oder TÜV-IT wie herkömmlich, oder ich kann mir auch den CCC vorstellen. [...] Man kann sich genauso gut vorstellen, dass diese vertrauenswürdigen Instanzen national gerufen werden oder, wenn ich ein System global vertreiben will, dass ich mir international verteilt verschiedene Instanzen ins Boot hole. Und man könnte sich sogar vorstellen, dass hier neue organisatorische Strukturen geschaffen werden, im Zusammenhang mit der Open Source-Bewegung z.B. eine <@1 fliess normal>Security Task Force. <@$p>Eine ex-post-Analyse eines Riesenberges von Source Code ist genauso schwierig für solche Experten, wie für das BSI, wie für jeden anderen. Wichtiger und besser ist, diese Experten gleich in den Entwicklungsprozess einzubeziehen.«<@3 hoch fliess>40<@$p>@1 fliess mit:@1 fliess ohne:Bei den vertrauenswürdigen Instanzen ist eine gewisse Vielfalt gefordert, da eine Instanz, die für eine Person oder eine Firma vertrauenswürdig ist, <t-2>es nicht unbedingt auch für andere ist. Die Enquete-Kommission »Deut<\h>sch<\h>l<t$>ands Weg in die Informationsgesellschaft« des Deutschen Bundestages der letzten Legislaturperiode hat in ihrem Bericht zur IT-Sicherheit eine »Stiftung Software-Test« gefordert, die sich mit den Kriterien und mit der Evaluation von Software unter IT-Sicherheitsaspekten auseinandersetzen sollte (<@6 Caps>Deutscher Bundestag<@$p>, 1998). Statt einer solchen zentralisti<t-2>schen Stiftung wäre auch ein Netzwerk von vertrauenswürdigen Instanzen <t$>vorstellbar. In jeden Fall ist deutlich, dass eine begründete Vertrauensbildung eine komplexe Sicherheitsinfrastruktur erfordert. Dokumenta<\h>tion des Quellcodes und Werkzeuge, die sein Studium erleichtern, wären für eine Überprüfung auch von zertifizierter Software hilfreich. Auch, ob eine gegebene Software tatsächlich aus einem zertifizierten Quellcode kompiliert worden ist, müsste überprüfbar sein. Auch an die Informatikausbildung stellen sich neue Anforderungen. Ein Sicherheitsbewusstsein, die Einhaltung von RFCs, die Kommentierung von Code sollten im Curriculum stärker gewichtet werden. Bogk zufolge handelt es sich bei 95 Prozent aller Sicherheitsprobleme um triviale Fehler, deren Vermeidung in der Universität gelehrt werden müsste. @1 fliess mit:Zur Vermittlung der Prinzipien freier Software wäre es zudem an den öffentlichen Universitäten, mit gutem Beispiel voranzugehen. TCP/IP, BSD, der Linux-Kernel und zahlreiche weitere Software ist an Universitäten entwickelt worden. In den USA muss Software, die mit staatlichen Mitteln entwickelt wurde, allen zugute kommen. An deutschen Hochschulen gibt es diese Verpflichtung nicht. Durch die Zunahme der Drittmittelforschung gestaltet sich die Situation noch schwieriger. Auch hier gibt es – nicht zuletzt im Interesse der Sicherheit – einen Bedarf nach strukturellen Änderungen. »Ich denke, dass diese ganze Auseinandersetzung um IT-Sicherheitsfragen ..., die momentan nur als Dichotomie gesehen wird von <@1 fliess kursiv>Security by Obscurity<@$p>, die niemand überprüfen kann, versus Offenheit, die jeder überprüfen kann, eigentlich nur ein erster Schritt ist. Dieser erste Schritt wird dazu führen, dass wir eine Infrastruktur von Überprüfbarkeit und von vertrauenswürdigen Instanzen brauchen, die das auch beglaubigen kann, was wir im offenen Code lesen können.«<@3 hoch fliess>41<@$p>@2  ZÜ 1:<\c>Für einen informationellen Umweltschutz@1 fliess mit:@1 fliess ohne:Die offene Softwarekooperation geht, wie gezeigt wurde, bis auf die Frühzeit des Computers zurück – und die freie Wissenskooperation bis auf den Beginn der Menschheit überhaupt. Die Schließung des Wissens unter dem Konzept des »geistigen Eigentums« beginnt mit dem gutenbergschen Buchdruck und entwickelte besonders im 20. Jahrhunderts ein komplexes Gefüge von rechtlichen, technischen und ethischen Regularien. »Information« wird zum Schlüsselbegriff dieses Jahrhunderts. Mathematisch-technisch wird sie von der Informationstheorie erschlossen, philosophisch als eigenständige ontische Kategorie entdeckt. Norbert Wieners Ur-Satz lautet: »Information ist Information und nicht Materie oder Energie.«<@3 hoch fliess>1<@$p> Militärisch entschieden Information und Kommunikation den Zweiten Weltkrieg. Im Brennpunkt der Kybernetik bündeln sich die Konzepte aus den unterschiedlichen Disziplinen: Die Welt wird durch die Brille der Information und Kommunikation sichtbar.@1 fliess mit:Die aus dem Zweiten Weltkrieg hervorgegangenen Innovationen wie Mikroelektronik und Prozesssteuerung, Raketentechnologie und Hochfrequenzfunk schufen neue expansive Industrien. Medienkonzerne traten hervor, die die Kritische Theorie als »Kulturindustrie« reflektiert. Im Mainstream-Diskurs setzte sich in den 1960ern der Begriff der »Informationsgesellschaft« durch. Als volkswirtschaftlich relevante Größe wird Information von Fritz Machlup Ende der 1950er erstmals quantifiziert und ökonometrisch erhoben. Autoren wie Peter Drucker, Daniel Bell und Yujiro Hayashi schreiben die Theorie dieser Gesellschaft. Hayashi (1969) z.<\!q>B. unterscheidet die funktionellen und die informationellen Aspekte von Wirtschaftsgütern. In der Informationsgesellschaft, so schreibt er, steigen die Anteile, die den sozialen Status oder die Persönlichkeit ihres Besitzers widerspiegeln und damit die enthaltenen ›Informationskosten‹ (Forschung und Entwicklung, Design usw.). Die Computerisierung erfasst zunächst die Produktion und Verwaltung. Mit dem PC hält sie Einzug in die Lebenswelt aller. In der Arbeitswelt dieser Gesellschaft dreht sich alles um informationelle Produktionsmittel: Plattformen und Arbeitsumgebungen, mit deren Hilfe der beliebig formbare Werkstoff Information zu Produkten und Dienstleistungen wird. Im 20. Jahrhundert wurden wesentliche Teile der Wissensinfrastruktur in direkter staatlicher oder öffentlich-rechtlicher Regie entwickelt (Bildung, Rundfunk, Telekommunikation). An seinem Ende zog sich der Staat auf eine regulierende Funktion des Wissens<\h>mar<t-1>kts zurück. Die Informations- und Kommunikationstechnologie lässt nichts unberührt, ob Alltag und Bildung, Wirtschaft und Verwaltung, Kultur oder Wissenschaft. Der Siegeszug der Universalmaschine Computer lässt alles andere als Anhängsel der informationellen Welt erscheinen. <t$>Markt braucht Mangel. Die immer wieder benannten Kennzeichen der Ware Wissen sind jedoch ihr nicht ausschließlicher und ihr nicht erschöpflicher Konsum. Wenn B weiß, was nur A wusste, verfügt A immer noch über dieses Wissen, und das auch dann noch, wenn C, D und der Rest der Menschheit ebenfalls davon Kenntnis erlangen. Information zu verkaufen, heißt in den allermeisten Fällen, sie zu publizieren. Doch was bereits öffentlich ist, will niemand mehr kaufen. Solange Wissen an physische Verbreitungsstücke gebunden ist, beruht der Mangel auf der Verfügbarkeit der Produktionsmittel für solche Datenträger. Wird es jedoch von körperlichen Medien freigesetzt, so tritt es in eine Wissensumwelt ein, die ihrer »Natur«<@3 hoch fliess>2<@$p> nach<@3 hoch fliess> <@$p>keinen Mangel kennt. Der vernetzte Computer ist eine ideale Umwelt für den Überfluss an Wissen. Als Software konstituiert Wissen diese Welt und ist zugleich die Form, die alle Wissensgüter annehmen. Um daraus eine Marktplattform zu machen, bedarf es einer Schließung. Diese setzte in den 1970ern ein, zunächst durch moralische Appelle (<@1 fliess kursiv>»Letter to Fellow Hobbyists«<@$p>), dann durch Einbeziehung von Software in das Urheber- und Patentrecht. »Geis<\h>tiges Eigentum ist die Rechtsform des Informationszeitalters«, schreibt der amerikanische Rechtsgelehrte James Boyle (1997) bündig. Der »Wert«, der heute als geistiges Eigentum geschützt ist, beziffert sich in den Hunderten von Milliarden Dollar. Den alten Medien<\h>oligopolen gesellten sich neue wie Microsoft hinzu. Software ist zwar erst vor kurzem, aber doch vermeintlich für alle Zeiten zu einem Industrieprodukt geworden. Genauso wie Autos oder Medikamente wird Software seither hinter verschlossenen Türen und eingezäunt von Geheimhaltungsvorschriften und Patenten hergestellt. Wie in anderen Massenmärkten werden Intransparenz, gezielte Inkompatibilitäten und FUD-Strategien (<@1 fliess kursiv>Fear, Uncertainty, Doubt<\!q><@3 hoch fliess>3<@$p>) eingesetzt, um Konkurrenten auszustechen. Die Rechtsabteilungen der Wettbewerber tragen ihre Dauergefechte abseits der öffentlichen Aufmerksamkeit aus. Die Endkunden schließlich werden als potenzielle Diebe vorgestellt, denen man mit allen verfügbaren technischen, rechtlichen und propagandistischen Mitteln das Handwerk schwer machen will. <@1 fliess kursiv>Business als ususal<@$p>. Und das alles, um einen Mangel zu erzeugen, wo ohne diese Maßnahmen keiner wäre. <*h"mehr">Wissen ist kein Zuschauersport. Ein weiteres »natürliches« Merkmal der Information ist die Symmetrie der Wissenskompetenz. Wer lesen kann, kann auch schreiben. Man erwirbt die eine Kulturtechnik nicht <\n>ohne die andere. Was in der Medienumwelt mittelalterlicher Manuskripte noch galt, wird mit ihrer Industrialisierung zu einer Einbahnstraße. Immer mehr können zwar öffentlich Geschriebenes lesen, doch wer <\n>öffentlich schreiben will, muss das Nadelöhr zu den industriellen Produktionsmitteln passieren. Der Mangel ergibt sich wiederum aus den <\n>materiellen Trägern des Wissens, die Investitionsgüter wie Produktionsstudios, Presswerke und Sendeanlagen erforderlich machen. Hektografiegeräte, Fotokopierer, Audio- und Videokassetten weichen diese Beschränkung auf und bringen erste »Kleine Medien« (<@6 Caps>Arns<@$p>/<@6 Caps>Broeckmann<@$p>, 1998) hervor, doch erst mit dem vernetzten PC in jedem Haushalt entsteht eine neue »Waffengleichheit«. In derselben Umgebung, in der Texte, Webseiten, Bilder und Klänge gelesen werden, können sie auch geschrieben werden. Über denselben Kanal, das Internet, über den mich die Information erreicht, kann ich antworten und meine eigene Information veröffentlichen. Für die ungebändigte Entfaltung dieser dialogischen Kraft des Internet stehen die <@1 fliess kursiv>Usenet Newsgroups<@$p>.<@3 hoch fliess>4<@$p> In den Anfangsjahren des Internet, als Entwickler und Nutzer noch identisch waren und Werkzeuge für ihren eigenen Gebrauch entwickelten, statt für »Konsumenten«, war eine solche Symmetrie selbstverständlich. Mail, News und das junge Web erlaubte jeder, die lesen konnte, im selben Format zu antworten. Das leitende Kommunikationsmodell war der Dialog und die Kooperation unter Gleichen. Netscape folgt noch heute dem Designkriterium der Symmetrie, indem es seinem Web-Browser den <@1 fliess kursiv>Editor Composer<@$p> beigibt. HTML ist quelloffen im besten Sinne. <*h"Standard">Diese offene Umwelt begann sich zu verändern, als Mitte der 90er-Jahre die alten Massenmedien das Internet entdeckten. Neue Formate wie Java, Flash und PDF bringen das Design von Hochglanzbroschüren und die <@1 fliess kursiv>Special Effects<@$p> von Hollywood ins Web. Zugleich sind es <@1 fliess kursiv>Blackboxes<@$p>, die die Spaltung in wenige professionelle Macher und Heerscharen passiver Rezipienten wieder einführen. Konsumptionsmittel (die in der Regel kostenlosen Betrachter) und Produktionsmittel werden getrennt. Das weitverbreitete PDF ist ein reines Publikationsformat, das keine Weiterverarbeitung der Information erlaubt und dazu Rechtekontrollmechanismen vorsieht. Technische Rechtekontrollsysteme, die inzwischen zusätzlich durch gesetzliche Umgehungsverbote gesichert sind, ziehen ungekannte Barrieren in den digitalen Wissensraum ein und gefährden vor allem den Vermittlungsauftrag der Bibliotheken und Bildungseinrichtungen. Zu den aktuellen Bedrohungen gehört vor allem auch die Softwarepatentierung, die, seit 20 Jahren in den USA gängige Praxis, seit 1999 auch für Europa diskutiert wird.<@3 hoch fliess>5<@$p>Digitalisierung, Privatisierung und Globalisierung der Medienkanäle führen zu einer Vervielfältigung der »Wertschöpfungsketten« für Wissenswaren.  Der unstillbare Hunger nach »Content,« um die Medien-Container zu füllen, treibt den Rechtehandel mit Medieninhalten zu ungekannten Dimensionen. Ein Beispiel: Die Zeitung von gestern galt bislang als Synomym für veraltete, wertlose Information. Tatsächlich betreiben viele Tageszeitungen (<@1 fliess kursiv>F.A.Z., Süddeutsche Zeitung, Wallstreet Journal<@$p>) ihre Online-Ausgaben auf eine ganz andere Weise. Die aktuelle Ausgabe ist für 24 Stunden zugänglich, dann wandert sie in ein Archiv, aus dem der Bestand für fünf Mark pro Artikel verkauft wird – Artikel aus einer Zeitung, die zwei Mark gekostet hat und über 100 Artikel enthielt: eine magische Wertsteigerung um das 200-fache, die die Verlage zudem nichts kostet, da die laufende Webproduktion ohnehin in der Datenbank steht, und die Autoren an dieser Zweitverwertung bislang in keiner Weise beteiligt waren. Alle an der Produktion Beteiligten (Autoren, Redakteure, Gestalter, Vorstandsvorsitzende bis zu den Zeitungsausträgern) sind vollständig entlohnt, wenn die Printausgabe der Zeitung verkauft ist. Keine wirtschaftlichen, technischen oder praktischen Gründe sprächen somit dagegen, die Archive als veröffentlichtes und damit öffentliches Wissensgut zu betrachten. Alles, was der Anbieter machen müsste, ist, den Zugang zu der Datenbank nicht zu sperren. Doch Gelegenheit macht aus Archi<t-1>ven Kaufwaren. Wie Midas wird der Rechteindustrie alles, was sie anfasst, <t$>zu Gold. Wie er vergisst sie in ihrer Gier, dass man Gold nicht essen kann. Flankiert wird die Marktwerdung der Wissensumwelt durch Kampag<\h>nen der Software- und der Medienindustrie gegen das »Raubkopieren«. Stallman vergleicht die klimatische Verschärfung mit dem anhaltenden »Krieg gegen Drogen« der US-Regierung, der bereits eine Million Menschen in die Gefängnisse gebracht hat. Er führe zu Korruption und Verzerrung der Bürgerrechte. Der Krieg gegen das Kopieren könne noch schlimmere Folgen haben.@1 fliesskursiv Zitat:»Stellt euch nur vor, wieviel Angst erforderlich sein wird, um Menschen davon abzuhalten, Kopien von Dingen auf ihren Computern weiterzugeben. Ich hoffe, ihr wollt nicht in einer Welt mit so viel Angst leben. Die Sowjetunion hat versucht, Menschen davon abzuhalten, Kopien von Dingen weiterzugeben, und sie haben eine Reihe sehr interessanter Methoden dafür gefunden. Heute schlägt die US-Regierung genau dieselben Methoden vor und erlässt entsprechende Gesetze. Es hat sich he<\h>rausgestellt, dass es nur bestimmte wirkungsvolle Methoden gibt, um Menschen daran zu hindern, Kopien von Dingen miteinander auszutauschen. Es spielt keine Rolle, ob der Beweggrund politische Zensur ist oder einfach die Durchsetzung der Monopolmacht einiger Unternehmen – sie benutzen dieselben Methoden, und sie machen die Gesellschaft auf dieselbe Weise monströs.«<@3 hoch fliess>6<@$p>@1 fliess mit:@2  ZÜ 2:Die kontrafaktische Wissensallmende @1 fliess mit:@1 fliesskursiv Zitat:»Dass wir Marktsystem und Warentausch hinter uns lassen, ist derzeit für viele Menschen noch genauso unvorstellbar, wie es die Einhegung und Privatisierung von Land und Arbeit und damit die Einbindung in Verhältnisse des Privateigentums vor einem halben Jahrtausend gewesen sein mögen.« <@1 fliess normal>(<@6 Caps>Jeremy Rifkin<@1 fliess normal>)@1 fliess mit:@1 fliess ohne:Max Weber beschreibt den Grund für das Allmende-Dilemma so: »Die Chance z. B. aus einem bestimmten Ackerstück durch dessen Bearbeitung Unterhalts- oder Erwerbsgüter zu gewinnen, ist an ein sinnfälliges und eindeutig abgrenzbares sachliches Objekt: eben das konkrete unvermehrbare Ackerstück gebunden ...« (<@6 Caps>Weber<@$p>, 1995, S. 144). Wissensobjekte dagegen sind beliebig, und wenn sie digital vorliegen, verlustfrei vermehrbar und nur schwer abzugrenzen. Der digitale Acker ist ein Zauberhut, aus dem sich ein Kaninchen nach dem anderen ziehen lässt, ohne dass er jemals leer würde.<@3 hoch fliess>7<@$p> Die »Verbraucher« von Informations<\h>gütern »verbrauchen« ja eben gerade nichts. Der Mangel an Mangel erlaubt die geistige Speisung aller. @1 fliess mit:Bei materiellen Gütern besagt die Lehre vom »Allmende-Dilemma«, dass ohne Regulierung jeder versuchen werde, den maximalen Nutzen aus einer gemeinsamen Ressource zu ziehen, ohne Rücksicht auf deren Erhaltung. Generationen von Forschern in Wirtschaft, Politik und Umweltschutz fanden zahlreiche empirische und theoretische Belege für Übernutzung und Trittbrettfahrerei. Was frei ist für alle, wird von niemandem geachtet. Individuen handeln rational in ihrem Eigeninteresse und produzieren dadurch Ergebnisse, die für das Kollektiv und damit wiederum für jeden Einzelnen irrational sind. Oder im Ergebnis des strukturverwandten »Gefangenen-Dilemmas«: Kooperieren lohnt sich nicht.Die Ökonomen sehen üblicherweise zwei mögliche Antworten auf diese »Tragödie«: eine zentrale Regulierung der gemeinsamen Güter z.B. durch den Staat oder ihre Privatisierung, da der alleinige Besitzer einer Ressource ein egoistisches Interesse daran habe, sie nachhaltig zu bewirtschaften. Die Politikwissenschaftlerin Elinor Ostrom hat in ihrem Buch »Die Verfassung der Allmende. Jenseits von Staat und Markt«  (<@6 Caps>Ostrom<@1 fliess normal>,<@$p> 1999) einen dritten Satz von Alternativen aufgezeigt: Die Nutzer eines gemeinsamen knappen Gutes einigen sich untereinander auf Regeln, deren Einhaltung sie wechselseitig, vielleicht unterstützt durch externe Mediatoren, überwachen. Lokales Wissen stützt lokale Entscheidungen. An einer Fülle von Fallbeispielen vor allem aus den Bereichen offen zugänglicher Wasserressourcen, der Küstenfischerei und der Bergweiden zeigt Ostrom, dass es möglich ist, dem Entweder-Oder von zentraler Kontrolle und Privatisierung zu entkommen. Weitere funktionierende Alternativen zeigen sich in Genossenschaften, Kooperativen, selbstverwalteten Betrieben und Tauschringen.Wie stellt sich das Problem nun bei Wissensgütern dar? Informationelle Produktionsmittel (Compiler, Editoren, CVSe) und Güter sind nicht erschöpflich, weil beliebig kopierbar. Ihre Nutzung schließt die gleichzeitige Nutzung durch andere nicht aus. Was entspräche dann aber einer »Übernutzung«? Sie meint, etwas aus dem gemeinschaftlichen Pool zu nehmen, es zu verändern und ohne Quellcode zu verkaufen, ohne die Änderungen an die anderen Beteiligten zurückzugeben. Die Geschichte von Unix demonstriert den Effekt. Solange AT&T Unix aus kartellrechtlichen Gründen nicht kommerzialisieren durfte, wurde es weitgehend offen von einer wachsenden Gemeinde von Studenten und Informatikern in Universitäten und Unternehmen weiterentwickelt. Dann setzte die private Bewirtschaftung von Unix ein. Der »freie« Markt, so hören wir unablässig, bringe Vielfalt hervor, und so entstanden »Aix«, »Xenix«, »HP/UX«, »Sinix«, »Sun/OS« und anderes mehr. Alle setzten auf eine gemeinsame Ressource auf, fügten ihr proprietäre Funktionalitäten hinzu, um sich von der Konkurrenz abzugrenzen und die Anwender in der eigenen Unix-Geschmacksrichtung einzuschließen. Die Einzäunung der Allmende begann. Der »freie Wettbewerb«, den die »unsichtbare Hand« des Marktes angeblich zum Wohl der Gesamtheit führt, brachte eine Zerstückelung in eingezäunte inkompatible Parzellen hervor. Die Berkeley-Gruppe und dann GNU und Linux stehen schließlich für Ostroms drittes Modell. Erst sie verwandelten Unix wieder in ein Gemeinschaftsgut. Alle drei ergriffen die Initiative und erwarben sich durch ihre Arbeit die Anerkennung der Gemeinschaft. Sie wurden zu »natürlichen« Autoritäten in einem Prozess der Selbstorganisation, in dem neue Regeln und Durchsetzungsmechanismen entstehen. Rifkins Motto lässt sich hinzufügen, dass es für viele Menschen unvorstellbar ist, dass wir den ebenfalls seit einem halben Jahrtausend laufenden Prozess der Einhegung und Privatisierung des Wissens hinter uns lassen.Begreift man die Erzeugung einer Struktur des Mangels in einem Schlaraffenland des Überflusses als Hauptprojekt des 20.<\!q>Jahrhunderts, so erscheint die Erfolgsgeschichte der freien Software um so verblüffender. Unverbesserliche, die sich den dominanten Tendenzen nicht fügen wollen, gibt es immer. Doch dass aus der untergründigen Entwicklungslinie der offenen Zusammenarbeit schließlich ein gewaltiger Strom wurde, der den Glauben an die »Kathedralen« des digitalen Wissens als einzig »realistisches« Modell zu unterhöhlen begann und die Schwächen ihrer Wissensprodukte aufzeigte, kann nur mit einem habermasschen Wort als »kontrafaktisch« bezeichnet werden. Noch gibt es diejenigen, die sich, wie Stallman, aus eigener Erfahrung an die Zeit erinnern, da die freie Kooperation in der Software selbstverständlich war. Neal Stephenson argumentiert überzeugend, dass Betriebssysteme Arbeitsumgebungen und Werkzeuge von Entwicklern und keine Konsumprodukte seien, und sie daher ihrer »Natur« nach frei zu sein haben (1999). Peter Deutsch bechränkt sich nicht auf Produktionsmittel und schreib rundweg: »Software ist im Wesentlichen ein öffentliches Gut« (1996). Linux und andere jüngere Softwareprojekte zeigen, dass die Werte der Hackerethik aus der Frühzeit in der heutigen Generation weiterleben. An vielen Stellen korrespondieren sie mit Verfassungswerten. David Harris, neuseeländischer Autor des E-Mailprogrammes »Pegasus Mail« erklärt seine Motivation, es zu verschenken, so: »Ich erkannte, dass Kommunikation als ein Recht, nicht als ein Privileg angesehen werden muss. Ich dachte 1989 und denke noch heute, dass das Recht auf freie Meinungsäußerung nutzlos ist, wenn niemand einen hören kann. Pegasus Mail zu verschenken schien mir ein Weg, durch den ich Kommunikation einem viel breiteren Spektrum von Menschen zugänglich machen konnte, die sie benötigen.« Ian Clarke, der in seinem Napster-artigen <@1 fliess kursiv>Peer-to-Peer<@$p>-<@1 fliess normal>Netz FreeNet Ve<@$p>rschlüsselung und Anonymisierung vorsieht, nennt als Grund, »den freien Fluss von Informationen und Ideen im Internet zu gewährleisten, ohne Angst vor Zensurmaßnahmen irgendeiner Art.«<t0.5>Aber Unverbesserliche gibt es, wie gesagt, immer. Erstaunlich ist jedoch, dass das, was innerhalb einer vergleichsweise kleinen Gemeinde praktiziert und genutzt wurde, Ende der 90er-Jahre zum Stoff für Titelgeschichten wurde. Freie Software beweist, dass Freiheit, Offenheit und Gemeinschaft funktionieren. Und das zumal in einem Technologiefeld, das den Kern der digitalen »Wissensgesellschaft« bildet und somit Austragungsort eines scharfen Wettbewerbs um Produkte und Dienstleis<\h>tungen, Autorinnen und Kunden, Aufmerksamkeit und Kapital ist. Wirtschaftskreise, die überzeugt waren, dass es so etwas wie ein kostenloses Mittagessen nicht gebe, begannen sich dafür zu begeistern. Selbst auf dem Weltwirtschaftsforum in Davos gab es einen Workshop über Open Source-Software.<x@3 hoch fliess><t0.5>8<@$p><t0.5> An den soziologischen, wirtschaftswissenschaftlichen, anthropologischen und informatischen Fakultäten werden Diplom- und Doktorarbeiten über das Phänomen geschrieben. Ministerien, Verwaltungen und Schulen beginnen, die Vorteile von freier Software zu erkennen, sie einzusetzen und zu fördern. Ein Beispiel ist das genannte Engagement des Bundeswirtschaftsministeriums für den <x@1 fliess kursiv><t0.5>GNU Privacy Guard.<@$p><t0.5> <t$>Ein jüngeres ist die Förderung der freien 3D-Grafik-Engine <@1 fliess kursiv>Open SG Plus<@$p> durch das Bundesforschungsministerium in Höhe von nahezu sechs Millionen Mark. Zwar gibt es bereits Hunderte solcher Engines, doch keine genügt den Anforderungen der Wissenschaftler, was das Vertrauen in die Zuverlässigkeit und langfristige Verfügbarkeit derartiger Lösungen aus dem kommerziellen Bereich erschüttert hat. Nur ein freies Softwareprojekt gibt nach Ansicht der Beteiligten die notwendige Investitionssicherheit, da es von niemandem willkürlich eingestellt werden kann. »Unser Ministerium sieht ein echtes nationales Interesse an der Schaffung einer standardisierten Basis für Anwendungen der Virtuellen <\h><\n>und Erweiterten Realität,« so der zuständige Referatsleiter im BMBF.<@3 hoch fliess>9<@1 fliess normal> <@$p>Ein weiteres Projekt ist das »Open Source-Zentrum« <@1 fliess kursiv>BerliOS<@$p>,<@3 hoch fliess>10<@$p> initiiert vom Forschungszentrum für Informationstechnik GmbH der Gesellschaft für Mathematik und Datenverarbeitung (GMD), das seit Herbst 2000 den Entwicklern und Anwendern Infrastruktur, Geld und Info<t-3>rmationen zur Verfügung stellt. Mit einer Anschubfinanzierung vom Bundeswirtschaftsministerium soll <x@1 fliess kursiv><t-3>BerliOS <@$p><t-3>auch aktiv Anwenderbedürfnisse erforschen und Anstöße in Bereichen geben, in denen das Angebot freier Software heute noch ungenügend ist. Eine ähnlich geartete Initiative der Bundeszentrale für politische Bildung ist FOS (Freie Online Systeme).<x@3 hoch fliess><t-3>11<@$p><t-3> Hier soll vor allem Software für Schulen und andere Bildungsträger zusammen- und zum Ausprobieren bereitgestellt, getestet und nach Nutzbarkeitskriterien bewertet werden. <t$>Einiges bliebe noch zu wünschen, z.B. dass Ausschreibungen der öffentlichen Hand für Software, Systeme und verwandte Dienstleistungen technologieneutral spezifiziert würden, so dass nicht ohnehin immer nur Microsoft-Produkte in Frage kommen, sondern freie Software konkurrieren kann: »Das würde einerseits die kleineren und mittleren Unternehmen – denn die sind es, die freie Software im Moment einsetzen und anbieten – fördern und auf der anderen Seite auch eine Menge Geld für den Bundeshaushalt sparen« (<@6 Caps>Hetze<@$p>, Fachgespräch, 7/1999). Frankreich geht hier mit gutem Beispiel voran. Ein Gesetzentwurf, nach dem bei der öffentlichen Beschaffung quelloffene Software vorzuziehen ist, steht vor der Verabschiedung.<*h"mehr"><t-1>Es ist deutlich geworden, dass freie Software nicht nur technische, sondern kulturelle, politische und ökonomische Dimensionen hat. Sie <t-2>belegt, dass eine Selbstorganisation in der gemeinschaftlichen und öffent<\h>li<t-1>chen Domäne alternativ zur staatlichen Domäne und zur profitmaximierenden<t$> privatwirtschaftlichen Domäne machbar und in vieler Hinsicht äußerst attraktiv ist. Nicht die Software, sondern unser Wissens<\h>regime und letztlich unsere Gesellschaft steht für GNU-Gründer Stallman im Zentrum der Bewegung: »Der Kern des GNU-Projekts ist die Vorstellung von freier Software als sozialem, ethischem, politischem Gegenstand: In was für einer Gesellschaft wollen wir leben?« (<@6 Caps>Stallman<@$p>, <@1 fliess normal>WOS 1, 7/1999<@$p>).<*h"Standard">@2  ZÜ 2:Kollektive Intelligenz@1 fliess ohne:<t-1>Die Freiheit, die die freie Software meint, ist die Freiheit zu kooperieren. Vor der Gesellschaft kommt die Gemeinschaft, von der es im Motto des LinuxTag 2001 zurecht heißt, sie sei das Herz der ganzen Bewegung.<x@3 hoch fliess><t-1>12 <@$p><t-1>Die Industrialisierung von Software und softwaregestütztem Wissen hat die Vorstellung etabliert, dass nur Experten wertvolle Werke herstellen können, und das nur in einer Unternehmensumgebung, die sie mit optimalen Produktionsmitteln und einer Führung durch ein kompetentes Management unterstützt. Darin stimmt sie mit der weitverbreiteten Ansicht überein, dass alle großen Fragen unserer Zeit so komplex sind, dass allein Experten sie noch überschauen und beantworten können. Im Horizont der Massenmedien herrscht eine klare Trennung zwischen professionellen Produzenten – von Nachrichten, Politik, Kultur, Software usw. – und deren Konsumenten. Freie Software dagegen wird vor allem von »Amateuren« geschrieben. Damit ist keine Aussage über ihre Qualifikation getroffen, denn tatsächlich sind viele dieser »Liebhaber« hauptberufliche Programmierer. Vielmehr geht es um den Unterschied in der Motivation. Der Amateur schafft etwas aus Liebe zur Sache. In dem Moment, in dem er etwas mit dem Ziel schafft, damit Geld zu verdienen, hört er auf, Amateur zu sein. Wird der Profi von der Gelderwerbsintention geleitet, springt der Amateur aus dem Referenzrahmen Geld heraus. Was zählt, ist allein die Freude am eigenen und gemeinsamen Werk. Sie kann bis zu einer potlatch-artigen Verausgabung gehen, in der jede freie Minute und jeder verfügbare Pfennig in die jeweilige Leidenschaft gesteckt wird.@1 fliess mit:Geld ist ein »individuierender« Wert. Selbst eine Gruppe von Bankräubern zerstreitet sich im Film und im Leben, weil jeder die gesamte Beute für sich haben will. Dem Geld ist es egal, wer sich damit die Welt kauft. Der Käufer bedarf keiner persönlichen Anerkennung, um sich ein gutes Leben leisten zu können. Die Freude an der eigenen Hände Arbeit steigt hingegen, wenn man sie mit anderen teilt. Reputation ist ein rein relationales Gut, das von anderen gewährt und entzogen wird. Zwar ist auch Geld ein relationales Phänomen – eine »kollektive Halluzination«, um es mit dem klassischen Wort von William Gibson über den Cyberspace zu sagen – doch ist es auf anonyme Austauschprozesse in der <\n>Gesellschaft bezogen, während Reputation persönliche Beziehungen in einer Gemeinschaft voraussetzt. Während in den Softwarefirmen kleine Teams eng koordiniert an ihren Aufgaben arbeiten, kann in den internetgestützten Austauschpraktiken der freien Software eine große Zahl von Menschen locker gekoppelt und zwanglos koordiniert zusammenarbeiten. Hier gibt es in der Regel ein Kontinuum von wenigen, die sehr viel Code schreiben, vielen, die <\n>kleinere Beiträge zum Programmtext, zur Dokumentation, zur Arbeits<\h>umgebung leisten, und sehr vielen, die Fehlerberichte und <@1 fliess kursiv>Feature-Requests <@$p>beitragen. Alle sind sie Koproduzenten, die meisten sind <@1 fliess kursiv>Volun<\h>teers<@$p>, viele davon Vollprofis. Die Netzwerkgesellschaft wird nicht von einer Expertenintelligenz getragen, die für andere denkt, sondern von einer <@1 fliess kursiv>kollektiven Intelligenz<@$p>, die die Mittel erhalten hat, sich auszudrücken.Der Anthropologe des Cyberspace, Pierre Lévy, hat sie untersucht: »Was ist kollektive Intelligenz? Es ist eine Intelligenz, die überall verteilt ist, sich ununterbrochen ihren Wert schafft, in Echtzeit koordiniert wird und Kompetenzen effektiv mobilisieren kann. Dazu kommt ein wesentlicher Aspekt: Grundlage und Ziel der kollektiven Intelligenz ist gegenseitige Anerkennung und Bereicherung ...« (<@6 Caps>Lévy<@$p>, 1997, S.<\!q>29). Um allen Miss<\h>verständnissen zuvor zu kommen, richtet er sich ausdrücklich gegen einen Kollektivismus nach dem Bild des Ameisenstaates. Vielmehr geht es ihm um eine Mikrovernetzung des Subjektiven. »Es geht um den aktiven Ausdruck von Singularitäten, um die systematische Förderung von Kreativität und Kompetenz, um die Verwandlung von Unterschiedlichkeit in Gemeinschaftsfähigkeit« (ebd., S. 66). Eine besondere Rolle weist auch er den technischen Voraussetzungen zu, die solche Gemeinschaften ohne Orte der Anwesenheit ermöglichen. Er bezeichnet sie als »die technische Infrastruktur des kollektiven Gehirns, [die] <@1 fliess kursiv>Hyperkortex<@$p> der lebenden Gemeinschaften« (ebd. S. 25). Sieht man von einigen Einschätzungen ab, die aus heutiger Sicht allzu optimistisch erscheinen und die der Euphorie über das neu entdeckte Internet – das Buch wurde 1994 geschrieben – geschuldet sein mögen, so zeichnet es doch ein gutes Bild der Gemeinschaften der freien Software. Den Begriff »Hyperkortex« übernimmt Lévy von Roy Ascott. Dessen Inspiration wiederum ist der Jesuitenpater Pierre Teilhard de Chardin und seine Vision von der »Noosphäre.« Bei der Noosphäre handelt es sich um eine planetare »Hülle aus denkender Substanz ... im Ausgang und oberhalb der Biosphäre« (1966, S. 91). 1947, als es nicht einmal eine Handvoll Computer auf der Welt gab und von ihrer Vernetzung noch keine Rede sein konnte, schrieb er, das »radiophonische und televisionelle Nachrichtennetz [verbindet] uns alle schon jetzt in einer Art ›ätherischem‹ Mitbewusstsein. Vor allem denke ich hier aber an den fallenreichen Aufstieg dieser erstaunlichen Rechenmaschinen, die mit Hilfe von kombinierten Signalen in der Größenordnung von mehreren Hunderttausend in der Sekunde ... die Denkgeschwindigkeit, also einen wesentlichen Faktor, in uns erhöhen« (ebd., S. 212 f.). Teilhard sieht darin »die Grundzüge einer besonderen Art von Supergehirn, das fähig ist, sich zu steigern, bis es irgendeinen Superbereich im Universum und im Denken meistert!«. Wie Lévys kollektive Intelligenz ist die Noosphäre ein Gehirn aus Gehirnen. Alles geht vom Individuum aus, »doch alles vollendet sich oberhalb des Individuums« (ebd., S. 224). Man muss Teilhard nicht in die Schwindel erregenden Höhen des Gottesproblems folgen, um zu sehen, dass die Internetkulturen viele der Qualitäten seiner Noosphäre verkörpern. Ihr Potenzial einer freien Assoziation von Amateuren hat sich vielfach bewiesen. Auch der Werkzeugkasten, auf den sie zurückgreifen können, ist heute umfangreicher und subtiler als zu Zeiten von Teilhards Noosphäre oder Lévys kollektiver Intelligenz. So stehen z. B. seit Herbst 1999 Protokolle zur Verfügung, die die Dezentralität des Internet in eine neue Dimension befördert haben. Seit Shawn Fannings <@1 fliess kursiv>Napster<@3 hoch fliess>13 <@$p>hat das »ätherische Mitbewusstsein« einen neuen Namen: <@1 fliess kursiv>Peer-to-Peer <@$p>(P2P). Das Phänomen ist nicht wirklich neu. Die Wissenschaft stützt ihren Gültigkeitsbegriff maßgeblich auf ein <@1 fliess kursiv>Peer-Review<@$p>-Verfahren. Das weltumspannende Internet beruht netztechnisch nicht auf zentraler Planung, sondern auf lokalen <@1 fliess kursiv>Peering<@$p>-Abkommen von Netzbetreibern. Medienlogisch sind die Sozialbeziehungen, <\n>die das Punkt-zu-Punkt-Netz schafft, <@1 fliess kursiv>Peer-to-Peer<@$p> (E-Mail) und <@1 fliess kursiv>Peer-to-many-Peers<@$p> (Newsgroups, Mailinglisten).<@3 hoch fliess>14<@$p> Doch Client-Server-Strukturen unterhöhlen diese Stimmengleichheit. Wer selbst im Web veröffentlichen möchte, muss die finanzielle und technische Hürde einer Domainnamen-Anmeldung und die Anschaffung eines Servers sowie <\h>einer Standleitung bewältigen oder auf kommerzielle Hostingdienste zurückgreifen. <@1 fliess kursiv>Peer-to-Peer<@$p> dagegen macht jeden Client zum Server. Der Datenaustausch erfolgt von Festplatte zu Festplatte, wobei das Internet nur als Transportkanal dient. Indexinformation erlaubt es, die freigegebenen Verzeichnisse der Angeschlossenen nach den gewünschten Dateien zu durchsuchen. Der Schwerpunkt des Geschehens verlagert sich <\h>somit an die Peripherie, in die Hand der »Endanwender«. Die symmetrische Wechselseitigkeit, die die frühen Dienste im Internet kennzeichnete, kehrt auf neuer Stufe zurück. »Tatsächlich könnten einige der P2P-Projekte das Netz, wie wir es kennen, neu erfinden. Basistechnologien wie E-Mail und FTP sind technisch längst überholt, und P2P-Systeme bieten echte Vorteile. Nicht nur das, <@1 fliess kursiv>Peer-to-Peer<@$p> könnte auch die zentralisierten Medien als Meinungsmacher ablösen und eine echte demokratische Informationsinfrastruktur schaffen, in der nicht der mit dem meisten Geld am lautesten ist, sondern der mit dem besten Wissen« (<@6 Caps>Möller<@$p>, 2001). P2P-Journalismus, den Möller hier anspricht, beseitigt die Autorität einer Redaktion. Alle Beteiligten können Beiträge liefern, die sie gegenseitig bewerten. Diese gemeinschaftliche Filterung sorgt dafür, dass nicht eine Gemengelage aus Beiträgen der unterschiedlichsten Güte dabei he<\h>rauskommt, sondern dass die Artikel die größte Sichtbarkeit erlangen, die viele interessant und wertvoll finden. Nichts wird unterdrückt, und dennoch bringt die Auswahl durch die kollektive Intelligenz »Publikationen« von hoher Qualität hervor.<@3 hoch fliess>15<@$p> Hier beweist sich erneut Lévys noch ohne Kenntnis dieser neuen selbstorganisierenden Gemeinschaften geschriebene Einschätzung: »Das Projekt der kollektiven Intelligenz verschiebt nicht das Glück auf später. Es geht dabei darum, dem Einzelnen und der Gruppe, abseits von jedem Opferdenken, täglich und in jeder neuen Situation zu größerer Freiheit zu verhelfen; es geht darum, Spielregeln aufzustel<t-2>len, bei denen es keine Verlierer, sondern nur Gewinner gibt, sowie Wissen <t$>und Wissende transversal zu verbinden und die daraus resultierenden Synergien zu nutzen. Die kollektive Intelligenz hat keine Feinde. Sie bekämpft die Macht nicht, sondern geht ihr aus dem Weg. Sie will nicht dominieren, sondern fördern« (<@6 Caps>Lévy<@$p>, 1997, S. 248).@2  ZÜ 2:Von freier Software zu freiem Wissen@1 fliess mit:@1 fliess ohne:Der freie Geist breitet sich aus. Dominante Leitbilder funktionieren wie der berühmte Hammer, der jedes Problem als Nagel erscheinen lässt. Wo Probleme sich stellen, finden sich Leute, die freie, offene, kooperative Lösungen ausprobieren. In einem Artikel im <@1 fliess kursiv>Salon Magazine<@$p> über die Querbezüge zwischen freier Software und freier Sexualität heißt es: »Ein Wesensmerkmal der Open Source und freien Softwarecommunities ist es idealerweise, dass sie für Experimente jeder Art offen sind, sowohl im Bereich der technischen Disziplinen – die Erstellung von effektivem Code – wie der angewandten Sozialwissenschaften – die Konstruktion neuer Arten, auf der Welt zu sein. Und diese neuen Daseinsformen sind beileibe nicht nur auf sexuelle Aspekte beschränkt. Open Source-Begeisterten fällt es leicht, Anwendungen für Open Source-Strategien in einem weiten Feld von Einsatzbereichen zu sehen, darunter Politik, Literatur und selbst Hardwaredesign« (<@6 Caps>Newitz<@$p>, 2000). @1 fliess mit:Betrachtet man die verschiedenen häufig direkt aus der Erfahrung der freien Software gespeisten Projekte dieser neuen Bewegung des freien Wissens, so stellt man fest, dass bestimmte Klassen von Wissensobjekten der kooperativen Intelligenz und Kreativität besonders entgegenkom<t-1>men. Vor allem modulare Wissensstrukturen, die inkrementell in kleinen <t$>Schritten fehlerbereinigt und weiterentwickelt werden können, eignen sich für offene kontributorische Wissenskulturen und eine anwendergestützte Innovation. Beispiele finden sich in mindestens drei Kategorien, die sich nach der inneren Abhängigkeit der Bestandteile und dem damit notwendigen Maß an Abstimmung unterscheiden:<@3 hoch fliess>16<@$p>@1 fliess ohne:<@1 fliess kursiv>1. Offene Sammlungen von Einzelelementen <@$p>Sammlungen von unabhängigen Werken (Musikstücke, Bilder, Gitarrentabulaturen) erfordern wenig vertikale Kommunikation zwischen Kontributor und Maintainer und keine horizontale Kommunikation unter den Kontributoren, obwohl sie über Newsgroups und <@1 fliess normal>Chat-<@$p>Kanäle meist ohnehin stattfindet. Es ist die Summe der gesammelten Einzelinformationen, die ihren Wert ausmacht. Sie ist das aktive Produkt der kollektiven Intelligenz. Eine Veränderungsfreiheit an diesem Gesamtpool besteht darin, dass jeder Dinge hinzufügen, kommentieren und zum Teil auch korrigieren kann. @1 fliess mit:In der Frühzeit des Web stellten viele thematische Linklisten ins Netz. Fanden andere sie nützlich, steuerten sie eigene Fundstücke bei. Daraus gingen umfassende Verzeichnisse, wie das <@1 fliess kursiv>Open Directory Project<@3 hoch fliess>17<@$p> hervor. Zu den frühen Beispielen für kontributorengestütze Informationssammlungen gehören auch die <@1 fliess kursiv>WWW Virtual Library<@3 hoch fliess>18 <@$p>und die inzwischen kommerzialisierte Internet <@1 fliess kursiv>Movie Database<@$p>.<@3 hoch fliess>19<@$p> Ein verteiltes Klangarchiv ist ORANG (<@1 fliess kursiv>Open Radio Archive Network Group<@$p>). Aus RIS (<@1 fliess kursiv>Radio Internationale Stadt<@$p>) hervorgegangen, besteht es heute aus Knoten in Berlin, Riga und London, die ihre Indexinformationen untereinander abgleichen. Im Juli 2001 enthielt es fast 1<\!q>000 Stunden Audiomaterial, das von etwa 250 Autorinnen beigesteuert wurde.<@3 hoch fliess>20<@$p> Ein ebenfalls weltweit verteiltes System für Graswurzeljournalismus ist <@1 fliess kursiv>Indymedia<@$p>.<@3 hoch fliess>21 <@$p>1999 anlässlich des Treffens der Welthandelsorganisation (WTO) in Seattle gegründet, ist die Text- und Bildberichterstattung des Netzwerks von Brennpunkten des Geschehens oft schneller und unmittelbarer als die der herkömmlichen Medien. Die Ressourcensammlung zur Open Source-Idee <@1 fliess kursiv>Osculture<@3 hoch fliess>22<@$p> <t-3>und die Netzkunstdatenbank <x@1 fliess kursiv><t-3>Verybusy<\!q><x@3 hoch fliess><t-3>23<@$p><t-3> mit mehr als 1<\!q>000 Einträgen<t$> gingen beide aus Hochschulabschlussprojekten hervor und zeigen damit, dass die Akademien weiterhin einen wichtigen Nährboden für den offenen Austausch darstellen. P2P-Dateiaustauschsysteme enthalten große Mengen an Musik und anderen von den Nutzern bereitgestellten Informationen, doch anders als bei servergestützten Archiven hängt das, was jeweils verfügbar ist, von den Rechnern ab, die gerade angemeldet sind. Und schließlich sei noch ein Beispiel für offene Kontribution aus dem akademischen Bereich genannt. Wie wollte man den Fluss des Vokabulars einer lebenden Sprache anders erfassen, denn durch eine vieläugige, ständige Aufmerksamkeit? Der Verlag Oxford University Press hat bei der Erstauflage des <@1 fliess kursiv>Oxford English Dictionary<@$p> dazu aufgefordert, neue Wörter beizusteuern und tut dies heute im Internet wieder. Zwar behält sich die Redaktion die Entscheidung vor, was letztlich in die autoritative Druckversion aufgenommen wird, dennoch ist die Begründung für die Einladung zur Zuarbeit interessant. Dort heißt es, die Vielfalt der englischen Sprache lasse sich nicht anderes kartografieren, als durch eine Vielzahl von freiwilligen Beiträgen aus aller Welt: »Der Aufruf hat gezeigt, dass das Erstellen eines Wörterbuchs eine Ausnahme von den meisten Bereichen der Wissenschaft darstellt: Jeder kann einen wertvollen Beitrag leisten.«<@3 hoch fliess>24<@$p> Doch wie groß ist die Ausnahme tatsächlich?@1 fliess ohne:<@1 fliess kursiv>2. Offene, weiterschreibbare Strukturen <@$p>Kann unter (1) die Autorin ihren Beitrag allenfalls einer Rubrik zuordnen, unter der dieser dann in chronologischer Ordnung aufgelistet wird, bilden sich hier aus der Interaktion komplexere Strukturen heraus. Die Anordnung und Gewichtung der Einzelelemente kann verändert und es können ihnen Kommentare hinzugefügt werden, doch die Elemente anderer werden nicht verändert. So fallen gewöhnliche <@4 Pfeil (Umschalt/Alt #)>’<@$p> <@1 fliess kursiv>Weblogs<@$p> unter (1), während solche mit einem kollektiven Bewertungsmechanismus, also P2P-Journalismussysteme im eigentlichen Sinne, in diese Kategorie ge<\h>hören. @1 fliess mit:Gruppengestützte kreative Prozesse finden sich in der Kunst, etwa in der japanischen Kettendichtung, bei der ein Vers jeweils ein Motiv aus dem vorangegangenen aufgreift. Solche rengaartigen Kooperationen finden natürlich auch im Netz statt. Der »längste Satz der Welt«<@3 hoch fliess>25<@$p> ist ein Beispiel dafür. Literarische Textkooperationen im Netz zielen in der Regel nicht auf einen Abschluss, der es erlauben würde, das Ergebnis zwischen Buchdeckel zu pressen. Die »Odysseen im Netzraum«<@3 hoch fliess>26 <@$p>z.<\!q>B. ist eine gemeinschaftliche Hyper-Science-Fiction, bei der die einzelnen Textelemente nicht nur linear aneinander anschließen, sondern sich dank der Links von HTML komplex miteinander vernetzen lassen.<@1 fliess kursiv>Frequently Asked Questions<@$p>-Dokumente (FAQs) werden traditionell aus dem unaufhörlichen Strom der Konversation in einer Newsgroup oder Mailingliste zusammengestellt. Sie haben zwar einen Maintainer, doch Autorin dieses Kompendiums des gesammelten Kollektivwissens ist die Community. Was läge näher, als eine Summe des Wissens aus dem Netz der Millionen emergieren zu lassen? Tatsächlich gibt es bereits einige offene Enzyklopädieprojekte.<@3 hoch fliess>27<@$p> Eines der ersten war die »Interpedia«, die vor allem in der Newsgroup comp.infosystems.interpedia lebte. Ein kommerzieller Nachfolger der »Interpedia«, aber kontributorengestützt und unter einer offenen Contentlizenz,<@3 hoch fliess>28<@1 fliess normal> <@$p>ist »Nupedia«.<@3 hoch fliess>29 <@$p>Während die Artikel, die in »Nupedia« aufgenommen werden, einen ausführlichen <@1 fliess kursiv>Peer-Review<@$p> hinter sich haben, ist das Begleitprojekt »Wikipedia«<@3 hoch fliess>30<@1 fliess normal> <@$p>eher mit einem kollektiven Brainstorming zu vergleichen. An dem Beispiel wird der Unterschied zwischen den beiden ersten Wissenskategorien deutlich. »Wikipedia« ist eine Sammlung ungefilterter Einträge von sehr unterschiedlicher Durchdachtheit. »Nupedia« dagegen strebt die Systematik und Qualität einer klassischen Enzyklopädie an.Das in der Schweiz ansässige Projekt »nic-las« präsentiert sich als eine autopoietische Informationslandschaft mit offenem Design und vielfältigen Verknüpfungsmöglichkeiten.<@3 hoch fliess>31<@$p> Im Sinne des Zettelkastens von Niklas Luhmann will es ein Werkzeug sein, das es den Menschen ermöglicht, die Welt zu referenzieren und auf einer Oberfläche um sich zusammenzuziehen. Auch dieses wissenschaftliche Online-Schreib- und Publikationsprojekt sieht einen mehrstufigen Prozess von Schreiben, Ergänzen, Kommentieren und Editieren vor. Am Ende steht hier ein geschlossener Korpus, der im Druck (per <@1 fliess kursiv>Publishing-on-Demand<@$p>) und auf CD-ROM verlegt werden soll. Damit steht »nic-las« auf der Grenze zur Kategorie (3).Schließlich ein Beispiel aus der offenen Wissenschaftskooperation in <t-1>der Humangenetik. In Laboren auf der ganzen Welt wird daran gearbeitet,<t$> die Ergebnisse des <@1 fliess kursiv>Human Genome Projects <@$p>auszuwerten. »Ensembl«<@3 hoch fliess>32<@$p> erlaubt es, mit Hilfe eines verteilten p2p-artigen Systems sämtliche verteilten Annotationen zu den einzelnen Gensequenzen zu sichten. Der Ansatz ist einem zentralen Server, auf dem alle Forschergruppen ihre Ergebnisse ablegen, an Flexibilität und Geschwindigkeit überlegen. @1 fliess ohne:<@1 fliess kursiv>3. In sich abgeschlossene funktionale Strukturen <@$p>Hier handelt es sich um Werke, die durch Fehlerbehebung, Verbesserungen und Erweiterungen funktional leistungsfähiger werden können. Da sie zwar modular aufgebaut sein können, aber letztlich ein Ganzes ergeben sollen, liegen hier die stärksten inneren Abhängigkeiten vor. Durch die starken Wechselwirkungen der Elemente ist es unerlässlich, bei der eigenen Arbeit auch die Werke anderer zu modifizieren. Dies macht ausgeprägtere Mechanismen für Konfliktlösungen und strategische Entscheidungen erforderlich. @1 fliess mit:Dazu gehört natürlich die freie Software selbst, die sich in der Entwicklerversion in ständiger Veränderung befindet, aber periodisch in einer stabilen Version freigegeben wird. Ebenso ihre Dokumentation, die ebenfalls frei verbreitet und verändert werden können muss, um die Veränderungen des Programmcode widerzuspiegeln.<@3 hoch fliess>33<@$p> Auch an freier Hardware wird gearbeitet.<@3 hoch fliess>34<@$p> <@1 fliess kursiv>Open Theory<@3 hoch fliess>35<@$p> ist der explizite Versuch, das Modell freier Softwareentwicklung auf die Entwicklung von Theorie zu übertragen. Wie bei einem Softwareprojekt macht jemand eine Vorgabe, hier einen Text, dessen Absätze einzeln kommentiert werden können. Dieselben Einwürfe gehen auch über eine Mailingliste und werden vom Maintainer des Projekts regelmäßig in den Gesamttext integriert. Auch Pädagogen haben die Idee aufgegriffen und erstellen Sammlungen quell<\h>offener und gemeinsam weiterschreibbarer Unterrichtsmaterialien wie <t-4>das von einem Berliner Grundschullehrer 1999 gestartete <x@1 fliess kursiv><t-4>Open Web School<@$p><t-4>.<x@3 hoch fliess><t-4>36<@$p><t-4> <t$>@1 fliess ohne:Bei diesen drei Kategorien der Offenheitsfähigkeit handelt es sich nur um <t-1>eine erste Annäherung. Weitere empirische Forschung würde ohne Frage ein differenzierteres Bild der offenen Wissensgemeinschaften zu Tage <t$>fördern. Auch die unterschiedlichen Wissenspraktiken der Wissenschaften, des Journalismus, der Künste, der Politik usw. müssten jeweils auf ihre Eignung für offene Kooperationen hin untersucht werden. Theoretische Fragestellungen richten sich auf die Werkzeuge und Standards offener Wissensgemeinschaften, auf die Motivationen der Autorinnen, ihre Werke als <@1 fliess kursiv>Open Content <@$p>in den dialogischen Wissensprozess zu stellen, und die Metriken der Reputation. Selbst wenn man unterstellt, dass für viele Kreative der Akt der Schöpfung in sich bereits einen hinreichenden Anreiz darstellt, bleibt die Frage, wie sich die Anerkennung in einen Beitrag zum Lebensunterhalt übersetzen kann, ohne den Amateurstatus zu unterhöhlen. Nicht zuletzt geht es immer wieder um Probleme des »geistigen Eigentums«, wie das <@1 fliess kursiv>On-Line Guitar Archive<@$p> (OLGA)<@3 hoch fliess>37<@$p> erfahren muss<\h>te. OLGA ging 1992 aus zwei Newsgroups hervor und enthielt zu Hochzeiten etwa 33<\!q>000 von Benutzern beigesteuerte Gitarrentabulaturen. Tabulaturen sind Spielanweisungen, die nicht etwa von kommerziell vertriebenen Noten kopiert, sondern nach dem Gehör aus Musikaufnahmen transkribiert werden. 1998 verklagte eine Musikagentur OLGA wegen mutmaßlicher Copyright-Verstöße und zwang das Archiv zur radikalen Reduktion seiner Bestände. Die Gefahr von privatwirtschaftlichen Angriffen gegen die kollektive Intelligenz kann nur durch eine gesamtgesellschaftliche Neubewertung der Wissenslandschaft abgewehrt werden. Sie ist Thema der verbleibenden Seiten dieses Kapitels. @1 fliess mit:Vorläufig lässt sich feststellen, dass Werke, oder doch bestimmte Werkkategorien, sich von fixierten, paketierten Produkten zu Prozessen wandeln, zu permanenten <@1 fliess kursiv>Works in Progress.<@$p> Aus dem auratischen Werk des einzigartigen Autors wird ein Fluss der ständigen Rekombination, Verbesserung, Hinzufügung, Ersetzung – ein Baukasten aus Elementen, wiederverwendbar als Module oder Samples. Es verschwindet die Fiktion des Werkes, das voraussetzungslos und aus einem Guss dem Haupt des Genius entspringt. An ihre Stelle tritt (wieder) das Bewusstsein, dass jede individuelle Schöpfung zwergenhaft auf den Schultern eines Riesen mit seinem Kollektivwissen steht. Der aktuelle Trend bedeutet natürlich nicht, dass individuelle Intelligenz und Kreativität an Bedeutung verlieren würden, doch möglicherweise schwingt das Pendel nach 200 Jahren Entkollektivierung, Individualisierung, Genie- und Star-Kult jetzt wieder in die andere Richtung. <t1>Das überlieferte Versprechen des Internet ist es, das »Große Wissensreservoir« und der »Große Dialog« der Menschheit zu werden. <\h>Haben die Massenmedien Leser und Zuschauer in die Konsumentenrolle gebannt, macht das Internet sie idealerweise zu Aktivisten, Interakteuren und Koproduzenten. Es ist fast wie ein Naturgesetz, das Eben Moglen, New Yorker Rechtsprofessor und juristischer Berater des GNU-Projekts, in seiner Entsprechung zum faradayschen Gesetz so formuliert: <t$>@1 fliesskursiv Zitat:»Wenn man das Internet um jeden Menschen auf dem Planeten wickelt und den Planeten in Drehung versetzt, fließt Software <@1 fliess normal>[gemeint sind nicht nur Computerprogramme, sondern jede Art von digitaler Information, VG]<@$p> im Netz. Es ist eine emergente Eigenschaft von miteinander verbundenen menschlichen Intelligenzen, dass sie Dinge schaffen, um einander eine Freude zu machen und ihr beunruhigendes Gefühl zu überwinden, alleine zu sein« <@1 fliess normal>(<@6 Caps>Moglen<@1 fliess normal>, 1999)<@$p>.@1 fliess mit:@2  ZÜ 2:Lizenzen@1 fliess mit:@1 fliess ohne:Das Phänomen der freien Wissenskooperation hat technisch-ökonomische Voraussetzungen in erschwinglichen Computern und Zugang zum Internet, ganz maßgeblich aber auch in einer rechtlichen Konstruktion. Autoren, die ihre Software verschenken und andere zum Weiterschreiben einladen, hat es immer gegeben. Häufig genug führte dies jedoch dazu, dass sich Dritte die Früchte der kollektiven Intelligenz aneigneten, ohne sich an der gemeinsamen Weiterentwicklung zu beteiligen. Dank der wundersamen Vermehrbarkeit digitaler Daten kennt die Wissens-Allmende keine Übernutzung, die die Güter selbst verbraucht. Im Gegenteil steigt der Wert z. B. von GNU/Linux für alle mit jeder neuen Nutzerin, weil sie potenziell zu seiner Verbesserung beiträgt. Der Übernutzung entspricht hier, wie das Beispiel Unix gelehrt hat, die proprietäre Schlie<\h>ßung von Gemeingütern, ähnlich der Privatisierung und Einzäunung der Allmende. @1 fliess mit:Die knappe Ressource sind hier also nicht die <@1 fliess kursiv>Bits<@$p>, sondern die Zeit und die Aufmerksamkeit der freien Entwickler, Dokumentatoren und Tes<\h>ter. Deren Motivation geht jedoch in die Binsen, wenn sie feststellen, dass Dritte von ihrer Hände Arbeit ihr eigenes Süppchen kochen. Das Problem ist nicht, dass Leute sich aus dem gemeinsamen Pool bedienen, ohne etwas zurückzugeben (zumindest, solange es noch genug andere gibt, die Arbeit hineinstecken), sondern dass Firmen ihre eigenen Weiterentwicklungen nicht mit der Gemeinschaft teilen. Sie vernichten damit die erschöpfliche Ressource Motivation. Midas Berührung lässt den lebendigen Prozess zu Gold erstarren. Um dies zu verhindern, erzeugen und schützen die Lizenzen der freien Software etwas, das dem alten Allmendekonzept sehr nahe kommt. Softwareautoren, die ihre Werke unter eine solche Lizenz stellen, teilen sie mit anderen, ohne ihre Urheberrechte daran aufzugeben. Vielmehr gewähren sie den Allmendgenossen, also jedem, der die Lizenz akzeptiert und ihr Folge leistet, bestimmte Freiheiten in Bezug auf das gemeinschaftliche Gut. Im FAQ-Dokument zu einer weniger freien Lizenz heißt es auf die Frage »Wenn die Quellen frei sind, wozu braucht ihr dann eine Lizenz?«: »So gesehen kann die Codebasis als Ressource angesehen werden, wie ein See oder die Luft, die wir atmen, und wir brauchen Grund<\h>regeln, die sicherstellen, dass diese Ressource erhalten und verbessert wird, zum Beispiel, indem wir dazu ermuntern und sogar zu einem gewissen Grad vorschreiben, dass Entwickler etwas in die gemeinsame Codebasis zurückgeben.«<@3 hoch fliess>38<@$p>Nach Boyles Aussage, das »geistige Eigentum« sei die Rechtsform des Informationszeitalters, ist es zwar nur folgerichtig, aber dennoch bemerkenswert, dass die Philosophie, genauer die Ethik im Reich des digitalen Wissens die Form von Lizenzen annimmt. Das Debian-Projekt hat sich einen Sozialkontrakt als Lizenz gegeben (oder umgekehrt?).<@3 hoch fliess>39 <@$p>Diese bestimmt den Ort des Werkes im Wissensraum und regelt zugleich das Innen- und Außenverhältnis der Gemeinschaft von Autoren. Bekenntnisse zur Freiheit werden heutzutage nicht mehr an Kirchentüren genagelt oder von den Zinnen eroberter Bastionen verkündet, sondern als Lizenz einem Softwarewerk beigegeben. Die stärkste und raffinierteste Formulierung einer Freiheitsphilosophie für Software findet sich in der <@1 fliess kursiv>GNU General Public License <@$p>(GPL). Sie erzeugt eine Allmende durch eine Schließung nach außen bei freiem Austausch nach innen. Stallman benennt klar die Strategie einer kollektiven Chancenmonopolisierung.<@3 hoch fliess>40<@$p> Die GPL wendet das Copyright zu einem Copyleft. Einmal frei, immer frei. Für Helmut Spinner steht an ers<\h>ter Stelle der Werteskala in einer liberalen Informationsgesellschaft die Wahrung und der Ausbau der Wissensfreiheiten, die er folgendermaßen benennt:@1 fliess ohne:<@5 Klötzchen (kleines p)>p<@1 fliess normal>  <\i><@$p>Veränderungsfreiheit zum Zwecke der Verbesserung durch Kritik undWiderlegung,<@5 Klötzchen (kleines p)>p<@1 fliess normal>  <@5 Klötzchen (kleines p)><\i><@$p>Beeinträchtigungsfreiheit zum Schutz gegenüber Informationsübergriffen,<@5 Klötzchen (kleines p)>p <@1 fliess normal>  <@5 Klötzchen (kleines p)><\i><@$p><t-2>Verbreitungsfreiheit für öffentliche Information wie Forschungsergebnisse und Nachrichten, aber auch Unterhaltung (<x@6 Caps><t-2>Spinner<@$p><t-2>, 1998a, S. 58).<t$>@1 fliess mit:@1 fliess ohne:Wenn man unter Beeinträchtigungsfreiheit nicht nur das Recht auf informationelle Selbstbestimmung, sondern auch die Verhinderung der proprietären Schließung freien Wissens sieht, bilden die Freiheiten des Wissensphilosophen eine verblüffende Parallele zu den Regularien der GPL. Daher ist es vielleicht gar nicht so abwegig, dass sich bereits jemand Gedanken über eine GPL-Gesellschaft macht.<@3 hoch fliess>41<@$p> @1 fliess mit:@2  ZÜ 2:Urheberrecht@1 fliess ohne:Moglens erstem Gesetz folgt ein zweites. Seine metaphorische Entsprechung zum ohmschen Gesetz besagt: »Der Widerstand des Netzwerkes steht in direktem Verhältnis zur Feldstärke des Systems des geistigen <\n>Eigentums« (1999). Was müsste also geschehen, um den Widerstand des <t-1>Netzes für die Energien der Wissens-Allmende möglichst gering zu halten?<t$> @1 fliess mit:Das Versprechen auf eine Vervielfältigung der Wertschöpfungsketten durch Digitalmedien treibt den Rechtehandel zu ungekannten Dimensionen. In Goldrauschstimmung über einen möglichen E-Commerce mit »Content« werden aggressiv Claims abgesteckt. Doch die Erträge blieben bislang aus. Was dagegen funktioniert, sind offene Wissensgemeinschaften und öffentliches Wissen. Genau diese werden aber durch die konträren Trends aus der Rechteindustrie bedroht: »Unser Recht schafft <\n>Anreize, so viel von einer intellektuellen Allmende wie möglich einzuzäunen. Es arbeitet gegen Öffentlichkeit und Transparenz und schafft im Effekt eine massive Geheimregierung. ... Hier ist der Ort für konkrete <\n>juristische Veränderungen« (<@6 Caps>Lessig<@$p>, 1999). Das Urheberrecht müsste vom Kopf auf die Füße gestellt werden. Autorinnen und Öffentlichkeit müssten wieder ins Zentrum gerückt werden, das jetzt von der Rechteindustrie – also denjenigen, die nur abgeleitete Verwertungsrechte vertreten – beansprucht wird. Von der Rechtsintention aus betrachtet wedelt heute der Schwanz mit dem Hund.Eine philosophische Erblast bildet das Konzept des Subjekts. Die Figuren des Autors im Urheberrecht und die des Erfinders im Patentrecht sind Individuen. Gruppenurheberschaft ist zwar vorgesehen, aber offenbar ist hier an Gruppen von zwei bis drei Personen gedacht. Daneben gibt es den Geistesarbeiter, der keine Rechte an seinen Werken hat, weil er sie im Dienste seines Arbeitgebers erstellt. Schließlich kennt das Recht neben dem geistigen auch das industrielle Eigentum von korporativen Akteuren. Patente und Markennamen können und werden in der Regel von Unternehmen angemeldet. Nicht vorgesehen sind jedoch Werke der kollektiven Intelligenz.<t-2h99.001>Dies ist kein Plädoyer für die Abschaffung des Urheberrechts, doch die gnadenlose Präzision bei der Rechteverwertung, die technische Schutzsys<\h>teme einführen, ist schädlich. Das staatlich gewährte Monopol zielt darauf, den Autoren einen hinreichenden Anreiz für die Schaffung neuer Werke zu bieten. »›Hinreichender Anreiz‹ ist etwas Geringeres als ›perfekte Kontrolle‹«, gibt Lessig zu bedenken und spricht sich dafür aus, dass die Rechtspraxis »durchlässig« sein solle (1999, S. 133). Dies stelle nur eine Fortführung der bisherigen Praxis dar, nach der Urheberrecht und Copyright eine geringe Zahl von Kopien durch viele Nutzer zuließen und gleichzeitig die Mittel bereitstellten, um gegen einige kommerzielle Hersteller vorzugehen, die eine große Zahl von nicht autorisierten Kopien vertreiben. Ohne ein gewisses Maß an »Raubkopien« von Software gäbe es keine »Informationsgesellschaft«. Ausgerechnet Mark Stefik, Vater der Rechtkon<\h>trollsysteme, merkt an: »Ironischerweise haben die Verleger von Werken, die regelmäßig aktualisiert werden müssen, wie Computersoftware, festgestellt, dass eine gewisse Durchlässigkeit ihren Kundenstamm erweitert, selbst wenn oft berichtet wird, dass mehr unauthorisierte Kopien eines Programms im Einsatz sind als authorisierte. Softwareverleger haben entschieden, dass die Einnahmenverluste durch illegales Kopieren tragbar sind, auch wenn sie zu einer unfairen Gebührenstruktur führen« (1996, S. 10).<t$h$>Eine Urheberrechtspolitik, die die Rechteindustrie nicht bei ihren Worten, sondern bei ihren Taten nimmt, würde diesen <@1 fliess kursiv>de facto-<@$p>Zustand in einen <@1 fliess kursiv>de jure-<@$p>Status überführen – zuallererst, um die Kinderzimmer zu entkriminalisieren. Will sagen, Kopien für private, nicht kommerzielle, bildungsrelevante Nutzungen sind erlaubt, wobei den Autorinnen über die Pauschalvergütung für Aufzeichnungsgeräte und Leermedien eine Entlohnung zugeht. Kopien für profitorientierte Nutzungen (Weiterverkauf der Kopie, Integration in andere Produkte, Verwendung bei der Produktion von Waren oder Dienstleistungen) werden gebührenpflichtig <\n>li<t-1>zenziert. Andy Müller-Maguhn, Sprecher des <x@1 fliess normal><t-1>Chaos Computer Clubs<x@1 fliess kursiv><t-1> <@$p><t-1>rich<\h>tet<t$>e sich in seinem 1995 vor dem Europaparlament vorgetragen »Thesen<\h>papier Informationsgesellschaft« dagegen, dass Jugendliche und Arbeitslose, die nicht autorisierte Kopien von Software verwenden, zu Straftätern erklärt werden: »Software muss für nicht kommerzielle Anwendungen kostenlos nutzbar sein; insbesondere Schüler, Studenten, Arbeits- und Mittellose müssen die Möglichkeit haben, sich Qualifikationen am Computer anzueignen, um überhaupt eine Chance auf dem Arbeitsmarkt zu haben. Die Verfolgung von Urheberechtsdelikten sollte von der Art der Nutzung abhängig gemacht werden. Produktionsmittel dürfen Geld kosten, Lernmittel nicht.«Die Sozialbindung des Eigentums bietet eine weitere Handhabe, die in der Wissensordnung zum Tragen kommen sollte. Das Materialrecht kennt die Option der Enteignung von Privatbesitz im Interesse der Allgemeinheit und gegen Entschädigung. In Analogie zu dieser Institution, die im US-amerikanischen Recht <@1 fliess kursiv>Eminent Domain<@$p> heißt, ist die <@1 fliess kursiv>Public Domain <@$p>als ein Modell vorgeschlagen worden, das dem öffentlichen Interesse dient, Monopole verhindert und Entwickler entlohnt. Zu den intellektuellen Objekten, die darunter fallen könnten, gehören chemische Formeln (z. B. Medikamente gegen Krebs oder AIDS) und Software wie der Quellcode von Betriebssystemen (vgl. <@6 Caps>Nadeau<@$p>, 1998).Ziel müsste ein Wissensklima sein, das den Schwerpunkt auf gesellschaftlichen Reichtum statt auf privatwirtschaftliche Bereicherung legt. Wissen ist kein Artefakt, sondern eine Tätigkeit, etwas Lebendiges, das im Dialog aktualisiert und weitergedacht wird. Der dialogische Prozess der kollektiven Intelligenz sollte das vorrangige Gut sein, das es zu schützen gilt. @2  ZÜ 3:<\c>@2  ZÜ 2:Wissensumweltschutz@2  ZÜ 3:@1 fliess ohne:Boyle schreibt, das »geistige Eigentum« sei die Rechtsform des Informationszeitalters, und er fährt fort: »Doch derzeit haben wir keine Politik des geistigen Eigentums – auf dieselbe Weise wie wir eine Umwelt- oder Steuerpolitik haben.« In seinem viel beachteten Aufsatz »A Politics of <\n>Intellectual Property: Environmentalism For the Net?« (1997) plädiert Boyle für eine Art Umweltbegriff für das geistige Eigentum. »Eine Art«, weil er dabei nicht wirklich an ein Greenpeace für bedrohte Daten denkt. Die Umweltschutzbewegung dient ihm als Analogie. @1 fliess mit:<*h"mehr">Boyle vergleicht unsere Situation heute mit dem Umweltschutz in den 50er-Jahren. Es gab Proteste gegen verseuchte Seen und Smog produzierende Schlote, was aber noch fehlte, war ein allgemeines Rahmenkonzept, ein Satz von analytischen Werkzeugen und damit eine Wahr<t-1>nehmung gemeinsamer Interessen in vermeintlich zusammenhanglosen <t$>Einzelsituationen. Ähnlich nehmen wir heute Katastrophen in der Wissensumwelt wahr: Micrososofts inzwischen gerichtlich bestätigte monopolistische Praktiken, die Patentierung von menschlichen Genen und der Einsatz von Copyrights, um Kritiker der <@1 fliess kursiv>Scientology <@$p>zum Schweigen zu bringen. Ähnlich fehlt heute ein übergreifendes Konzept wie die Ökologie, das unsere Wissensumwelt als komplexes System aus interagierenden Bestandteilen sichtbar macht, ihr eine vergleichbare Aufmerksamkeit in der öffentlichen Debatte verschafft und es erlaubt, Koalitionen aus der freien Software, den Kampagnen gegen Gen- und Softwarepatentierung, den Bibliotheken und Hochschulen, den Bewegungen für eine informationelle Selbstbestimmung des Südens usw. zu bilden. Wir brauchen, fordert Boyle, eine politische Ökonomie des geistigen Eigentums. Wie die Umweltbewegung die Umwelt erfunden habe, damit sich unterschied<\h>liche Interessengruppen als Umweltschützer verstehen konnten, so müssten wir heute die <@1 fliess kursiv>Public Domain<@$p>, das Gemeingut Wissen erfinden, um die Allianz hervorzubringen, die es schützt. @1 fliesskursiv Zitat:»Hinter dem Versagen im Entscheidungsprozess liegt ein Versagen in der Art, wie wir über Probleme nachdenken. Die Umweltschutzbewegung erlangte viel von ihrer Überzeugungskraft, indem sie darauf hinwies, dass es strukturelle Gründe gab, die es wahrscheinlich machten, dass wir schlechte Umweltentscheidungen treffen: ein Rechtssystem, das auf einer bestimmten Vorstellung von ›Privateigentum‹ beruht und ein System von Ingenieurkunde und Wissenschaft, das die Welt als einen schlichten linearen Satz von Ursachen und Wirkungen behandelte. Tatsächlich verschwand die Umwelt in beiden konzeptionellen Systemen einfach. Es gab keinen Platz für sie in der Analyse. Es überrascht also nicht, dass wir sie nicht besonders gut erhalten haben. Ich habe argumentiert, dass dasselbe für die <@1 fliess normal>Public Domain<@$p> zutrifft. Die grund<\h>sätzliche Aporie in der ökonomischen Analyse von Informationsfragen, die Quellblindheit eines auf den ›originären Autor‹ zentrierten Modells von Eigentumsrechten und eine politische Blindheit für die Bedeutung der <@1 fliess normal>Public Domain<@$p> als Ganzer (nicht ›mein See‹, sondern ›Die Umwelt‹) kommen alle zusammen, um die <@1 fliess normal>Public Domain <@$p>zum Verschwinden zu bringen, erst als Konzept und dann in zunehmenden Maße als Wirklichkeit« <@1 fliess normal>(<@6 Caps>Boyle<@1 fliess normal> 1997)<@$p><f"FFScala">.@1 fliess mit:@1 fliess ohne:Auf ganz ähnliche Weise argumentiert auch Ostrom dafür, unsere tief sitzenden Vorstellungen über das Allmende- oder Gefangenen-Dilemma zu korrigieren. Solange Individuen als »Gefangene« betrachtet werden, beziehen sich politische Programme zu Gemeingütern auf Bilder von hilflosen Einzelnen, die, gefangen in einem unerbittlichen Prozess, ihre eigenen Ressourcen vernichten und denen man nur mit Staat oder Markt beikommen kann. Wie Boyle sieht auch Ostrom, dass wir intellektuelle Werkzeuge benötigen, um die Logik des kollektiven Handelns und die Möglichkeiten von selbstverwaltenden Institutionen, gemeinsame Ressourcen zu regulieren, besser zu verstehen <@6 Caps>(Ostrom, 1999)<@$p>. @1 fliess mit:Ein Ansatzpunkt wäre es, die Mechanismen zur Entscheidungsfindung und Konfliktlösung, die sich in den verschiedenen Projekten der freien Software und des softwaregestützten offenen Wissens entwickelt haben, zu untersuchen, um nicht nur den Code, sondern auch die Umwelt zu optimieren, in der er entsteht. Vor allem ginge es darum, ein Bewusstsein für die Landschaft des Wissens zu schaffen, für seine Freizonen und die umzäunten Regionen, die Bulldozer, die in die Urwälder rollen, nicht um Bodenschätze, sondern um verwertbares Wissen zu »ernten«. Ihnen müsste sich eine Theorie und Empirie der Wissensfreiheiten entgegenstellen.<@3 hoch fliess>42<@$p> Die Wissens-Allmende muss immer wieder zum Thema gemacht werden, damit die grundlegenden Fragen der Wissensgesellschaft nicht unter Ausschluss der Öffentlichkeit verhandelt werden: Wieviel »Wissen als Ware« können wir uns leisten? Wieviel öffentliches und gemeinschaftliches Wissen brauchen wir? <*h"mehr">Eine Gesamtschau der Wissensordnung macht deutlich, dass es irreführend ist, vom Copyright und seinen Ausnahmen zu sprechen. Vielmehr ist das Copyright die Ausnahme. Eine Gesellschaft, die sich tatsächlich als Wissensgesellschaft versteht, würde größtmögliche Anstrengungen unternehmen, um das kulturelle Erbe zu erhalten und gemeinfreie Werke nicht nur als den »Rest« betrachten, der im Urheberrecht nicht vorkommt. Es geht nicht an, dass Bibliothekare hilflos zusehen müssen, wie große Teile des schriftlichen Wissensbestandes aus gut 100 Jahren säurehaltiger Papierproduktion zerfallen, weil ihnen die Mittel für groß angelegte Rettungsaktionen fehlen. Im Interesse einer informationellen Nachhaltigkeit würde eine solche Gesellschaft bei allem technologiegestützten Wissen darauf insistieren, dass ausschließlich offene Standards zur Anwendung kommen. Nur so kann gewährleistet werden, dass zukünftige Generationen noch auf die heutigen Daten zurückgreifen können. Da die Informationsmenge immer schneller wächst und damit Wissen immer schneller veraltet, wäre es nur folgerichtig, die Schutzfrist zu verkürzen. Die absurde Schutzdauer für das Monopolrecht an geistigem Eigentum von 70 Jahren nach dem Tod des Autors müsste zurückgeschraubt werden. Eine selbstbewusste zivile Wissensgesellschaft würde sich nicht – weitgehend ohne es auch nur zu bemerken – von Mickey Mouse auf der Nase herumtanzen lassen.<*h"Standard">»Die grundlegende Idee einer demokratischen Rechenschaftspflicht für die öffentliche Verfügung über extrem wertvolle Rechte würde eine weit informiertere Politik des geistigen Eigentums im Informationszeitalter verlangen. Wenn eine solche Rechenschaftspflicht bestehen soll, müsste die <@1 fliess kursiv>Public Domain<@$p> systematischer diskutiert und verteidigt werden, als es bislang der Fall ist« (<@6 Caps>Boyle<@$p>, 1997). Das Bundesverfassungsgericht hat mit der »informationellen Selbstbestimmung« und der »informationellen Grundversorgung« zwei Leitlinien formuliert, denen immer aufs Neue nachgestrebt werden muss. Vor allem aus Letzterer ließe sich der Begriff einer »öffentlichen Wissensinfrastruktur« entwickeln, der die Basiswerkzeuge wie Standards, Protokolle, Metadaten, Suchmaschinen, das elektromagnetische Spektrum ebenso umfasst wie die Wissensvermittlung im Bildungssystem und die Wissensbestände in den Biblio<\h>theken, Museen, Archiven und Fachinformationssystemen. Das Potenzial für eine freie Entfaltung der kollektiven und individuellen Intelligenz ist gegeben, das hat die freie Software auf eindrucksvolle Weise belegt. Es muss gepflegt und gegen die aggressiven Schließungsbestrebungen der globalen Rechteindustrie in Schutz genommen werden. »Ich denke die gegenwärtige Situation ist derart, dass sie rechtfertigt, was man als Haltung der vorbeugenden Warnung bezeichnen könnte. Es wäre eine Schande, wenn das grundlegende Eigentumsregime der Informationswirtschaft hinter unserem Rücken konstruiert würde. Wir brauchen eine Politik – eine politische Ökonomie – des geistigen Eigentums, und wir brauchen sie jetzt.« (<@6 Caps>Boyle<@$p>, 1997) Auch der Wirtschaftswissenschaftler Norbert Szyperski kommt zu dem Schluss: »Die Freiheit des Wissens zu verteidigen, ist wahrscheinlich die wichtigste Aufgabe, die wir in der Zukunft vor uns haben.«<@3 hoch fliess>43 <@$p>In dieser Überzeugung treffen sich Ökonom, Jurist und Hacker, dem das letzte Wort gehören soll: »Wir können die Zukunft der Freiheit nicht als gegeben betrachten. Seht sie nicht als selbstverständlich an! Wenn ihr eure Freiheit behalten möchtet, müsst ihr bereit sein, sie zu verteidigen« (<@6 Caps>Stallman<@$p>, 1999a, S.<\!q>70).@0  Über 100%:Anhang@2  ZÜ 1:Glossar@99 Glossar fliess:<@7 Glossar fett>Backup<@$p> – Sicherungskopie von Programmen, Daten und ganzen Computersystemen. Sollte man regelmäßig machen <@7 Glossar normal>:-)<@7 Glossar fett>Beta-Version<@$p> – die letzte Entwicklungsstufe eines Programmes vor der Freigabe der stabilen Produktionsversion (gelegentlich auch als Alpha-Version bezeichnet). Die Entwicklung ist gestoppt, es werden nur noch Fehler bereinigt, aber keinen neuen Features mehr aufgenommen. Der Beta-Test prüft die B.V. auf Herz und Nieren in einer Umgebung, die dem späteren Produktionseinsatz entspricht.<@7 Glossar fett>Binary<@$p> – ausführbare Binärversion eines Programmes, bei der die Anweisungen in einer prozessorspezifischen Maschinensprache durch Nullen und Einsen repräsentiert werden; auch als »Objektcode« bezeichnet. B.s sind für Menschen nur schwer lesbar, die daher in einer höheren Programmiersprache (z.B. C) programmieren. Der dabei entstehende Quellcode wird von einem <@7 Glossar Pfeil Umsch/alt#>’<@$p> Compiler oder einem Assembler übersetzt, um daraus ein B. zu erzeugen. Vgl. <@7 Glossar Pfeil Umsch/alt#>’<@$p> Executable<@7 Glossar fett>Bit<@$p> –<t0> in der Informationstheorie: <x@7 Glossar kursiv><t0>Basic Indissolu<t$>ble Infomation Unit<@$p>, kleinste nicht weiter teilbare Informationseinheit; heute in der <\n>Regel: <@7 Glossar kursiv>Binary Digit,<@$p> Binärzahl, kleinste nicht weiter teilbare Zeichendifferenz von 0 und 1, in deren Darstellung der Computer sämtliche Information, ob Programme oder Daten, verarbeitet.<@7 Glossar fett>Blackbox-System<@$p> – Modell eines Informa<\h>tionsverarbeitungssystems mit unbekanntem Aufbau: ein verschlossener Kasten mit einem Eingang und einem Ausgang. Man gibt ein Signal auf den Eingang und liest das Ausgangssignal ab. Mit einer genügend großen Zahl und Varianz von Ein- und Ausgangsmus<\h>tern lässt sich eine Hypothese darüber <\n>bilden, was im Inneren der Box geschieht, <\n>gar eine Box bauen, die dasselbe In/Output-Verhalten zeigt, auch wenn sie anders auf<\h>gebaut seien kann als die Vorlage. Wird <\n>verwendet, um Hard- oder Software nachzuentwickeln, Verschlüsselungssysteme zu knacken oder Signale ins All zu schicken, um <t1>nach extraterrestrischer Intelligenz zu suchen. <t$>Auch Menschen sind einander Blackboxes, die Signalabfrage nennt sich bei ihnen »Kommunikation«. Hier als »undurchschaubares System« verwendet. Während ein Programm im <@7 Glossar Pfeil Umsch/alt#>’<@$p> Quellcode eine offene Schachtel darstellt, ist eines allein im <@7 Glossar Pfeil Umsch/alt#>’<@$p> Objektcode eine Blackbox.<@7 Glossar fett>Bloatware<@$p> – Software, die mit Features überladen ist, eierlegende Wollmilchsau; kann in der Regel sehr viel, aber nichts davon wirklich gut. Typische Vertreter dieser Gattung sind MS Word und Netscape.<@7 Glossar fett>Booten<@$p> – eigentlich <@7 Glossar kursiv>bootstrapping,<@$p> sich an den eigenen Schnürsenkeln aus dem Sumpf ziehen, ein Computersystem starten. Beim Hochfahren der einzelnen Systemkomponenten werden diese einem mehr oder weniger ausführlichen Check unterzogen.<@7 Glossar fett>Bug<@$p> – Fehler in einem Programm; nach einer Motte, die 1945 einen Kurzschluss in einem elektromagnetischen Relais eines der ersten Computer überhaupt verursachte, als B. bezeichnet. Die Motte wurde von US-Marine<\h>offizier Grace Murray Hopper entfernt (das <\h>erste <@7 Glossar Pfeil Umsch/alt#>’<@$p> De-Bugging) und in ihr Logbuch eingeklebt, das heute am Smithsonian-Institut aufbewahrt wird.<@7 Glossar fett>Bug Fix<@$p> – Fehlerbehebung; ein Stück Code <\n>(<@7 Glossar Pfeil Umsch/alt#>’<@$p> Patch), das einen Fehler repariert. <\n>Zwischen den <@7 Glossar Pfeil Umsch/alt#>’<@$p> Releases einer Software können so schnell kleinere Berichtigungen verbreitet und von den Nutzern in ihre Installation eingefügt werden.<@7 Glossar fett>Bug Report <@$p>– Fehlerbericht; Mitteilung von Nutzern über einen aufgetretenen Fehler an die Entwickler des jeweiligen Programms. Viele Projekte der freien Software haben ausgefeilte Verfahren für die Bearbeitung von B.R.s. Ein guter B.R. enthält genaue Angaben über die verwendete Programmversion und Betriebsumgebung und beschreibt das Fehlverhalten auf nachvollziehbare Weise. Ein wirklich guter Bugreport enthält gleich noch den passenden <@7 Glossar Pfeil Umsch/alt#>’<@$p> Bug Fix.<@7 Glossar fett>Byte<@$p> – die kleinste adressierbare Speicherstelle; besteht aus 8 Bits. Da ein <@7 Glossar Pfeil Umsch/alt#>’<@$p> Bit zwei Zustände einnehmen kann, ermöglicht ein Byte 2<+>8<$> = 256 Kombinationen und damit die Darstellung von 256 verschiedenen Zuständen oder Zeichen.<@7 Glossar fett>Citation Index<@$p> – Verzeichnis zitierter wissenschaftlicher Literatur; selbst organisierendes Reputationssystem: Werke, auf die besonders häufig Bezug genommen wird, müssen folglich besonders bedeutend und einflussreich sein. <@7 Glossar fett>Clone<@$p> – Nachbau einer Hard- oder Software. Ein Hardware-C. funktioniert mit den gleichen Treibern wie die Orginalhardware. Ein Software-C. hat die gleiche Funktion und Bedienung wie das Originalprogramm, kann aber z.B. unter einer freien Lizenz stehen. Bekannte Beispiele sind die verschiedenen Norton Commander-C.s (ein DOS-Filemanager) wie sc oder mc. <@7 Glossar fett>Cluster<@$p> – Verbund zusammengeschlossener Rechner zur Steigerung der Rechenleistung; ein C. aus vielen handelsüblichen PCs kann dieselbe Rechenleistung erzielen wie ein Supercomputer, zu einem Bruchteil von dessen Preis. Die fotorealistischen Animationen für den Kinohit »Titanic« wurden auf einem C. aus 350 GNU/Linux-Computern gerechnet, die Suchmaschine »Google« läuft auf einem C. aus 500 GNU/Linux-Rechnern. »Feierabend-C.« schalten die nachts brachliegenden Prozessoren in einem bestehenden LAN zusammen, um leistungshungrige Anwendungen laufen zu lassen. <@7 Glossar fett>Compiler<@$p> – Programm, das den <@7 Glossar Pfeil Umsch/alt#>’<@$p> Quellcode von Programmen, die in einer höheren Programmiersprache (z.B. C) geschrieben sind, in die Maschinensprache des jeweiligen Prozessors übersetzt und optimiert (z.B. der GNU C Compiler (gcc)). Ergebnis der Kompilierung ist ein <@7 Glossar Pfeil Umsch/alt#>’<@$p> Binary. <@7 Glossar fett>Content<@$p> – mediale Inhalte aller Formate (Text, Bild, Klang, Bewegtbild), insbesondere im Internet. Als in der Frühzeit des Internet Wissenschaftler und andere Gemeinschaften Wissen aller Art miteinander austauschten, sprach niemand von C. Der Begriff verbreitete sich Mitte der 90er-Jahre, als die Medienindustrie der Ansicht war, dass, nachdem die Vorgeschichte des Netzes zur Einrichtung der »Container« (der Infrastruktur aus Kabeln, Protokollen und Diensten) gedient habe, es nun an der Zeit sei, dass sie das Netz mit ihrem »geistigen <\h>Eigentum« (ein anderes Wort für C.) beglücke. <@7 Glossar fett>Cookie<@$p> – kein Keks, sondern eine Variable, <\n>die ein Webserver auf der Festplatte des <\n>Browsers setzt, um eine Session (z.B. einen eShopping-Vorgang) zu verwalten, d.h. das Verhalten des Besuchers zu protokollieren (z.B. den Inhalt seines Warenkorbes) und <\n>server-seitig darauf zu reagieren (z.B. eine Gesamtrechnung zu erstellen). Kann Nütz<\h>liches tun, wie einen authentifizierten und verschlüsselten Webdialog zulassen, aber auch den Anbietern von Werbebannern erlauben, Information über das Surfverhalten einer großen Zahl von Menschen zu sammeln, die die Standardeinstellung der meisten Browser (»alle Cookies akzeptieren«) unverändert <\n>lassen, statt eine bewusste Cookie-Politik zu betreiben.<@7 Glossar fett>Core-Team<@$p> – Kerngruppe eines Softwareprojekts, die die Hauptarbeit an der Entwicklung trägt und koordiniert. Im Gegensatz zu der sehr viel größeren Zahl der Entwickler, die nur gelegentlich periphere Teile zum Programm beisteuern, arbeitet das kleine C.T. ständig an den Grundlagen und Kernfunktionen.<@7 Glossar fett>dBase<@$p> – Datenbankprogramm der Ashton-<\n>Tate Corporation; in den 80er-Jahren weit <\n>verbreitet.<@7 Glossar fett>Debugger<@$p> – Programm, das während der <\n>Entwicklung von Software die Fehlersuche <\n>(<@7 Glossar Pfeil Umsch/alt#>’<@$p> Bug) erleichtert. Ein D. überprüft den Quellcode auf formale Fehler und überwacht das Ablaufen des <@7 Glossar Pfeil Umsch/alt#>’<@$p> Excecutables. Trifft der D. auf einen Fehler, hält er das Programm an, gibt Information zum Fehler und bietet Korrekturen an.<@7 Glossar fett>Dongle<@$p> – Hardware-Kopierschutz; der D. wird in eine Schnittstelle des Rechners (seriell, parallel, USB usw.) gesteckt und von der geschützten Software beim Starten und in regelmäßigen Intervallen abgefragt. Antwortet die <@7 Glossar Pfeil Umsch/alt#>’<@$p> Blackbox nicht mit dem erwarteten Validierungs<\h>code, bricht die Software ab. Sie kann daher kopiert, aber nur vom Besitzer des D. verwendet werden. Ähnlich gibt es Software, die nicht ohne die richtige CD im Laufwerk läuft.<@7 Glossar fett>Encoder<@$p> – bringt eine Datenquelle (z.B. ein digitales Audiosignal) in ein gewünschtes Format (z.B. <@7 Glossar Pfeil Umsch/alt#>’<@$p> MP3); wird meist im Zusammenhang mit der Kompression von fortlaufenden Datenströmen wie Audio und Video verwendet.<@7 Glossar fett>Ethernet<@$p> – weit verbreitetes Protokoll für <\n>lokale Netzwerke. Robert Metcalfe umriss in seiner Doktorarbeit an der Harvard Universität das Konzept für ein <@7 Glossar Pfeil Umsch/alt#>’<@$p>LAN mit multiplen Zugangskanälen, das er »E.« nannte. Am Xerox PARC entwickelte er 1976 das Konzept zu seiner heutigen Form weiter. Bald darauf als Industriestandard (IEEE 802.3) verabschiedet. Das ursprüngliche E. war für 10 Mbit/s ausgelegt, heute verbreiten sich Gigabit-E’s.<@7 Glossar fett>Executable<@$p> – ausführbares Programm; abhängig von der Programmiersprache, eine Quellcode-Datei (z.B. ein Perl-Skript), die zur Laufzeit des Programmes von einem Interpreter oder <@7 Glossar Pfeil Umsch/alt#>’<@$p> Compiler übersetzt wird, oder ein bereits kompiliertes <@7 Glossar Pfeil Umsch/alt#>’<@$p> Binary oder Objekt<\h>code, wobei Objektcode ein Linking mit anderen Objektcode-Dateien erfordern kann, bevor daraus ein vollständiges E. wird.<@7 Glossar fett>Fair Use<@$p> – Rechtsdoktrin des angloamerikanischen Copyright-Rechts, die bestimmte nicht autorisierte Nutzungen von geschütztem Material zugesteht, sofern sie der öffent<\h>lichen Bildung und der Anregung geistiger Produktionen dienen; entspricht in etwa den <\h>Schrankenbestimmungen des kontinentaleuropäischen Urheberrechts. Einige US-amerikanische Rechtsgelehrte (z.B. Tom W. Bell) sehen in der F.U.-Doktrin nur einen Mechanismus, um ein »Marktversa<t1>gen« zu reparieren. <t$> Andere (z.B. Julie Cohen) sehen darin ein positives Recht, das selbst dann greift, wenn der Rechteinhaber das Werk durch ein technisches Rechtekontroll<\h>sys<\h>tem schützt. (<t4>Vgl. Ka<t$>pitel »Für eine informationelle Nachhaltigkeit«)<t0> <t$><@7 Glossar fett>Firewall<@$p> – »Brandschutzmauer« die alle externen Zugänge überwacht; entweder als Software auf einem Rechner am Internet oder als <@7 Glossar Pfeil Umsch/alt#>’<@$p> Gateway-Rechner zwischen einem lokalen Netz und seiner Umwelt (dem Internet, dem Telefon- oder anderen Netzen). Erlaubt es, Datenpakete zu filtern und Ports von Dien<\h>sten (Telnet, http, Chat, Napster), Adress<\h>räume und einzelne Adressen zu <\h>sperren. <@7 Glossar fett>Freeware<@$p> – Software, die von ihrem Autor kostenlos, frei weitergebbar, in der Regel ohne <@7 Glossar Pfeil Umsch/alt#>’<@$p> Quellcode und oft ohne Lizenz veröffentlicht wird.<@7 Glossar fett>Gateway<@$p> – der »Torweg« zwischen verschiedenen Netzen, z.B. einem lokalen Netz und dem Internet. Der G.-Rechner kann Protokolle ineinander übersetzen und kennt die Adress<\h>bereiche, die hinter ihm liegen.<@7 Glossar fett>Gigabyte<@$p> – Milliarden <@7 Glossar Pfeil Umsch/alt#>’<@$p> Byte<@7 Glossar fett>Header<@$p> – einer Datei vorangestellte Informationen; kann Angaben zum verwendeten Anwendungsprogramm, zu Rechtekontrolle, eine Datei-ID und vieles andere mehr enthalten. Die H. einer E-Mail z.B. tragen die zum Versand notwendigen Verwaltungsinformationen und eine Art elektronischen »Poststempel«, an dem versiertere Nutzer sehen können, welchen Weg die E-Mail gegangen ist. Üblicherweise werden von <@7 Glossar Pfeil Umsch/alt#>’<@$p> Mail-Clients nur die Header »From:«, »To:« und »Subject:« angezeigt. <@7 Glossar fett>Host<@$p> – wörtlich »Gastgeber«; Rechner, der ständig mit dem Internet verbunden ist und Dienste anbietet.<@7 Glossar fett>Hosting<@$p> – das Vermieten von Zugang zu und Plattenplatz auf einem <@7 Glossar Pfeil Umsch/alt#>’<@$p> Host. Im Unterschied zum H., stellt beim »Housing« der Kunde einen eigenen Rechner im Rechenzentrum des <@7 Glossar Pfeil Umsch/alt#>’ <@$p>ISP unter.<@7 Glossar fett>Hypertext<@$p> – im einfachsten Sinne: ein Text mit aktiven Links, die den Leser per Maus<\h>klick zu einer anderen Information bringen; im schwierigen Sinne: Gegenstand eines anhaltenden Diskurses, ob mit dem H. die 5<\!q>000-jährige Linearität der Schrift zu Ende geht oder ob nicht vielmehr bereits James Joyce’ Roman »Ulysses« ein H. <@7 Glossar kursiv>avant la lettre<@$p> war. <@7 Glossar fett>Intranet<@$p> – ein auf Internet-Technologie (also RFC-) basiertes internes Netzwerk mit keinem oder einem stark regulierten Zugang zum Internet.<@7 Glossar fett>Kernel<@$p> – der »Kern« eines Betriebssystems, der die Betriebsmittel eines Rechners (CPU, Speicher, Ein/Ausgabegeräte) verwaltet und sie den Anwendungen zur Verfügung stellt das, was im Flugzeug das Cockpit ist.<@7 Glossar fett>Lock<@$p> – »Sperre«; hat z.B. ein Prozess eine <\n>Datei zum Schreiben geöffnet, ist sie für alle anderen Prozesse gesperrt. Blockieren sich zwei Prozesse gegenseitig, spricht man von einem »Deadlock«. Hier vor allem als Mechanismus, um Software mit der individuellen Kennung einer anderen Software oder einer Hardware zu koppeln (vgl. <@7 Glossar Pfeil Umsch/alt#>’<@$p> Dongle).<@7 Glossar fett>Mail-Client<@$p> – Programm, mit dem man <\h><\n>E-Mail schreiben und lesen kann.<@7 Glossar fett>Mainframe<@$p> – bezeichnete ursprünglich den Schrank, der die Einheit mit dem Zentralprozessor oder eben »main frame« eines Raum füllenden, Stapel verarbeitenden Großrechners enthielt. Seit sich Anfang der 70er-Jahre das Minicomputer-Design verbreitete, <\n>wurden alle früheren »Großen Eisen« M. <\n>genannt. Heute nur noch für Maschinen aus der Steinzeit der Computerei verwendet. <@7 Glossar fett>Multitasking<@$p> – die parallele Ausführung von mehreren Prozessen (tasks) auf einem Rechner. Tatsächlich werden die Prozesse nicht zeitgleich, sondern nacheinander abgearbeitet. Dauert ein Prozess länger als die vorgesehene Zeit, wird er unterbrochen, und der Prozessor arbeitet ein bisschen am nächsten wartenden Prozess weiter und am nächsten, bis er sich wieder dem ersten Prozess zuwendet. Da es dabei um Zeiträume von Sekundenbruchteilen geht, ergibt sich für den Benutzer der Eindruck, als arbeiteten mehrere Prozesse gleichzeitig.<@7 Glossar fett>Netiquette<@$p> – Konventionen für einen <\n>höf<\h>lichen Umgang im Netz, vor allem auf <\n><@7 Glossar Pfeil Umsch/alt#>’<@$p> Newsgroups und Mailinglisten. Der <\n>»Knigge« des Netzes ist Virginia Shea, die sich seit Mitte der 80er-Jahre um das zivilisierte Benehmen der Netizens – der Bewohner des Netzes – verdient gemacht und ihr Lebenswerk in einem gleich<\h>namigen Buch niedergelegt hat. Meint heute nahe liegende, aber leider nicht selbstverständliche Grundregeln wie: Du sollst nicht <@7 Glossar Pfeil Umsch/alt#>’<@$p> spammen, Du sollst dringende Aufrufe über Viren oder krebsleidende Kinder nicht unüberprüft weiterversenden, Du sollst nicht betrunken <\h>posten usw. <@7 Glossar fett>Newsgroups<@$p> – elektronische »schwarze Bretter« auf denen buchstäblich über Gott und die Welt diskutiert wird. 1979 von Steve Bellovin entwickeltes verteiltes Client-Server-System. N. können moderiert oder unmoderiert sein und ihre Postings an eine parallele Mailing<\h>liste weiterschicken oder von dort <\n>bekommen. Zusammen werden die N. oder NetNews als »Usenet« bezeichnet. <@7 Glossar fett>Objektcode<@$p> – s. <@7 Glossar Pfeil Umsch/alt#>’ <@$p>Excecutable<@7 Glossar fett>Patch<@$p> – ein Code-Flicken, mit dem sich ein Loch oder ein Fehler (<@7 Glossar Pfeil Umsch/alt#>’<@$p> Bug-Fix) vorüber<\h>gehend ausbessern lässt. Statt darauf zu warten, dass eine vollständige verbesserte <t1>Version des jeweiligen Programmes erscheint, <t$>fügt man den P. in den <@7 Glossar Pfeil Umsch/alt#>’<@$p> Quellcode ein, kompiliert ihn neu und hat die entsprechende Funktionalität zur Verfügung. Im besten Fall wird der Anlass für den P. in der nächsten vollen Release sauber behoben, im schlimmsten <\n>folgen weitere P’s, die unerwartete Nebenwirkungen des ersten P. beseitigen. Dann spricht man davon, dass »Narbengewebe« wächst.<@7 Glossar fett>Plugin<@$p> – externes Programm, das einem Basissystem neue Funktionen hinzufügt; viele Programme wie Browser, Web-Server oder Grafikprogramme bieten Schnittstellen, an denen P. der Hersteller oder Dritter ansetzen können, um sie zu erweitern. Beim Websurfen stößt man alle Nase lang auf neue Dienste, und der Browser fragt, ob man das benötigte P. jetzt herunterladen möchte.<@7 Glossar fett>Port<@$p> – Schnittstelle eines Servers für einen <\h>bestimmten Dienst, repräsentiert durch eine P.Nummer. So verbirgt sich z.B. hinter Port 80 ein Server, der http-Anfragen (WWW) bedient, und hinter Port 25 ein Server, der Mail entgegen nimmt. P.Nummern werden von der <@7 Glossar Pfeil Umsch/alt#>’<@$p> ICANN weltweit einheitlich zugewiesen. Sinnbildlich ist ein P. wie eine Steckdose, die auf den passenden Stecker wartet.<@7 Glossar fett>Portierung<@$p> – Übersetzung eines Programmes auf eine andere Rechnerplattform<@7 Glossar fett>Proxy-Server<@$p> – Stellvertreter; ein http-P. <\n>in einem lokalen Netz z.B. speichert die an<\h>gefragten Webseiten zwischen. Fordert der nächste Nutzer eine solche Seite an, bekommt er sie nicht vom entfernten Server, sondern sehr viel schneller vom lokalen Stellvertreter. In Verbindung mit einer <@7 Glossar Pfeil Umsch/alt#>’<@$p> Firewall kann ein P. dazu dienen, von abgeschirmten Rechnern aus Webseiten aufzurufen, ohne sich ungebetene Gäste ins Haus zu holen.<@7 Glossar fett>Public Domain<@$p> – im angloamerikanischen Copyright-Recht: gemeinfreies Wissen; Werke ohne Eigentümer. Häufig unspezifisch für öffentliches Wissen im Gegensatz zu Wissen im Privatbesitz verwendet<@7 Glossar fett>Public/Private-Key-Verfahren<@$p> – asymmetrischer Kryptografiealgorithmus, zuerst 1976 von Whitfield Diffie und Martin E. Hellman veröffentlicht; löst das Urproblem der symmetrischen Kryptografie: da derselbe Schlüssel für Ver- und Entschlüsselung verwendet wird, muss der Absender einer verschlüsselten Nachricht dem Empfänger auch den Schlüssel zukommen lassen. Den kann er aber nicht über denselben Kanal schicken wie die Botschaft, da er dem Kanal nicht traut, sonst würde er ja nicht verschlüsseln. Beim P.V. erzeugt jeder Teilnehmer ein Schlüsselpaar aus einem geheimen und einem öffentlichen Schlüssel, die mathematisch miteinander verbunden sind, ohne dass man den geheimen aus dem öffentlichen Schlüssel errechnen könnte. Der öffentliche Schlüssel wird auf einem öffentlich zugänglichen Key-Server abgelegt. Dort holt sich der Absender den öffentlichen Schlüssel des Empfängers und verschlüsselt damit seine Nachricht, die jetzt nur der Empfänger mit seinem privaten Schlüssel wieder entschlüsseln kann. Neben der Verhinderung unliebsamer Mitleser kann das P.V. auch für die Authentifizierung von digitalen Dokumenten, also einen Echtheitsnachweis, verwendet werden. Dabei signiert man ein Dokument mit dem eigenen privaten Schlüssel. Die Echtheit der Signatur kann mit Hilfe des passenden öffentlich verfügbaren Schlüssels überprüft werden.<@7 Glossar fett>Pull-Down-Menü <@$p>– ein selbst ausklappendes Menü.<@7 Glossar fett>Quellcode<@$p> – Programmtext in einer höheren Programmiersprache; da die binär repräsentierte Maschinensprache des Computers für Menschen nur schwer verständlich ist, programmieren sie in einer höheren Programmiersprache wie C, die aus alphanumerischen Zeichen und Wörtern besteht, die an die englische Sprache erinnern. Q. muss von einem <@7 Glossar Pfeil Umsch/alt#>’<@$p> Compiler oder einem Interpreter erst in eine maschinennahe Form übersetzt werden, bevor ein Computer das Programm tatsächlich ausführen kann. Will man Änderungen an <\h>einem Programm vornehmen, benötigt man Zugang zu seinem Q.<@7 Glossar fett>Release<@$p> – »Freigabe«; eine bestimmte Version eines Programms.<@7 Glossar fett>Reverse Engineering<@$p> – Verfahren, bei <\n>dem ein System unbekannten Aufbaus (vgl. <\n><@7 Glossar Pfeil Umsch/alt#>’<@$p> Blackbox) analysiert und seine Komponenten und ihre Wechselwirkungen identifiziert werden, um ein System mit demselben <\n>Verhalten zu bauen. Dabei kann es sich um das <@7 Glossar Pfeil Umsch/alt#>’<@$p> Excecutable eines Programmes, einen integrierten Schaltkreis oder ein anderes technisches System handeln.<@7 Glossar fett>Router<@$p> – ein Rechner, der Datenpakete <\n>zwischen Netzwerken weiterschickt. Die <\n><t1>Entscheidung, wohin er Pakete jeweils weiter<\h>sc<t$>hickt, stützt er auf Routing-Tabellen über die verfügbaren Verbindungen und Informa<\h>tionen über deren aktuelle Auslastung. <@7 Glossar fett>Sample<@$p> – in der digitalen Signalverarbeitung: die Abtastung der Amplitude eines analogen Signals, die in regelmäßigen Abständen <\n>erfolgt, deren Zeitdauer über die Auflösung des S. entscheidet. In der Musik: ein kurzer Klang, der in den Loop eines Samplers eingelesen und von dort aus verfremdet, rhythmisiert und ausgespielt werden kann.<@7 Glossar fett>Screen-Shot<@$p> – ein Abbild (Hardcopy) des derzeitigen Bildschirminhaltes, das dann ausgedruckt oder anderweitig weiter verwendet werden kann.<@7 Glossar fett>Security by Obscurity<@$p> – »Sicherheit durch Undurchsichtigkeit«; die naive, aber immer noch weit verbreitete Annahme, dass ein <\n>System dann sicher ist, wenn man seine Funktionsweise geheim hält. Der gegenteilige Ansatz verwendet Algorithmen, die auch dann sicher sind, wenn potenzielle Angreifer sie kennen. Dabei heißt es bereits im Lehrbuch: »Zum Ersten soll der Entwurf eines Systems öffentlich zugänglich sein. Die Annahme, dass ein Eindringling nicht weiß, wie ein System funktioniert, dient lediglich der Täuschung der Entwickler.« (Tanenbaun, 1995, S. 231)<@7 Glossar fett>Server<@$p> – ein Programm, das anderen, nämlich Client-Programmen einen bestimmten Dienst (Domain-Namen-Information, FTP, HTTP, <\n>News) anbietet. Client und S. sprechen über ein Protokoll miteinander und können auf demselben Rechner oder auf verschiedenen Rechnern laufen, die über ein Netzwerk miteinander verbunden sind. <@7 Glossar fett>Set-Top-Box<@$p> – Dekoder für digitales TV<@7 Glossar fett>Shared Library<@$p> – gemeinsam benutzte Bibliothek. In komplexeren Programmiersprachen werden häufig gebrauchte Funktionen (z.B. das Umbenennen einer Datei) in eigenen Modulen abgelegt und in Bibliotheken zusammengefasst. Programmierer brauchen diese Funktionen nicht immer wieder neu zu schreiben, sondern können sie aus der S.L. abrufen. Da verschiedene Programme sie parallel nutzen können, müssen die Funktionen nicht mit jedem Programm erneut auf dem System installiert werden.<@7 Glossar fett>Shareware<@$p> – Software, die von ihrem Autor (in der Regel ohne <@7 Glossar Pfeil Umsch/alt#>’<@$p> Quellcode und ohne Veränderungserlaubnis) kostenlos, aber mit der verbindlichen Bitte veröffentlicht wird, ihm bei regelmäßiger Nutzung des Programms einen bestimmten Geldbetrag zukommen zu lassen. Da sie es der Nutzerin erlaubt, das Programm weiterzugeben und auszuprobieren, bevor sie es in täglichen Gebrauch nimmt, überschneidet sich S. mit der »Demoware« oder »Crippleware«, die einen gegenüber der Vollversion eingeschränkten Leistungsumfang bietet. Wenn die Software ihren Ansprüchen genügt, kann sie beim Autor einen Schlüssel kaufen, mit dem die Software zum vollen <\h>Leistungsumfang kommt. <@7 Glossar fett>Slackware<@$p> – eine der ersten kommerziellen GNU/Linux-Distributionen, heute nur noch sehr gering verbreitet.<@7 Glossar fett>Source-Baum<@$p> – in einem <@7 Glossar Pfeil Umsch/alt#>’<@$p> CVS baumartig organisierter <@7 Glossar Pfeil Umsch/alt#>’<@$p> Quellcode einer Software, <\h>inklusive der Dokumentation; kann man verwenden, um damit das Programm zu kompilieren oder in die Weiterentwicklung des Programms einzusteigen.<@7 Glossar fett>Spam<@$p> – kommunikationelle Umweltverschmutzung; Massen-E-Mails, die Foren und Mailboxen überfluten, häufig zweifelhaften kommerziellen Ursprungs: Kettenbriefe, unerwünschte Werbung jeder Art, Bettelbriefe. S. ist ursprünglich gepökeltes Dosenfleisch der US-Firma Hormel mit schlechtem Ruf: salziges Fastfood in Dosen ohne jeglichen Nährwert. Die Komikergruppe Monty Python machte daraus einen Sketch, in dem es kein Entkommen vor S. gibt. Dieser war Anlass für die neue Verwendung des Wortes im digitalen Netzraum. Als Begründer des Genres gilt die Anwaltsfirma Canter & Siegel, die im April 1994 eine Werbebotschaft auf sämtlichen (mehr als 10 000) Newsgroups postete.<@7 Glossar fett>Toolkit<@$p> – Werkzeugsammlung; Zusammenstellung mehrerer Programme, die einen gemeinsamen Einsatzzweck haben. Das gtk-T. stellt z.B. eine komplette Entwicklungsumgebung für gtk-basierte Programme bereit.<@7 Glossar fett>Trojanisches Pferd<@$p> – gefährlicher Parasit im Inneren eines scheinbar normalen <\n>Programmes, z.B. eine Login-Prozedur, die Nutzername und Passwort abfragt, ein Fehlermeldung ausgibt und den Nutzer dann an das echte Login übergibt. Jeder vertippt sich mal, wenige werden durch eine solche Fehler<\h>meldung misstrauisch. Das T.P. jedoch <\n>sammelt Passwörter, die es dann an seinen Autor übermittelt. <@7 Glossar fett>Turing-Maschine<@$p> – eine von dem britischen Mathematiker Alan Mathison Turing (1912-1954) zur Lösung des von Gödel formulierten Vollständigkeitsproblems erdachte Maschine, bestehend aus einem programmgesteuerten Lese- und Schreibkopf, der auf einem endlosen Papierband ein Feld nach links oder rechts rücken, ein Zeichen lesen, schreiben oder löschen und schließlich stehen bleiben kann. Turing zeigte, dass diese Maschine jedes algorithmisierbare Problem lösen kann. Sie wird als universale T.M. bezeichnet und liegt fast allen heutigen Computerarchitekturen zugrunde.<@7 Glossar fett>Upgrade<@$p> – eine neue oder verbesserte <\n>Version einer Software oder Hardware. Nicht selten verwendet, um <@7 Glossar Pfeil Umsch/alt#>’<@$p> Bug-Fixes zu ver<\h>breiten und Anwender durch den U. einer Komponente zu zwingen, auch andere Hard- oder Softwarekomponenten seines Systems neu zu kaufen.<@7 Glossar fett>Usenet<@$p> – s. <@7 Glossar Pfeil Umsch/alt#>’ <@$p>Newsgroups<@7 Glossar fett>Von-Neumann-Architektur<@$p> – eine von dem ungarischen, über Berlin und Zürich in die USA ausgewanderten Mathematiker John von Neumann (1903-1957) erfundene Rechnerarchitektur, bei der Programme wie Daten im selben Speicher vorliegen, aus dem der <\n>Prozessor die Anweisungen des jeweiligen Programmes holt, sie interpretiert, dann die Daten holt, um eine der elementaren Maschinenoperationen auf ihnen auszuführen, und schließlich das Ergebnis in den Speicher zurückzuschreiben. Bei jedem Takt der Uhr wird eine der Grundoperationen durchgeführt. Diese strikt sequenzielle Abarbeitung im einzigen Zentralprozessor (CPU) wird als V.-N.-A. bezeichnet und liegt fast allen heute gebauten Computern zugrunde. Zu »Non-von«-Architekturen gehören Parallelrechner mit mehreren Prozessoren und neuronale Netze. <@7 Glossar fett>Workstation<@$p> – Arbeitsplatzrechner in einem Netz, in dem Arbeitsmittel wie Festplatten und Drucker meist in zentralen Pools bereitgestellt werden. @2  ZÜ 1:Abkürzungsverzeichnis@99 Glossar fliess:<@7 Glossar fett>AHRA<@$p> – <@7 Glossar kursiv>Audio Home Recording Act,<@$p> 1992 verabschiedeter Zusatz zum US-Copyright-Gesetz, auf die DAT-Technologie zugeschnitten; verbietet die Herstellung, Einfuhr und Verbreitung von digitale Audioaufnahmegeräten, die nicht über ein <@7 Glossar Pfeil Umsch/alt#>’<@$p> SCMS oder vergleichbares System verfügen; verbietet Geräte, deren primärer Zweck es ist, solche Kopierschutzmechanismen zu umgehen, zu entfernen oder zu deaktivieren; etablierte in den USA ein Pauschalvergütungssystem für digitale Audioaufnahmegeräte und -leermedien; eines der frühesten Beispiele für ein lex digitalis.<@7 Glossar fett>ARPA<@$p> – s. <@7 Glossar Pfeil Umsch/alt#>’<@$p> DARPA <@7 Glossar fett>BBS<@$p> – <@7 Glossar kursiv>Bulletin Board System<@$p>, elektronisches »schwarzes Brett« in Deutschland auch als ›Mailbox‹ bekannt; abfangs allein stehende PCs mit Telefoneinwahl, die sich seit Ende der 70er-Jahre u.a. mit dem FidoNet-Protokoll untereinander vernetzten. Zu den kommerziellen BBSs gehören AOL und CompuServe, die Ende der 80er-Jahre <@7 Glossar Pfeil Umsch/alt#>’<@$p> Gateways zum Internet einrichteten. <@7 Glossar fett>BIOS<@$p> – <@7 Glossar kursiv>Basic Input/Output System<@$p>; Firmware, die einen Computer in die Lage versetzt, ein Betriebssystem zu laden.<@7 Glossar fett>BITNET<@$p> – <@7 Glossar kursiv>Because It’s Time NETwork<@$p>; 1981 als ein kooperatives Netzwerk an der City University of New York gestartet, eine Art <\n>›Internet des armen Mannes‹, das Dateiübertragung per E-Mail erlaubt. 1987 überschritt die weltweite Zahl der BITNET-Hosts 1 000.<@7 Glossar fett>BSD<@$p> – <@7 Glossar kursiv>Berkeley Software Distribution,<@$p> die an der University of California in Berkeley selbst erstellten und von anderen Hochschulen gesammelten Weiterentwicklungen von AT&Ts Betriebssystem Unix, die 1977 erstmals von Bill Joy unter dem Namen BSD veröffentlicht wurden. Zu den bedeutenden Beiträgen der Universität Berkeley gehört die Integration von <@7 Glossar Pfeil Umsch/alt#>’<@$p> TCP/IP in Unix und die BSD-Lizenz. Nachdem die Forschungsgruppe an der Berkeley Universität aufgelöst wurde, lebt das BSD-Unix heute in den drei freien Projekten NetBSD, FreeBSD und OpenBSD weiter.<@7 Glossar fett>CERN<@$p> –<@7 Glossar kursiv>Centre Européenne pour la Recherche Nucléaire<@$p>, europäisches Hochenergiephysik-Zentrum in Genf, Geburtsort des <@7 Glossar Pfeil Umsch/alt#>’<@$p> WWW.<@7 Glossar fett>CGMS<@$p> – <@7 Glossar kursiv>Copy Generation Management <\n>System<@$p>; Kopierschutzmechanismus u.a. für DVDs, der von einem Vertriebsstück nur eine Generation von Kopien zulässt. Für DAT-Technologie eingeführt, heißt das Verfahren dort <@7 Glossar Pfeil Umsch/alt#>’<@$p> SCMS.<@7 Glossar fett>CISAC<@$p> – <@7 Glossar kursiv>Confédération Internationale des <\n>Sociétés d’Auteurs et Compositeurs;<@$p> inter<\h>nationaler Dachverband der Verwertungs<\h>gesellschaften.<@7 Glossar fett>CIX<@$p> – <@7 Glossar kursiv>Commercial Internet eXchange<@$p>; <\n>Verkehrsknotenpunkt zwischen kommerziellen Internet-Anbietern. Der erste wurde 1991 in Kalifornien eingerichtet. <@7 Glossar fett>CONTU<@$p> – <@7 Glossar kursiv>Commission on New Technological Uses of Copyrighted Works,<@$p> 1974 vom US-Kongress einberufener Ausschuss zur Vorbereitung der Revision des Copyright-Gesetzes für die Anforderungen der Digitaltechnik; empfahl, Computerprogramme zukünftig als »literarische« Werke unter den Schutz des Copyright zu stellen.<@7 Glossar fett>CP/M<@$p> – <@7 Glossar kursiv>Control Program for Microcomputers<@$p>; das 1973 von Gary Kildall entwickelte Betriebssystem für Kleincomputer; wurde auf zahlreichen Prozessoren implementiert und ermöglichte daher erstmals die einfache Portierung von Programmen auf verschiedene Rechnertypen. In den 80er-Jahren das am weitesten verbreitete PC-Betriebssystem, wurde CP/M durch einen leistungsschwächeren <@7 Glossar Pfeil Umsch/alt#>’<@$p> Clone namens MS-DOS verdrängt.<@7 Glossar fett>CPU<@$p> – <@7 Glossar kursiv>Central Processing Unit<@$p>; der Haupt<\h>prozessor eines Computers<@7 Glossar fett>CSS<@$p> – <@7 Glossar kursiv>Content Scrambling System,<@$p> für <@7 Glossar Pfeil Umsch/alt#>’<@$p> DVD eingeführter Kopierschutzmechanismus, enthält u.a. die Regionalsperre <@7 Glossar Pfeil Umsch/alt#>’<@$p> RPC; hauptsächlich von Matsushita und Toshiba entwickelt.<@7 Glossar fett>CTSS<@$p> – <@7 Glossar kursiv>Compatible Time-Sharing System<@$p>; <\n>das erste Anfang der 60er-Jahre am <@7 Glossar Pfeil Umsch/alt#>’<@$p> MIT <\n>entwickelte Multiuser-Betriebssystem. Jeder Nutzer bekommt reihum den Prozessor für ein kurzes Zeitsegment zugeteilt, so dass er das Gefühl hat, den Computer alleine zu nutzen. S.a. <@7 Glossar Pfeil Umsch/alt#>’<@$p> ITS<@7 Glossar fett>CVS<@$p> – <@7 Glossar kursiv>Concurrent Versions System<@$p>, Online-<\h>System, das den <@7 Glossar Pfeil Umsch/alt#>’<@$p> Source-Baum eines Softwareprojekts enthält und es mehreren Entwicklern erlaubt, zeitgleich an denselben <@7 Glossar Pfeil Umsch/alt#>’<@$p><\!q>Quellcode-Abschnitten zu arbeiten.<@7 Glossar fett>DARPA<@$p> – <@7 Glossar kursiv>Department of Defense Advanced Projects Agency,<@$p> Forschungsförderungsbehörde des US-amerikanischen Verteidigungs<\h>minis<\h>teriums; bis Anfang der 70er-Jahre hieß sie ARPA.<@7 Glossar fett>DFN<@$p> – Deutsches ForschungsNetz e.V.; 1985 gegründetes Wissenschaftsnetz, betreibt heute mit dem Gigabit-Wissenschaftsnetz <\n>G-WiN den Kern der Kommunikations-Infrastruktur von Wissenschaft, Forschung und <\n>Bildung in Deutschland. <@7 Glossar fett>DNS<@$p> – <@7 Glossar kursiv>Domain Name System<@$p>; 1983 u.a. von Jon Postel entwickeltes System, um die <\n>numerischen Internet-Adressen <\n>(z.B. 141.20.1.3) auf Namen abzubilden <\n>(z.B. www.hu-berlin.de).<@7 Glossar fett>DOS, MS-DOS<@$p> – <@7 Glossar kursiv>Microsoft Disk Operating System,<@$p> in den 80er-Jahren dominierendes Betriebssystem für PCs. Wie viele seiner Produkte hat Microsoft MS-DOS nicht selbst entwickelt, sondern von einer Firma namens Seattle Computer gekauft. Es handelte sich um eine abgespeckte Version von <@7 Glossar Pfeil Umsch/alt#>’<@$p> CP/M, von der Digital Research noch dazu behauptete, dass Seattle Computer seinen Quellcode gestohlen habe. Die Industrie war sich einig, dass CP/M ein weit überlegenes Betriebssys<\h>tem war, doch mit der Unterstützung durch IBM dominierte MS-DOS nach 1980 in kürzes<\h>ter Zeit den Markt.<@7 Glossar fett>DVD<@$p> – <@7 Glossar normal>ursprünglich:<@7 Glossar kursiv> Digital Video Disc, <@7 Glossar normal>heute: <@7 Glossar kursiv>Digital Versatile Disc,<@$p> Massenspeicher für Software und Daten jeder Art, der die CD ablösen wird. <@7 Glossar fett>EARN<@$p> – <@7 Glossar kursiv>European Academic Research <\n>Network<@$p>; 1984 gestartet, das erste <\n>europäische Rechnernetz, das über einen <\n>E-Mail-Link in die USA und dort über ein <\n><@7 Glossar Pfeil Umsch/alt#>’ <@$p>Gateway ins Internet verfügte.<@7 Glossar fett>EBLIDA<@$p>– <@7 Glossar kursiv>European Bureau of Library<@$p>, <@7 Glossar kursiv>Infor<\h>mation and Documentation Associations; <@$p><\n>europäischer Dachverband der Bibliotheken<@7 Glossar fett>EBONE<@$p> – <@7 Glossar kursiv>European Backbone,<@$p> ab 1991 errichtete »Hauptverkehrsstraße« des Internet in Europa<@7 Glossar fett>EFF<@$p> – <@7 Glossar kursiv>Electronic Frontier Foundation<@$p>; 1990 von »Greatful Dead«-Texter John Perry Barlow und Lotus-Gründer Mitch Kapor gegründete NGO, die sich für die Wahrung der <\n>Bürgerrechte im Cyberspace engagiert.<@7 Glossar fett>FITUG<@$p> – Förderverein Informationstechnik <\n>und Gesellschaft; NGO mit dem thematischen Schwerpunkt Kryptografie.<@7 Glossar fett>FSF<@$p> – <@7 Glossar kursiv>Free Software Foundation,<@$p> 1985 von Richard Stallman gegründete gemeinnützige Stiftung, die die Distribution von Emacs und dann auch für andere <@7 Glossar Pfeil Umsch/alt#>’<@$p> GNU-Software übernahm. Erlöse und Spenden verwendet die FSF, um Entwickler dafür zu bezahlen, dass sie bestimmte, für eine vollständige Betriebssys<\h>temumgebung notwendige Programme schreiben. 1989 legte die FSF mit der <@7 Glossar Pfeil Umsch/alt#>’<@$p> GPL die wichtigste Lizenz der freien Software vor. Um besser regional handeln zu können, gründeten sich 2001 die FSF Europe sowie die FSF India.<@7 Glossar fett>FTP<@$p> – <@7 Glossar kursiv>File Transfer Protocol<@$p>, Protokoll zur <\n>Dateiübertragung zwischen Computern<@7 Glossar fett>GEMA<@$p> – Gesellschaft für musikalische Aufführungs- und mechanische Vervielfältigungsrechte, Verwertungsgesellschaft für Musik<@7 Glossar fett>GIF<@$p> – <@7 Glossar kursiv>Graphics Image Format,<@$p> im Internet weitverbreitetes Grafikformat, erlaubt die Darstellung von 256 Farben; ist von CompuServe patentrechtlich geschützt.<@7 Glossar fett>GNU<@$p> – <@7 Glossar kursiv>GNU’s Not Unix,<@$p> von Richard Stallman 1984 gestartetes Projekt, eine freie, vollständige Unix-artige Betriebsumgebung zu schaffen.<@7 Glossar fett>GPL<@$p> – <@7 Glossar kursiv>GNU General Public License,<@$p> von der <@7 Glossar Pfeil Umsch/alt#>’<@$p> FSF herausgegebene Lizenz, ursprünglich für das <@7 Glossar Pfeil Umsch/alt#>’<@$p> GNU-Projekt geschaffen, heute die am häufigsten verwendete Lizenz für freie Software.<@7 Glossar fett>HTML<@$p> – <@7 Glossar kursiv>HyperText Markup Language<@$p>; die <\n>Auszeichnungssprache für Webseiten<@7 Glossar fett>HTTP<@$p> – <@7 Glossar kursiv>HyperText Transfer Protocol<@$p>; das <\n>Übertragungsprotokoll für Webseiten<@7 Glossar fett>IAB<@$p> – <@7 Glossar kursiv>Internet Architecture Board<@$p>, offenes Gremium für die Weiterentwicklung der Internet-Protokolle, 1983 als <@7 Glossar kursiv>Internet Activities Board <@$p>gegründet, 1992 in der <@7 Glossar Pfeil Umsch/alt#>’<@$p> ISOC aufgegangen und in AB umbenannt. <@7 Glossar fett>IANA<@$p> – <@7 Glossar kursiv>Internet Assigned Numbers Authority<@$p>; 1988 unter Leitung von Jon Postel für die <\h>Verwaltung u.a. des <@7 Glossar Pfeil Umsch/alt#>’<@$p> DNS eingerichtet, 1998 von der <@7 Glossar Pfeil Umsch/alt#>’<@$p> ICANN ersetzt.<@7 Glossar fett>ICANN<@$p> – <@7 Glossar kursiv>Internet Corporation for Assigned Names and Numbers<@$p>; 1998 errichtete Institution für die Verwaltung der Domain-Namen, den IP-Adressen, den Protokollparametern und Portnummern sowie den Root-Servern des Internet.<@7 Glossar fett>IEC<@$p> – <@7 Glossar kursiv>International Electrotechnical Commission,<@$p> Standardisierungsgremium mit Sitz in Genf<@7 Glossar fett>IETF<@$p> – <@7 Glossar kursiv>Internet Engineering Task Force<@$p>; das <\n>offene Gremium im Rahmen der <@7 Glossar Pfeil Umsch/alt#>’<@$p> ISOC, in dem die technischen Fragen der Internet<\h>infrastuktur verhandelt werden.<@7 Glossar fett>IFPI<@$p> – <@7 Glossar kursiv>International Federation of the Phonographic Industry<@$p>, London; Dachverband der weltweiten Schallplattenindustrie<@7 Glossar fett>IMP<@$p> – <@7 Glossar kursiv>Interface Message Processor,<@$p> Vorläufer der heutigen <@7 Glossar Pfeil Umsch/alt#>’<@$p> Router im Internet<@7 Glossar fett>IN<@$p> – Individual Network e.V., 1992 gegründete Initiative, die <@7 Glossar Pfeil Umsch/alt#>’<@$p> IP-Kapazitäten bei den wenigen <@7 Glossar Pfeil Umsch/alt#>’<@$p> ISPs einkaufte und als erste in Deutschland Individuen außerhalb von Hochschulen und Unternehmen Internetzugang anbot; spielte eine wichtige Rolle bei der Verbreitung des Internet und bei der Gestaltung der deutschen IP-Landschaft.<@7 Glossar fett>IP<@$p> – <@7 Glossar kursiv>Internet Protocol,<@$p> der in den <@7 Glossar Pfeil Umsch/alt#>’<@$p> RFCs spezifizierte Satz von Protokollen für digitale Datennetze<@7 Glossar fett>IRC<@$p> – <@7 Glossar kursiv>Internet Relay Chat<@$p>; 1988 von Jarkko <\n>Oikarinen entwickelte synchrone Textkommunikation.<@7 Glossar fett>ISDN<@$p> – <@7 Glossar kursiv>Integrated Services Digital Network,<@$p> digitale Telefonie und Datenkommunikation über das bestehende Kupferkabelnetz, im Gegensatz zum analogen POTS (Plain Old Telephone Service).<@7 Glossar fett>ISO<@$p> – <@7 Glossar kursiv>International Organization for Standardization,<@$p> (von gr. isos, gleich), internationale Vereinigung der Standardisierungsgremien von über 140 Ländern; verabschiedet internationale Standards in allen technischen Bereichen (außer in Elektrik und Elektronik, für die die <@7 Glossar Pfeil Umsch/alt#>’<@$p> IEC zuständig ist), darunter technische (z.B. <@7 Glossar Pfeil Umsch/alt#>’<@$p> MP3 oder Telefonkarten), klassifikatorische (z.B. Ländercodes wie .de, .nl, .jp.) und Verfahrensstandards (z.B. Qualitätsmanagement nach ISO 9000).<@7 Glossar fett>ISOC<@$p> – <@7 Glossar kursiv>Internet Society,<@$p> 1992 gegründete NGO für die Pflege und Weiterentwicklung der Internetinfrastruktur; beherbergt die für die Internetstandards zuständigen Gremien <@7 Glossar Pfeil Umsch/alt#>’<@$p> IETF und <@7 Glossar Pfeil Umsch/alt#>’<@$p> IAB.<@7 Glossar fett>ISP<@$p> – <@7 Glossar kursiv>Internet Service Provider,<@$p> Anbieter von Zugang zum und anderen Dienstleistungen im Internet.<@7 Glossar fett>ITS<@$p> – <@7 Glossar kursiv>Incompatible Timesharing System<@$p>; <\n>von den Hackern am Labor für Künstliche <\n>Intelligenz des <@7 Glossar Pfeil Umsch/alt#>’<@$p> MIT geschriebenes freies <\n>Betriebssystem für die Rechner der PDP-10-Serie von DEC, Wortspiel auf <@7 Glossar Pfeil Umsch/alt#>’<@$p> CTSS; bis zu Beginn der 80er-Jahre der von Hackern bevorzugte Tummelplatz unter den Betriebs<\h>systemen. <x@7 Glossar fett><t1>JPEG<@$p><t1> – <x@7 Glossar kursiv><t1>Joint Photographic Experts Group,<@$p><t1> (1) Ausschuss von <x@7 Glossar Pfeil Umsch/alt#><t1>’<@$p><t1> ISO und <x@7 Glossar Pfeil Umsch/alt#><t1>’<@$p><t1> IEC für die Entwicklung von Codierungsstandards für Halbtonbilder; (2) das standardisierte Bildkompressionsformat selbst; wird gern für Echt<\h>farb- bilder im Internet verwendet. Im Gegensatz zum lizenzpflichtigen <x@7 Glossar Pfeil Umsch/alt#><t1>’<@$p><t1> GIF ist JPEG frei.<t$><@7 Glossar fett>JUNET<@$p> – <@7 Glossar kursiv>Japan Unix Network;<@$p> 1984 u.a. von Jun Murai gegründetes, erstes außerakademisches IP-Netz in Japan<@7 Glossar fett>Kbps<@$p> – <@7 Glossar kursiv>Kilo bit per second,<@$p> tausend Bit pro Sekunde, Datendurchsatzrate einer Leitung<@7 Glossar fett>KI<@$p> – Künstliche Intelligenz, Zweig der Informatik, der sich bemüht, den Computer mit menschenähnlichen Kognitions- und Informationsverarbeitungsfähigkeiten auszustatten.<@7 Glossar fett>LAN<@$p> – <@7 Glossar kursiv>Local Area Network<@$p> – lokales Computer-Netz; Mitte der 80er-Jahre entstanden parallel zum Weitverkehrsnetz <@7 Glossar Pfeil Umsch/alt#>’<@$p> ARPANet Techniken, um Rechner auf einem Universitätskampus oder innerhalb eines Unternehmens miteinander zu verbinden. Verbreitetes Protokoll ist Robert Metcalfes <@7 Glossar Pfeil Umsch/alt#>’<@$p> Ethernet. <x@7 Glossar fett><t$>LGPL<@$p> – <@7 Glossar kursiv>Library General Public License<@$p>, 1999 ersetzt durch: <@7 Glossar kursiv>Lesser General Public License<@$p>; stimmt in weiten Teilen mit der <@7 Glossar Pfeil Umsch/alt#>’<@$p> GPL über<\h>ein, nur verlangt sie nicht, dass Programme, die ein freies Programm unter dieser Lizenz einlinken, selbst diesen Frei<\h>heiten unterstehen müssen (vgl. das Kapitel »Library/Lesser GPL«).<@7 Glossar fett>MBONE<@$p> – <@7 Glossar kursiv>Multimedia Backbone<@$p>; 1992 <\n>zwischen breitbandig angeschlossenen <\n>Informatiklaboren entwickelter Protokollsatz vor allem für Sprach- und Bewegtbild<\h>kommunikation<@7 Glossar fett>MCPS<@$p> – <@7 Glossar kursiv>Mechanical Copyright Protection Society,<@$p> britische Musikverwertungsgesellschaft für mechanische Reproduktionsrechte<@7 Glossar fett>MILNET<@$p> – <@7 Glossar kursiv>Military Network<@$p>; der militärisch genutzte Teil des <@7 Glossar Pfeil Umsch/alt#>’<@$p> ARPANet, den das US-Verteidigungsministerium 1983 von den akademischen Teilen abtrennte.<@7 Glossar fett>MIME<@$p> – <@7 Glossar kursiv>Multimedia Internet Mail Extensions<@$p>; eine Erweiterung des E-Mail-Standards, der das Anhängen von Binärinformationen (Bilder, Klänge, Programme) erlaubt. Spezifiziert in <\n><@7 Glossar Pfeil Umsch/alt#>’<@$p> RFC 1437.<@7 Glossar fett>MIT<@$p> – <@7 Glossar kursiv>Massachusetts Institute of Technology,<@$p> führende US-amerikanische Universität und Hochburg der Technologieentwicklung<@7 Glossar fett>MP3<@$p> – kurz für <@7 Glossar Pfeil Umsch/alt#>’<@$p> MPEG 1 Layer 3; psycho<\h>akustisches Kompressionsverfahren für Musik in nahezu CD-Qualität; federführend vom Fraunhofer-Institut für Integrierte Schaltungen (IIS) entwickelt und im Rahmen der <@7 Glossar Pfeil Umsch/alt#>’<@$p> ISO standardisiert. <@7 Glossar fett>MPAA<@$p> – <@7 Glossar kursiv>Motion Picture Association of <\n>America<@$p>; Industrievereinigung der größten Hollywood-Studios<@7 Glossar fett>MPEG<@$p> – <@7 Glossar kursiv>Moving Pictures Experts Group<@$p>; <\n>eine 1988 gegründete Arbeitsgruppe von <@7 Glossar Pfeil Umsch/alt#>’<@$p><\!q>ISO und <@7 Glossar Pfeil Umsch/alt#>’<@$p> IEC, die sich mit der Entwicklung und Standardisierung von Codierungsverfahren für digitales Audio und Video befasst. Die berühmteste aus der MPEG-Familie von Codierungen ist <@7 Glossar Pfeil Umsch/alt#>’<@$p> MP3<@7 Glossar fett>MUD<@$p> – <@7 Glossar kursiv>Multi-User Dungeon<@$p>; textbasierte Spieleumgebung für synchrone Kommuni<\h>kation und Interaktion der Spieler. Seit 1979 in diversen Technologiegenerationen und <\n>thematischen Ausrichtungen entwickelt. <@7 Glossar fett>NCP<@$p> – <@7 Glossar kursiv>Network Controll Protocol<@$p>, das erste Verbindungsprotokoll des <@7 Glossar Pfeil Umsch/alt#>’<@$p> ARPANET, Vorläufer des heutigen <@7 Glossar Pfeil Umsch/alt#>’<@$p> TCP/IP.<@7 Glossar fett>NCSA<@$p> – <@7 Glossar kursiv>National Center for Supercomputing Applications<@$p>; US-amerikanische Einrichtung für Hochleistungscomputer, hier entstand 1992 der erste Web-Browser »Mosaic« und ein früher WWW-Server, der NCSA HTTP-d.<@7 Glossar fett>NDA<@$p> – <@7 Glossar kursiv>NonDisclosure Agreement<@$p>, Vertraulichkeitsvereinbarung; macht ein Unternehmen Angestellten oder Externen (Kunden, <\n>Partnern) sensitive Information zugänglich, <\n>verpflichtet es sie mit einem NDA zu Nichtweitergabe.<@7 Glossar fett>NIC<@$p> – (1) <@7 Glossar kursiv>Network Information Center,<@$p> nationale Organisation zur Koordinierung der Domain-Vergabe und Standort der <@7 Glossar Pfeil Umsch/alt#>’<@$p> DNS-Rootserver; in Deutschland das DeNIC (www.denic.de); (2) <@7 Glossar kursiv>Network Interface Card,<@$p> Netzwerkkarte.<@7 Glossar fett>NSA<@$p> – <@7 Glossar kursiv>National Security Agency<@$p>; Auslands<\h>geheimdienst der USA, bis vor wenigen <\n>Jahren so geheim, dass schon allein seine Existenz offiziell »weder bestätigt noch <\n>geleugnet« wurde.<@7 Glossar fett>NSF<@$p> – <@7 Glossar kursiv>National Science Foundation<@$p>; die zivile Forschungsförderungsbehörde der USA (vgl. <@4 Pfeil (Umschalt/Alt #)>’<@$p> DARPA).<@7 Glossar fett>NSFnet<@$p> – Hochgeschwindigkeitsnetzwerk der <@7 Glossar Pfeil Umsch/alt#>’<@$p> NSF für die US-amerikanische Wissenschaftswelt, besonders zur entfernten <\n>Nutzung von Supercomputern. 1986 in Betrieb genommen, wurde es 1995 eingestellt, da zu dem Zeitpunkt die kommerzielle Backbone-<\n>Infrastruktur seine Aufgabe übernehmen konnte.<@7 Glossar fett>OEM<@$p> – <@7 Glossar kursiv>Original Equipment Manufacturer<@$p>; <\n>Firma, die Hard- oder Software anderer Firmen zusammenstellt und unter eigenem <\n>Label verkauft. Im Gegensatz zum <@7 Glossar kursiv>Value <\n>Added Reseller<@$p> (VAR) braucht der OEM nichts als seinen Namen hinzuzufügen. OEM-Versionen von Software wird von einem Hardwarehersteller zusammen mit seinem Computer, Drucker oder Scanner ausgeliefert. <@7 Glossar fett>OSD<@$p> – <@7 Glossar kursiv>Open Source Definition,<@$p> Versuch der <@7 Glossar Pfeil Umsch/alt#>’<@$p><\!q>OSI (2), freie Software unter Vermeidung des Begriffs »frei« neu zu definieren; abgeleitet von den <@7 Glossar kursiv>Debian Free Software Guidelines.<@$p> OSD-konforme Lizenzen werden von der OSI (2) zertifiziert.<@7 Glossar fett>OSI<@$p> – <@7 Glossar normal>(1)<@$p> <@7 Glossar kursiv>Open Systems Interconnection<@$p>; ein <\n>ab 1982 von der <@7 Glossar Pfeil Umsch/alt#>’<@$p> ISO entwickelter ver<\h>bindungsorientierter Internetzwerkstandard, der an die Stelle von <@7 Glossar Pfeil Umsch/alt#>’<@$p> TCP/IP treten sollte, heute jedoch weitgehend in Vergessenheit geraten ist. (2) <@7 Glossar kursiv>Open Source Initiative,<@$p> im Februar 1998 von Eric Raymond, Bruce Perens u.a. mit dem Ziel gegründete Vereinigung, freie Software unter Vermeidung des Begriffs »frei« neu zu definieren und <@7 Glossar Pfeil Umsch/alt#>’<@$p> OSD-konforme Lizenzen zu zertifizieren.<@7 Glossar fett>PDF<@$p> – <@7 Glossar kursiv>Portable Document Format,<@$p> von Adobe entwickeltes, zwischen verschiedenen Plattformen austauschbares und heute weit verbreitetes Format für gestaltete Textdokumente.<@7 Glossar fett>PGP<@$p> – <@7 Glossar kursiv>Pretty Good Privacy<@$p>; ein asymmetrisches ( <@7 Glossar Pfeil Umsch/alt#>’<@$p> <@7 Glossar kursiv>Public/Private Key<@$p>) Verschlüsselungsverfahren für E-Mails und andere Dateien, 1991 von Philip Zimmerman freigegeben.<@7 Glossar fett>PHP<@$p> – <@7 Glossar kursiv>Personal Home Page construction kit<@$p>; eine server-seitige, plattformübergreifende <\n>in HTML eingebettete Skriptsprache für die Generierung von dynamischen Webseiten; ein Projekt der <@7 Glossar kursiv>Apache Software Foundation.<@$p><@7 Glossar fett>RAM<@$p> – <@7 Glossar kursiv>Random Access Memory,<@$p> Hauptspeicher eines Computers mit sehr schnellem und wahlfreiem Zugriff<@7 Glossar fett>RBÜ<@$p> – Revidierte Berner Übereinkunft, das 1971 grundlegend revidierte internationale Berner Abkommen zum Schutz von Werken der Literatur und Kunst von 1886<@7 Glossar fett>RCS<@$p> – <@7 Glossar kursiv>Rights Control System,<@$p> technisches Rechtekontrollsystem; Mechanismen in Übertragungs-, Abspiel- und Aufzeichnungsgeräten und auf Datenträgern (z.B. DAT, <@7 Glossar Pfeil Umsch/alt#>’<@$p> DVD), die kryptografisch die Einhaltung der vom Verwerter lizenzierten Nutzungsrechte an einem urheberrechtlich geschützten Werk erzwingen; oft auch als <@7 Glossar kursiv>Digital Rights Management<@$p> (DRM) bezeichnet (s. das Kapitel  »Code statt Recht : Rechtekontrollsysteme«).<@7 Glossar fett>RFC<@$p> – <@7 Glossar kursiv>Request for Comments<@$p>; die Dokumente, in denen die Internet-Gemeinde ihre technischen Standards dokumentiert; technisch ist »internet« gleichbedeutend mit »RFC-Konform«. Alle RFC sind unter http://www.rfc-editor.org/<@7 Glossar fett>RIAA<@$p> – <@7 Glossar kursiv>Recording Industry Association of America<@$p>; Industrievereinigung der US-amerikanischen Schallplattenfirmen<@7 Glossar fett>RIPE<@$p> – <@7 Glossar kursiv>Reseaux IP Européen<@$p>; 1989 auf Inititiative von Rob Blokzijl gegründeter Zusammenschluss der europäischen Netzbetreiber<@7 Glossar fett>RPC<@$p> – <@7 Glossar kursiv>Regional Playback Control<@$p>; Bestandteil von <@7 Glossar Pfeil Umsch/alt#>’<@$p> CSS, soll dafür sorgen, dass DVDs in einer von acht Weltregionen ausschließlich auf Geräten dieser Region abgespielt werden können. <@7 Glossar fett>RSA<@$p> – Verschlüsselungssystem nach dem <@7 Glossar Pfeil Umsch/alt#>’<@$p> <@7 Glossar kursiv>Public-Private-Key-<@$p>Verfahren, entwickelt von Ron Rivest, Adi Shamir und Len Adelman, die ihr System mit den Anfangsbuchstaben ihrer Namen als RSA bezeichneten. <\n>s. http://www.rsa.com/<@7 Glossar fett>SACEM<@$p> – <@7 Glossar kursiv>Société des Auteurs Compositeurs Éditeurs de Musique pour la gestion collective du droit d'auteur et de la propriété intellectuelle,<@$p> französische Musikverwertungsgesellschaft<@7 Glossar fett>SCMS<@$p> – <@7 Glossar kursiv>Serial Copy Management System,<@$p> Kopierschutzmechanismus für DAT-Rekorder, der von einem Vertriebsstück nur eine Generation von Kopien zulässt; vom US-amerikanischen <@7 Glossar Pfeil Umsch/alt#>’<@$p> AHRA für digitale Audioaufnahmegeräte gesetzlich vorgeschrieben. Das gleiche Verfahren wird unter dem Namen <@7 Glossar Pfeil Umsch/alt#>’<@$p> CGMS für <@7 Glossar Pfeil Umsch/alt#>’<@$p> DVDs eingesetzt.<@7 Glossar fett>SDMI<@$p> – <@7 Glossar kursiv>Secure Digital Music Initiative<@$p>; 1998 aus einem Zusammenschluss der <@7 Glossar Pfeil Umsch/alt#>’<@$p> RIAA mit ihrem japanischen Pendant RIAJ und der <\n><@7 Glossar Pfeil Umsch/alt#>’<@$p> IFPI entstanden, umfasst der Industrie<\h>verband heute auch Computer- und HiFi-Hard- und Softwarehersteller, die Rechteindustrie und <@7 Glossar Pfeil Umsch/alt#>’<@$p> ISPs. Ziel der SDMI ist es, technische Rechtekontrollsysteme zu entwickeln und flächendeckend durchzusetzen.<@7 Glossar fett>SDRM<@$p> – <@7 Glossar kursiv>Société pour l'administration du Droit de Reproduction Mécanique des auteurs, compositeurs et éditeurs,<@$p> französische Musikverwertungsgesellschaft für die mechanischen Reproduktionsrechte<@7 Glossar fett>SPI<@$p> – <@7 Glossar kursiv>Software in the Public Interest,<@$p> 1997 gegründeter, gemeinnütziger Dachverband verschiedener freier Softwareprojekte wie Debian GNU/Linux und Gnome<@7 Glossar fett>SRI<@$p> – <@7 Glossar kursiv>Stanford Research Institute,<@$p> nonprofit Forschungsunternehmen, erarbeitete 1968 im Auftrag der <@7 Glossar Pfeil Umsch/alt#>’<@$p> ARPA die Spezifikationen des Netzwerks, aus dem das Internet hervorging. <@7 Glossar fett>TCP/IP<@$p> – <@7 Glossar kursiv>Transmission Control Protocol / <\n>Internet Protocol<@$p>, technische Grundlage des Internet. Das ab 1973 entwickelte TCP dient der zuverlässigen Übertragung von Datenpaketen über unzuverlässige Kanäle. Für die zeitkritische Sprachübertragung kam 1978 IP hinzu, das das <@7 Glossar Pfeil Umsch/alt#>’<@$p> UDP spezifiziert. 1982 wurde das gesamte <@7 Glossar Pfeil Umsch/alt#>’<@$p> ARPANET von <@7 Glossar Pfeil Umsch/alt#>’<@$p> NCP auf TCP/IP umgestellt. Heute wird der Wechsel von der seit 1982 verwendeten Version 4 des <\n>Protokolls auf TCP/IP v 6 vorbereitet. <@7 Glossar fett>UCC<@$p> – <@7 Glossar kursiv>Uniform Commercial Code,<@$p> US-amerikanische Entsprechung zu den Allgemeinen Geschäftsbedingungen (AGB)<@7 Glossar fett>UCLA<@$p> – <@7 Glossar kursiv>University of California at Los Angeles<@$p><@7 Glossar fett>UDP<@$p> – <@7 Glossar kursiv>User Datagram Protocol<@$p>; Protokoll für zeitkritische Datenübertragung im Internet können bei einer Dateiübertragung verloren gegangene Pakete erneut geschickt werden (<@7 Glossar Pfeil Umsch/alt#>’<@$p> TCP), gewährleistet UDP (spezifiziert in <@7 Glossar Pfeil Umsch/alt#>’<@$p> IP) für synchrone Sprachkommunikation eine verzögerungsfreie Übertragung, bei der der Verlust einiger Pakete inkauf genommen wird.<@7 Glossar fett>UMTS<@$p> – <@7 Glossar kursiv>Universal Mobile Telecommunications System,<@$p> neuer Übertragungsstandard im Mobilfunk für sehr hohe Datenübertragungsraten. Wegen der exorbitanten Einnahmen aus der Versteigerung des erforderlichen öffentlichen Frequenzspektrums auch als <@7 Glossar kursiv>Unerwartete Mehreinnahmen zur Tilgung von Staatsschulden<@$p> bezeichnet.<@7 Glossar fett>UNIX<@$p> – ursprünglich UNICS, <@7 Glossar kursiv>Uniplexed Information and Computing Service<@$p>; ab 1969 von Ken Thompson auf den Ruinen von MULTICS errichtetes modulares und portierbares <\n>Betriebssystem; heute als Oberbegriff für alle Unix-artigen Betriebssysteme wie GNU/Linux oder BSD verwendet (s. Kapitel »Unix«).<@7 Glossar fett>URL<@$p> – <@7 Glossar kursiv>Uniform Resource Locator,<@$p> universelles Adressierungsformat für Dienste im Internet (z.B. http://www.bpb.de)<@7 Glossar fett>UUCP<@$p> – <@7 Glossar kursiv>Unix to Unix Copy<@$p>, 1976 an den AT&T Bell Labs entwickeltes Protokoll, um über Telefonwahlleitungen Daten zwischen Unix-Rechnern auszutauschen; ermöglicht Einrichtungen, die sich keine Standleitung leisten können, die Nutzung von asynchronen Internet-Diensten wie E-Mail oder NetNews. <@7 Glossar fett>WAIS<@$p> – <@7 Glossar kursiv>Wide Area Information Servers<@$p>; <\n>1991 von Brewster Kahle entwickelter <\n>Vorläufer des <@7 Glossar Pfeil Umsch/alt#>’<@$p> WWW.<@7 Glossar fett>WCT<@$p> – <@7 Glossar kursiv>WIPO Copyright Treaty,<@$p> 1996 von der <@7 Glossar Pfeil Umsch/alt#>’<@$p> WIPO verabschiedete Urheberrechtsrichtlinie; bildet den Rahmen für die Anpassung der nationalen Urheberrechtsgesetze an die Anforderungen digitaler Netzmedien.<@7 Glossar fett>WIPO<@$p> – <@7 Glossar kursiv>World Intellectual Property Organization<@$p>; 1970 aus Institutionen zur Durchführung von Verwaltungsaufgaben im Zusammenhang mit internationalen Urheberrechtsabkommen gegründet, 1974 in eine Behörde der Vereinten Nationen umgewandelt, verhandelt im Auftrag der Mitgliedsstaaten der UNO Fragen des geistigen Eigentums.<@7 Glossar fett>WWW<@$p> – <@7 Glossar kursiv>World-Wide Web<@$p>; von Tim Berners-Lee ab 1989 entwickeltes Hypertext-Protokoll, besteht aus <@7 Glossar Pfeil Umsch/alt#>’<@$p> HTTP und <@7 Glossar Pfeil Umsch/alt#>’<@$p> HTML.@2  ZÜ 1:Internet-Ressourcen@2  ZÜ 2:Wissensfreiheit und <\h>Bürgerrechte <\h><\n>im Internet@99 Glossar fliess:@2  ZÜ 3:<f"Univers-CondensedBold">Virtuelles Datenschutzbüro @99 Glossar fliess fett:<x@7 Glossar normal><*p(0,0,0,11,0,0,g,"Deutsch")><z9.5>ein gemeinsamer Service von Datenschutz<\h>institutionen aus aller Welt <\h>www.datenschutz.de<@$p><*p(5.669,-5.669,0,11,0,0,g,"Deutsch")>Global Internet Liberty Campaign (GILC) <\n><t2f"Univers-CondensedLight">eine Koalition von mehr als 60 Netzaktivismusgruppen aus aller Welt<\n>www.gilc.org<t$f$>Center for the Public Domain<\n><t2f"Univers-CondensedLight">Stiftung zur Förderung einer robusten Public Domain; die Liste der »Supported Organizations« ist ein guter Einstieg in die Organisationen, die sich für die Wissens-Allmende engagieren <\n>www.centerforthepublicdomain.org<t$f$>Internet Societal Task Force (ISTF)<\n><t2f"Univers-CondensedLight">Bereich der ISOC, der sich mit sozialen und wirtschaftlichen Fragen des Internet beschäftigt <\n>www.istf.org<t$f$>Chaos Computer Club (CCC)<\n><t2f"Univers-CondensedLight">Hackervereinigung, die sich grenzüberschreitend für Informationsfreiheit einsetzt<\n>www.ccc.de<t$f$>Electronic Frontier Foundation (EFF)<\n><t2f"Univers-CondensedLight">engagiert sich für die Wahrung der Bürgerrechte im Cyberspace, besonders mit Rechtsbeistand, besonders in den USA <\n>www.eff.org<t$f$>@2  ZÜ 2:Internet@99 Glossar fliess:<@7 Glossar fett>Sicherheit im Internet<@$p><\n><t1>eine Initiative des Bundesministeriums für Wirtschaft und Technologie, des Bundes<\h>minis<\h>teriums des Innern und des Bundesamtes für Sicherheit in der Informationstechnologie<t$> <\n>www.sicherheit-im-Internet.de@99 Glossar fliess fett:Internet Society (ISOC)<\n><t2f"Univers-CondensedLight">Mitgliedsorganisation für die Pflege und Weiterentwicklung der Internetinfrastruktur<\n>www.isoc.org<t$f$>Internet Engineering Task Force (IETF)<\n><t2f"Univers-CondensedLight">ISOC-Gremium für die technischen Fragen der Internetinfrastuktur<\n>www.ietf.org<t$f$>Request for Comments (RFCs)<\n><t2f"Univers-CondensedLight">die IETF-Dokumente, die die technischen Standards des Internet spezifizieren<\n>www.rfc-editor.org<t$f$>World Wide Web Consortium (W3C)<\n><t2f"Univers-CondensedLight">Gremium für die Pflege und Weiterentwicklung des Web<\n>www.w3c.org<t$f$>Internet Corporation for Assigned <\h>Names and Numbers (ICANN)<\n><t2f"Univers-CondensedLight">globale Organisation, die für Domain-Namen, IP-Adressen und Portnummern im Internet zuständig ist<\n>www.icann.org@99 Glossar fliess:@2  ZÜ 2:Freie Software allgemein@99 Glossar fliess:<@7 Glossar fett>»Alternative Betriebssysteme.<@$p> Open-Source-Software. Ein Leitfaden für kleine und mittlere Unternehmen« <\n>Informationsbroschüre des Bundesministeriums für Wirtschaft und Technologie <\n>www.bmwi.de/Homepage/download/infogesellschaft/Open-Source-Software.pdf@99 Glossar fliess fett:EU Working Group on Libre Software<\n><t2f"Univers-CondensedLight">Arbeitsgruppe zu freier Software des EU-Generaldirektorats Informationsgesellschaft<\n>eu.conecta.it<t$f$>SourceForge<\n><t2f"Univers-CondensedLight">wichtigste Entwicklerressource, hostet rund 30 000 freie Softwareprojekte<\n>sourceforge.net<t$f$>FreshMeat<\n><t2f"Univers-CondensedLight">laufende Rezensionen zu freien Softwareprojekten<\n>freshmeat.net<t$f$>Slashdot - News for Nerds<\n><t2f"Univers-CondensedLight">Nachrichten, Klatsch und Tratsch aus der Hackerwelt<\n>slashdot.org@99 Glossar fliess:<@7 Glossar fett>BerliOS<@$p> – Der Open-Source-Mediator<\n>Informationen und Ressourcen für Entwickler und Anwender freier Software, insbesondere mittelständische Unternehmen und öffentliche Verwaltungen, betrieben von GMD Fokus<\n>www.berlios.de@99 Glossar fliess fett:LinuxTag<\n><t2f"Univers-CondensedLight">größte europäische Messe und Konferenz für freie Software<\n>www.linuxtag.org@99 Glossar fliess:<@7 Glossar fett>The Jargon File<@$p>, auch bekannt als <@7 Glossar kursiv>»The Hacker's Dictionary«,<@$p> Wörterbuch der Hackerkultur bis zurück in die 70er-Jahre<\n>www.tuxedo.org/jargon@2  ZÜ 2:Unix@99 Glossar fliess fett:<@7 Glossar fett>Das GNU-Projekt <@$p><t2f"Univers-CondensedLight"><\n>www.gnu.org<t$f$>Die Free Software Foundation<\n><t2f"Univers-CondensedLight">www.fsf.org<t$f$>Die Free Software Foundation Europe<\n><t2f"Univers-CondensedLight">www.fsfeurope.org<t$f$>Brave GNU World<\n><t2f"Univers-CondensedLight">monatliche Kolumne aus der GNU-Welt<\n>www.brave-gnu-world.org<t$f$>Linux Online<\n><t2f"Univers-CondensedLight">gute Einstiegsseite mit übersichtlich zusammengestellten Distributionen, Applikationen und Dokumentation<\n>www.linux.org<t$f$>Dave Central<\n><t2f"Univers-CondensedLight">großes Portal für GNU/Linux-Applikationen von Dave Franklin<\n>linux.davecentral.com<t$f$>Linux Documentation Project<\n><t2f"Univers-CondensedLight">Dokumentation vom GNU/Linux-Start bis zu fortgeschrittenen Fragen<\n>www.linuxdoc.org<t$f$>Linux-Anwenderhandbuch<\n><t2f"Univers-CondensedLight">wichtigstes deutschsprachiges Handbuch von Hetze/Hohndel/Müller/Kirch in der Fassung von 1997, vollständig online<\n>www1.lunetix.de/LHB<t$f$>Simple End User Linux (SEUL)<\n><t2f"Univers-CondensedLight">ein weiterer guter Startpunkt und eine Projektplattform, um eine breitere Basis kundiger GNU/Linux-Nutzer besonders in Bildung und Wissenschaft zu fördern<\n>www.seul.org<t$f$>Debian GNU/Linux-Distribution <\n><t2f"Univers-CondensedLight">www.debian.org<t$f$>Linux.de<\n><t2f"Univers-CondensedLight">deutschsprachige Nachrichten-Site<\n>www.linux.de<t$f$>Linux-Magazin<\n><t2f"Univers-CondensedLight">Online-Version der gleichnamigen wichtigsten deutschsprachigen Zeitschrift<\n>www.linux-magazin.de<t$f$>Pro-Linux<\n><t2f"Univers-CondensedLight">Nachrichten-Site zu GNU/Linux und anderer freier Software<\n>www.pro-linux.de<t$f$>Linux Weekly News<\n><t2f"Univers-CondensedLight">eine der besten Nachrichten-Sites zu GNU/Linux und anderer freier Software<\n>lwn.net<t$f$>Drei BSD-Varianten:<@7 Glossar fett>FreeBSD <@$p><t2f"Univers-CondensedLight">das BSD mit der größten Verbreitung, für Intel und Alpha<\n>www.freebsd.org<t$f$>NetBSD<\n><t2f"Univers-CondensedLight">das BSD, das auf den meisten Plattformen läuft<\n>www.netbsd.org<t$f$>Open BSD<\n><t2f"Univers-CondensedLight">auf Sicherheit hin optimiert<\n>www.openbsd.org@99 Glossar fliess:@2  ZÜ 2:Andere Freie Softwareprojekte@99 Glossar fliess:@99 Glossar fliess fett:GNU Privacy Guard<\n><t2f"Univers-CondensedLight">freie PGP-kompatible Verschlüsselungssoftware für E-Mails und andere Dateien<\n>www.gnupg.org<t$f$>Pretty Good Privacy (PGP)<\n><t2f"Univers-CondensedLight">quelloffene Verschlüsselungssoftware <\n>www.pgpi.org<t$f$>OpenSSH<\n><t2f"Univers-CondensedLight">freie Version des SSH (Secure SHell) Protokoll<\h>satzes; sicherer Ersatz für ftp, telnet und rlogin, bei dem auch die Passwörter verschlüsselt übertragen werden; vor allem aus dem OpenBSD-Projekt heraus entwickelt.<\n>www.openssh.com<t$f$>XFree86<\n><t2f"Univers-CondensedLight">freies X-Window-System<\n>www.xfree86.org<t$f$>KDE<\n><t2f"Univers-CondensedLight">grafischer Desktop für GNU/Linux<\n>www.kde.org<t$f$>Gnome<\n><t2f"Univers-CondensedLight">grafischer Desktop für GNU/Linux<\n>www.gnome.org<t$f$>Apache Webserver<\n><t2f"Univers-CondensedLight">www.apache.org<t$f$>Samba<\n><t2f"Univers-CondensedLight">Datei und-Druck-Server, der auch MS-Win<\h>dows-Rechner unterstützt<\n>www.samba.org<t$f$>Perl<\n><t2f"Univers-CondensedLight">Skriptsprache<\n>www.perl.org<t$f$>Perl-Archiv<\n><t2f"Univers-CondensedLight">cpan.org<t$f$>Ghostscipt<\n><t2f"Univers-CondensedLight">PostScript-Viewer<\n>www.ghostscript.com<t$f$>Mozilla<\n><t2f"Univers-CondensedLight">die freie Version von Netscape<\n>www.mozilla.org<t$f$>Mailman<\n><t2f"Univers-CondensedLight">GNU-Mailinglisten-Manager<\n>www.list.org<t$f$>Wine<\n><t2f"Univers-CondensedLight">MS-Windows-Emulator für GNU/Linux<\n>www.winehq.com<t$f$>Zope<\n><t2f"Univers-CondensedLight">Content Management System und Web-Applikationsumgebung<\n>www.zope.org<t$f$>PHP<\n><t2f"Univers-CondensedLight">Skriptsprache für die Generierung von dynamischen Webseiten<\n>www.php.net<t$f$>GIMP<\n><t2f"Univers-CondensedLight">Bildbearbeitungsprogramm<\n>www.gimp.org<t$f$>Concurrent Versions System (CVS)<\n><t2f"Univers-CondensedLight">die wichtigste Arbeitsumgebung für freie Softwareprojekte<\n>http://www.cvshome.org/@99 Glossar fliess:@2  ZÜ 2:Freie Software für Schulen@99 Glossar fliess:@99 Glossar fliess fett:Freie Software und Bildung (FSuB)<\n><t2f"Univers-CondensedLight">fsub.schule.de<t$f$>kmLinux<\n><t2f"Univers-CondensedLight">einfach zu installierendes Linux-Komplettsys<\h>tem für den Arbeitsplatzrechner, vom Landesbildungsserver Schleswig-Holstein in Zusammenarbeit mit dem Verein Freie Software und Bildung entwickelt<\n>www.lernnetz-sh.de/kmlinux<t$f$>Simple End User Linux/edu<\n><t2f"Univers-CondensedLight">Diskussionsgruppe für alle, die an Linux in der Bildung interessiert sind, Lehrer, Eltern und Schüler; sammelt Hunderte von bildungsrelevanter freier Software, hostet eigene Projekte, sammelt Fallstudien einzelner Schulen<\n>www.seul.org/edu/<t$f$>Organization for Free Software in Education and Teaching (OFSET)<\n><t2f"Univers-CondensedLight">sammelt schulrelevante freie Software (Freeduc) und unterstützt Projekte<\n>www.ofset.org<t$f$>Schulen ans Netz e.V.<\n><t2f"Univers-CondensedLight">Initiative des Bundesministeriums für Bildung und Forschung und der Deutschen Telekom<\!q>AG<\n>www.san-ev.de<t$f$>Schulen ans Netz<\n><t2f"Univers-CondensedLight">die c't und das Offenen Deutschen Schulnetz (ODS) haben einen Kommunikationsserver für Schulen entwickelt<\n>www.heise.de/ct/schan<t$f$>GEE-Internetserver für Schulen<\n><t2f"Univers-CondensedLight">www.gesamtschule-eiserfeld.de/gee/index.html<t$f$>Linux für Schulen<\n><t2f"Univers-CondensedLight">Aktion von c't und Linux.de<\n>www.linux.de/schulen<t$f$>PingoS. Linux-User helfen Schulen<\n><t2f"Univers-CondensedLight">hilft Schulen bei der Einrichtung und Wartung von GNU/Linux-Rechnern<\n>www.pingos.schulnetz.org<t$f$>OpenWebSchool<\n><t2f"Univers-CondensedLight">freie Unterrichtsmaterialien, von Schülern (der Oberstufen) für Schüler (der Unterstufen oder Grundschulen) entwickelt<\n>www.openwebschool.de<t$f$>KDE Edutainment Project<\n><t2f"Univers-CondensedLight">im September 2001 gestartetes Projekt, um Bildungssoftware zu entwickeln<\n>edu.kde.org<t$f$>Freie Online Systeme (FOS)<\n><t2f"Univers-CondensedLight">Demonstrations- und Evaluations-Center für webbasierte freie Anwendungen, betrieben von Thomax Kaulmann und Herbert A. Meyer, im Auftrag der Bundeszentrale für politische Bildung. Hier soll vor allem Software für Schulen zusammen- und zum Ausprobieren bereitgestellt, getestet und nach Nutzbarkeitskriterien bewertet werden.<\n>fos.bpb.de<t$f$>Linux for Kids<\n><t2f"Univers-CondensedLight">www.linuxforkids.org@99 Glossar fliess:@2  ZÜ 2:Gesetze@99 Glossar fliess:	<@7 Glossar fett>Gesetz über Urheberrecht<@$p> und verwandte Schutzrechte in der Fassung vom 1. November 1998 <\n>www.uni-muenster.de/Jura.itm/hoeren/materialien/urheberrechtsgesetz.html	<@7 Glossar fett>Richtlinie 2001/29/EG<@$p> des Europäischen Parlaments und des Rates zur Harmonisierung bestimmter Aspekte des Urheberrechts und der verwandten Schutzrechte in der Informationsgesellschaft (Juni 2001) muss bis Dezember 2002 in deutsches Recht übersetzt <\h>werden<\n>europa.eu.int/comm/internal_market/en/intprop/news/com29de.pdf	<@7 Glossar fett>Copyright Law of the United States of America<@$p> in der Fassung vom April 2000, darin enthalten, der Digital Millennium Copyright Act lcweb.loc.gov/copyright/title17	<@7 Glossar fett>Britischer Copyright, Designs and <\h>Patents Act<@$p> von 1988 (CDPA), in der Fassung vom 1.2.2000 <\n>www.jenkins-ip.com/patlaw/index1.htm	<@7 Glossar fett>WIPO Copyright Treaty <@$p>(WCT) von 1996 <\n>www.wipo.int/treaties/ip/copyright/	<@7 Glossar fett>WIPO Performances and Phonograms Treaty<@$p> (WPPT) von 1996<\n>www.wipo.int/treaties/ip/performances/	<@7 Glossar fett>Abkommen über Trade-Related Aspects of Intellectual Property Rights<@$p> (TRIPS) der Welthandelsorganisation von 1994 www.wto.org/english/tratop_e/ trips_e/t_agm1_e.htm 	<@7 Glossar fett>Revidierte Berner Übereinkunft<@$p> (RBÜ)<\n>geht auf das Berner Abkommen zum Schutz von Werken der Literatur und Kunst von 1886 zurück, 1971 grundlegend revidiert, in der Fassung von 1979<\n>www.wipo.int/treaties/ip/berne/	<@7 Glossar fett>Eurolinux Petition for a Software <\h>Patent Free Europe<@$p> umfangreiche <\h>Ressourcensammlung zur Frage der <\h>Softwarepatentierung petition.eurolinux.org<\c>@2  ZÜ 1:Literatur@99 Glossar fliess:4C, Copy Protection Framework for DVD Audio, o.J., http://www.dvdcca.org/4centity/data/<\n>tech/dvd_audio_cp.pdf<Ht10>Albini, Steve<$t$>, The Problem With Music, o.J., http://www.negativland.com/albini.html.<@8 Kapitäl>Alvestrand, Harald Tveit<@$p>, »The Internet Standardisation Process«, in: T. Buland/ H. Finne/<\n>S. Helmers/U. Hoffmann/J. Hofmann (Hrsg.), Managements and Network Technology, <\n>Proceedings from COST A3 Workshop in Trondheim, 22.–24. Nov. 1995, WZB Paper 1996, S. 59–65.<@8 Kapitäl>Arns, Inke & Andreas Broeckmann<@$p>, »Kleine östliche Mediennormalität – Ein Blick in die Werkzeugkisten des Medienalltags im ehemaligen ‚Ostblock’”, in: springerin. Hefte für Gegenwartskunst, Bd. IV, Heft 3, Wien, September – November 1998, S. 22–25; http://www.springerin.at/d/springer_2/springer_2_502.htm<@8 Kapitäl>Baase, Sara<@$p>, »IBM: Producer or Predator«, in: Reason, April 1974, S. 4–10, http://www-<\n>rohan.sdsu.edu/faculty/giftfire/ibm.html<@8 Kapitäl>Bachrach, Steven/ R. Stephen Berry/ Martin Blume/Thomas von Foerster/ Alexander <\h>Fowler/ Paul Ginsparg/ Stephen Heller/ Neil Kestner/Andrew Odlyzko/ Ann Okerson/ <\n>Ron Wigington/ Anne Moffat<@$p>: Inetellectual Property: Who Should Own Scientific Papers?, in: Science Volume 281, No. 5382 (4. Sep 1998), S. 1459–1460, http://www.sciencemag.org/cgi/ content/full/281/5382/1459 <@8 Kapitäl>Baker, Steven/ Net Worth, Desktop TCP/IP At Middle Age<@$p>, in: UNIX Review, Februar 1998; http://www.unixreview.com/articles/1998/9802/index.htm<@8 Kapitäl>Barlow, John Perry<@$p>, »The Economy of Ideas. A Framework for Patents and Copyrights in the Digital Age. (Everything you know about intellectual property is wrong.)«, in: Wired 2.03, März 1994, http://www.wired.com/wired/2.03/features/economy.ideas.html<@8 Kapitäl>Barlow, John Perry<@$p>, »The Best of All Possible Worlds«, in: Communications of the ACM, 50th Anniversary Issue, 1996, http://www.nettime.org/nettime.w3archive/199612/msg00011.html<@8 Kapitäl>Becker, Jürgen/ Thomas Dreier<@$p> (Hrsg.), Urheberrecht und digitale Technologie, UFITA-Schriftenreihe, Nomos, Baden-Baden 1994.<@8 Kapitäl>Bell, Bruce<@$p>, The Coming Storm, 3. Juni 2000, http://eon.law.harvard.edu/openlaw/DVD/<\n>articles/comingstorm.html<@8 Kapitäl>Bell, Tom W.<@$p>, Fair Use Vs. Fared Use: The Impact of Automated Rights Management on Copyright’s Fair Use Doctrine, in: North Carolina Law Rev. 76/557, S. 558-618, 1998, http://www.tomwbell.com/writings/FullFared.html<@8 Kapitäl>Berners-Lee, Tim<@$p>, Der Web-Report, Econ, München 1999.<@8 Kapitäl>Bezroukov, Nikolai<@$p>, A Second Look at the Cathedral and the Bazaar, First Monday, 12/1999, http://firstmonday.org/issues/issue4_12/bezroukov/<@8 Kapitäl>Bolz, Norbert/ Friedrich A. Kittler/Christoph Tholen<@$p> (Hrsg.), Computer als Medium, München 1993.<@8 Kapitäl>Borchers, Detlev<@$p>, »Patente, die die Welt erschüttern«, in: ZD-Net Deutschland, August 1999; http://www.zdnet.de/kolumne/199908/mdb16_00-wc.html<@8 Kapitäl>Bortloff, Nils<@$p>, Legal Advisor, IFPI, »Internet Piracy – notice and take-down«, Präsentation auf dem WIPO Workshop über die Haftbarkeit von Sevice-Providern, Genf, 9.–10. Dezember 1999, http://www.wipo.int/eng/meetings/ 1999/osp/doc/osp_lia3.doc<@8 Kapitäl>Boyle, James<@$p>, »A Politics of Intellectual Property: Environmentalism For the Net?«, 47 Duke Law Journal 87 (1997), http://www.law.duke.edu/boylesite/intprop.htm<@8 Kapitäl>Brennan<@$p>, <@8 Kapitäl>Patricia<@$p>, Association of Research <\h>Libraries, Timeline: A History of Copyright in the U.S., o.J., http://arl.cni.org/info/frn/copy/timeline.html<@8 Kapitäl>Buma/Stemra<@$p>, Annual Report 1997.<@8 Kapitäl>Bundesministerium für Wirtschaft und Technologie<@$p>, »Eckpunkte der deutschen Kryptopolitik«, 2. Juni 1999, http://www.sicherheit-im-internet.de/download/krypto-d.pdf<@8 Kapitäl>Bundesministerium für Wirtschaft und Technologie<@$p>, »Alternative Betriebssysteme. Open <\h>Source-Software. Ein Leitfaden für kleine und mittlere Unternehmen«, Berlin 2001. http://www.bmwi.de/Homepage/download/ infogesellschaft/Open-Source-Software.pdf<@8 Kapitäl>Bundesverband der Phonographischen Indus<\h>trie<@$p> / Deutsche IFPI, »Musik als geistiges <\h>Eigentum«, in: Jahrbuch 2001, http://www. <\h>ifpi.de/jb/2001/jb01e.html<@8 Kapitäl>Burckhardt, Jacob<@$p>, Die Kultur der Renaissance in Italien, Kröner, Stuttgart 1976.<@8 Kapitäl>Busch, Christoph/Michael Arnold/Wolfgang Funk<@$p>, »Schutz von Urheberrechten durch digitale Wasserzeichen«, in: Gerhard Banse/ Christian J. Langenbach (Hrsg.), Geistiges Eigentum und Copyright im multimedialen Zeitalter. Positionen, Probleme, Perspektiven. Eine fachübergreifende Bestandsaufnahme, Europäische Akademie, Bad Neuenahr-Ahrweiler 1999.<@8 Kapitäl>Carter, Joseph<@$p>, Why Debian Doesn’t Include KDE, 17. Juni 2000, http://freshmeat.net/<\n>news/2000/06/17/961300740.html<@8 Kapitäl>Cerf, Vinton<@$p>, How the Internet Came to Be, as told to Bernard Aboba, in: Bernard Aboba, The <\h>Online User’s Encyclopedia, Addison-Wesley, November 1993, http://www.isoc.org/guest/zakon/ Internet/History/How_the_Internet_came_to_Be<@8 Kapitäl>Chapman, D. Brent<@$p>, »Majordomo: How I Manage 17 Mailing Lists Without Answering ›-request‹ Mail«, 1992, http://www.greatcircle.com/majordomo/majordomo.lisa6.ps.Z<@8 Kapitäl>Chardin, Pierre Teilhard de<@$p>, Die Zukunft des Menschen, Olten 1966.<@8 Kapitäl>CISAC<@$p>, The Common Information System. A Digital Rights Architecture for the Information Age, <\h>Paris o.J.; http://www.worksnet.org/Web_cisac/cisac_communication.nsf/ e9e109c34d8a0c2ec1256713004a879a/297492bec1062cb04125676a00401c38/$FILE/CIS_A.pdf<@8 Kapitäl>Clemins, Admiral Archie<@$p>, Commander in Chief, U.S. Pacific Fleet, IT-21: The Path to Information Superiority, o.J., http://www.chips.navy.mil/chips/archives/97_jul/file1.htm<@8 Kapitäl>Cohen, Julie E.<@$p>, Some Reflections on Copyright Management Systems and Laws <\h>Designed to Protect Them, in: Berkeley Law Journal, (12/ 161) 1997, http://www.lawberkeley.edu:80/journals/ <\h>btlj/articles/12_1/Cohen/html/text.html<@8 Kapitäl>Cohen, Julie E.<@$p>, A Right to Read Anonymously: A Closer Look at »Copyright Management« in <\h>Cybercpace, Connecticut Law Review, (981, 991) Summer 1996, http://cyber.law.harvard.edu/ <\h>property/alternative/Cohen.html<@8 Kapitäl>Cusumano, Michael/Richard Selby<@$p>, »How Microsoft Builds Software«, in: Communica<\h>tions of the ACM, Juni 1997, S. 53-61.<@8 Kapitäl>Czychowski, Christian<@$p>, Ein Muster im Ringen um die Unabhängigkeit der Urheber – Anhalt-Dessau-Wörlitz und seine Selbstverlagsunternehmen (1781–1785), in: forum historiae iuris, 19. August 1998, http://www.rewi.hu-berlin.de/FHI/98_08/czych_t.htm<@8 Kapitäl>Dell, Michael/Catherine Fredman<@$p>, Direct From Dell: Strategies That Revolutionized an Industry,  Harper Business, 1999.<@8 Kapitäl>Demiralp, Emre<@$p>, Linux in der akademischen Ausbildung, Linux-Focus, Oktober 1998, <\h>http://www.linuxfocus.org/Deutsch/October1998/article9.html<@8 Kapitäl>Deutsch, L. Peter<@$p>, Licensing Alternatives for Freely Redistributable Software, 1996, <\h>ftp://ftp.cs.wisc.edu/ghost/papers<@8 Kapitäl>Deutscher Bundestag<@$p>, Schlussbericht der Enquete-Kommission »Zukunft der Medien in Wirtschaft und Gesellschaft – Deutschlands Weg in die Informationsgesellschaft«, <\n>Drucksache 13/11004, 22. 06. 98, http://www.dpg-hv.de/eq_schluss.htm<@8 Kapitäl>Dietrich, Oliver<@$p>, »In den Tiefen des Kernel. Interview mit Linux-Entwickler Alan Cox«,<\n>in: c’t, 25/1999, S.34.<@8 Kapitäl>Druey, Jean Nicola<@$p>s, Information als Gegenstand des Rechts, Nomos, Baden-Baden 1995.<@8 Kapitäl>DTLA<@$p> (Digital Transmission Licensing Adminis<\h>trator), Policy Statements Regarding DTCP Adopters, January 26, 1999; http://www.dtcp.com/adopt.pdf<@8 Kapitäl>Dwyer, Michael<@$p>, Linux has Window of Opportunity in China, in: Austrian Financial Review,  30. Juni 2000, http://www.afr.com.au/information/20000630/A42633-2000Jun29.html<@8 Kapitäl>Eblida<@$p> (European Bureau of Library, Information and Documentation Associations), »Save Future Access to Information Now! EBLIDA Position Paper on the proposed Directive on the harmonisation of certain aspects of copyright and related rights in the Information Society«, März 1998, http://www.eblida.org/posharmo.htm<@8 Kapitäl>Elkin-Koren, Niva<@$p>, Law School, Haifa Univer<\h>sity, The Future of Public/Private Boundaries for Copyright in Cyberspace, Journal of Computer Mediated Communication, Vol. 2, No.2 Special Issue: »Emerging Law on the Electronic Frontier« (September 1996), http://www. ascusc.org/jcmc/vol2/ issue2/elkin.html<@8 Kapitäl>Elkin-Koren, Niva<@$p>, Public/Private and Copyright Reform in Cyberspace, 12:1 Berkeley Technology Law Journal, Spring 1997, http://eon.law.harvard.edu/property99/alternative/Elkin-Koren.html<@8 Kapitäl>Ellins, Julia<@$p>, Copyright Law, Urherberrecht und ihre Harmonisierung in der Europäischen Gemeinschaft. Von den Anfängen bis zum Informationszeitalter, Duncker&Humblot, Berlin 1997.<@8 Kapitäl>Encyclopaedia Britannica<@$p>, CD 2000 Deluxe Edition, London 2000.<@8 Kapitäl>Evers, Steffen<@$p>, An Introduction to Open Source Software Development, Diplomarbeit an der TU-Berlin, Juli 2000.Fachgespräch Open Source-Software des Bundeswirtschaftsministeriums, am 15.7.1999, im Haus der Kulturen der Welt in Berlin<@8 Kapitäl>Flusser, Vilém<@$p>, Ins Universum der technischen Bilder, Göttingen 1985.<@8 Kapitäl>Fonda, Daren<@$p>, Copyright Crusader, in: The Boston Globe, 29. August 1999, http://www.boston.com/globe/magazine/8-29/featurestory1.shtml<@8 Kapitäl>Fritsch, Lothar<@$p>, Die Geburt des Mikroprozessors, Saarbrücken, April 1992, http://fsinfo.<\n>cs.uni-sb.de/~fritsch/Papers/PC/node9.html#SECTION00033000000000000000<@8 Kapitäl>Gabriel, Richard P./ William N<@$p>. <@8 Kapitäl>Joy<@$p>, Sun Community Source License Principles, 20. Januar 1999, http://www.sun.com/981208/scsl/principles.html<@8 Kapitäl>Gartner Group Inc<@$p>., The E-Market Maker Revolution, 1999, http://www.gartner.com/webletter/softlock/issue1/<@8 Kapitäl>Gehring, Robert<@$p>, Freeware, Shareware und Public Domain. Geschichte, Begrifflichkeit, Urheberrecht und Haftung, Studienarbeit an der TU-Berlin, Juli 1996, http://ig.cs.tu-berlin.de/sa/043/ <\h>index.html<@8 Kapitäl>Gehring,Robert<@$p>, »Schneller, höher, weiter. Veranstaltungsbericht ›Effizienter Staat 2000‹, in: Linux-Magazin 6/2000, S. 56–57.<@8 Kapitäl>GEMA<@$p>, Digitaltechnik und Urheberrecht im Bereich der Musik, Mai 1997, http://www.gema.de/ service/digi.shtml<@8 Kapitäl>Gieseke, Michael<@$p>, Der Buchdruck in der frühen Neuzeit, Suhrkamp, Frankfurt/M. 1991.<@8 Kapitäl>Gilder, George<@$p>, Metcalfe’s Law and Legacy, in: Forbes ASAP, 13. September 1993, http://www. <\h>forbes.com/asap/gilder/telecosm4a.htm<@8 Kapitäl>Gomulkiewicz, Robert<@$p>, »How Copyleft Uses <\h>License Rights to Succeed in the Open Source Software Revolution and the Implications for Article 2B«,  in: Houston Law Review, Spring 1999, S.<\!q>180 ff., http://eon.law.harvard.edu/h2o/property/alternatives/gomulkiewicz.html<@8 Kapitäl>Götting, Horst-Peter<@$p> (Hrsg.), Multimedia, Internet und Urheberrecht, University Press, Dresden – München 1998.<@8 Kapitäl>Grassmuck, Volker<@$p>, Vom Animismus zur Animation, Junius Verlag, Hamburg 1988.<@8 Kapitäl>Grassmuck, Volker/Christian Unverzagt<@$p>, Das Müll-System. Eine metarealistische <\n>Bestandsaufnahme, Suhrkamp, Frankfurt/M. 1991.<@8 Kapitäl>Grassmuck, Volker<@$p>, Open Source – Betriebssystem für eine freiheitliche Gesellschaft, in: Linux-Magazin 9/2000, S.54-61, http://waste.informatik.hu-berlin.de/Grassmuck/Texts/OSS-Tutzing-<\h><\h><\n>5-00.html <@8 Kapitäl>Gravell, Tom<@$p>, The Wizard of Watermarks, DU PONT Magazine, January/February 1990, S. 4–6, http://ebbs.english.vt.edu/gravell/wizard/wizard.html<@8 Kapitäl>Grimm, Jacob und Wilhelm<@$p>, Deutsches Wörterbuch, Bd. 1, (A – Biermolke, Leipzig 1854), dtv, München 1984.<@8 Kapitäl>Grimm, Jacob und Wilhelm<@$p>, Deutsches Wörterbuch, Neubearbeitung, Berlin-Brandenburgische Akadmie der Wissenschaften und Akademie der Wissenschaften Göttingen, S. Mirzel Verlag, Stuttgart – Leipzig 1998.<@8 Kapitäl>Günther, Gotthard<@$p>, Das Bewusstsein der Maschinen. Eine Metaphysik der Kybernetik, Baden-<\h>Baden – Krefeld 1963.<@8 Kapitäl>Guibault, Lucie<@$p>, für Imprimatur, Legal SIG Workshop »Contracts and Copyright Exemp<\h>tions«, <\h>Institute for Information Law, Amsterdam, Dezember 1997, http://www.imprimatur.alcs.co.uk/ IMP_FTP/except.pdf<@8 Kapitäl>Gulbin, Jürgen/Karl Obermayr<@$p>, Unix System V.4. Begriffe, Konzepte, Kommandos, Schnittstellen, Springer, Berlin – New York 1995.<@8 Kapitäl>Halbert, Debora J<@$p>., Weaving Webs of Owner<\h>ship: Intellectual Property in an Information Age, Dissertation Draft, Hawaii University 1998, http://www.soc.hawaii.edu/~future/ dissertation/TOC.html <@8 Kapitäl>H<t5>ardin, Garrett<@$p><t-3>, »The Tragedy of the Commons«, <t$>in: Science, 162 (1968), S. 1243–1248, <\h>http://dieoff.com/page95.htm<@8 Kapitäl>Hardin, Garrett<@$p>, Ethical Implications of Carrying Capacity, 1977, http://dieoff.com/page96.htm<@8 Kapitäl>Harsh, B<@$p>., »New Computer Operating System takes India by Storm«, India Abroad News Service, auf: Corporate Watch, Globalization and Microsoft, 9. Feb. 1999, http://www.igc.org/trac/ <\h>feature/microsoft/globalization/india.html<@8 Kapitäl>Hauben, Michael/ Ronda Hauben<@$p>, Netizens: On the History and Impact of Usenet and the Internet, 6. Dez. 1996, http://www.columbia.edu/~hauben/netbook/<@8 Kapitäl>Hautsch, Gert<@$p>, Zur Entwicklung der Medienwirtschaft in Deutschland 1997/98, 9. August 1998, http://www.igmedien.de/publikationen/m/1998/08-09/10.html<@8 Kapitäl>Hayashi, Yûjirô<@$p>, <@7 Glossar kursiv>Jôhôka Shakai: Hâdo na Shakai Kara Sofuto na Shakai e<@$p>, (Informationsgesellschaft: Von der harten (Hardware-) zur sanften (Software-)Gesellschaft), Tokyo 1969.<@8 Kapitäl>Helmers, Sabine/Kai Seidler<@$p>, Linux: Coope<\h>rative Software Development and Internet, Proceedings of the First Dutch International Symposium on Linux – December 1994 at Amsterdam, S. 56–59, State University of Gronigen, 1995, http://duplox.wz-berlin.de/docs/linux/index.html<@8 Kapitäl>Helmers, Sabine, Ute Hoffmann, Jeanette Hofmann,<@$p> Endbericht des Forschungsprojekts Kulturraum Internet am Wissenschaftszentrum Berlin für Sozialforschung, 1998, http://duplox.wz-<\h>berlin.de/endbericht/<@8 Kapitäl>Hetze, Sebastian/Dirk Hohndel/Martin Müller/ Olaf Kirch<@$p>, Linux Anwenderhandbuch. Leitfaden für die Systemverwaltung, LunetIX, Berlin 1997 und online: http://www1.lunetix.de/LHB//<@8 Kapitäl>Hetze, Sebastian<@$p>, Wirtschaftliche Aspekte von Freier Software und Open Source, 1999, <\h><t-3>http://mikro.org/Events/OS/ref-texte/hetze.html<t$><@8 Kapitäl>Hitachi Ltd./ Intel Corporation/ Matsushita Electric Industrial Co. Ltd./ Sony Corporation/ Toshiba Corporation<@$p>, 5C Digital Transmission Content Protection White Paper, Revision 1.0, 14. Juli 1998; http://www.dtcp.com/wp_spec.pdf<@8 Kapitäl>Hitachi, Ltd./ Intel Corporation/ Matsushita Electric Industrial Co. Ltd./ Sony Corporation/ Toshiba Corporation<@$p>, Digital Transmission Content Protection Specification Volume 1 (Informational Version), Revision 1.0, 12. April 1999; http://www.dtcp.com/dtcp_spec1.pdf<@8 Kapitäl>Hohmann, Robin<@$p>, Interview mit Richard Stallman, in: Computerwoche 4/ 29. Januar 1999; http://www.computerwoche.de/info-point/heftarchiv/index.cfm?id=46864&cfid= <\h>7105550&cftoken=37716226&nr=1<x@8 Kapitäl><t8>Horns, Axel H<@$p><t0>., DeCSS-Spruch – Modell für Europa?, in: Digest »Netz und Politik« 19, Februar 2000, <t$>http://www.fitug.de/netpol/00/19.html#1<@8 Kapitäl>Intel Corporation/ International Business Machines Corporation/ Matsushita Electric Industrial Co. Ltd./ Toshiba Corporation<@$p>, »Content Protection System Architecture. A Comprehen<\h>sive Framework for Content Protection«, 17. Februar 2000, Revision 0.81; http://www.dvdcca.org/ 4centity/data/tech/cpsa/cpsa081.pdf<H>Jeter, K.W., <$>Noir, Bantam Books, New York etc. 1999.<@8 Kapitäl>Kahn, David<@$p>, The Codebreakers: The Comprehensive History of Secret Communication from <\h>Ancient Times to the Internet, Scribner, New York 1996.<@8 Kapitäl>Kant, Immanuel<@$p>, Von der Unrechtsmäßigkeit des Büchernachdruckens, 1785; Nachdruck in UFITA 106, 1987, S. 137.<@8 Kapitäl>Katz, Ronald S./Janet Arnold Hart<@$p>, Turning On and Turning Off. Can Merely Turning on a Computer Constitute Copyright Infringement? In the Ninth Circuit, yes, The Recorder, 1996, http://www.ipmag.com/katz.html<@8 Kapitäl>KBSt-Brief<@$p> Nr. 2/2000, Koordinierungs- und Beratungsstelle der Bundesregierung für Informationstechnik, »Open Source-Software in der Bundesverwaltung«, http://www.kbst.bund.de/papers/briefe/02-2000<@8 Kapitäl>Kelsey, John/Bruce Schneier<@$p>, The Street Performer Protocol and Digital Copyrights, in: Firstmonday, Jahrg. 4, #6, 1999, http://www.firstmonday.dk/issues/issue4_6/kelsey/<@8 Kapitäl>Kingdon, Jim<@$p>, (Cyclic), Free Software Business Models, Dezember 1996, http://www.stromian.com/bizmod.html<@8 Kapitäl>Kirch, John<@$p>, Microsoft Windows NT Server 4.0 versus Unix, März 1998, <\h><\n>http://www.unix-vs-nt.org/kirch/<@8 Kapitäl>Kloke-Lesch, Adolf<@$p>, »Funktionale Positionsbestimmung der Entwicklungspolitik«, in: Politik und Gesellschaft Online; International Politics and Society 3/1998 (Friedrich Ebert Stiftung), <t1>http://www.fes.de/ipg/ipg3_98/artkloke.html<@8 Kapitäl>Köhntopp, Kristian<@$p>, »RPS III: Wenn geistiges Eigentum zum Unwesen wird«, auf: »Netz und Politik« (NETPOL-Digest) 20, 10. Feb. 2000, http://www.fitug.de/netpol/00/20.html#3<@8 Kapitäl>Kohn, Alfie<@$p>, Studies Find Reward Often No Motivator, in: Boston Globe, 19. Jan.1987, http://www.naggum.no/motivation.html<@8 Kapitäl>Kollock, Peter<@$p>, »The Economics of Online Cooperation: Gifts and Public Goods in Cyberspace«, Working Draft, 14. August 1996, http://research.microsoft.com/vwg/papers/ koolockeconomies.htm<x@8 Kapitäl><t7>Kreile, Reinhold/Jürgen Becker<@$p><t-1>, »Verwertungs<\h>g<t$>esellschaften«, in: Rolf Moser/ Andreas Scheuermann (Hrsg.), Handbuch der Musikwirtschaft, J. Keller Verlag, Starnberg – <\n>München 1997a, S. 621–647.<@8 Kapitäl>Kreile, Reinhold/Jürgen Becker<@$p>, »Tarife für Multimedianutzung«, in: Rolf Moser/ Andreas Scheuermann (Hrsg.), Handbuch der Musikwirtschaft, J. Keller Verlag, Starnberg – München 1997b, S.<\!q>715–741.<@8 Kapitäl>Krempl, Stefan<@$p>, »Mit URL-Blocker gegen MP3-Server«, in: Telepolis, 3. Sept. 1999, http://www.heise.de/tp/deutsch/inhalt/te/5259/1.html<@8 Kapitäl>Krempl, Stefan<@$p>, EU Commissioner: We Need a Sense of Urgency, Interview with Erkki Liikanen, auf: Telepolis, 17. Apr. 2000, http://www.heise.de/tp/english/inhalt/te/8052/1.html<@8 Kapitäl>Kretschmer, Martin<@$p>, Centre for Intellectual Property Policy & Managment, School of Finance and Law, Bournemouth University, auf dem MICA-Fokus »Reales Musikschaffen für einen virtuellen Markt«, 18. März 2000, Wien, http://www.mica.at/focus3.html<@8 Kapitäl>Kunze, Carol<@$p>, Hotton Button Issue: Mass Market Licenses, 13. März 1997, http://www.2bguide.com/hbimmvc.html<@8 Kapitäl>Kuri, Jürgen<@$p>, »Alice im Bücherland. Online-<t-1>Buchläden: Die Speerspitze des E-Commerce?«,<t$> in: <\h>c’t<\!q>19/99, S. 164, http://www.heise.de/ct/99/19/164/<@8 Kapitäl>Leonard, Andrew<@$p>, »The Saint of Free Software. Maverick Richard Stallman Keeps his Faith – and Gives Bill Gates the Finger«, <\n>in: Salon Magazine 8/1998, http://www.salon.com/21st/feature/1998/08/cov_31feature2.html.<@8 Kapitäl>Leonard, Andrew<@$p>, The Richard Stallman Saga, <t-3>Redux, in: Salon Magazine, 9/1998, <\h>http://www<t$>.<t-4>salon.com/21st/feature/1998/09/11feature.ht<t$>ml<@8 Kapitäl>Lessig, Lawrence<@$p>, Jefferson’s Nature, Draft 1, University of Virginia, Charlottesville, Va, November 19, 1998; http://cyber.law.harvard.edu/works/lessig/NatureD3.pdf<@8 Kapitäl>Lessig, Lawrence<@$p>, Code and Other Laws of Cyberspace, Basic Books, New York 1999 (dt: Code und andere Gesetze des Cyberspace), Berlin Verlag 2001.<@8 Kapitäl>Lessig, Lawrence<@$p>, The Law of the Horse: What Cyberlaw Might Teach (final draft), 1999a, http://cyber.law.harvard.edu/works/lessig/finalhls.pdf<@8 Kapitäl>Lettice, John<@$p>, »French senators propose making open source compulsory«, in: The Register, 24. Okt. 1999, http://www.theregister.co.uk/991024-000005.html<@8 Kapitäl>Lévy, Pierre<@$p>, Die Kollektive Intelligenz. Eine Anthropologie des Cyberspace, Bollmann Verlag, Mannheim 1997.<@8 Kapitäl>Levy, Steven<@$p>, Hackers. Heroes of the Computer Revolution (1984), Bantam Doubleday Dell <\n>Publishing, New York 1994.<@8 Kapitäl>Lewis, Jeff<@$p>, The Cathedral and the Bizarre, auf: Mac-Opinion, 7. Juli 2000, http://www.maco<\n>pinion.com/columns/macskeptic/00/07/07/<@8 Kapitäl>Loren, Lydia Pallas<@$p>, The Purpose of Copyright, in: Open Spaces Quarterly, 7. Feb. 2000; http://www.public.asu.edu/~dkarjala/publicdomain/Loren2-7-00.html<@8 Kapitäl>Loukides, Mike<@$p>, Some Thoughts on the Sun Community Source License, O’Reilly Java Center, 9.<\!q>Dez. 1999, http://java.oreilly.com/news/loukides_0399.html<@8 Kapitäl>Loukides, Mike<@$p>, The Chameleon and the Virus: More Thoughts on Java’s Community License, o.J., O’Reilly-News, o.J., http://java.oreilly.com/news/java_license_0399.html<@8 Kapitäl>Love, Courtney<@$p>, Courtney Love does the math. The controversial singer takes on record label profits, Napster and ›sucka VCs’‹, in: Salon Magazine, 14. Juni 2000, http://www.salon.com/tech/feature/2000/06/14/love/<@8 Kapitäl>Lühr, Rüdiger<@$p>, Diskussionsentwurf zum Urheberrechtsgesetz von Justizminister Schmidt-Jortzig vorgelegt, 29. Okt. 1998,  http://www.igmedien.de/publikationen/m/1998/10/29.html<@8 Kapitäl>Malkin, G<@$p>., Who’s Who in the Internet. Biographies of IAB, IESG and IRSG Members, Mai 1992, http://www.ietf.org/rfc/rfc1336.txt?number=1336<@8 Kapitäl>Marx, Reiner/Gerhard Sauder<@$p> (Hrsg.), Moritz contra Campe. Ein Streit zwischen Autor und Verleger im Jahr 1789, Werner Röhrig Verlag, St. Ingbert 1993.<@8 Kapitäl>McKusick, Marshall Kirk<@$p>, Twenty Years of Berkeley Unix. From AT&T-Owned to Freely Redistributable, in: C. DiBona7 S. Ockman/M. Stone (Hrsg.), Open Sources. Voices from the Open Source Revolution, O’Reilly Verlag, Sebastopol 1999, S. 31–46.<@8 Kapitäl>Merten, Stefan<@$p>, Gnu/Linux – Meilenstein auf dem Weg in die GPL-Gesellschaft?, 2000 opentheory.org/proj/gplgesellschaft/v0001.phtml<@8 Kapitäl>Metzger, Axel/Till Jaeger<@$p>, »Open Source-Software und deutsches Urheberrecht«, in: GRUR Int., Okt. 1999, S. 839-848, http://www.ifross.de/ifross_html/art1.html<@8 Kapitäl>Möller, Erik<@$p>, »JXTA, Slashdot, Open-Source-Cola und Metadaten. Treffen unter Gleichen oder die Zunkunft des Internet, Teil II«, Telepolis, 5.3.2001, http://www.heise.de/tp/deutsch/ inhalt/te/7051/1.html<@8 Kapitäl>Moglen, Eben<@$p>, Anarchism Triumphant: Free Software and the Death of Copyright, First Monday, August 1999, http://old.law.columbia.edu/my_pubs/anarchism.html <@8 Kapitäl>Molnar, Daniel<@$p>, Pop Muzik. Join the New Folkateers!, auf: Nettime-L, 29. Sept. 1998.<@8 Kapitäl>Müller-Maguhn, Andy<@$p>, »Thesenpapier Informationsgesellschaft«, http://www.datenreisen.de<@8 Kapitäl>Münker, Reiner<@$p>, Urheberrechtliche Zustimmungserfordernisse beim Digital Sampling, Peter Lang,  Frankfurt/M. 1995.<@8 Kapitäl>Nadeau, Tom,<@$p> The Case for Eminent Domain, first posted: 1. Januar 1998, http://www.vcnet.com/bms/features/domain.html<@8 Kapitäl>Negroponte, Nicholas<@$p>, Being Digital, Knopf, New York 1995.<@8 Kapitäl>Neumann, A. Lin<@$p>, Information Wants to Be Free – But This Is Ridiculous. We go on a Shopping Spree for Pirate Software in Asia, in: Wired 3.10 – Okt. 1995, http://www.<\n>wired.com/wired/archive/3.10/piracy.html<@8 Kapitäl>Newitz, Annalee<@$p>, »If Code is Free, Why not Me? Some Open Source Geeks are as Open-Minded about Sex as they are About Hacking« in: Salon Magazine, 26. Mai 2000;http://www.salon.com/ tech/feature/2000/05/26/free_love/index.html<@8 Kapitäl>Newman, Nathan<@$p>, From Microsoft Word to Microsoft World: How Microsoft is Building a Global Monopoly. A NetAction White Paper, <t0>1997, http://www.netaction.org/msoft/world/<t$><@8 Kapitäl>Nicholson, Bradley J<@$p>., The Ghost In The Machine: MAI Systems Corp. v. Peak Computer, Inc. and the Problem of Copying in RAM, Berkeley Law Journal, 10/1995, http://www.law.berkeley.edu/ journals/btlj/articles/10_1/Nicholson/html/text.html<@8 Kapitäl>O‘Reilly, Tim<@$p>, »Gated Source« Communities?, 7. Mai 2000, http://weblogs.oreillynet.com/<\n>tim/stories/storyReader$39<@8 Kapitäl>Oresme, Nicolas von<@$p>, Traktat über Geldabwertungen, Kadmos Verlag, Berlin 1999.<@8 Kapitäl>Ostrom, Elinor<@$p>, Die Verfassung der Allmende. Jenseits von Staat und Markt, Mohr Siebeck, 1999 (Engl.: Governing the commons. The evolution of institutions for collective action, Cambridge University Press, Cambridge 1990).<@8 Kapitäl>Paolini, George,<@$p> »Sun's New Java™ Source Licencing Policies«, an Interview with George Paolini, Vice President of Marketing, Sun <\h>Java Software, http://java.sun.com/<\h>features/1998/12/source_license_QA.html<@8 Kapitäl>Patel, Prakesh<@$p>, Open Source: Sizing the Linux Opportunity, WR Hambrecht + Co, 18. Mai 2000, http://www.wrhambrecht.com/research/coverage/opensource/ir/ir20000523.pdf<@8 Kapitäl>Perens, Bruce<@$p>, The Open Source Definition, in: C. DiBona/ S. Ockman/ M. Stone (Hrsg.), Open Sources. Voices from the Open Source Revolution, O’Reilly Verlag, Sebastopol 1999, S. 171–188.<@8 Kapitäl>Perens, Bruce<@$p>, It’s Time to Talk about Free Software Again, 1999a, http://www.perens.<\n>com/perens_com/Articles/ItsTimeToTalkAboutFreeSoftwareAgain.html<@8 Kapitäl>Petitcolas, Fabien A. P./ Ross J. Anderson/ Markus G. Kuhn<@$p>, Attacks on Copyright <\n>Marking Systems, in: David Aucsmith (Hrsg.), Information Hiding, Second International Workshop, IH’98, Portland, Oregon, U.S.A., 15.-17. April 1998, Proceedings, LNCS 1525, S. 219-239, Springer-Verlag, Berlin – New York 1998, http://www.cl.cam.ac.uk/~fapp2/papers/ih98-attacks/<@8 Kapitäl>Petitcolas, Fabien A. P./ Ross J. Anderson<@$p>, Evaluation of Copyright Marking Systems, in: Proceedings of IEEE Multimedia Systems (ICMCS’99), Vol. 1, S. 574–579, 7.–11. Juni 1999, Florenz, http://www.cl.cam.ac.uk/~fapp2/papers/ieeemm99-evaluation<@8 Kapitäl>Pfennig, Gerhard<@$p>, »Die Funktionärsclique aus Frankfurt. Die Geschichte der Wahrnehmung visueller Urheberrechte am Beispiel der VG Bild-Kunst«, in: Gerhard Pfennig/ Michael Schwarz (Hrsg.), Die Zukunft der Bilder. Medienentwicklung und Recht – 25 Jahre VG Bild-Kunst, Steidl, Göttingen 1993, S. 12–29.<@8 Kapitäl>Philipkoski, Kristen<@$p>, The Student Jukebox Sting. The Internet may no longer be the Wild West. Seventy-one college students at Carnegie Mellon University (CMU) in Pittsburgh, Pennsylvania, have been disciplined for the illegal use of MP3 files on the University’s intranet, Wired News 4:20 p.m., 9. Nov. 1999 PST; http://www.wired.com/news/culture/0,1284,32444,00.html<@8 Kapitäl>Platt, Charles<@$p>, Satellite Pirates, Wired 2.08 – Aug. 1994, http://www.wired.com/wired/ <\h>archive/2.08/satellite.html<@8 Kapitäl>Plotz, David<@$p>, Luke Skywalker Is Gay? Fan fiction is America’s literature of obsession, <t-1>14. April 2000, auf: Slate http://slate.msn.com/<t$>Features/fanfic/fanfic.asp<x@8 Kapitäl><t9>Pomian, Krzystof<@$p><t1>, Der Ursprung des Museums. <t$>Vom Sammeln, Wagenbach, Berlin 1998.<@8 Kapitäl>Powell, Dennis E<@$p>., Judgement Day for the GPL?, 26. Juni 2000, http://www.linux<\n>planet.com/linuxplanet/reports/2000/1/<@8 Kapitäl>Powell, Dennis E<@$p>., Lawyers, Guns, and Money. KDE and Corel: Working in the Real World, LinuxPlanet, 14. Juni 2000a, http://www.linuxplanet.com/linuxplanet/reports/1960/1/<@8 Kapitäl>Raymond, Eric S.<@$p>, The Cathedral and the Bazaar, Januar 1998, http://www.tuxedo.org/<\n>~esr/writings/cathedral-bazaar/cathedral-bazaar.html<@8 Kapitäl>Raymond, Eric S.<@$p>, »The Cathedral and the Bazaar«, http://sagan.earthspace.net/~esr/writings/ <\h>cathedral-bazaar/ <@8 Kapitäl>Rice, Dan/Robert Pecot<@$p>, Microprocessor History, o.J., http://members.tripod.com/nikkicox/<@8 Kapitäl>Rifkin, Jeremy<@$p>, Vorabdruck aus: Access. Das Verschwinden des Eigentums. Wenn alles im Leben zur bezahlten Ware wird, Suhrkamp, Frankfurt/M. 2000, in: FAZ, 12.8.2000.<@8 Kapitäl>Rötzer, Florian<@$p>, Napster-ähnliche Tauschbörse für die Wissenschaft. Besonders die Zusammenführung der Erläuterungen zum mensch<\h>lichen Genom von verschiedenen Websites lässt neue Programme notwendig werden, in: Telepolis, 17.08.2000, http://www.heise.de/tp/deutsch/inhalt/ lis/8561/1.htm<@8 Kapitäl>Rosenberg, Scott<@$p>, Why the Music Industry has Nothing to Celebrate. Napster’s Shut<\h>down will only Cause a Thousand Alternatives to Bloom, in: Salon Magazine, 27. Juli 2000<t3>,http://www.salon.com/tech/col/rose/2000/07/27/napster_shutdown/print.html<t$><@8 Kapitäl>Rost, Martin<@$p>, »Vorschläge zur Entwicklung einer wissenschaftlichen Diskurs-Markup-Language« in: Heibach, Christiane/ Bollmann, Stefan (Hrsg.), Kursbuch Internet – Anschlüsse an Wirtschaft und Politik, Wissenschaft und Kultur, Bollmann Verlag, Köln 1996, http://www.netzservice.de/ <\h>Home/maro/home/mr_dml.html<@8 Kapitäl>Rump, Niels<@$p>, Protecting Intellectual Property Rights using the Multimedia Protection <\n>Protocoll (MMP), Fraunhofer IIS, Erlangen, 19. August 1997, (nicht länger online).<x@8 Kapitäl><t9>Saacke, Astrid<@$p><t1>, »Schutzgegenstand und Rechts<\h>inhaberschaft bei Multimediaprodukten – Grundfragen des Rechtsschutzes und der Lizenzierung«, in: Horst-Peter Götting (Hrsg.), Multimedia, Internet und Urheberrecht, University Press, Dresden 1998, S. 19–45.<t$><@8 Kapitäl>Samuelson, Pamela<@$p>, »Is information property? (Legally Speaking)«, in: Communications of the ACM, March 1991 v34 n3 p15(4), http://www.ifla.org/documents/infopol/copyright/samp6.txt<@8 Kapitäl>Samuelson, Pamela<@$p>, Legally Speaking: Does Information Really Want To Be Licensed?, in: Communications of the ACM, September 1998; http://www.press.umich.edu/jep/04-03/samuelson.html<@8 Kapitäl>Sanders, Glenn/ Wade Roush<@$p>, Cracking the <t-1>Bullet: Hackers Decrypt PDF Version of Stephen <t$>King eBook, eBookNet.com, 23. März 2000, http://www.ebooknet.com/story.jsp?id=1671<@8 Kapitäl>Scanlon, Jessie<@$p>, Open Source: Power Ex<\h>change, Wired, Nov. 1999, http://www.wired.com/ <\h>wired/archive/7.11/mustread.html? pg=2<@8 Kapitäl>Schreiner, Klaus<@$p>, »Bücher, Bibliotheken und ‘gemeiner Nutzen’ im Spätmittelalter und <\n>in der frühen Neuzeit«, in: Bibliothek und Wissenschaft (9) 1975, S. 216–249.<@8 Kapitäl>Schultz, Pit<@$p>, »Was ist digitaler Kapitalismus?«, in: de:bug, 12/99. <@8 Kapitäl>Schulze, Marcel<@$p>, Materialien zum Urheberrechtsgesetz. Texte – Begriffe – Begründungen, VCH, Weinheim – New York 1993.<@8 Kapitäl>Schulze, Martin<@$p>, »Debian GNU«, in: FIFF-Kommunikation 3/1999, S. 27–33.<@8 Kapitäl>Schulzki-Haddouti, Christiane<@$p>, »Dialog mit Hindernissen, Bundeskriminalamt will Berührungs<\h>ängste der Provider abbauen«, in: Telepolis, 17.02.2000, http://www.heise.de/tp/deutsch/ inhalt/te/5806/1.html<@8 Kapitäl>Seiferth, C. Justin<@$p>, Maj USAF, Open Source and these United States, A Research Report Submitted to the Faculty, In Partial Fulfillment of the Graduation Requirements, Maxwell Air Force Base, Alabama, April 1999 http://skyscraper.fortunecity.com/mondo/841/documents/99-184.html<@8 Kapitäl>Siepmann, Jürgen<@$p>, »Lizenz- und haftungsrechtliche Fragen bei der kommerziellen Nutzung Freier Software«, in: JurPC. Internet-Zeitschrift für Rechtsinformatik, 1999, http://www.jurpc.de/ <\h>aufsatz/19990163.htm<@8 Kapitäl>Sollfrank, Cornelia<@$p>, Woman Hackers, Rotterdam 1999, http://www.obn.org/hackers/text.htm<@8 Kapitäl>Sommerfeld, Bernd<@$p>, »Wie es begann... Eine kurze Geschichte von Linux und ›Open Source‹«, 1999, http://www.de.uu.net/shop/jfl/linux/BSo7.html<@8 Kapitäl>Spinner, Helmut F<@$p>., Die Wissensordnung. Ein Leitkonzept für die dritte Grundordnung des Informationszeitalters, Leske + Budrich, Opladen 1994.<@8 Kapitäl>Spinner, Helmut F<@$p>., Die Architektur der Informationsgesellschaft, Philo, Bodenheim 1998.<@8 Kapitäl>Spinner, Helmut F<@$p>., aus: Ettersburger Gespräche: Zur Ohnmacht verdammt? – Der Staat in der Informationsgesellschaft, hrg. vom  Thüringer Ministerium für Bundesangelegenheiten i.d. Staatskanzlei, Weimar, Feb. 1998a.<@8 Kapitäl>Spinner, Helmut F./ Michael Nagenborg/ Karsten Werber<@$p>, Baustein zu einer neuen <\n>Informationsethik, Philo, Berlin – Wien 2001.<@8 Kapitäl>Stallman, Richard<@$p>, The GNU Manifesto, 1985, http://www.gnu.org/gnu/manifesto.html<@8 Kapitäl>Stallman, Richard<@$p>, The Right Way to Tax DAT, in: Wired 1992, http://www.gnu.org/philosophy/dat.html<@8 Kapitäl>Stallman, Richard<@$p>, Why Software Should Not Have Owners, 1994, http://www.gnu.org/<\n>philosophy/why-free.html<@8 Kapitäl>Stallman, Richar<@$p><H>d<$>, The Right to Read, in: Communications of the ACM, Vol. 20, No. 2, Februar 1997, http://www.gnu.org/philosophy/right-to-read.html<@8 Kapitäl>Stallman, Richard<@$p>, Free Software and Free Manuals, 1997b, http://www.fsf.org/<\n>philosophy/free-doc.html<@8 Kapitäl>Stallman, Richard<@$p>, Copyleft: Pragmatic Idealism, 1998, http://www.gnu.org/<\n>philosophy/pragmatic.html<@8 Kapitäl>Stallman, Richard<@$p>, The GNU Operating System and the Free Software Movement, in: C. DiBona/ S. Ockman/ M. Stone (Hrsg.), Open Sources. Voices from the Open Source Revolution, Sebastopol 1999, S. 53–70.<@8 Kapitäl>Stallman, Richard<@$p>, Why you shouldn’t use the <t-1>Library GPL for your next library, Februar 1999a, <t$>http://www.gnu.org/philosophy/why-not-lgpl.html<@8 Kapitäl>Stallman, Richard<@$p>, Why »Free Software« is <t-1>better than »Open Source«, 1999b, http://www.<t$>gnu.org/philosophy/free-software-for-freedom.html<@8 Kapitäl>Starrett, Robert A<@$p>., Copying Music to CD: The Right, the Wrong, and the Law, Februar 1998, http://www.cdpage.com/Audio_Compact_Disc/rightwrong.html<@8 Kapitäl>Stefik, M. J.<@$p>, Letting Loose the Light: Igniting Commerce in Electronic Publication, in: Stefik, M. (Hrsg.), Internet Dreams: Archetypes, Myths, and Metaphors, MIT Press, Cambridge Mass. 1996; http://www.parc.xerox.com/istl/projects/uir/pubs/pdf/UIR-R-1996-10-Stefik-Internet<\n>Commerce-IgnitingDreams.pdf<@8 Kapitäl>Stefik, Mark<@$p>, Shifting the Possible: How Digital Property Rights Challenge Us to Rethink Digital Publishing, 12 Berkeley Technology Law Journal 1997a, S. 137-159, http://www.law.berkeley.edu/journals/btlj/articles/12_1/Stefik/html/reader.html<@8 Kapitäl>Stefik, Mark<@$p>, Special Report: Trusted Systems. Devices that Enforce Machine-Readable Rights to Use the Work of a Musician or Author may Create Secure Ways to Publish over the Internet, Scientific American 3/1997b, 276(3), S. 78-81; http://www.sciam.com/0397issue/0397stefik.html<@8 Kapitäl>Stephenson, Neal<@$p>, In the Beginning was the Command Line, 1999, http://www.cryptonomicon.com/beginning.html<@8 Kapitäl>Steurer, Reinhard<@$p>, »Die schwierige Psycho<\h>logie der Umweltpolitik: Das Beispiel Österreich« (vorläufige Fassung), in: Politik und Gesellschaft Online, International Politics and Society 4/1999, http://www.fes.de/ipg/ipg4_99/artsteur.htm<@8 Kapitäl>Straub, Craig<@$p>, Living in a World of Limits. An Interview with Noted Biologist Garrett Hardin, in: The Social Contract, TSC Press, Fall, 1997, http://www.lrainc.com/swtaboo/ stalkers/tsc_hard.html<@8 Kapitäl>Strauss, Neil<@$p>, »Hong Kong Film: Exit the Dragon?«, in: New York Times, 1. Aug. 1998, http://www.geocities.com/Athens/Forum/2496/vcd-hkfilm.txt<@8 Kapitäl>Tiemann, Michael<@$p>, Future of Cygnus Solutions: An Entrepreneur’s Account, in: C. DiBona/ <\n>S. Ockman/ M. Stone (Hrsg.), Open Sources. Voices from the Open Source Revolution, <\n>Sebastopol 1999, S. 71–89.<@8 Kapitäl>Tushnet, Rebecca<@$p>, Using Law and Identity to Script Cultural Production: Legal Fictions: Copyright, Fan Fiction, and a New Common Law, 17 Loyola of Los Angeles Entertainment Law Journal, ( 651) 1997, http://cyber.law.harvard.edu/property/respect/fanfiction.html<@8 Kapitäl>Ulmer, Eugen<@$p>, Urheber- und Verlagsrecht, Berlin 1983 .<@8 Kapitäl>U.S. Department of Defense,<@$p> »Department of Defense Trusted Computer System Evaluation Criteria«, DoD 5200.28-STD, December 26, 1985.<@8 Kapitäl>Valloppillil, Vinod<@$p>, Open Source Software. A (New?) Development Methodology (aka »The Halloween Document«), Microsoft Confidential (mit Kommentaren versehen von Eric Raymond), 11. Aug. 1998 – v1.00., http://www.tuxedo.org/~esr/halloween.html<@8 Kapitäl>Vanpelt, Lauren<@$p>, Mickey Mouse – A Truely Public Character, Frühjahr 1999, http://www.<\n>public.asu.edu/~dkarjala/publicdomain/Vanpelt-s99.html<@8 Kapitäl>VG Wort,<@$p> Bericht des Vorstands über das <\h>Geschäftsjahr 2000, München 2001.<@8 Kapitäl>Vermeer, Marti<@$p>n, Linux and Ethnodiversity, 21. Jan. 1998, http://linuxtoday.com/stories/2465.html.<@8 Kapitäl>Verzola, Roberto<@$p>, Cyberlords: The Rentier Class of the Information Sector, Nettime, 15. März 1998, http://amsterdam.nettime.org/ Lists-Archives/nettime-l-9803/msg00079.html<@8 Kapitäl>Vobruba, G<@$p>., Strukturwandel der Sozialpolitik – Lohnarbeitszentrierte Sozialpolitik und soziale Grundsicherung, Frankfurt/M. 1990.<@8 Kapitäl>Walsh, Mary Williams<@$p>, »Windows won’t Compute into Ancient Icelandic Llanguage«, in: Seattle Times, 30. Juni 1998, http://archives.seattletimes.nwsource.com/cgi-bin/texis/web/vortex/ <\h>display?slug=icel&date=19980630<@8 Kapitäl>Weber, Max<@$p>, Schriften zur Soziologie, Stuttgart 1995.<@8 Kapitäl>Wiener, Norbert,<@$p> Kybernetik, Düsseldorf - <\h>Wien, 1963.<x@8 Kapitäl><K>Wos<$$> 1<@$p>, Wizards of OS. Offene Quellen und Freie Software, Konferenz, am 16.–17.7.1999, im Haus der Kulturen der Welt, Berlin, http://www.mikro.org/wos<@8 Kapitäl>Wulffen, Thomas,<@$p> »Betriebssystem Kunst«, in: Kunstforum Bd. 125, Januar/Februar 1994, <\n>S. 50-58.<@8 Kapitäl>Zawinski, Jamie<@$p>, Fear and Loathing on the Merger Trail, 23. Nov. 1998, http://www.mozilla.org/ <\h>fear.html<@8 Kapitäl>Zawinski, Jamie<@$p>, Resignation and Postmortem, März 1999, http://www.jwz.org/gruntle/nomo.html<@8 Kapitäl>Zentralinstitut für Sprachwissenschaft Berlin<@$p>, Etymologisches Wörterbuch des Deutschen, München 1995.<@8 Kapitäl>Zingel, Wolfgang-Peter<@$p>, (Südasien-Institut der Universität Heidelberg, Abteilung Internationale Wirtschafts- und Entwicklungspolitik), »Bodenrecht in Indien«, Eine leicht gekürzte Fassung erschien in : »entwicklung + länd<\h>licher raum. Beiträge zur internationalen Zusammenarbeit«,  DLG. 29, Frankfurt 1995 (6), S. 7–10, http://www.sai.uni-heidelberg.de/intwep/zingel/bodenr95.htm<@8 Kapitäl>Zingel, Wolfgang-Peter<@$p>, Genug Nahrung für eine Milliarde Inder?, in: Werner Draguhn (Hrsg.), <\h>Indien 1999: Politik, Wirtschaft, Gesellschaft, Institut für Asienkunde, Hamburg 1999, http://www.sai.uni-heidelberg.de/intwep/zingel/jbindi99.htm